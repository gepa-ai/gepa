{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eefb064",
   "metadata": {},
   "source": [
    "# GEPA: Optimize Anything using ASI (additional side information)\n",
    "\n",
    "GEPA is a text evolution engine: Given a target metric, GEPA can efficiently search for the right parameters (including numerical, textual and code) to improve that metric. This way, GEPA can optimize essentially represent _anything_ that has a textual representation. In this post, we leverage this insight to present GEPA's optimize-anything API, which leverages the reflective capabilities of LLMs, to optimize anything representable as text. Crucially, GEPA can leverage any additional information available from the optimization environment simply by serializing into text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52b3cc",
   "metadata": {},
   "source": [
    "## The optimize_anything API\n",
    "\n",
    "At its core, the API is remarkably simple. You provide just two things:\n",
    "\n",
    "1. **A seed candidate** — your starting point, represented as a dictionary mapping parameter names to their values. \n",
    "2. **A fitness function** — tells GEPA how good each candidate is. The fitness function also returns any additional information available from the environment about the evaluated candidate, like compiler error messages, that can guide the optimization.\n",
    "\n",
    "That's it. GEPA handles the rest — selecting candidates, reflecting on failures, proposing improvements, and tracking the optimization trajectory, finally returning the optimized parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b5553",
   "metadata": {},
   "source": [
    "### The Fitness Function: Your Optimization Signal\n",
    "\n",
    "The fitness function is where you define *what* you're optimizing for. It takes a candidate and a batch of data instances, returning scores and diagnostic information:\n",
    "\n",
    "```python\n",
    "def fitness_fn(candidate: dict[str, str], batch: list[DataInst]) -> list[tuple[float, Any, dict]]:\n",
    "    results = []\n",
    "    for instance in batch:\n",
    "        # Run your system with the candidate parameters\n",
    "        output = run_my_system(candidate, instance)\n",
    "        \n",
    "        # Compute a score (higher is better)\n",
    "        score = compute_score(output, instance)\n",
    "        \n",
    "        # Collect diagnostic info for LLM reflection\n",
    "        side_info = {\n",
    "            \"input\": instance[\"input\"],\n",
    "            \"output\": output,\n",
    "            \"expected\": instance[\"expected\"],\n",
    "            \"error_analysis\": analyze_errors(output, instance)\n",
    "        }\n",
    "        \n",
    "        results.append((score, output, side_info))\n",
    "    return results\n",
    "```\n",
    "\n",
    "The magic happens in `side_info` — this is GEPA's secret weapon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf16cd",
   "metadata": {},
   "source": [
    "### The Power of Side Information\n",
    "\n",
    "The `side_info` dictionary is where GEPA shines. Unlike traditional optimization that only sees a scalar score, GEPA's LLM-based reflection can understand *why* a candidate performed poorly:\n",
    "\n",
    "- **Error messages**: Compiler errors, runtime exceptions, validation failures\n",
    "- **Execution traces**: What the candidate actually did vs. what was expected\n",
    "- **Partial results**: Which sub-tasks succeeded, which failed\n",
    "- **Domain-specific feedback**: Any signal that helps explain performance\n",
    "\n",
    "The more informative your `side_info`, the better GEPA can reason about improvements. This is what enables GEPA to optimize complex artifacts like code and agent architectures — not just tweak numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f143f",
   "metadata": {},
   "source": [
    "## Example 1: Code Optimization — Evolving Optimization Algorithms\n",
    "\n",
    "Our first example demonstrates GEPA's ability to evolve code. We'll optimize Python code that minimizes blackbox functions from the [evalset benchmark](https://github.com/sigopt/evalset/tree/main) — a collection of challenging optimization test functions (Ackley, Rosenbrock, Rastrigin, etc.).\n",
    "\n",
    "**The task**: Given a blackbox function, write code that finds its minimum. The code can use any optimization library (Optuna, scipy, etc.) and must return the best `x` found.\n",
    "\n",
    "**What GEPA optimizes**: The Python code itself — its structure, algorithm choice, hyperparameters, and implementation details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ae873",
   "metadata": {},
   "source": [
    "### Setting up the dataset\n",
    "\n",
    "Each data instance is a blackbox optimization problem with bounds, dimension, and problem characteristics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ced13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.polynomial.evalset import problems\n",
    "\n",
    "# Create dataset from benchmark problems\n",
    "dataset = []\n",
    "for problem_name, problem in problems.items():\n",
    "    dataset.append({\n",
    "        \"problem_name\": problem_name,\n",
    "        \"problem_description\": f\"\"\"Blackbox optimization problem.\n",
    "        Minimize a function that takes a numpy array of shape ({problem.dim},) and returns a scalar.\n",
    "        Bounds: {problem.bounds}\"\"\",\n",
    "        \"dim\": problem.dim,\n",
    "        \"bounds\": problem.bounds,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a91948d",
   "metadata": {},
   "source": [
    "### The seed candidate\n",
    "\n",
    "We start with a trivial baseline — code that just guesses the center of the search space:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcad3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_candidate = {\n",
    "    \"code\": \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def solve(dim):\n",
    "    # A trivial baseline: guess the center of the search space\n",
    "    x = [0.5] * dim\n",
    "    y = evaluator.evaluate(np.array(x))\n",
    "    print(\"y:\", y)\n",
    "    return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global x\n",
    "    x = solve(dim)\n",
    "\"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd33611",
   "metadata": {},
   "source": [
    "### The fitness function\n",
    "\n",
    "The fitness function executes the candidate code in a sandboxed environment, captures the result, and returns rich diagnostic information:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Sequence\n",
    "\n",
    "def execute_code(code: str, global_vars: dict, timeout: int = 30) -> dict:\n",
    "    \"\"\"Execute code in a sandboxed environment with timeout.\"\"\"\n",
    "    # Implementation handles: stdout/stderr capture, timeout, exception handling\n",
    "    # Returns: {\"output\": str, \"logs\": str, \"results\": dict, \"error\": str}\n",
    "    ...\n",
    "\n",
    "def fitness_fn(candidate: dict[str, str], batch: Sequence[Any]) -> list[tuple[float, Any, dict]]:\n",
    "    \"\"\"Evaluate optimization code on a batch of blackbox problems.\"\"\"\n",
    "    code = candidate[\"code\"]\n",
    "    results = []\n",
    "    \n",
    "    for problem_instance in batch:\n",
    "        problem = problems[problem_instance[\"problem_name\"]]\n",
    "        \n",
    "        # Execute the candidate code with problem context\n",
    "        execution = execute_code(\n",
    "            code,\n",
    "            global_vars={\"dim\": problem.dim, \"evaluator\": evaluator},\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        # Compute score: negative function value (higher is better)\n",
    "        if \"x\" in execution[\"results\"] and execution[\"error\"] == \"\":\n",
    "            x = np.array(execution[\"results\"][\"x\"])\n",
    "            score = -problem.do_evaluate(x)  # Negate because we minimize\n",
    "        else:\n",
    "            score = -99999  # Penalize failed executions\n",
    "        \n",
    "        # Rich side_info for LLM reflection\n",
    "        side_info = {\n",
    "            \"scores\": {\"score\": score},\n",
    "            \"Input\": {\"problem_description\": problem_instance[\"problem_description\"]},\n",
    "            \"code_side_info\": {\n",
    "                \"X\": execution[\"results\"].get(\"x\", \"not found\"),\n",
    "                \"Prints\": execution[\"output\"],       # Captured stdout\n",
    "                \"Logs\": execution[\"logs\"],           # Captured stderr  \n",
    "                \"Error\": execution[\"error\"],         # Any exceptions\n",
    "                \"Num evaluation calls\": evaluator.local_evaluation_calls,\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        results.append((score, {\"code\": code, **side_info}, side_info))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218c04c",
   "metadata": {},
   "source": [
    "Notice how `side_info` captures everything the LLM needs to understand *why* the code failed or succeeded: error messages, print output, the actual result found, and evaluation budget used.\n",
    "\n",
    "### Running GEPA optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa.optimize_anything import (\n",
    "    optimize_anything,\n",
    "    GEPAConfig,\n",
    "    EngineConfig,\n",
    "    ReflectionConfig,\n",
    ")\n",
    "\n",
    "result = optimize_anything(\n",
    "    seed_candidate=seed_candidate,\n",
    "    fitness_fn=fitness_fn,\n",
    "    dataset=dataset,\n",
    "    config=GEPAConfig(\n",
    "        engine=EngineConfig(\n",
    "            max_metric_calls=1000,\n",
    "            track_best_outputs=True,\n",
    "        ),\n",
    "        reflection=ReflectionConfig(\n",
    "            reflection_lm=\"openai/gpt-4o\",  # LLM for proposing improvements\n",
    "            reflection_minibatch_size=3,     # Problems shown per reflection\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Access the optimized code\n",
    "print(result.best_candidate[\"code\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54affe23",
   "metadata": {},
   "source": [
    "GEPA evolves the code from a trivial baseline into sophisticated optimization strategies — discovering the use of libraries like Optuna, implementing proper bounds handling, and tuning algorithm hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352303a5",
   "metadata": {},
   "source": [
    "## Example 2: Agent Optimization — Evolving DSPy Programs for ARC-AGI\n",
    "\n",
    "Our second example pushes GEPA further: optimizing not just prompts or hyperparameters, but the *entire structure* of an AI agent. We'll evolve a DSPy program to solve ARC-AGI tasks — a challenging benchmark requiring visual reasoning and pattern recognition.\n",
    "\n",
    "**The task**: Given input-output matrix pairs as training examples, produce the correct output for test inputs.\n",
    "\n",
    "**What GEPA optimizes**: The entire DSPy program source code — signatures, modules, control flow, and prompting strategies.\n",
    "\n",
    "**Result**: GEPA improves Gemini-2.5-Pro's performance from **44% to 49.5%** by discovering an elaborate 5-step reasoning pipeline with self-refinement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58492d",
   "metadata": {},
   "source": [
    "### Setting up the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00eba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import dspy\n",
    "\n",
    "ds = load_dataset(\"dataartist/arc-agi\")\n",
    "\n",
    "# Each example has training pairs and test inputs/outputs\n",
    "dataset = [\n",
    "    dspy.Example(\n",
    "        training_examples=ex[\"train\"],\n",
    "        test_inputs=[x[\"input\"] for x in ex[\"test\"]],\n",
    "        test_outputs=[x[\"output\"] for x in ex[\"test\"]],\n",
    "    ).with_inputs(\"training_examples\", \"test_inputs\")\n",
    "    for ex in ds[\"training\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e1e27",
   "metadata": {},
   "source": [
    "### The seed candidate\n",
    "\n",
    "We start with a simple Chain-of-Thought program — just a single DSPy module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_program = \"\"\"import dspy\n",
    "from typing import List\n",
    "import pydantic\n",
    "\n",
    "MATRIX = List[List[int]]\n",
    "\n",
    "class TrainingExample(pydantic.BaseModel):\n",
    "    input: MATRIX\n",
    "    output: MATRIX\n",
    "\n",
    "class SolveTaskSignature(dspy.Signature):\n",
    "    training_examples: List[TrainingExample] = dspy.InputField(\n",
    "        description=\"Input and output examples demonstrating the task.\"\n",
    "    )\n",
    "    test_inputs: List[MATRIX] = dspy.InputField(\n",
    "        description=\"Input matrices to be solved.\"\n",
    "    )\n",
    "    test_outputs: List[MATRIX] = dspy.OutputField(\n",
    "        description=\"Output matrices corresponding to the test inputs.\"\n",
    "    )\n",
    "\n",
    "program = dspy.ChainOfThought(SolveTaskSignature)\n",
    "\"\"\"\n",
    "\n",
    "seed_candidate = {\"program\": seed_program}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e441e",
   "metadata": {},
   "source": [
    "### The fitness function\n",
    "\n",
    "The fitness function compiles and runs the DSPy program, comparing outputs against ground truth. Crucially, it provides detailed feedback about *what went wrong*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001eb5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_lm = dspy.LM(model=\"gemini/gemini-2.5-pro\", max_tokens=32000)\n",
    "\n",
    "def validate_matrix(pred_matrix, gold_matrix):\n",
    "    \"\"\"Check if prediction matches gold, returning (is_valid, feedback).\"\"\"\n",
    "    if not isinstance(pred_matrix, list):\n",
    "        return False, f\"Expected List[List[int]], got {type(pred_matrix)}\"\n",
    "    \n",
    "    if len(pred_matrix) != len(gold_matrix):\n",
    "        return False, f\"Wrong dimensions: {len(pred_matrix)} rows vs {len(gold_matrix)} expected\"\n",
    "    \n",
    "    wrong_indices = []\n",
    "    for i, (pred_row, gold_row) in enumerate(zip(pred_matrix, gold_matrix)):\n",
    "        for j, (pred_val, gold_val) in enumerate(zip(pred_row, gold_row)):\n",
    "            if pred_val != gold_val:\n",
    "                wrong_indices.append((i, j))\n",
    "    \n",
    "    if wrong_indices:\n",
    "        return False, f\"Incorrect values at indices: {wrong_indices[:10]}... Correct: {gold_matrix}\"\n",
    "    return True, \"Correct!\"\n",
    "\n",
    "def fitness_fn(candidate: dict, batch: Sequence, **kwargs) -> list[tuple[float, Any, dict]]:\n",
    "    \"\"\"Evaluate a DSPy program on ARC-AGI tasks.\"\"\"\n",
    "    program_src = candidate[\"program\"]\n",
    "    results = []\n",
    "    \n",
    "    for example in batch:\n",
    "        # Compile and run the program\n",
    "        try:\n",
    "            exec(program_src, globals())\n",
    "            with dspy.context(lm=task_lm):\n",
    "                pred = program(\n",
    "                    training_examples=example.training_examples,\n",
    "                    test_inputs=example.test_inputs\n",
    "                )\n",
    "            pred_outputs = pred.test_outputs\n",
    "            error = None\n",
    "        except Exception as e:\n",
    "            pred_outputs = []\n",
    "            error = str(e)\n",
    "        \n",
    "        # Score each test output\n",
    "        feedbacks = []\n",
    "        correct = 0\n",
    "        for i, (pred_out, gold_out) in enumerate(\n",
    "            zip(pred_outputs, example.test_outputs)\n",
    "        ):\n",
    "            valid, feedback = validate_matrix(pred_out, gold_out)\n",
    "            correct += int(valid)\n",
    "            feedbacks.append(f\"Test {i}: {feedback}\")\n",
    "        \n",
    "        score = correct / len(example.test_outputs) if example.test_outputs else 0\n",
    "        \n",
    "        # Rich side_info enables targeted reflection\n",
    "        side_info = {\n",
    "            \"scores\": {\"accuracy\": score},\n",
    "            \"TrainingExamples\": str(example.training_examples)[:500],\n",
    "            \"TestInputs\": str(example.test_inputs)[:500],\n",
    "            \"PredictedOutputs\": str(pred_outputs)[:500],\n",
    "            \"ExpectedOutputs\": str(example.test_outputs)[:500],\n",
    "            \"Feedback\": \"\\n\".join(feedbacks),\n",
    "            \"Error\": error,\n",
    "        }\n",
    "        \n",
    "        results.append((score, pred_outputs, side_info))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec5232",
   "metadata": {},
   "source": [
    "### Running GEPA optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_lm = dspy.LM(model=\"gemini/gemini-2.5-pro\", max_tokens=32000)\n",
    "\n",
    "result = optimize_anything(\n",
    "    seed_candidate=seed_candidate,\n",
    "    fitness_fn=fitness_fn,\n",
    "    dataset=dataset,\n",
    "    config=GEPAConfig(\n",
    "        engine=EngineConfig(\n",
    "            max_metric_calls=4000,\n",
    "            track_best_outputs=True,\n",
    "        ),\n",
    "        reflection=ReflectionConfig(\n",
    "            reflection_lm=lambda x: reflection_lm(x)[0],\n",
    "            reflection_minibatch_size=3,\n",
    "        ),\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31781e",
   "metadata": {},
   "source": [
    "### What GEPA discovered\n",
    "\n",
    "After optimization, GEPA evolved the simple ChainOfThought into an elaborate 5-step pipeline:\n",
    "\n",
    "1. **Hypothesize Rule**: Ask LLM to deduce a natural language transformation rule from training examples\n",
    "2. **Generate Code**: Ask LLM to implement the rule as a Python function\n",
    "3. **Validate on Training**: Run the code on all training examples, collecting feedback on failures\n",
    "4. **Refine if Needed**: If validation fails, ask LLM to fix the code using gathered feedback\n",
    "5. **Execute on Test**: Run the refined code on test inputs\n",
    "\n",
    "Remarkably, **GEPA discovered reflective self-refinement** — having the LLM check and fix its own code before producing final outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de947640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the evolved program\n",
    "print(result.best_candidate[\"program\"][:2000])  # First 2000 chars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238b846",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "The `optimize_anything` API demonstrates GEPA's power as a general-purpose text evolution engine:\n",
    "\n",
    "1. **Unified interface**: Whether you're optimizing prompts, code, or agent architectures, the API is the same — just define your fitness function with rich `side_info`.\n",
    "\n",
    "2. **Side information is key**: The more diagnostic information you provide, the better GEPA's LLM-based reflection can understand failures and propose targeted improvements.\n",
    "\n",
    "3. **Beyond scalar optimization**: Traditional optimizers only see scores. GEPA sees error messages, execution traces, and domain-specific feedback — enabling it to optimize complex artifacts that would be impossible to search blindly.\n",
    "\n",
    "4. **Emergent capabilities**: GEPA can discover sophisticated strategies (like self-refinement in the ARC-AGI example) that weren't explicitly programmed — they emerge from the optimization process itself.\n",
    "\n",
    "Try `optimize_anything` on your own optimization problems. If you can express your system's parameters as text and compute a score with diagnostic feedback, GEPA can optimize it.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
