{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eefb064",
   "metadata": {},
   "source": [
    "# GEPA: Optimize Anything with LLMs\n",
    "\n",
    "LLM-based optimization algorithms like GEPA, OpenEvolve, ShinkaEvolve, and AlphaEvolve have shown real promise. But their focus has been fragmented: GEPA originally focused on evolving LLM prompts, while others targeted scientific and algorithmic discovery. Today, we're announcing GEPA's new `optimize_anything` API—an open-source library you can plug into any scenario to optimize text of any kind: prompts, code, agents, or even compositions of them.\n",
    "\n",
    "With this API, GEPA becomes a general-purpose text evolution engine. Given a target metric, GEPA efficiently searches for the right parameters to improve that metric. It uses LLMs to generate proposals and leverages side information from the optimization environment to guide the search. This means GEPA can optimize essentially *anything* with a textual representation.\n",
    "\n",
    "In this post, we provide the following minimal, actionable examples to demonstrate the flexibility of our API:\n",
    "1. **Mathematical optimization** that competes Optuna\n",
    "2. **Prompt optimization** that beats GRPO and MIPRO\n",
    "3. **Agent optimization** on ARC-AGI\n",
    "4. **Combinatorial optimization** for circle packing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52b3cc",
   "metadata": {},
   "source": [
    "## The optimize_anything API\n",
    "\n",
    "At its core, the API is remarkably simple. You provide just two things:\n",
    "\n",
    "1. **A seed candidate** — your starting point, represented as a dictionary mapping parameter names to their values. \n",
    "2. **A fitness function** — tells GEPA how good each candidate is. The fitness function also returns any additional information available from the environment about the evaluated candidate, like compiler error messages, that can guide the optimization.\n",
    "\n",
    "That's it. GEPA handles the rest — selecting candidates, reflecting on failures, proposing improvements, and tracking the optimization trajectory, finally returning the optimized parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b5553",
   "metadata": {},
   "source": [
    "### The Fitness Function: Your Optimization Signal\n",
    "\n",
    "The fitness function is where you define *what* you're optimizing for. It takes a candidate and a data instance, returning scores and diagnostic information:\n",
    "\n",
    "```python\n",
    "def fitness_fn(candidate: dict[str, str], instance: DataInst) -> list[tuple[float, Any, dict]]:\n",
    "    # Run your system with the candidate parameters\n",
    "    output = run_my_system(candidate, instance)\n",
    "    \n",
    "    # Compute a score (higher is better)\n",
    "    score = compute_score(output, instance)\n",
    "    \n",
    "    # Collect diagnostic info for LLM reflection\n",
    "    side_info = {\n",
    "        \"input\": instance,\n",
    "        \"output\": output,\n",
    "        \"expected\": instance[\"expected\"],\n",
    "        \"error_analysis\": analyze_errors(output)\n",
    "    }\n",
    "    return (score, output, side_info)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf16cd",
   "metadata": {},
   "source": [
    "### The Power of Side Information\n",
    "\n",
    "The `side_info` dictionary empowers GEPA. Unlike traditional optimization that only sees a scalar score, GEPA's LLM-based reflection can understand *why* a candidate performed poorly:\n",
    "\n",
    "- **Error messages**: Compiler errors, runtime exceptions, validation failures\n",
    "- **Execution traces**: What the candidate actually did vs. what was expected\n",
    "- **Partial results**: Which sub-tasks succeeded, which failed\n",
    "- **Domain-specific feedback**: Any signal that helps explain performance\n",
    "\n",
    "You have complete control over what to put inside `side_info.` The more informative your `side_info`, the better GEPA can reason about improvements. This enables GEPA to optimize complex artifacts like code and agent architectures — not just tweak numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f143f",
   "metadata": {},
   "source": [
    "## Example 1: Mathematical optimization\n",
    "\n",
    "We first demonstrate GEPA's ability to evolve a search code that minimizes blackbox functions from the [evalset benchmark](https://github.com/sigopt/evalset/tree/main) — a collection of challenging optimization test functions (Ackley, Rosenbrock, Rastrigin, etc.) benchmarked by the [Optuna paper](https://arxiv.org/pdf/1907.10902). \n",
    "\n",
    "**The task**: Given a blackbox function, write code that finds its minimum. The code can use any optimization library (Optuna, scipy, etc.) and returns the best `x`.\n",
    "\n",
    "**What GEPA optimizes**: The Python code itself — its structure, algorithm choice, hyperparameters, and implementation details.\n",
    "\n",
    "The figure below shows GEPA competing with Optuna. Starting from minimal baseline code, GEPA initially underperforms. However, it progressively discovers more effective optimization strategies, eventually finding better solutions than Optuna in later stages. \n",
    "\n",
    "<img src=\"./assets/blog/polynomial_optimization_normalized_arithmetic.png\" width=\"50%\">\n",
    "\n",
    "Here is the code to reproduce this experiment: [link].\n",
    "\n",
    "While Optuna requires users to select sampling algorithms and techniques for advanced use cases, GEPA frees you from such decisions. By simply defining a baseline code template and the fitness function, GEPA automatically evolves the search code—from experimenting with high-level search strategies to fine-tuning hyperparameters. \n",
    "\n",
    "Now, let's walk through a simple example of optimizing code on a single problem from evalset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ae873",
   "metadata": {},
   "source": [
    "### Setting up the dataset\n",
    "\n",
    "Here, the dataset is a single blackbox optimization problem with bounds, dimension, and problem characteristics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ced13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.polynomial.evalset import sample_problem\n",
    "\n",
    "dataset = [sample_problem]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a91948d",
   "metadata": {},
   "source": [
    "### The seed candidate\n",
    "\n",
    "We start with a trivial baseline that randomly samples a solution. The function signature exposes the problem structure—`dim` and `bounds` define the search space, `total_evaluation_budgets` limits how many times the code can call the objective, and `prev_best_x` provides the best solution found so far (if any). The solver code can use `objective_function` to evaluate and compare candidates before returning its best guess.\n",
    "\n",
    "(is this function too complicated?)\n",
    "- maybe i could specify total_evaluation_budgets in the LLM prompt instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcad3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_candidate = \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def solve(dim, bounds, objective_function, prev_best_x):\n",
    "    bounds_arr = np.array(bounds)\n",
    "    x = np.random.uniform(bounds_arr[:, 0], bounds_arr[:, 1])\n",
    "    y = objective_function(x)\n",
    "    return x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd33611",
   "metadata": {},
   "source": [
    "### The fitness function\n",
    "\n",
    "The fitness function executes the candidate code in a sandboxed environment, captures the result, and returns rich diagnostic information:\n",
    "\n",
    "(update the code below to use our real code. but currently it looks quite complicated. let's modify it. let's abstrsact awway none-gepa-related codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from examples.new_polynomial.evaluator import execute_code, compute_score\n",
    "\n",
    "# TODO: add best_side_info to the fitness function\n",
    "def fitness_fn(candidate: dict[str, str], problem: Any, best_side_info: dict) -> list[tuple[float, Any, dict]]:\n",
    "    code = candidate[\"code\"]\n",
    "    execution = execute_code(code, 300, {\n",
    "        \"dim\": problem[\"dim\"], \n",
    "        \"bounds\": problem[\"bounds\"], \n",
    "        \"objective_function\": problem[\"objective_function\"],\n",
    "        \"prev_best_x\": best_side_info[\"X\"],\n",
    "    })\n",
    "    score = compute_score(execution)\n",
    "    \n",
    "    side_info = {\n",
    "        \"scores\": {\"score\": score},\n",
    "        \"Input\": {\"problem_description\": problem[\"problem_description\"]},\n",
    "        \"code_side_info\": {\n",
    "            \"X\": execution[\"results\"].get(\"x\", \"not found\"),\n",
    "            \"Prints\": execution[\"output\"],       # Captured stdout\n",
    "            \"Logs\": execution[\"logs\"],           # Captured stderr  \n",
    "            \"Error\": execution[\"error\"],         # Any exceptions\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    return score, {\"code\": code, **side_info}, side_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218c04c",
   "metadata": {},
   "source": [
    "Notice how `side_info` captures everything the LLM needs to understand *why* the code failed or succeeded: error messages, print output, and the result found.\n",
    "\n",
    "### Running GEPA optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca32d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fitness_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgepa\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize_anything\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     optimize_anything,\n\u001b[32m      3\u001b[39m     GEPAConfig,\n\u001b[32m      4\u001b[39m     EngineConfig,\n\u001b[32m      5\u001b[39m     ReflectionConfig,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m result = optimize_anything(\n\u001b[32m      9\u001b[39m     seed_candidate=seed_candidate,\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     fitness_fn=\u001b[43mfitness_fn\u001b[49m,\n\u001b[32m     11\u001b[39m     dataset=dataset,\n\u001b[32m     12\u001b[39m     config=GEPAConfig(\n\u001b[32m     13\u001b[39m         engine=EngineConfig(\n\u001b[32m     14\u001b[39m             max_metric_calls=\u001b[32m100\u001b[39m,  \u001b[38;5;66;03m# tweak the number\u001b[39;00m\n\u001b[32m     15\u001b[39m             track_best_outputs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     16\u001b[39m         ),\n\u001b[32m     17\u001b[39m         reflection=ReflectionConfig(\n\u001b[32m     18\u001b[39m             reflection_lm=\u001b[33m\"\u001b[39m\u001b[33mopenai/gpt-5.1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m             reflection_minibatch_size=\u001b[32m1\u001b[39m,     \u001b[38;5;66;03m# Problems shown per reflection. In this example, we have only one problem, and thus we set it 1.\u001b[39;00m\n\u001b[32m     20\u001b[39m         ),\n\u001b[32m     21\u001b[39m     ),\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Access the optimized code\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.best_candidate[\u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'fitness_fn' is not defined"
     ]
    }
   ],
   "source": [
    "from gepa.optimize_anything import (\n",
    "    optimize_anything,\n",
    "    GEPAConfig,\n",
    "    EngineConfig,\n",
    "    ReflectionConfig,\n",
    ")\n",
    "\n",
    "result = optimize_anything(\n",
    "    seed_candidate=seed_candidate,\n",
    "    fitness_fn=fitness_fn,\n",
    "    dataset=dataset,\n",
    "    config=GEPAConfig(\n",
    "        engine=EngineConfig(\n",
    "            max_metric_calls=100,  # tweak the number\n",
    "            track_best_outputs=True,\n",
    "            cache_evaluation=True,  # TODO: add\n",
    "        ),\n",
    "        reflection=ReflectionConfig(\n",
    "            reflection_lm=\"openai/gpt-5.1\",\n",
    "            reflection_minibatch_size=1,     # Problems shown per reflection. In this example, we have only one problem, and thus we set it 1.\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Access the optimized code\n",
    "print(result.best_candidate[\"code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54affe23",
   "metadata": {},
   "source": [
    "**The evolved solution:** GEPA discovered a hybrid optimizer that combines adaptive evolutionary search with surrogate-assisted trust-region methods—automatically escalating from cheap linear models to richer quadratic approximations as the search stalls.\n",
    "\n",
    "**Why it works:** Rather than relying on a fixed algorithm, GEPA learned to dynamically balance exploration (orthogonalized sampling, Cauchy jumps) and exploitation (gradient probes, Nelder-Mead) based on observed progress—a strategy no human specified, but one that outperforms hand-tuned baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774db146",
   "metadata": {},
   "source": [
    "## Example 2: Prompt Optimization\n",
    "\n",
    "In our [GEPA paper](link) at ICLR 2025, we showed that GEPA outperforms the previous state-of-the-art optimizer, MIPROv2, by over 10%—and even beats GRPO using only 2% of the rollouts across four tasks.\n",
    "\n",
    "<img src=\"./assets/blog/gepa_aime.png\" width=\"70%\">\n",
    "\n",
    "<img src=\"./assets/blog/aime_best_comparison.png\" width=\"70%\">\n",
    "\n",
    "In this tutorial, we take the AIME 2025 benchmark and show a minimal example: GEPA evolves a prompt for GPT-4.1-mini, boosting its score from 46.67% to 53.33%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f526e920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45 training examples\n",
      "Loaded 45 validation examples\n",
      "Loaded 30 test examples\n"
     ]
    }
   ],
   "source": [
    "from examples.math.dataset import load_math_dataset\n",
    "\n",
    "trainset, valset, testset = load_math_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec163d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "\n",
    "# Use GPT-4.1-mini as the language model to solve the math problems.\n",
    "lm = dspy.LM(\"gpt-4.1-mini\", api_key=os.environ.get(\"OPENAI_API_KEY\"), temperature=1.0, max_tokens=32000)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Define a simple base prompt that we will optimize.\n",
    "SEED_PROMPT = \"\"\"Solve the math problem carefully. Break down the steps and provide the final answer as a single number.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff6ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa.optimize_anything import SideInfo\n",
    "\n",
    "from examples.math.main import run_llm, math_metric\n",
    "\n",
    "def fitness_fn(candidate: dict[str, str], example: Any) -> list[tuple[float, Any, SideInfo]]:\n",
    "    prediction = run_llm(example, candidate[\"prompt\"])\n",
    "    metric_result = math_metric(example, prediction)\n",
    "    score = metric_result.score\n",
    "    feedback = metric_result.feedback\n",
    "\n",
    "    output = {\n",
    "        \"prompt\": candidate[\"prompt\"],\n",
    "        \"answer\": prediction.answer,\n",
    "        \"score\": score,\n",
    "    }\n",
    "\n",
    "    side_info = {\n",
    "        \"Input\": example.input,\n",
    "        \"Output\": prediction.answer,\n",
    "        \"Reasoning\": getattr(prediction, \"reasoning\", \"\"),\n",
    "        \"ExecutionFeedback\": feedback,\n",
    "    }\n",
    "\n",
    "    return (score, output, side_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14431a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa.optimize_anything import (\n",
    "    EngineConfig,\n",
    "    GEPAConfig,\n",
    "    ReflectionConfig,\n",
    "    optimize_anything,\n",
    ")\n",
    "\n",
    "gepa_config = GEPAConfig(\n",
    "    engine=EngineConfig(\n",
    "        max_metric_calls=800,\n",
    "        track_best_outputs=True,\n",
    "    ),\n",
    "    reflection=ReflectionConfig(\n",
    "        reflection_minibatch_size=3,\n",
    "        skip_perfect_score=False,\n",
    "        reflection_lm=\"openai/gpt-5.1\",\n",
    "    )\n",
    ")\n",
    "\n",
    "result = optimize_anything(\n",
    "    seed_candidate={\"prompt\": SEED_PROMPT},\n",
    "    fitness_fn=fitness_fn,\n",
    "    dataset=trainset,\n",
    "    valset=valset,\n",
    "    config=gepa_config,\n",
    ")\n",
    "\n",
    "best_prompt = result.best_candidate[\"prompt\"]\n",
    "best_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.math.main import evaluate_on_dataset\n",
    "\n",
    "# Baseline Evaluation\n",
    "print(\"\\nEvaluating Baseline (Initial Prompt)...\")\n",
    "baseline_score = evaluate_on_dataset(SEED_PROMPT, testset)\n",
    "\n",
    "# Optimized Evaluation\n",
    "print(\"\\nEvaluating Best Optimized Program...\")\n",
    "best_prompt = result.best_candidate[\"prompt\"]\n",
    "print(f\"Best Prompt Found:\\n{best_prompt}\")\n",
    "\n",
    "optimized_score = evaluate_on_dataset(best_prompt, testset)\n",
    "\n",
    "print(f\"Baseline Score: {baseline_score:.2%}\")\n",
    "print(f\"Optimized Score: {optimized_score:.2%}\")\n",
    "print(f\"Improvement: {optimized_score - baseline_score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd938c",
   "metadata": {},
   "source": [
    "TODO: show the evolved prompt\n",
    "Also run the test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352303a5",
   "metadata": {},
   "source": [
    "## Example 3: Agent Optimization — Evolving DSPy Programs for ARC-AGI\n",
    "\n",
    "Our third example pushes GEPA further: optimizing not just prompts or hyperparameters, but the *entire structure* of an AI agent. We'll evolve a DSPy program to solve ARC-AGI tasks — a challenging benchmark requiring visual reasoning and pattern recognition.\n",
    "\n",
    "**The task**: Given input-output matrix pairs as training examples, produce the correct output for test inputs.\n",
    "\n",
    "**What GEPA optimizes**: The entire DSPy program source code — signatures, modules, control flow, and prompting strategies.\n",
    "\n",
    "**Result**: GEPA improves GPT5's performance from **X% to Y%** by discovering an [elaborate 5-step reasoning pipeline with self-refinement.]\n",
    "\n",
    "<!-- ![ARC AGI Graph](./assets/blog/arc_agi_optimization_progress.png) -->\n",
    "<img src=\"./assets/blog/arc_agi_best_comparison.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58492d",
   "metadata": {},
   "source": [
    "### Setting up the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00eba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 200\n",
      "Val set: 200\n",
      "Test set: 400\n"
     ]
    }
   ],
   "source": [
    "from examples.arc_agi.data import load_arc_agi_dataset\n",
    "\n",
    "train_set, val_set, test_set = load_arc_agi_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e1e27",
   "metadata": {},
   "source": [
    "### The seed candidate\n",
    "\n",
    "We start with a simple Chain-of-Thought program — just a single DSPy module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f3c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_candidate = \"\"\"import dspy\n",
    "from typing import List\n",
    "import pydantic\n",
    "\n",
    "MATRIX = List[List[int]]\n",
    "\n",
    "class TrainingExample(pydantic.BaseModel):\n",
    "    input: MATRIX\n",
    "    output: MATRIX\n",
    "\n",
    "class SolveTaskSignature(dspy.Signature):\n",
    "    training_examples: List[TrainingExample] = dspy.InputField(description=\"Input and output examples demonstrating the task to be performed.\")\n",
    "    test_inputs: List[MATRIX] = dspy.InputField(description=\"Input matrices to be solved following the task described in the training examples.\")\n",
    "    test_outputs: List[MATRIX] = dspy.OutputField(description=\"Output matrices corresponding to the test inputs.\")\n",
    "\n",
    "program = dspy.ChainOfThought(SolveTaskSignature)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193aa56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "\n",
    "from gepa.adapters.dspy_full_program_adapter.full_program_adapter import DspyAdapter\n",
    "from examples.arc_agi.main import metric_fn\n",
    "\n",
    "# Create LMs\n",
    "task_lm = dspy.LM(\n",
    "    model=\"openai/gpt-5\",\n",
    "    temperature=1.0,\n",
    "    max_tokens=32000,\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Create adapter\n",
    "adapter = DspyAdapter(\n",
    "    task_lm=task_lm,\n",
    "    metric_fn=metric_fn,\n",
    "    num_threads=64,\n",
    "    reflection_lm=\"openai/gpt-5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e441e",
   "metadata": {},
   "source": [
    "### The fitness function\n",
    "\n",
    "The fitness function compiles and runs the DSPy program, comparing outputs against ground truth. Crucially, it provides detailed feedback about *what went wrong*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001eb5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_fn(candidate, example):\n",
    "    program = candidate[\"program\"]\n",
    "    print(\"Example: \", type(example))\n",
    "\n",
    "    try:\n",
    "        evaluation_results = adapter.evaluate([example], candidate, capture_traces=True)\n",
    "    except Exception as e:\n",
    "        side_info = {\n",
    "            \"input\": example,\n",
    "            \"error\": str(e),\n",
    "            \"program\": program\n",
    "        }\n",
    "        return (0.0, side_info, side_info)\n",
    "\n",
    "    # Program error\n",
    "    if not isinstance(evaluation_results.trajectories, list) or len(evaluation_results.trajectories) == 0:\n",
    "        print(\"Error: \")\n",
    "        print(evaluation_results.trajectories)\n",
    "        side_info = {\n",
    "            \"input\": example,\n",
    "            \"error\": f\"All examples failed. Program error: {str(evaluation_results.trajectories)}\",\n",
    "            \"program\": program\n",
    "        }\n",
    "        return (0.0, side_info, side_info)\n",
    "\n",
    "    # Process evaluations with no program errors\n",
    "    trajectory = evaluation_results.trajectories[0]\n",
    "    metric_result = trajectory.get(\"score\")\n",
    "    score = metric_result.get(\"score\")\n",
    "    feedback = metric_result.get(\"feedback\")\n",
    "    prediction = trajectory.get(\"prediction\")\n",
    "\n",
    "    side_info = {\n",
    "        \"input\": example,\n",
    "        \"reasoning\": prediction.get(\"reasoning\"),\n",
    "        \"feedback\": feedback,\n",
    "        \"output\": prediction.get(\"test_outputs\"),\n",
    "    }\n",
    "\n",
    "    return (score, side_info, side_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec5232",
   "metadata": {},
   "source": [
    "### Running GEPA optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa.optimize_anything import (\n",
    "    EngineConfig,\n",
    "    GEPAConfig,\n",
    "    ReflectionConfig,\n",
    "    optimize_anything,\n",
    ")\n",
    "from examples.arc_agi.prompt import REFLECTION_PROMPT\n",
    "\n",
    "gepa_config = GEPAConfig(\n",
    "    engine=EngineConfig(\n",
    "        max_metric_calls=4000,\n",
    "        track_best_outputs=True,\n",
    "        parallel=True,\n",
    "        max_workers=64,\n",
    "    ),\n",
    "    reflection=ReflectionConfig(\n",
    "        reflection_minibatch_size=3,\n",
    "        reflection_lm=\"openai/gpt-5\",\n",
    "        reflection_prompt_template=REFLECTION_PROMPT,\n",
    "    )\n",
    ")\n",
    "\n",
    "result = optimize_anything(\n",
    "    seed_candidate={\"program\": seed_candidate},\n",
    "    fitness_fn=fitness_fn,\n",
    "    dataset=train_set,\n",
    "    valset=val_set,\n",
    "    config=gepa_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31781e",
   "metadata": {},
   "source": [
    "### What GEPA discovered\n",
    "\n",
    "After optimization, GEPA evolved the simple ChainOfThought into an elaborate 5-step pipeline:\n",
    "\n",
    "1. **Hypothesize Rule**: Ask LLM to deduce a natural language transformation rule from training examples\n",
    "2. **Generate Code**: Ask LLM to implement the rule as a Python function\n",
    "3. **Validate on Training**: Run the code on all training examples, collecting feedback on failures\n",
    "4. **Refine if Needed**: If validation fails, ask LLM to fix the code using gathered feedback\n",
    "5. **Execute on Test**: Run the refined code on test inputs\n",
    "\n",
    "Remarkably, **GEPA discovered reflective self-refinement** — having the LLM check and fix its own code before producing final outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de947640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the evolved program\n",
    "print(result.best_candidate[\"program\"][:2000])  # First 2000 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a86150",
   "metadata": {},
   "source": [
    "## Circle Packing\n",
    "\n",
    "Circle packing is a classic example used by ShinkaEvolve, OpenEvolve, and AlphaEvolve.\n",
    "Here, we also show how GEPA can conduct an algorithmic discovery for circle packing.\n",
    "\n",
    "### Single mode (Evolving a search code for a single instance)\n",
    "\n",
    "<img src=\"./assets/blog/circle_packing/circle_packing_26_comparison.png\" width=\"80%\">\n",
    "\n",
    "<!-- <img src=\"./assets/blog/circle_packing/circle_packing_21.png\" width=\"50%\">\n",
    "\n",
    "<img src=\"./assets/blog/circle_packing/circle_packing_26.png\" width=\"50%\">\n",
    "\n",
    "<img src=\"./assets/blog/circle_packing/circle_packing_32.png\" width=\"50%\"> -->\n",
    "\n",
    "<!-- ### Batch mode (Evolving a search code for 13 instances)\n",
    "\n",
    "num_circles = [7, 13, 19, 21, 22, 26, 29, 31, etc.]\n",
    "\n",
    "<img src=\"./assets/blog/circle_packing/gepa_vs_shinka.png\" width=\"50%\">\n",
    "\n",
    "We take Shinka as a baseline and run the same gpt5.1 for a batch mode. \n",
    "We can see that more data instances -> GEPA save computes while Shinka performs a full validation. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a867ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.635977394754397"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best circles\n",
    "best_circles = [\n",
    "    [0.08468730125813197, 0.08468730125813197, 0.08468730125813197],\n",
    "    [0.8889367079091394, 0.1110632920908606, 0.1110632920908606],\n",
    "    [0.08468730125813231, 0.9153126987418675, 0.08468730125813231],\n",
    "    [0.8889367079091401, 0.8889367079091401, 0.11106329209085986],\n",
    "    [0.2735316496509986, 0.10527607855641154, 0.10527607855641154],\n",
    "    [0.48251901284944326, 0.10371709930579165, 0.10371709930579165],\n",
    "    [0.682251639361118, 0.09615849835819791, 0.09615849835819791],\n",
    "    [0.2735316496509994, 0.8947239214435884, 0.10527607855641163],\n",
    "    [0.4825190128494443, 0.8962829006942082, 0.10371709930579176],\n",
    "    [0.6822516393611192, 0.9038415016418019, 0.09615849835819812],\n",
    "    [0.13236009424048484, 0.2964345012443294, 0.13236009424048484],\n",
    "    [0.07826927088831243, 0.49999999999999845, 0.07826927088831243],\n",
    "    [0.13236009424048586, 0.7035654987556688, 0.13236009424048586],\n",
    "    [0.9075441882905443, 0.31372998221417503, 0.09245581170945572],\n",
    "    [0.9061808044177737, 0.5000000000000004, 0.09381919558222629],\n",
    "    [0.907544188290544, 0.686270017785826, 0.09245581170945605],\n",
    "    [0.2697939589756203, 0.500000000000001, 0.11325541719900234],\n",
    "    [0.38112506495598114, 0.7008876686603106, 0.11641928880622215],\n",
    "    [0.38112506495598014, 0.2991123313396901, 0.11641928880622275],\n",
    "    [0.7632491637451031, 0.7594985921321933, 0.06935728482284345],\n",
    "    [0.5965286484003857, 0.7268072190485636, 0.10053814216358756],\n",
    "    [0.742587601069024, 0.40428027755541857, 0.09571972244458213],\n",
    "    [0.7425876010690241, 0.5957197224445819, 0.09571972244458193],\n",
    "    [0.5965286484003852, 0.273192780951437, 0.1005381421635882],\n",
    "    [0.5325196677311786, 0.5000000000000006, 0.1351282835717167],\n",
    "    [0.7632491637451023, 0.24050140786780766, 0.06935728482284145],\n",
    "]\n",
    "\n",
    "best_circles_sum = np.array(best_circles)[:, 2].sum()\n",
    "best_circles_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ebdd3",
   "metadata": {},
   "source": [
    "Within just 150 evaluations, GEPA found the solution 99.9999880668% of AlphaEvolve, 99.9997836004% of ShinkaEvolve, and 100.063963765% of OpenEvolve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238b846",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "The `optimize_anything` API demonstrates GEPA's power as a general-purpose text evolution engine:\n",
    "\n",
    "1. **Unified interface**: Whether you're optimizing prompts, code, or agent architectures, the API is the same — just define your fitness function with rich `side_info`.\n",
    "\n",
    "2. **Side information is key**: The more diagnostic information you provide, the better GEPA's LLM-based reflection can understand failures and propose targeted improvements.\n",
    "\n",
    "3. **Beyond scalar optimization**: Traditional optimizers only see scores. GEPA sees error messages, execution traces, and domain-specific feedback — enabling it to optimize complex artifacts that would be impossible to search blindly.\n",
    "\n",
    "4. **Emergent capabilities**: GEPA can discover sophisticated strategies (like self-refinement in the ARC-AGI example) that weren't explicitly programmed — they emerge from the optimization process itself.\n",
    "\n",
    "Try `optimize_anything` on your own optimization problems. If you can express your system's parameters as text and compute a score with diagnostic feedback, GEPA can optimize it.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
