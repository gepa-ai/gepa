{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eefb064",
   "metadata": {},
   "source": [
    "# GEPA: Optimize Anything using ASI (additional side information)\n",
    "\n",
    "GEPA is a text evolution engine: Given a fitness function, GEPA can efficiently search for the right parameters (including numerical, textual and code) to improve the blackbox fitness. GEPA can optimize essentially _anything_ that has a textual representation. We leverage this insight to present GEPA's optimize-anything API, which leverages the reflective capabilities of LLMs, to optimize anything representable as text. Crucially, GEPA can leverage any additional information available from the optimization environment simply by serializing into text.\n",
    "\n",
    "In this blog, we will walk through using GEPA to optimize the parameters of a blackbox polynomial optimization problem, prompts of an agent for math tasks, optimizing the harness of the agent itself, and discovering efficient circle packing algorithms using GEPA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52b3cc",
   "metadata": {},
   "source": [
    "## The optimize_anything API\n",
    "\n",
    "At its core, the API is remarkably simple. You provide just the following things:\n",
    "\n",
    "1. **A seed candidate** — your starting point, represented as a dictionary mapping parameter names to their values. \n",
    "2. **A fitness function** — tells GEPA how good each candidate is. The fitness function also returns any additional information available from the environment about the evaluated candidate, like compiler error messages, that can guide the optimization.\n",
    "3. **(Optional) A datasets** - A dataset if the task domain consists of multiple related problem instances, especially if the parameters being optimized must generalize to unseen datapoints, OR, if there are several closely related problems being solved (like circle packing with different number of circles).\n",
    "\n",
    "That's it. GEPA handles the rest — selecting candidates, reflecting on failures, proposing improvements, and tracking the optimization trajectory, finally returning the optimized parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b5553",
   "metadata": {},
   "source": [
    "### The Fitness Function: Your Optimization Signal\n",
    "\n",
    "The fitness function is where you define *what* you're optimizing for. It takes a candidate and a batch of data instances, returning scores and diagnostic information:\n",
    "\n",
    "```python\n",
    "def fitness_fn(candidate: dict[str, str], example: DataInst) -> tuple[float, Any, dict]:\n",
    "    # Run your system with the candidate parameters\n",
    "    output = run_my_system(candidate, example)\n",
    "    \n",
    "    # Compute a score (higher is better)\n",
    "    score = compute_score(output, example)\n",
    "    \n",
    "    # Collect diagnostic info for LLM reflection\n",
    "    side_info = {\n",
    "        \"input\": example[\"input\"],\n",
    "        \"output\": output,\n",
    "        \"expected\": example[\"expected\"],\n",
    "        \"error_analysis\": analyze_errors(output, instance)\n",
    "    }\n",
    "    \n",
    "    return score, output, side_info\n",
    "```\n",
    "\n",
    "The magic happens in `side_info` — this is GEPA's secret weapon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf16cd",
   "metadata": {},
   "source": [
    "### The Power of Side Information\n",
    "\n",
    "The `side_info` dictionary is where GEPA shines. Unlike traditional optimization that only sees a scalar score, GEPA's LLM-based reflection can understand *why* a candidate performed poorly:\n",
    "\n",
    "- **Error messages**: Compiler errors, runtime exceptions, validation failures\n",
    "- **Execution traces**: What the candidate actually did vs. what was expected\n",
    "- **Partial results**: Which sub-tasks succeeded, which failed\n",
    "- **Domain-specific feedback**: Any signal that helps explain performance\n",
    "\n",
    "The more informative your `side_info`, the better GEPA can reason about improvements. Note the keys within `side_info` are entirely flexible. You can include any contextual metadata that helps an LLM interpret performance and propose meaningful iterations.  This is what enables GEPA to optimize complex artifacts like code and agent architectures — not just tweak numbers. \n",
    "\n",
    "In this notebook, we provide minimal, actionable examples to demonstrate the flexibility of the GEPA API.\n",
    "1. Optimizing code to solve a black-box polynomial function.\n",
    "2. Optimizing an LLM prompt for a math dataset.\n",
    "3. Optimizing agent architecture for the ARC-AGI benchmark.\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f143f",
   "metadata": {},
   "source": [
    "## Example 1: Code Optimization — Evolving Optimization Algorithms\n",
    "\n",
    "Our first example demonstrates GEPA's ability to evolve code. We'll optimize Python code that minimizes blackbox functions from the [evalset benchmark](https://github.com/sigopt/evalset/tree/main) — a collection of challenging optimization test functions (Ackley, Rosenbrock, Rastrigin, etc.).\n",
    "\n",
    "**The task**: Given a blackbox function, write code that finds its minimum. The code can use any optimization library (Optuna, scipy, etc.) and must return the best `x` found.\n",
    "\n",
    "**What GEPA optimizes**: The Python code itself — its structure, algorithm choice, hyperparameters, and implementation details.\n",
    "\n",
    "![Polynomial Graph](./assets/blog/polynomial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ae873",
   "metadata": {},
   "source": [
    "### Setting up the dataset\n",
    "\n",
    "Each data instance is a blackbox optimization problem with bounds, dimension, and problem characteristics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ced13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.polynomial.evalset import problems\n",
    "\n",
    "problem = problems[\"ackley\"]\n",
    "dataset = [{\n",
    "    \"problem_description\": f\"\"\"Blackbox optimization problem.\n",
    "    Minimize a function that takes a numpy array of shape ({problem.dim},) and returns a scalar.\n",
    "    Bounds: {problem.bounds}\"\"\",\n",
    "    \"dim\": problem.dim,\n",
    "    \"bounds\": problem.bounds,\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a91948d",
   "metadata": {},
   "source": [
    "### The seed candidate\n",
    "\n",
    "We start with a trivial baseline — code that just guesses the center of the search space:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcad3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_candidate = \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def solve(dim):\n",
    "    # A trivial baseline: guess the center of the search space\n",
    "    x = [0.5] * dim\n",
    "    y = evaluator.evaluate(np.array(x))\n",
    "    print(\"y:\", y)\n",
    "    return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global x\n",
    "    x = solve(dim)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd33611",
   "metadata": {},
   "source": [
    "### The fitness function\n",
    "\n",
    "The fitness function executes the candidate code in a sandboxed environment, captures the result, and returns rich diagnostic information:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Sequence\n",
    "\n",
    "def execute_code(code: str, global_vars: dict, timeout: int = 30) -> dict:\n",
    "    \"\"\"Execute code in a sandboxed environment with timeout.\"\"\"\n",
    "    # Implementation handles: stdout/stderr capture, timeout, exception handling\n",
    "    # Returns: {\"output\": str, \"logs\": str, \"results\": dict, \"error\": str}\n",
    "    ...\n",
    "\n",
    "def fitness_fn(candidate: dict[str, str], batch: Sequence[Any]) -> list[tuple[float, Any, dict]]:\n",
    "    \"\"\"Evaluate optimization code on a batch of blackbox problems.\"\"\"\n",
    "    code = candidate[\"code\"]\n",
    "    results = []\n",
    "    \n",
    "    for problem_instance in batch:\n",
    "        problem = problems[problem_instance[\"problem_name\"]]\n",
    "        \n",
    "        # Execute the candidate code with problem context\n",
    "        execution = execute_code(\n",
    "            code,\n",
    "            global_vars={\"dim\": problem.dim, \"evaluator\": evaluator},\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        # Compute score: negative function value (higher is better)\n",
    "        if \"x\" in execution[\"results\"] and execution[\"error\"] == \"\":\n",
    "            x = np.array(execution[\"results\"][\"x\"])\n",
    "            score = -problem.do_evaluate(x)  # Negate because we minimize\n",
    "        else:\n",
    "            score = -99999  # Penalize failed executions\n",
    "        \n",
    "        # Rich side_info for LLM reflection\n",
    "        side_info = {\n",
    "            \"scores\": {\"score\": score},\n",
    "            \"Input\": {\"problem_description\": problem_instance[\"problem_description\"]},\n",
    "            \"code_side_info\": {\n",
    "                \"X\": execution[\"results\"].get(\"x\", \"not found\"),\n",
    "                \"Prints\": execution[\"output\"],       # Captured stdout\n",
    "                \"Logs\": execution[\"logs\"],           # Captured stderr  \n",
    "                \"Error\": execution[\"error\"],         # Any exceptions\n",
    "                \"Num evaluation calls\": evaluator.local_evaluation_calls,\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        results.append((score, {\"code\": code, **side_info}, side_info))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218c04c",
   "metadata": {},
   "source": [
    "Notice how `side_info` captures everything the LLM needs to understand *why* the code failed or succeeded: error messages, print output, the actual result found, and evaluation budget used.\n",
    "\n",
    "### Running GEPA optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa.optimize_anything import (\n",
    "    optimize_anything,\n",
    "    GEPAConfig,\n",
    "    EngineConfig,\n",
    "    ReflectionConfig,\n",
    ")\n",
    "\n",
    "result = optimize_anything(\n",
    "    seed_candidate=seed_candidate,\n",
    "    fitness_fn=fitness_fn,\n",
    "    dataset=dataset,\n",
    "    config=GEPAConfig(\n",
    "        engine=EngineConfig(\n",
    "            max_metric_calls=1000,\n",
    "            track_best_outputs=True,\n",
    "        ),\n",
    "        reflection=ReflectionConfig(\n",
    "            reflection_lm=\"openai/gpt-4o\",  # LLM for proposing improvements\n",
    "            reflection_minibatch_size=3,     # Problems shown per reflection\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Access the optimized code\n",
    "print(result.best_candidate[\"code\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54affe23",
   "metadata": {},
   "source": [
    "GEPA evolves the code from a trivial baseline into sophisticated optimization strategies — discovering the use of libraries like Optuna, implementing proper bounds handling, and tuning algorithm hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774db146",
   "metadata": {},
   "source": [
    "## Example 2: Prompt Optimization — AIME\n",
    "\n",
    "Baseline Score: 46.67%  \n",
    "Optimized Score: 53.33%  \n",
    "Improvement: 6.66%\n",
    "\n",
    "![AIME Graph](./assets/blog/aime_optimization_progress.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ef999",
   "metadata": {},
   "source": [
    "### Setting up the dataset and the seed candidate\n",
    "\n",
    "We use a split of AIME problems for training, validation, and testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f526e920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45 training examples\n",
      "Loaded 45 validation examples\n",
      "Loaded 30 test examples\n"
     ]
    }
   ],
   "source": [
    "from examples.math.dataset import load_math_dataset\n",
    "\n",
    "trainset, valset, testset = load_math_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec163d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "\n",
    "# Define the language model\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "lm = dspy.LM(\"gpt-4.1-mini\", api_key=api_key, temperature=1.0, max_tokens=32000)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Let's optimize the prompt of the following dspy reasoning module.\n",
    "class MathSolverSignature(dspy.Signature):\n",
    "    input = dspy.InputField(desc=\"The math problem to solve.\")\n",
    "    answer = dspy.OutputField(desc=\"The final numerical answer.\")\n",
    "\n",
    "predictor = dspy.ChainOfThought(MathSolverSignature)\n",
    "\n",
    "# This is the initial prompt that we will optimize.\n",
    "SEED_PROMPT = \"\"\"Solve the math problem carefully. Break down the steps and provide the final answer as a single number.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81d7e6",
   "metadata": {},
   "source": [
    "### The fitness function\n",
    "\n",
    "The fitness function runs the predictor on each example and collects detailed feedback about the reasoning process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff6ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Sequence\n",
    "from examples.math.main import math_metric\n",
    "\n",
    "def fitness_fn(candidate: dict[str, str], batch: Sequence[Any]) -> list[tuple[float, Any, dict]]:\n",
    "    predictor.predict.signature.instructions = candidate[\"prompt\"]\n",
    "\n",
    "    evaluator = dspy.Evaluate(\n",
    "        devset=list(batch),\n",
    "        metric=math_metric,\n",
    "        num_threads=16,\n",
    "        display_progress=True,\n",
    "    )\n",
    "    eval_result = evaluator(predictor)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for example, prediction, metric_result in eval_result.results:\n",
    "        score = metric_result.score\n",
    "        feedback = metric_result.feedback\n",
    "\n",
    "        artifact = {\n",
    "            \"prompt\": candidate[\"prompt\"],\n",
    "            \"answer\": prediction.answer,\n",
    "        }\n",
    "\n",
    "        side_info = {\n",
    "            \"Input\": example.input,\n",
    "            \"Output\": prediction.answer,\n",
    "            \"Reasoning\": prediction.reasoning,\n",
    "            \"ExecutionFeedback\": feedback,\n",
    "        }\n",
    "\n",
    "        results.append((score, artifact, side_info))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73eebf",
   "metadata": {},
   "source": [
    "### Running GEPA optimization\n",
    "\n",
    "Notice how `side_info` includes the model's reasoning trace — this helps GEPA understand *how* the model approached each problem, not just whether it got the right answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14431a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa.optimize_anything import (\n",
    "    EngineConfig,\n",
    "    GEPAConfig,\n",
    "    ReflectionConfig,\n",
    "    optimize_anything,\n",
    ")\n",
    "\n",
    "gepa_config = GEPAConfig(\n",
    "    engine=EngineConfig(\n",
    "        max_metric_calls=800,\n",
    "        track_best_outputs=True,\n",
    "    ),\n",
    "    reflection=ReflectionConfig(\n",
    "        reflection_minibatch_size=3,\n",
    "        skip_perfect_score=False,\n",
    "        reflection_lm=\"openai/gpt-5\",\n",
    "    )\n",
    ")\n",
    "\n",
    "result = optimize_anything(\n",
    "    seed_candidate={\"prompt\": SEED_PROMPT},\n",
    "    fitness_fn=fitness_fn,\n",
    "    dataset=trainset,\n",
    "    valset=valset,\n",
    "    config=gepa_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352303a5",
   "metadata": {},
   "source": [
    "## Example 3: Agent Optimization — Evolving DSPy Programs for ARC-AGI\n",
    "\n",
    "Our second example pushes GEPA further: optimizing not just prompts or hyperparameters, but the *entire structure* of an AI agent. We'll evolve a DSPy program to solve ARC-AGI tasks — a challenging benchmark requiring visual reasoning and pattern recognition.\n",
    "\n",
    "**The task**: Given input-output matrix pairs as training examples, produce the correct output for test inputs.\n",
    "\n",
    "**What GEPA optimizes**: The entire DSPy program source code — signatures, modules, control flow, and prompting strategies.\n",
    "\n",
    "**Result**: GEPA improves Gemini-2.5-Pro's performance from **44% to 49.5%** by discovering an elaborate 5-step reasoning pipeline with self-refinement.\n",
    "\n",
    "![ARC AGI Graph](./assets/blog/arc_agi_optimization_progress.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58492d",
   "metadata": {},
   "source": [
    "### Setting up the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00eba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.arc_agi.data import load_arc_agi_dataset\n",
    "\n",
    "train_set, val_set, test_set = load_arc_agi_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e1e27",
   "metadata": {},
   "source": [
    "### The seed candidate\n",
    "\n",
    "We start with a simple Chain-of-Thought program — just a single DSPy module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_candidate = \"\"\"import dspy\n",
    "from typing import List\n",
    "import pydantic\n",
    "\n",
    "MATRIX = List[List[int]]\n",
    "\n",
    "class TrainingExample(pydantic.BaseModel):\n",
    "    input: MATRIX\n",
    "    output: MATRIX\n",
    "\n",
    "class SolveTaskSignature(dspy.Signature):\n",
    "    training_examples: List[TrainingExample] = dspy.InputField(description=\"Input and output examples demonstrating the task to be performed.\")\n",
    "    test_inputs: List[MATRIX] = dspy.InputField(description=\"Input matrices to be solved following the task described in the training examples.\")\n",
    "    test_outputs: List[MATRIX] = dspy.OutputField(description=\"Output matrices corresponding to the test inputs.\")\n",
    "\n",
    "program = dspy.ChainOfThought(SolveTaskSignature)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193aa56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "\n",
    "from gepa.adapters.dspy_full_program_adapter.full_program_adapter import DspyAdapter\n",
    "from examples.arc_agi.main import metric_fn\n",
    "\n",
    "# Create LMs\n",
    "task_lm = dspy.LM(\n",
    "    model=\"openai/gpt-5\",\n",
    "    temperature=1.0,\n",
    "    max_tokens=32000,\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Create adapter\n",
    "adapter = DspyAdapter(\n",
    "    task_lm=task_lm,\n",
    "    metric_fn=metric_fn,\n",
    "    num_threads=64,\n",
    "    reflection_lm=\"openai/gpt-5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e441e",
   "metadata": {},
   "source": [
    "### The fitness function\n",
    "\n",
    "The fitness function compiles and runs the DSPy program, comparing outputs against ground truth. Crucially, it provides detailed feedback about *what went wrong*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001eb5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_error_results(program, batch, error_msg):\n",
    "    \"\"\"Create error results for all examples in batch.\"\"\"\n",
    "    results = []\n",
    "    for example in batch:\n",
    "        log = {\"error\": error_msg, \"program\": program}\n",
    "        side_info = {\n",
    "            \"input\": example,\n",
    "            \"error\": error_msg,\n",
    "        }\n",
    "        results.append((0.0, log, side_info))\n",
    "    return results\n",
    "\n",
    "def format_results(program, trajectories):\n",
    "    results = [] \n",
    "    for traj in trajectories:\n",
    "        \"\"\"Create a single result item from trajectory.\"\"\"\n",
    "        metric_result = traj.get(\"score\")\n",
    "        score = metric_result.get(\"score\")\n",
    "        feedback = metric_result.get(\"feedback\")\n",
    "        prediction = traj.get(\"prediction\")\n",
    "        model_answer = prediction.get(\"test_outputs\")\n",
    "\n",
    "        log = {\n",
    "            \"program\": program,\n",
    "            \"model_answer\": model_answer,\n",
    "            \"score\": score,\n",
    "        }\n",
    "\n",
    "        side_info = {\n",
    "            \"input\": traj.get(\"example\"),\n",
    "            \"reasoning\": prediction.get(\"reasoning\"),\n",
    "            \"feedback\": feedback,\n",
    "            \"output\": model_answer,\n",
    "        }\n",
    "\n",
    "        results.append((score, log, side_info))\n",
    "\n",
    "    return results\n",
    "\n",
    "def fitness_fn(candidate, batch):\n",
    "    \"\"\"Evaluate candidate program on batch and return results.\"\"\"\n",
    "    program = candidate[\"program\"]\n",
    "\n",
    "    try:\n",
    "        eval_batch = adapter.evaluate(batch, candidate, capture_traces=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating candidate: {e}\")\n",
    "        return format_error_results(program, batch, str(e))\n",
    "\n",
    "    # Program error\n",
    "    if not isinstance(eval_batch.trajectories, list):\n",
    "        error_msg = f\"All examples failed. Program error: {str(eval_batch.trajectories)}\"\n",
    "        return format_error_results(program, batch, error_msg)\n",
    "\n",
    "    # Process evaluations with no errors\n",
    "    return format_results(program, eval_batch.trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec5232",
   "metadata": {},
   "source": [
    "### Running GEPA optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa.optimize_anything import (\n",
    "    EngineConfig,\n",
    "    GEPAConfig,\n",
    "    ReflectionConfig,\n",
    "    optimize_anything,\n",
    ")\n",
    "\n",
    "gepa_config = GEPAConfig(\n",
    "    engine=EngineConfig(\n",
    "        max_metric_calls=100,\n",
    "        track_best_outputs=True,\n",
    "    ),\n",
    "    reflection=ReflectionConfig(\n",
    "        reflection_minibatch_size=3,\n",
    "        skip_perfect_score=False,\n",
    "        reflection_lm=\"openai/gpt-5\",\n",
    "    )\n",
    ")\n",
    "\n",
    "result = optimize_anything(\n",
    "    seed_candidate={\"program\": seed_candidate},\n",
    "    fitness_fn=fitness_fn,\n",
    "    dataset=train_set,\n",
    "    valset=val_set,\n",
    "    config=gepa_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31781e",
   "metadata": {},
   "source": [
    "### What GEPA discovered\n",
    "\n",
    "After optimization, GEPA evolved the simple ChainOfThought into an elaborate 5-step pipeline:\n",
    "\n",
    "1. **Hypothesize Rule**: Ask LLM to deduce a natural language transformation rule from training examples\n",
    "2. **Generate Code**: Ask LLM to implement the rule as a Python function\n",
    "3. **Validate on Training**: Run the code on all training examples, collecting feedback on failures\n",
    "4. **Refine if Needed**: If validation fails, ask LLM to fix the code using gathered feedback\n",
    "5. **Execute on Test**: Run the refined code on test inputs\n",
    "\n",
    "Remarkably, **GEPA discovered reflective self-refinement** — having the LLM check and fix its own code before producing final outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de947640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the evolved program\n",
    "print(result.best_candidate[\"program\"][:2000])  # First 2000 chars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a86150",
   "metadata": {},
   "source": [
    "## Circle Packing\n",
    "\n",
    "![Circle Packing 21](./assets/blog/circle_packing/circle_packing_21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e5952",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24519ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be7b262",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8244f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_candidate = \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def pack_circles(n_circles: int) -> list[tuple[float, float]]:\n",
    "    '''Pack n unit circles, returning their center positions.'''\n",
    "    positions = []\n",
    "    for i in range(n_circles):\n",
    "        # Place circles in a simple grid pattern\n",
    "        row = i // int(np.sqrt(n_circles) + 1)\n",
    "        col = i % int(np.sqrt(n_circles) + 1)\n",
    "        x = col * 2.1  # Slight spacing to avoid overlap\n",
    "        y = row * 2.1\n",
    "        positions.append((x, y))\n",
    "    return positions\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d63c95",
   "metadata": {},
   "source": [
    "![Circle Packing 26](./assets/blog/circle_packing/circle_packing_26.png)\n",
    "\n",
    "![Circle Packing 32](./assets/blog/circle_packing/circle_packing_32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238b846",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "The `optimize_anything` API demonstrates GEPA's power as a general-purpose text evolution engine:\n",
    "\n",
    "1. **Unified interface**: Whether you're optimizing prompts, code, or agent architectures, the API is the same — just define your fitness function with rich `side_info`.\n",
    "\n",
    "2. **Side information is key**: The more diagnostic information you provide, the better GEPA's LLM-based reflection can understand failures and propose targeted improvements.\n",
    "\n",
    "3. **Beyond scalar optimization**: Traditional optimizers only see scores. GEPA sees error messages, execution traces, and domain-specific feedback — enabling it to optimize complex artifacts that would be impossible to search blindly.\n",
    "\n",
    "4. **Emergent capabilities**: GEPA can discover sophisticated strategies (like self-refinement in the ARC-AGI example) that weren't explicitly programmed — they emerge from the optimization process itself.\n",
    "\n",
    "Try `optimize_anything` on your own optimization problems. If you can express your system's parameters as text and compute a score with diagnostic feedback, GEPA can optimize it.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
