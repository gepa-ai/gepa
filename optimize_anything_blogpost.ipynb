{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "outline",
      "metadata": {},
      "source": [
        "# `optimize_anything`: A Universal API for Text-Based Optimization\n",
        "\n",
        "**TL;DR**: We introduce `optimize_anything`, a single, declarative API that uses LLMs as intelligent proposers to optimize *anything* representable as text—code, prompts, configurations, agent architectures. The key insight: if it can be serialized to text, an LLM can reason about it and propose improvements. The secret sauce? **A**uxiliary **S**ide **I**nformation (ASI).\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Unified Interface**: Whether you're optimizing prompts, code, hyperparameters, or agent architectures, the API is the same—just provide a `seed_candidate` (starting point) and a `fitness_fn` (how good are we doing?).\n",
        "\n",
        "2. **The Convex Hull of Optimization**: `optimize_anything` is designed to be the \"convex hull\" of all text-based optimization problems. Different libraries optimize different things (Optuna for hyperparameters, evolutionary strategies for algorithms, gradient descent for neural networks). We unify them under one abstraction.\n",
        "\n",
        "3. **Side Information is Key**: Unlike traditional optimizers that only see scalar scores, GEPA's LLM-based reflection can understand *why* a candidate performed poorly through rich diagnostic information—error messages, execution traces, partial results.\n",
        "\n",
        "4. **Emergent Capabilities**: GEPA can discover sophisticated strategies (like self-refinement) that weren't explicitly programmed—they emerge from the optimization process itself.\n",
        "\n",
        "---\n",
        "\n",
        "## Results Summary\n",
        "\n",
        "| Domain | Task | Baseline | Optimized | Improvement |\n",
        "|--------|------|----------|-----------|-------------|\n",
        "| **Mathematical Optimization** | EvalSet Benchmark | Optuna TPE | GEPA | Outperforms Optuna |\n",
        "| **Prompt Engineering** | AIME 2025 (GPT-4.1 Mini) | 46.67% | 60.00% | +13.33% absolute |\n",
        "| **Agent Evolution** | ARC-AGI (GPT-5) | 56.5% | 68.0% | +11.5% absolute |\n",
        "| **Algorithmic Discovery** | Circle Packing (N=26) | 0.9798 | 2.6359 | Exceeds AlphaEvolve |\n",
        "| **Systems: Scheduling** | Can't Be Late | Cost 96.48 | Cost 89.86 | **6.9% savings** |\n",
        "| **Systems: Networking** | Cloudcast | $191 | $120 | **37.3% savings** |\n",
        "\n",
        "---\n",
        "\n",
        "## Outline\n",
        "\n",
        "1. **[The Landscape of Optimization (The \"Old\" Way)](#section-1)** — The fragmented world of optimization libraries\n",
        "2. **[The Unifying Abstraction: `optimize_anything`](#section-2)** — One API to rule them all\n",
        "3. **[The Secret Weapon: Auxiliary Side Information (ASI)](#section-3)** — Why GEPA outperforms traditional optimizers\n",
        "4. **[Example 1: Mathematical Optimization](#section-4)** — Evolving code to beat Optuna on EvalSet\n",
        "5. **[Example 2: Prompt Engineering](#section-5)** — Optimizing prompts for AIME 2025\n",
        "6. **[Example 3: Agent Program Evolution](#section-6)** — Evolving DSPy programs for ARC-AGI\n",
        "7. **[Example 4: Algorithmic Discovery](#section-7)** — Circle packing that matches AlphaEvolve\n",
        "8. **[Example 5: Systems Optimization](#section-adrs)** — Cloud infrastructure cost reduction\n",
        "9. **[How It Works Under the Hood](#section-8)** — The GEPA engine\n",
        "10. **[Conclusion](#section-9)** — From imperative to declarative optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-1",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-1\"></a>\n",
        "# 1. The Landscape of Optimization (The \"Old\" Way)\n",
        "\n",
        "The world of optimization is **fragmented**. Each problem domain has its own specialized library with its own API, paradigm, and learning curve. Let's look at the major categories:\n",
        "\n",
        "### Hyperparameter/Black-Box Optimization (Optuna)\n",
        "\n",
        "For hyperparameter tuning, you use Bayesian optimization or Tree-structured Parzen Estimators (TPE):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "optuna-example",
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
        "    \n",
        "    # Train model and return validation score\n",
        "    model = build_model(lr, n_layers)\n",
        "    return train_and_evaluate(model)\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scipy-intro",
      "metadata": {},
      "source": [
        "### Mathematical Optimization (SciPy)\n",
        "\n",
        "For continuous optimization of mathematical functions, you use classical algorithms like L-BFGS-B or SLSQP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ab70cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "def rosenbrock(x):\n",
        "    return sum(100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
        "\n",
        "result = minimize(\n",
        "    rosenbrock, \n",
        "    x0=[0, 0, 0, 0],\n",
        "    method='L-BFGS-B',\n",
        "    bounds=[(-5, 5)] * 4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1dcea18",
      "metadata": {},
      "source": [
        "### Evolutionary Algorithms (DEAP)\n",
        "\n",
        "For evolving programs or complex structures, you use genetic algorithms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scipy-example",
      "metadata": {},
      "outputs": [],
      "source": [
        "from deap import base, creator, tools, algorithms\n",
        "\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, 100)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"evaluate\", eval_func)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "population = toolbox.population(n=300)\n",
        "algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deap-intro",
      "metadata": {},
      "source": [
        "### The Problem: Fragmentation\n",
        "\n",
        "**A user needs to learn 3 different paradigms to solve 3 different optimization problems.**\n",
        "\n",
        "| Library | Domain | Paradigm | What You Must Know |\n",
        "|---------|--------|----------|-------------------|\n",
        "| Optuna | Hyperparameters | Bayesian/TPE | Samplers, pruners, search space definition |\n",
        "| SciPy | Mathematical Functions | Classical Methods | Algorithm selection (L-BFGS, SLSQP, etc.) |\n",
        "| DEAP | Evolutionary | Genetic Algorithms | Crossover, mutation, selection operators |\n",
        "\n",
        "Each library has:\n",
        "- **Different APIs and abstractions** — you can't just swap one for another\n",
        "- **Different optimization strategies** hard-coded into the implementation\n",
        "- **Different assumptions** about what can be optimized (differentiable? discrete? continuous?)\n",
        "\n",
        "### The Insight: Text is the Universal Representation\n",
        "\n",
        "Here's the key insight: **if something can be represented as text, an LLM can reason about it and propose improvements**.\n",
        "\n",
        "- **Code** is text → LLMs can write and improve code\n",
        "- **Prompts** are text → LLMs can refine instructions\n",
        "- **Configurations** are text → LLMs can tune JSON/YAML\n",
        "- **Agent architectures** are text → LLMs can evolve program structure\n",
        "\n",
        "What if we had **one API** that could optimize all of them—by leveraging the LLM's ability to understand and generate text?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fragmentation-problem",
      "metadata": {},
      "source": [
        "<!-- This cell intentionally left empty - placeholder for removal -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-2",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-2\"></a>\n",
        "# 2. The Unifying Abstraction: `optimize_anything`\n",
        "\n",
        "We introduce `optimize_anything`—a single entry point for optimizing any text-representable artifact. It's designed to be the **\"Convex Hull\"** of all optimization problems: every point in the space of text-based optimization can be reached through this API.\n",
        "\n",
        "## The API Signature\n",
        "\n",
        "The API is intentionally minimal. You need only two things:\n",
        "1. **A seed candidate** — your starting point\n",
        "2. **A fitness function** — how to measure success"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "api-signature",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gepa.optimize_anything import optimize_anything, GEPAConfig\n",
        "\n",
        "def optimize_anything(\n",
        "    # === REQUIRED ===\n",
        "    seed_candidate: dict[str, str],           # Your starting point (text parameters to optimize)\n",
        "    fitness_fn: FitnessFn,                    # How to measure success\n",
        "    \n",
        "    # === OPTIONAL: Data ===\n",
        "    dataset: list[DataInst] | None = None,   # Examples to optimize on (for example, multiple related tasks)\n",
        "    valset: list[DataInst] | None = None,    # Held-out set for ensuring generalization if required\n",
        "    \n",
        "    # === OPTIONAL: Natural Language Guidance ===\n",
        "    objective: str | None = None,            # What you're trying to achieve (e.g. \"Find a prompt that maximizes accuracy\")\n",
        "    background: str | None = None,           # Domain knowledge, constraints, strategies (e.g. Domain knowledge about the framework which the candidate is written in)\n",
        "    \n",
        "    # === OPTIONAL: Fine-Grained Control ===\n",
        "    config: GEPAConfig | None = None,        # Engine, reflection, tracking settings\n",
        ") -> GEPAResult:\n",
        "    \"\"\"\n",
        "    Optimize any parameterized system using evolutionary algorithms with LLM-based reflection.\n",
        "    \n",
        "    Returns:\n",
        "        GEPAResult containing best_candidate, optimization history, and metrics.\n",
        "    \"\"\"\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "api-philosophy",
      "metadata": {},
      "source": [
        "## The Philosophy: Declare, Don't Implement\n",
        "\n",
        "With `optimize_anything`, the user **declares** the optimization problem:\n",
        "\n",
        "| You Provide | Example | Purpose |\n",
        "|-------------|---------|---------|\n",
        "| `seed_candidate` | `{\"prompt\": \"Solve this math problem:\"}` | Your starting point |\n",
        "| `fitness_fn` | Returns (score, output, side_info) | How to measure success |\n",
        "| `dataset` (optional) | List of test cases | Multi-instance generalization |\n",
        "| `objective` (optional) | \"Find a prompt that maximizes accuracy\" | Natural language guidance |\n",
        "| `background` (optional) | \"Solutions must handle edge cases\" | Domain knowledge |\n",
        "\n",
        "GEPA handles the **how**: proposing mutations, reflecting on failures, selecting candidates, and tracking the optimization trajectory.\n",
        "\n",
        "## Two Modes of Operation\n",
        "\n",
        "### Per-Instance Mode (with `dataset`)\n",
        "\n",
        "For problems where you want parameters that **generalize** across examples:\n",
        "- **Prompt optimization**: The same prompt should work on many math problems\n",
        "- **Agent architecture search**: The same agent should solve many tasks\n",
        "\n",
        "```python\n",
        "# dataset is a list of examples\n",
        "result = optimize_anything(\n",
        "    seed_candidate={\"prompt\": \"Solve:\"},\n",
        "    fitness_fn=evaluate_prompt,\n",
        "    dataset=math_problems,  # ← Optimize across these\n",
        "    valset=held_out_problems,  # ← Test generalization\n",
        ")\n",
        "```\n",
        "\n",
        "### Single-Instance Mode (without `dataset`)\n",
        "\n",
        "For problems defined by a **single optimization target**:\n",
        "- **Circle packing**: Maximize sum of radii for N circles\n",
        "- **Code evolution**: Minimize a mathematical function\n",
        "\n",
        "```python\n",
        "# dataset=None triggers single-instance mode\n",
        "result = optimize_anything(\n",
        "    seed_candidate={\"code\": \"def solve(): ...\"},\n",
        "    fitness_fn=evaluate_code,\n",
        "    dataset=None,  # ← Single optimization target\n",
        ")\n",
        "```\n",
        "\n",
        "## The Fitness Function: Your Optimization Signal\n",
        "\n",
        "The fitness function is where you define *what* you're optimizing for:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fitness-fn-example",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "def fitness_fn(\n",
        "    candidate: dict[str, str],  # The parameters being optimized\n",
        "    example: Any | None = None  # A single data instance (None for single-instance mode)\n",
        ") -> tuple[float, Any, dict]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        score: Higher is better\n",
        "        output: The actual output produced (for tracking)\n",
        "        side_info: Diagnostic information for LLM reflection\n",
        "    \"\"\"\n",
        "    # Run your system with the candidate parameters\n",
        "    output = run_my_system(candidate, example)\n",
        "    \n",
        "    # Compute a score (higher is better)\n",
        "    score = compute_score(output, example)\n",
        "    \n",
        "    # Collect diagnostic info for LLM reflection\n",
        "    side_info = {\n",
        "        \"Input\": example[\"input\"],\n",
        "        \"Output\": output,\n",
        "        \"Expected\": example[\"expected\"],\n",
        "        \"Error\": get_error_message(output),\n",
        "    }\n",
        "    \n",
        "    return score, output, side_info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "side-info-power",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-3\"></a>\n",
        "# 3. The Secret Weapon: Auxiliary Side Information (ASI)\n",
        "\n",
        "The `side_info` dictionary is GEPA's secret weapon—we call it **ASI** (**A**uxiliary **S**ide **I**nformation). \n",
        "\n",
        "> *While the AI community debates when we'll achieve ASI (Artificial Superintelligence), you can achieve **your** ASI today—just return rich diagnostic information from your fitness function.*\n",
        "\n",
        "## Why ASI Matters\n",
        "\n",
        "Traditional optimizers only see a **scalar score**:\n",
        "\n",
        "```\n",
        "Candidate A → Score: 0.73  (Why did it fail? No idea.)\n",
        "Candidate B → Score: 0.85  (What made it better? Unknown.)\n",
        "```\n",
        "\n",
        "GEPA's LLM-based reflection can understand **why** a candidate performed the way it did:\n",
        "\n",
        "```\n",
        "Candidate A → Score: 0.73\n",
        "  side_info: {\n",
        "    \"Error\": \"Circle 3 and Circle 7 overlap by 0.02 units\",\n",
        "    \"Boundary violations\": [\"Circle 12 extends past x=1.0\"],\n",
        "    \"Best score achieved\": 2.847\n",
        "  }\n",
        "```\n",
        "\n",
        "Now the LLM knows *exactly* what to fix.\n",
        "\n",
        "## What to Include in ASI\n",
        "\n",
        "| Information Type | Example | Purpose |\n",
        "|-----------------|---------|----------|\n",
        "| **Error messages** | `\"SyntaxError: invalid syntax on line 42\"` | Helps LLM fix code bugs |\n",
        "| **Execution traces** | `\"Called API 3x, timeout on 3rd call\"` | Helps LLM understand behavior |\n",
        "| **Partial results** | `\"3/5 test cases passed\"` | Helps LLM identify failure patterns |\n",
        "| **Expected vs Actual** | `\"Expected: [1,2,3], Got: [1,2,4]\"` | Helps LLM understand what went wrong |\n",
        "| **Domain feedback** | `\"Circles overlap at positions (0.5, 0.3)\"` | Helps LLM make domain-aware improvements |\n",
        "| **Reasoning traces** | `\"Model's chain-of-thought: ...\"` | Helps LLM understand failure modes |\n",
        "\n",
        "## The ASI Design Principle\n",
        "\n",
        "**Be generous with information.** Include anything that would help a human expert understand why the candidate succeeded or failed. The LLM will use this to make targeted, intelligent improvements rather than random mutations.\n",
        "\n",
        "```python\n",
        "# Good ASI\n",
        "side_info = {\n",
        "    \"Input\": problem_description,\n",
        "    \"Output\": model_output,\n",
        "    \"Expected\": correct_answer,\n",
        "    \"Reasoning\": model_reasoning_trace,\n",
        "    \"Error\": \"Division by zero on line 15\",\n",
        "    \"Partial scores\": {\"accuracy\": 0.8, \"efficiency\": 0.3},\n",
        "}\n",
        "\n",
        "# Bad ASI (not enough information)\n",
        "side_info = {\"score\": 0.73}  # LLM can't help with just this!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-3",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-4\"></a>\n",
        "# 4. Example 1: Mathematical Optimization — Beating Optuna\n",
        "\n",
        "**Result: GEPA outperforms Optuna on the EvalSet benchmark.**\n",
        "\n",
        "This example demonstrates how `optimize_anything` can evolve **code** that implements optimization algorithms—essentially using LLMs to discover optimization strategies automatically.\n",
        "\n",
        "## The Challenge\n",
        "\n",
        "Optuna is the industry standard for black-box optimization. But using Optuna effectively requires:\n",
        "- Choosing sampling algorithms (TPE, CMA-ES, Random, etc.)\n",
        "- Defining search spaces manually\n",
        "- Tuning algorithm-specific hyperparameters\n",
        "- Deep knowledge of optimization theory\n",
        "\n",
        "(Luke: the claims above are too strong? Optuna is actually very simplistic)\n",
        "\n",
        "**What if we could just write code that finds minima, and let GEPA evolve the strategy?**\n",
        "\n",
        "## The Task\n",
        "\n",
        "**Given**: A black-box function `objective_function(x) → float` with bounds\n",
        "**Find**: Python code that discovers the minimum\n",
        "\n",
        "**What GEPA optimizes**: The Python code itself—algorithm choice, implementation, hyperparameters, heuristics.\n",
        "\n",
        "<img src=\"./assets/blog/mathematical_optimization.png\" width=\"60%\">\n",
        "\n",
        "*GEPA starts below Optuna but progressively discovers better strategies, eventually surpassing it.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poly-setup",
      "metadata": {},
      "source": [
        "## Setting Up the Problem\n",
        "\n",
        "We use the [EvalSet benchmark](https://github.com/sigopt/evalset)—a collection of challenging optimization test functions (Ackley, Rosenbrock, Rastrigin, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-dataset",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Ackley(11), {'name': 'Ackley', 'dim': 11, 'int': None, 'res': None})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from examples.polynomial.evalset.problems import problems, problem_configs\n",
        "\n",
        "problem_index = 0\n",
        "problems[problem_index], problem_configs[problem_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poly-seed",
      "metadata": {},
      "source": [
        "## The Seed Candidate\n",
        "\n",
        "We start with a trivial baseline—random sampling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "poly-seed-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_candidate = {\n",
        "    \"code\": \"\"\"import numpy as np\n",
        "\n",
        "def solve(objective_function, config, prev_best_x=None):\n",
        "    bounds = np.array(config['bounds'])\n",
        "    x = np.random.uniform(bounds[:, 0], bounds[:, 1])\n",
        "    y = objective_function(x)\n",
        "    return x\n",
        "\"\"\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poly-fitness",
      "metadata": {},
      "source": [
        "## The Fitness Function\n",
        "\n",
        "The fitness function executes the code in a sandbox and captures rich diagnostic information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-fitness-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Luke: need to either remove the whole custom objective tracking feature or abstract it away into GEPA\n",
        "\n",
        "from typing import Any\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from gepa.optimize_anything import SideInfo\n",
        "from gepa.utils.code_execution import execute_code as _execute_code, ExecutionMode\n",
        "\n",
        "\n",
        "class FitnessEvaluator:\n",
        "    \"\"\"Fitness evaluator for GEPA blackbox optimization.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        problem_index: int,\n",
        "        timeout: int = 300,\n",
        "        evaluation_budget: int = 100,\n",
        "        log_dir: str = None,\n",
        "        seed: int = 0,\n",
        "    ):\n",
        "        self.problem_index = problem_index\n",
        "        self.timeout = timeout\n",
        "        self.evaluation_budget = evaluation_budget\n",
        "        self.log_dir = Path(log_dir) if log_dir else None\n",
        "        self.seed = seed\n",
        "\n",
        "        # State tracking for warm-start (minimization: lower is better)\n",
        "        self.evaluation_history = []\n",
        "        self.best_score = float(\"inf\")\n",
        "        self.best_x = None\n",
        "\n",
        "    def evaluate(self, candidate: dict[str, str], **kwargs) -> tuple[float, Any, SideInfo]:\n",
        "        \"\"\"Evaluate code candidate on a single problem.\"\"\"\n",
        "        code = candidate[\"code\"]\n",
        "        function = problems[self.problem_index]\n",
        "        problem_config = problem_configs[self.problem_index]\n",
        "\n",
        "        # Track state for this candidate\n",
        "        eval_count = 0\n",
        "        best_candidate_score = float(\"inf\")\n",
        "        errors = []\n",
        "\n",
        "        def objective_function(x):\n",
        "            nonlocal eval_count, best_candidate_score\n",
        "            if eval_count >= self.evaluation_budget:\n",
        "                raise ValueError(f\"Evaluation budget exceeded: {eval_count} >= {self.evaluation_budget}\")\n",
        "            eval_count += 1\n",
        "\n",
        "            score = function.do_evaluate(np.array(x))\n",
        "\n",
        "            if score < best_candidate_score:\n",
        "                best_candidate_score = score\n",
        "            if score < self.best_score:\n",
        "                self.best_score = score\n",
        "                self.best_x = np.array(x).copy()\n",
        "\n",
        "            self.evaluation_history.append({\n",
        "                \"score\": score,\n",
        "                \"best_score\": self.best_score,\n",
        "            })\n",
        "            return score\n",
        "\n",
        "        # Execute code\n",
        "        result = _execute_code(\n",
        "            code=code,\n",
        "            timeout=self.timeout,\n",
        "            mode=ExecutionMode.IN_PROCESS,\n",
        "            entry_point=\"solve\",\n",
        "            entry_point_kwargs={\n",
        "                \"objective_function\": objective_function,\n",
        "                \"config\": {\"bounds\": function.bounds, \"dim\": function.dim, \"budget\": self.evaluation_budget},\n",
        "                \"prev_best_x\": self.best_x,\n",
        "            },\n",
        "            seed=self.seed,\n",
        "        )\n",
        "\n",
        "        x = result.variables.get(\"__return__\")\n",
        "        stdout = self._truncate(result.stdout)\n",
        "        stderr = self._truncate(result.stderr)\n",
        "\n",
        "        if result.error:\n",
        "            errors.append(result.error)\n",
        "        if result.traceback and result.traceback not in (result.error or \"\"):\n",
        "            errors.append(result.traceback)\n",
        "        if x is None or not isinstance(x, np.ndarray):\n",
        "            errors.append(\"Code did not return a valid numpy array.\")\n",
        "        if eval_count == 0:\n",
        "            errors.append(\"No objective_function calls were made.\")\n",
        "\n",
        "        # Use best score found, or inf if none\n",
        "        score = best_candidate_score if best_candidate_score < float(\"inf\") else float(\"inf\")\n",
        "        print(f\"Best score from {eval_count} calls: {score}\")\n",
        "\n",
        "        side_info = {\n",
        "            \"score\": score,\n",
        "            \"Input\": problem_config[\"name\"],\n",
        "            \"Prints\": stdout,\n",
        "            \"Logs\": stderr,\n",
        "            \"Error\": \"\\n\".join(errors) if errors else \"\",\n",
        "        }\n",
        "\n",
        "        output = {\n",
        "            **side_info,\n",
        "            \"code\": code,\n",
        "            \"X\": \" \".join(map(str, x.ravel())) if x is not None else \"not found\",\n",
        "        }\n",
        "\n",
        "        self.save()\n",
        "        gepa_score = -score if score < float(\"inf\") else -1e9\n",
        "        return (gepa_score, output, side_info)\n",
        "\n",
        "    def save(self, verbose: bool = False):\n",
        "        \"\"\"Save evaluation history to JSON.\"\"\"\n",
        "        if not self.log_dir:\n",
        "            return\n",
        "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
        "        filename = self.log_dir / f\"evaluation_history.json\"\n",
        "        try:\n",
        "            with open(filename, \"w\") as f:\n",
        "                json.dump(self.evaluation_history, f, indent=2, default=lambda o: o.tolist() if isinstance(o, np.ndarray) else o)\n",
        "            if verbose:\n",
        "                print(f\"Saved to {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to save: {e}\")\n",
        "            \n",
        "    def  _truncate(self, text: str, limit: int = 4000) -> str:\n",
        "        \"\"\"Truncate text to avoid token limits.\"\"\"\n",
        "        if len(text) <= limit:\n",
        "            return text\n",
        "        half = limit // 2\n",
        "        return text[:half] + \"\\n...[truncated]...\\n\" + text[-half:]\n",
        "\n",
        "\n",
        "total_evaluation_budgets = 8000\n",
        "num_proposals = 10\n",
        "evaluation_budget_per_proposal = total_evaluation_budgets // num_proposals\n",
        "seconds_per_trial=2\n",
        "timeout_per_candidate = evaluation_budget_per_proposal * seconds_per_trial\n",
        "\n",
        "# Create evaluator\n",
        "evaluator = FitnessEvaluator(\n",
        "    problem_index=problem_index,\n",
        "    timeout=timeout_per_candidate,\n",
        "    evaluation_budget=evaluation_budget_per_proposal,\n",
        "    seed=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poly-optimize",
      "metadata": {},
      "source": [
        "## Running GEPA Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-optimize-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best score from 1 calls: 21.109047957197003\n",
            "Iteration 0: Base program full valset score: -21.109047957197003 over 1 / 1 examples\n",
            "Iteration 1: Selected program 0 score: -21.109047957197003\n",
            "Best score from 1 calls: 21.109047957197003\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgepa\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize_anything\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     optimize_anything,\n\u001b[32m      4\u001b[39m     GEPAConfig,\n\u001b[32m      5\u001b[39m     EngineConfig,\n\u001b[32m      6\u001b[39m     ReflectionConfig,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m config = GEPAConfig(\n\u001b[32m     10\u001b[39m     engine=EngineConfig(\n\u001b[32m     11\u001b[39m         max_metric_calls=num_proposals,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     )\n\u001b[32m     19\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m result = \u001b[43moptimize_anything\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed_candidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed_candidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfitness_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvolve a code that minimizes a blackbox objective function.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBACKGROUND\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOptimized code:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.best_candidate[\u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/external/gepa-optimize-anything/src/gepa/optimize_anything.py:794\u001b[39m, in \u001b[36moptimize_anything\u001b[39m\u001b[34m(seed_candidate, fitness_fn, dataset, valset, objective, background, config)\u001b[39m\n\u001b[32m    792\u001b[39m \u001b[38;5;66;03m# --- 15. Run optimization ---\u001b[39;00m\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m experiment_tracker:\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m     state = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m GEPAResult.from_state(state, run_dir=config.engine.run_dir, seed=config.engine.seed)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/external/gepa-optimize-anything/src/gepa/core/engine.py:299\u001b[39m, in \u001b[36mGEPAEngine.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m.merge_proposer.last_iter_found_new_program = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;66;03m# 2) Reflective mutation proposer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m proposal = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreflective_proposer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpropose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m proposal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate.i\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Reflective mutation did not propose a new candidate\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/external/gepa-optimize-anything/src/gepa/proposer/reflective_mutation/reflective_mutation.py:168\u001b[39m, in \u001b[36mReflectiveMutationProposer.propose\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    167\u001b[39m     reflective_dataset = \u001b[38;5;28mself\u001b[39m.adapter.make_reflective_dataset(curr_prog, eval_curr, predictor_names_to_update)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     new_texts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpropose_new_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_prog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreflective_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor_names_to_update\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pname, text \u001b[38;5;129;01min\u001b[39;00m new_texts.items():\n\u001b[32m    171\u001b[39m         \u001b[38;5;28mself\u001b[39m.logger.log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Proposed new text for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/external/gepa-optimize-anything/src/gepa/proposer/reflective_mutation/reflective_mutation.py:107\u001b[39m, in \u001b[36mReflectiveMutationProposer.propose_new_texts\u001b[39m\u001b[34m(self, candidate, reflective_dataset, components_to_update)\u001b[39m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    104\u001b[39m         \u001b[38;5;66;03m# Use the single template for all parameters\u001b[39;00m\n\u001b[32m    105\u001b[39m         prompt_template = \u001b[38;5;28mself\u001b[39m.reflection_prompt_template\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     new_texts[name] = \u001b[43mInstructionProposalSignature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreflection_lm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcurrent_instruction_doc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_instruction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset_with_feedback\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_with_feedback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_template\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mnew_instruction\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_texts\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/external/gepa-optimize-anything/src/gepa/proposer/reflective_mutation/base.py:48\u001b[39m, in \u001b[36mSignature.run\u001b[39m\u001b[34m(cls, lm, input_dict)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mcls\u001b[39m, lm: LanguageModel, input_dict: Mapping[\u001b[38;5;28mstr\u001b[39m, Any]) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m     47\u001b[39m     full_prompt = \u001b[38;5;28mcls\u001b[39m.prompt_renderer(input_dict)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     lm_res = \u001b[43mlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# Handle both string and list of strings (common in DSPy/LiteLLM wrappers)\u001b[39;00m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lm_res, \u001b[38;5;28mlist\u001b[39m | \u001b[38;5;28mtuple\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/external/gepa-optimize-anything/src/gepa/optimize_anything.py:604\u001b[39m, in \u001b[36moptimize_anything.<locals>._reflection_lm\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m    602\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_reflection_lm\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    603\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m         completion = \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreflection_lm_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    608\u001b[39m         completion = litellm.completion(model=reflection_lm_name, messages=prompt)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/litellm/utils.py:1250\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1248\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1249\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1250\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1251\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1253\u001b[39m     kwargs=kwargs,\n\u001b[32m   1254\u001b[39m     call_type=call_type,\n\u001b[32m   1255\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/litellm/main.py:2130\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   2110\u001b[39m         response = base_llm_http_handler.completion(\n\u001b[32m   2111\u001b[39m             model=model,\n\u001b[32m   2112\u001b[39m             messages=messages,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2127\u001b[39m             provider_config=provider_config,\n\u001b[32m   2128\u001b[39m         )\n\u001b[32m   2129\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2130\u001b[39m         response = \u001b[43mopenai_chat_completions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2132\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2134\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2135\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2136\u001b[39m \u001b[43m            \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2137\u001b[39m \u001b[43m            \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2138\u001b[39m \u001b[43m            \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2139\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2140\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2141\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2142\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2143\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   2144\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2145\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[32m   2146\u001b[39m \u001b[43m            \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m=\u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2147\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2148\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshared_session\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshared_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2149\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2150\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2151\u001b[39m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n\u001b[32m   2152\u001b[39m     logging.post_call(\n\u001b[32m   2153\u001b[39m         \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m   2154\u001b[39m         api_key=api_key,\n\u001b[32m   2155\u001b[39m         original_response=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m   2156\u001b[39m         additional_args={\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: headers},\n\u001b[32m   2157\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:673\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params, shared_session)\u001b[39m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n\u001b[32m    659\u001b[39m logging_obj.pre_call(\n\u001b[32m    660\u001b[39m     \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m    661\u001b[39m     api_key=openai_client.api_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    667\u001b[39m     },\n\u001b[32m    668\u001b[39m )\n\u001b[32m    670\u001b[39m (\n\u001b[32m    671\u001b[39m     headers,\n\u001b[32m    672\u001b[39m     response,\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_sync_openai_chat_completion_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m logging_obj.model_call_details[\u001b[33m\"\u001b[39m\u001b[33mresponse_headers\u001b[39m\u001b[33m\"\u001b[39m] = headers\n\u001b[32m    681\u001b[39m stringified_response = response.model_dump()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:237\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    234\u001b[39m parent_otel_span = _get_parent_otel_span_from_logging_obj(logging_obj)\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:471\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_client, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    469\u001b[39m raw_response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     raw_response = \u001b[43mopenai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    476\u001b[39m         headers = \u001b[38;5;28mdict\u001b[39m(raw_response.headers)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/luke_optany/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.3-macos-aarch64-none/lib/python3.12/ssl.py:1233\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1230\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1231\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1232\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.3-macos-aarch64-none/lib/python3.12/ssl.py:1106\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from examples.polynomial.prompt import BACKGROUND\n",
        "from gepa.optimize_anything import (\n",
        "    optimize_anything,\n",
        "    GEPAConfig,\n",
        "    EngineConfig,\n",
        "    ReflectionConfig,\n",
        ")\n",
        "\n",
        "gepa_config = GEPAConfig(\n",
        "    engine=EngineConfig(\n",
        "        max_metric_calls=num_proposals,\n",
        "        track_best_outputs=True,\n",
        "        cache_evaluation=True,\n",
        "    ),\n",
        "    reflection=ReflectionConfig(\n",
        "        reflection_lm=\"openai/gpt-5\",\n",
        "        reflection_minibatch_size=1,\n",
        "    )\n",
        ")\n",
        "\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=evaluator.evaluate,\n",
        "    config=gepa_config,\n",
        "    objective=\"Evolve Python code that minimizes a blackbox objective function using the available evaluation budget efficiently.\",\n",
        "    background=BACKGROUND,\n",
        ")\n",
        "\n",
        "print(\"Optimized code:\")\n",
        "print(result.best_candidate[\"code\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poly-evolved",
      "metadata": {},
      "source": [
        "## What GEPA Discovered\n",
        "\n",
        "GEPA evolved the trivial random sampler into a sophisticated optimization strategy. Here's a snippet from the evolved code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-evolved-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evolved by GEPA - combines multiple strategies:\n",
        "\n",
        "evolved_code = '''\n",
        "import numpy as np                                                                                                                                                                                   \n",
        "                                                                                                                                                                                                       \n",
        "def solve(objective_function, config, prev_best_x=None):                                                                                                                                             \n",
        "    \"\"\"                                                                                                                                                                                              \n",
        "    Hybrid global-local blackbox minimization with adaptive budget allocation.                                                                                                                       \n",
        "                                                                                                                                                                                                    \n",
        "    Strategy:                                                                                                                                                                                        \n",
        "    - Warm-start from prev_best_x when available and compatible                                                                                                                                      \n",
        "    - Global search with quasi-random exploration and sampling around current best                                                                                                                   \n",
        "    - Local search via adaptive coordinate-wise pattern search with random                                                                                                                           \n",
        "    perturbations and occasional global kicks                                                                                                                                                      \n",
        "    - Budget and dimension aware: adapts exploration / exploitation split                                                                                                                            \n",
        "    and step sizes                                                                                                                                                                                 \n",
        "    \"\"\"                                                                                                                                                                                              \n",
        "                                                                                                                                                                                                    \n",
        "    bounds = np.array(config[\"bounds\"], dtype=float)                                                                                                                                                 \n",
        "    dim = int(config.get(\"dim\", len(bounds)))                                                                                                                                                        \n",
        "    budget = int(config.get(\"budget\", 1))                                                                                                                                                            \n",
        "                                                                                                                                                                                                    \n",
        "    if dim <= 0 or budget <= 0:                                                                                                                                                                      \n",
        "        mid = bounds[:, 0] + 0.5 * (bounds[:, 1] - bounds[:, 0])                                                                                                                                     \n",
        "        return np.asarray(mid, dtype=float)                                                                                                                                                          \n",
        "                                                                                                                                                                                                    \n",
        "    lower = bounds[:, 0].astype(float)                                                                                                                                                               \n",
        "    upper = bounds[:, 1].astype(float)                                                                                                                                                               \n",
        "    span = upper - lower                                                                                                                                                                             \n",
        "                                                                                                                                                                                                    \n",
        "    fixed_mask = span <= 0.0                                                                                                                                                                         \n",
        "    span_safe = np.where(fixed_mask, 1.0, span)                                                                                                                                                      \n",
        "                                                                                                                                                                                                    \n",
        "    def clamp(x):                                                                                                                                                                                    \n",
        "        return np.clip(x, lower, upper)                                                                                                                                                              \n",
        "                                                                                                                                                                                                    \n",
        "    evals_used = 0                                                                                                                                                                                   \n",
        "    best_x = None                                                                                                                                                                                    \n",
        "    best_y = None                                                                                                                                                                                    \n",
        "                                                                                                                                                                                                    \n",
        "    def eval_point(x):                                                                                                                                                                               \n",
        "        nonlocal evals_used, best_x, best_y                                                                                                                                                          \n",
        "        if evals_used >= budget:                                                                                                                                                                     \n",
        "            return best_y if best_y is not None else np.inf                                                                                                                                          \n",
        "        y = objective_function(x)                                                                                                                                                                    \n",
        "        evals_used += 1                                                                                                                                                                              \n",
        "        if best_x is None or y < best_y:                                                                                                                                                             \n",
        "            best_x = np.array(x, copy=True)                                                                                                                                                          \n",
        "            best_y = float(y)                                                                                                                                                                        \n",
        "        return y                                                                                                                                                                                     \n",
        "                                                                                                                                                                                                    \n",
        "    # ---- Initialization: warm start + LHS-like starts ----                                                                                                                                         \n",
        "    if prev_best_x is not None:                                                                                                                                                                      \n",
        "        x0 = np.asarray(prev_best_x, dtype=float)                                                                                                                                                    \n",
        "        if x0.shape[0] == dim:                                                                                                                                                                       \n",
        "            x0 = clamp(x0)                                                                                                                                                                           \n",
        "            eval_point(x0)                                                                                                                                                                           \n",
        "                                                                                                                                                                                                    \n",
        "    remaining = budget - evals_used                                                                                                                                                                  \n",
        "    if remaining > 0:                                                                                                                                                                                \n",
        "        if budget < 40:                                                                                                                                                                              \n",
        "            init_trials = min(remaining, 6)                                                                                                                                                          \n",
        "        else:                                                                                                                                                                                        \n",
        "            init_trials = min(max(10, budget // 20), 25)                                                                                                                                             \n",
        "                                                                                                                                                                                                    \n",
        "        n_init = init_trials                                                                                                                                                                         \n",
        "        if n_init > 0:                                                                                                                                                                               \n",
        "            cut = np.linspace(0.0, 1.0, n_init + 1)                                                                                                                                                  \n",
        "            u = np.random.rand(n_init, dim)                                                                                                                                                          \n",
        "            a = cut[:n_init]                                                                                                                                                                         \n",
        "            b = cut[1:n_init + 1]                                                                                                                                                                    \n",
        "            u = a[:, None] + (b - a)[:, None] * u                                                                                                                                                    \n",
        "            for d in range(dim):                                                                                                                                                                     \n",
        "                np.random.shuffle(u[:, d])                                                                                                                                                           \n",
        "            xs = lower + u * span_safe                                                                                                                                                               \n",
        "            for k in range(n_init):                                                                                                                                                                  \n",
        "                if evals_used >= budget:                                                                                                                                                             \n",
        "                    break                                                                                                                                                                            \n",
        "                eval_point(xs[k])                                                                                                                                                                    \n",
        "                                                                                                                                                                                                    \n",
        "    if best_x is None:                                                                                                                                                                               \n",
        "        x_mid = clamp(lower + 0.5 * span)                                                                                                                                                            \n",
        "        eval_point(x_mid)                                                                                                                                                                            \n",
        "                                                                                                                                                                                                    \n",
        "    if evals_used >= budget:                                                                                                                                                                         \n",
        "        return best_x                                                                                                                                                                                \n",
        "                                                                                                                                                                                                    \n",
        "    remaining = budget - evals_used                                                                                                                                                                  \n",
        "                                                                                                                                                                                                    \n",
        "    # ---- Budget split: global vs local ----                                                                                                                                                        \n",
        "    if remaining < 40 or dim > 20:                                                                                                                                                                   \n",
        "        global_frac = 0.7                                                                                                                                                                            \n",
        "    else:                                                                                                                                                                                            \n",
        "        global_frac = 0.55                                                                                                                                                                           \n",
        "                                                                                                                                                                                                    \n",
        "    global_budget = max(1, int(global_frac * remaining))                                                                                                                                             \n",
        "    global_budget = min(global_budget, remaining)                                                                                                                                                    \n",
        "    local_budget = remaining - global_budget                                                                                                                                                         \n",
        "                                                                                                                                                                                                    \n",
        "    # ---- Quasi-random utilities ----                                                                                                                                                               \n",
        "    def van_der_corput(n, base=2):                                                                                                                                                                   \n",
        "        if n <= 0:                                                                                                                                                                                   \n",
        "            return np.empty(0, dtype=float)                                                                                                                                                          \n",
        "        seq = np.empty(n, dtype=float)                                                                                                                                                               \n",
        "        for i in range(n):                                                                                                                                                                           \n",
        "            v = i                                                                                                                                                                                    \n",
        "            denom = 1.0                                                                                                                                                                              \n",
        "            x = 0.0                                                                                                                                                                                  \n",
        "            while v:                                                                                                                                                                                 \n",
        "                v, r = divmod(v, base)                                                                                                                                                               \n",
        "                denom *= base                                                                                                                                                                        \n",
        "                x += r / denom                                                                                                                                                                       \n",
        "            seq[i] = x                                                                                                                                                                               \n",
        "        return seq                                                                                                                                                                                   \n",
        "                                                                                                                                                                                                    \n",
        "    def quasi_random_points(n_points, dim_):                                                                                                                                                         \n",
        "        if n_points <= 0:                                                                                                                                                                            \n",
        "            return np.empty((0, dim_), dtype=float)                                                                                                                                                  \n",
        "        base_sequence = van_der_corput(n_points + 16, 2)[8:8 + n_points]                                                                                                                             \n",
        "        pts = np.empty((n_points, dim_), dtype=float)                                                                                                                                                \n",
        "        for d in range(dim_):                                                                                                                                                                        \n",
        "            perm = np.random.permutation(n_points)                                                                                                                                                   \n",
        "            pts[:, d] = base_sequence[perm]                                                                                                                                                          \n",
        "        jitter = (np.random.rand(n_points, dim_) - 0.5) / (4.0 * max(n_points, 1))                                                                                                                   \n",
        "        u = np.clip(pts + jitter, 0.0, 1.0)                                                                                                                                                          \n",
        "        return lower + u * span_safe                                                                                                                                                                 \n",
        "                                                                                                                                                                                                    \n",
        "    # ---- Global search ----                                                                                                                                                                        \n",
        "    n_global = global_budget                                                                                                                                                                         \n",
        "    refine_budget = max(0, int(0.3 * n_global))                                                                                                                                                      \n",
        "    pure_explore_budget = max(0, n_global - refine_budget)                                                                                                                                           \n",
        "                                                                                                                                                                                                    \n",
        "    if pure_explore_budget > 0 and evals_used < budget:                                                                                                                                              \n",
        "        xs = quasi_random_points(pure_explore_budget, dim)                                                                                                                                           \n",
        "        for i in range(pure_explore_budget):                                                                                                                                                         \n",
        "            if evals_used >= budget:                                                                                                                                                                 \n",
        "                break                                                                                                                                                                                \n",
        "            eval_point(xs[i])                                                                                                                                                                        \n",
        "                                                                                                                                                                                                    \n",
        "    if refine_budget > 0 and evals_used < budget and best_x is not None:                                                                                                                             \n",
        "        base_sigma = 0.10 * span_safe                                                                                                                                                                \n",
        "        base_sigma[fixed_mask] = 0.0                                                                                                                                                                 \n",
        "        for t in range(refine_budget):                                                                                                                                                               \n",
        "            if evals_used >= budget:                                                                                                                                                                 \n",
        "                break                                                                                                                                                                                \n",
        "            frac = 1.0 - (t / max(refine_budget - 1, 1))                                                                                                                                             \n",
        "            sigma = np.maximum(base_sigma * (0.5 + 0.5 * frac), 1e-16)                                                                                                                               \n",
        "            noise = np.random.randn(dim) * sigma                                                                                                                                                     \n",
        "            cand = clamp(best_x + noise)                                                                                                                                                             \n",
        "            eval_point(cand)                                                                                                                                                                         \n",
        "                                                                                                                                                                                                    \n",
        "    if evals_used >= budget or local_budget <= 0:                                                                                                                                                    \n",
        "        return best_x                                                                                                                                                                                \n",
        "                                                                                                                                                                                                    \n",
        "    # ---- Local search: adaptive coordinate-wise + global kicks ----                                                                                                                                \n",
        "    remaining = budget - evals_used                                                                                                                                                                  \n",
        "                                                                                                                                                                                                    \n",
        "    if remaining < 40:                                                                                                                                                                               \n",
        "        step_frac = 0.16                                                                                                                                                                             \n",
        "    elif dim <= 5:                                                                                                                                                                                   \n",
        "        step_frac = 0.15                                                                                                                                                                             \n",
        "    else:                                                                                                                                                                                            \n",
        "        step_frac = 0.11                                                                                                                                                                             \n",
        "                                                                                                                                                                                                    \n",
        "    step = step_frac * span_safe                                                                                                                                                                     \n",
        "    step[fixed_mask] = 0.0                                                                                                                                                                           \n",
        "    min_step = 1e-10 * np.maximum(span_safe, 1.0)                                                                                                                                                    \n",
        "                                                                                                                                                                                                    \n",
        "    per_iter_cost = max(2 * dim + 4, 1)                                                                                                                                                              \n",
        "    max_iters = max(1, min(60, local_budget // per_iter_cost))                                                                                                                                       \n",
        "                                                                                                                                                                                                    \n",
        "    it = 0                                                                                                                                                                                           \n",
        "    no_improve_iters = 0                                                                                                                                                                             \n",
        "    second_best_x = None                                                                                                                                                                             \n",
        "    second_best_y = None                                                                                                                                                                             \n",
        "                                                                                                                                                                                                    \n",
        "    while evals_used < budget and it < max_iters and np.any(step > min_step):                                                                                                                        \n",
        "        it += 1                                                                                                                                                                                      \n",
        "        improved = False                                                                                                                                                                             \n",
        "        current_best_x = best_x.copy()                                                                                                                                                               \n",
        "                                                                                                                                                                                                    \n",
        "        order = np.arange(dim)                                                                                                                                                                       \n",
        "        np.random.shuffle(order)                                                                                                                                                                     \n",
        "                                                                                                                                                                                                    \n",
        "        for j in order:                                                                                                                                                                              \n",
        "            if evals_used >= budget:                                                                                                                                                                 \n",
        "                break                                                                                                                                                                                \n",
        "            if fixed_mask[j]:                                                                                                                                                                        \n",
        "                continue                                                                                                                                                                             \n",
        "                                                                                                                                                                                                    \n",
        "            for direction in (-1.0, 1.0):                                                                                                                                                            \n",
        "                if evals_used >= budget:                                                                                                                                                             \n",
        "                    break                                                                                                                                                                            \n",
        "                cand = current_best_x.copy()                                                                                                                                                         \n",
        "                cand[j] = cand[j] + direction * step[j]                                                                                                                                              \n",
        "                cand = clamp(cand)                                                                                                                                                                   \n",
        "                y = eval_point(cand)                                                                                                                                                                 \n",
        "                if (second_best_x is None or y < second_best_y) and (best_y is None or y > best_y):                                                                                                  \n",
        "                    second_best_x = cand.copy()                                                                                                                                                      \n",
        "                    second_best_y = float(y)                                                                                                                                                         \n",
        "                if y < best_y:                                                                                                                                                                       \n",
        "                    improved = True                                                                                                                                                                  \n",
        "                    current_best_x = best_x.copy()                                                                                                                                                   \n",
        "                                                                                                                                                                                                    \n",
        "            if evals_used >= budget:                                                                                                                                                                 \n",
        "                break                                                                                                                                                                                \n",
        "                                                                                                                                                                                                    \n",
        "            if step[j] > 0:                                                                                                                                                                          \n",
        "                for _ in range(2):                                                                                                                                                                   \n",
        "                    if evals_used >= budget:                                                                                                                                                         \n",
        "                        break                                                                                                                                                                        \n",
        "                    rnd = (2.0 * np.random.rand() - 1.0) * step[j]                                                                                                                                   \n",
        "                    cand = current_best_x.copy()                                                                                                                                                     \n",
        "                    cand[j] = cand[j] + rnd                                                                                                                                                          \n",
        "                    cand = clamp(cand)                                                                                                                                                               \n",
        "                    y = eval_point(cand)                                                                                                                                                             \n",
        "                    if (second_best_x is None or y < second_best_y) and (best_y is None or y > best_y):                                                                                              \n",
        "                        second_best_x = cand.copy()                                                                                                                                                  \n",
        "                        second_best_y = float(y)                                                                                                                                                     \n",
        "                    if y < best_y:                                                                                                                                                                   \n",
        "                        improved = True                                                                                                                                                              \n",
        "                        current_best_x = best_x.copy()                                                                                                                                               \n",
        "                                                                                                                                                                                                    \n",
        "        if evals_used >= budget:                                                                                                                                                                     \n",
        "            break                                                                                                                                                                                    \n",
        "                                                                                                                                                                                                    \n",
        "        # Gaussian escape step                                                                                                                                                                       \n",
        "        full_noise = np.random.randn(dim) * (0.35 * step)                                                                                                                                            \n",
        "        full_noise[fixed_mask] = 0.0                                                                                                                                                                 \n",
        "        cand = clamp(best_x + full_noise)                                                                                                                                                            \n",
        "        y = eval_point(cand)                                                                                                                                                                         \n",
        "        if (second_best_x is None or y < second_best_y) and (best_y is None or y > best_y):                                                                                                          \n",
        "            second_best_x = cand.copy()                                                                                                                                                              \n",
        "            second_best_y = float(y)                                                                                                                                                                 \n",
        "        if y < best_y:                                                                                                                                                                               \n",
        "            improved = True                                                                                                                                                                          \n",
        "                                                                                                                                                                                                    \n",
        "        # Diversification from second-best when stuck                                                                                                                                                \n",
        "        if (not improved) and (no_improve_iters >= 4) and (second_best_x is not None):                                                                                                               \n",
        "            kick_scale = 0.25                                                                                                                                                                        \n",
        "            noise = np.random.randn(dim) * (kick_scale * np.maximum(step, min_step))                                                                                                                 \n",
        "            noise[fixed_mask] = 0.0                                                                                                                                                                  \n",
        "            cand = clamp(second_best_x + noise)                                                                                                                                                      \n",
        "            y = eval_point(cand)                                                                                                                                                                     \n",
        "            if (second_best_x is None or y < second_best_y) and (best_y is None or y > best_y):                                                                                                      \n",
        "                second_best_x = cand.copy()                                                                                                                                                          \n",
        "                second_best_y = float(y)                                                                                                                                                             \n",
        "            if y < best_y:                                                                                                                                                                           \n",
        "                improved = True                                                                                                                                                                      \n",
        "                                                                                                                                                                                                    \n",
        "        # Step size adaptation                                                                                                                                                                       \n",
        "        if not improved:                                                                                                                                                                             \n",
        "            no_improve_iters += 1                                                                                                                                                                    \n",
        "            if no_improve_iters <= 3:                                                                                                                                                                \n",
        "                decay = 0.7                                                                                                                                                                          \n",
        "            elif no_improve_iters <= 7:                                                                                                                                                              \n",
        "                decay = 0.5                                                                                                                                                                          \n",
        "            else:                                                                                                                                                                                    \n",
        "                decay = 0.35                                                                                                                                                                         \n",
        "            step *= decay                                                                                                                                                                            \n",
        "        else:                                                                                                                                                                                        \n",
        "            no_improve_iters = 0                                                                                                                                                                     \n",
        "            step *= 1.05                                                                                                                                                                             \n",
        "                                                                                                                                                                                                    \n",
        "        step = np.maximum(step, min_step)                                                                                                                                                            \n",
        "                                                                                                                                                                                                    \n",
        "    return best_x\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poly-takeaway",
      "metadata": {},
      "source": [
        "### Key Takeaway\n",
        "\n",
        "**GEPA vs. Traditional Optimization**:\n",
        "\n",
        "| Aspect | Optuna | GEPA |\n",
        "|--------|--------|------|\n",
        "| Algorithm selection | Manual (TPE, CMA-ES, etc.) | Automatic (evolved) |\n",
        "| Hyperparameter tuning | Required | Evolved |\n",
        "| Domain knowledge needed | High | Low |\n",
        "| What user provides | Search space + sampler config | Baseline code + fitness function |\n",
        "| What gets optimized | Parameter values | The optimization algorithm itself |\n",
        "\n",
        "While Optuna requires users to select algorithms and tune hyperparameters, GEPA automatically **discovers optimization strategies** by evolving code. The user just provides the problem and a baseline—GEPA evolves Halton sequences, surrogate models, local refinement, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-4",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-5\"></a>\n",
        "# 5. Example 2: Prompt Engineering — AIME 2025\n",
        "\n",
        "**Result: GEPA improves GPT-4.1 Mini's accuracy from 46.67% to 60.00% on AIME 2025.**\n",
        "\n",
        "This example demonstrates how `optimize_anything` can evolve **prompts**—the natural language instructions that guide LLM behavior.\n",
        "\n",
        "## The Challenge\n",
        "\n",
        "Prompt engineering is often done through **trial and error**:\n",
        "1. Write a prompt\n",
        "2. Test on a few examples\n",
        "3. Manually tweak based on intuition\n",
        "4. Repeat until it \"feels right\"\n",
        "\n",
        "This is slow, doesn't scale, and doesn't guarantee you've found the best prompt.\n",
        "\n",
        "## The Task\n",
        "\n",
        "**Given**: A dataset of AIME math competition problems\n",
        "**Find**: A system prompt that maximizes GPT-4.1 Mini's accuracy\n",
        "\n",
        "**What GEPA optimizes**: The instruction prompt—what guidance to give the model.\n",
        "\n",
        "<img src=\"./assets/blog/aime.png\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aime-setup",
      "metadata": {},
      "source": [
        "## Setting Up the Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aime-setup-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 45 training examples\n",
            "Loaded 45 validation examples\n",
            "Loaded 30 test examples\n",
            "Training: 45 problems\n",
            "Validation: 45 problems\n",
            "Test: 30 problems\n"
          ]
        }
      ],
      "source": [
        "import dspy\n",
        "import os\n",
        "\n",
        "# Configure the language model\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "lm = dspy.LM(\"gpt-4.1-mini\", api_key=api_key, temperature=1.0, max_tokens=32000)\n",
        "dspy.configure(lm=lm)\n",
        "\n",
        "# Load AIME dataset splits\n",
        "from examples.aime_math.dataset import load_math_dataset\n",
        "trainset, valset, testset = load_math_dataset()\n",
        "\n",
        "print(f\"Training: {len(trainset)} problems\")\n",
        "print(f\"Validation: {len(valset)} problems\")\n",
        "print(f\"Test: {len(testset)} problems\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aime-module",
      "metadata": {},
      "source": [
        "## The DSPy Module\n",
        "\n",
        "We use DSPy's `ChainOfThought` for step-by-step reasoning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "aime-module-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MathSolverSignature(dspy.Signature):\n",
        "    \"\"\"Solve a math competition problem.\"\"\"\n",
        "    input = dspy.InputField(desc=\"The math problem to solve.\")\n",
        "    answer = dspy.OutputField(desc=\"The final numerical answer.\")\n",
        "\n",
        "predictor = dspy.ChainOfThought(MathSolverSignature)\n",
        "\n",
        "def run_llm(example, prompt: str):\n",
        "    \"\"\"Run the LLM on a single example with the given prompt.\"\"\"\n",
        "    predictor.predict.signature.instructions = prompt\n",
        "    return predictor(input=example.input)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aime-seed",
      "metadata": {},
      "source": [
        "## The Seed Candidate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "aime-seed-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_candidate = {\n",
        "    \"prompt\": \"Solve the math problem carefully. Break down the steps and provide the final answer as a single number.\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aime-fitness",
      "metadata": {},
      "source": [
        "## The Fitness Function\n",
        "\n",
        "The fitness function runs the predictor and collects detailed feedback:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "aime-fitness-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def math_metric(example, prediction):\n",
        "    \"\"\"Compute score and detailed feedback for math problems.\"\"\"\n",
        "    correct_answer, written_solution = int(example.answer), getattr(example, \"solution\", \"\")\n",
        "    solution_suffix = f\" Here's the full step-by-step solution:\\n{written_solution}\\n\\nThink about what takeaways you can learn from this solution to improve your future answers and approach to similar problems\" if written_solution else \"\"\n",
        "\n",
        "    try:\n",
        "        llm_answer = int(prediction.answer)\n",
        "    except (ValueError, TypeError):\n",
        "        feedback_text = f\"The final answer must be a valid integer and nothing else. You responded with '{prediction.answer}', which couldn't be parsed as a python integer. Please ensure your answer is a valid integer without any additional text or formatting. The correct answer is '{correct_answer}'.{solution_suffix}{' and ensure your final answer is a valid integer.' if written_solution else ''}\"\n",
        "        return dspy.Prediction(score=0.0, feedback=feedback_text)\n",
        "\n",
        "    score = float(correct_answer == llm_answer)\n",
        "    status = \"correct\" if score == 1.0 else \"incorrect\"\n",
        "    feedback_text = f\"Your answer is {status}. The correct answer is '{correct_answer}'.{solution_suffix}\"\n",
        "    return dspy.Prediction(score=score, feedback=feedback_text)\n",
        "\n",
        "\n",
        "def fitness_fn(candidate: dict[str, str], example) -> tuple[float, Any, SideInfo]:\n",
        "    \"\"\"Fitness function for GEPA optimization with single example evaluation.\"\"\"\n",
        "    prediction = run_llm(example, candidate[\"prompt\"])\n",
        "    metric_result = math_metric(example, prediction)\n",
        "    score = metric_result.score\n",
        "    feedback = metric_result.feedback\n",
        "\n",
        "    output = {\n",
        "        \"prompt\": candidate[\"prompt\"],\n",
        "        \"answer\": prediction.answer,\n",
        "        \"score\": score,\n",
        "    }\n",
        "\n",
        "    side_info = {\n",
        "        \"Input\": example.input,\n",
        "        \"Output\": prediction.answer,\n",
        "        \"Reasoning\": getattr(prediction, \"reasoning\", \"\"),\n",
        "        \"ExecutionFeedback\": feedback,\n",
        "    }\n",
        "\n",
        "    return (score, output, side_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aime-optimize",
      "metadata": {},
      "source": [
        "## Running GEPA Optimization\n",
        "\n",
        "Note: We use `valset` for generalization testing—GEPA optimizes on `trainset` but tracks performance on held-out `valset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aime-optimize-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0: Base program full valset score: 0.4666666666666667 over 45 / 45 examples\n",
            "Iteration 1: Selected program 0 score: 0.4666666666666667\n"
          ]
        }
      ],
      "source": [
        "from gepa.optimize_anything import (\n",
        "    optimize_anything,\n",
        "    GEPAConfig,\n",
        "    EngineConfig,\n",
        "    ReflectionConfig,\n",
        ")\n",
        "\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=trainset,   # Optimize on training set\n",
        "    valset=valset,      # Track generalization on validation set\n",
        "    config=GEPAConfig(\n",
        "        engine=EngineConfig(\n",
        "            max_metric_calls=800,\n",
        "            track_best_outputs=True,\n",
        "            parallel=True,      \n",
        "            max_workers=32,\n",
        "            cache_evaluation=True,\n",
        "        ),\n",
        "        reflection=ReflectionConfig(\n",
        "            reflection_lm=\"openai/gpt-5\",\n",
        "            reflection_minibatch_size=3,  # Show 3 problems per reflection\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"\\nOptimized prompt:\")\n",
        "print(result.best_candidate[\"prompt\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aime-evolved",
      "metadata": {},
      "source": [
        "## The Optimized Prompt\n",
        "\n",
        "GEPA discovered a detailed, structured prompt with domain-specific strategies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aime-evolved-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This prompt was EVOLVED by GEPA, not written by a human!\n",
        "# Starting from a simple \"Solve carefully and provide the answer\" prompt,\n",
        "# GEPA discovered domain-specific strategies through reflection.\n",
        "\n",
        "optimized_prompt = \"\"\"\n",
        "Solve the math problem carefully and thoroughly. Your goal is to produce a correct, well‑structured solution that leads unambiguously to the requested final result.\n",
        "\n",
        "Follow these rules:\n",
        "\n",
        "1. Restate the problem briefly in your own words.\n",
        "   - Identify what is given and what must be found.\n",
        "   - Note any special conditions (e.g., ordering of variables, geometric configuration).\n",
        "\n",
        "2. Set up notation and equations cleanly before manipulating them.\n",
        "   - Define all variables explicitly (including any you introduce, like substitutions).\n",
        "   - State all constraints and domain conditions (positivity, integrality, ordering, angle ranges, etc.) before using them.\n",
        "   - When you make structural assumptions (e.g., “assume all first k terms are equal”), clearly justify why this does not reduce generality for maximizing/minimizing the desired expression.\n",
        "\n",
        "3. Plan the approach before detailed algebra.\n",
        "   - Briefly outline the main idea (e.g., symmetry, extremal principle, splitting into cases, using inequalities, introducing coordinates).\n",
        "   - For optimization / extremal problems, explain why your configuration is optimal (e.g., by convexity, rearrangement inequality, averaging arguments), not just by constructing one example.\n",
        "   - For existence/uniqueness, mention how you will show all solutions or the unique solution.\n",
        "\n",
        "4. Show clear, logically ordered reasoning.\n",
        "   - Justify each important algebraic, geometric, or inequality step.\n",
        "   - When you split into cases, state:\n",
        "     * why each case must be considered, and\n",
        "     * what assumptions define the case.\n",
        "   - If you invoke a known theorem or fact (e.g., Ptolemy, Power of a Point, Cauchy–Schwarz, AM–GM, similarity, Vieta, extremal principle), name it and show explicitly how it applies in this context.\n",
        "   - For inequality / optimization problems, be explicit about:\n",
        "     * where equalities can occur,\n",
        "     * why boundary or extreme configurations are considered,\n",
        "     * and why they indeed give a global optimum under the constraints.\n",
        "\n",
        "5. Handle dead ends correctly.\n",
        "   - If a line of reasoning leads to a contradiction or dead end, explicitly say so.\n",
        "   - Roll back to the last correct point and choose a new approach; do not keep using a flawed assumption.\n",
        "   - Do not discard potential solutions without checking them against all given conditions.\n",
        "\n",
        "6. Keep the reasoning focused but rigorous.\n",
        "   - Avoid unnecessary numerical approximations when an exact expression is available.\n",
        "   - Do not approximate exact values unless the problem explicitly asks for a decimal or numerical estimate.\n",
        "   - Prefer structural or conceptual arguments (symmetry, convexity, parity, monotonicity, invariants, etc.) over ad hoc trial‑and‑error or guess‑and‑check.\n",
        "   - You may test specific candidate values only after deriving strong constraints that sharply limit the possibilities, and you must explain why those candidates are exhaustive.\n",
        "\n",
        "7. Treat ordering and extremal indices with care.\n",
        "   - For sequences with ordering constraints (e.g., \\(x_1 \\le \\dots \\le x_n\\)), reason about which indices must be negative, zero, or positive to maximize or minimize a given difference.\n",
        "   - When you set many consecutive terms equal (e.g., all smallest terms equal to some \\(b\\), all largest terms equal to some \\(a\\), and middle ones zero), justify that any deviation from this would decrease (or not increase) the desired quantity, using monotonicity or averaging arguments.\n",
        "   - Ensure that sign and ordering constraints are respected in all constructed examples.\n",
        "\n",
        "8. Final consistency check.\n",
        "   - At the end, verify that your solution satisfies:\n",
        "     * all original equations/constraints,\n",
        "     * ordering or geometric conditions,\n",
        "     * and any side conditions (e.g., angle ranges, positivity, distinctness).\n",
        "   - For maximization/minimization problems, explicitly argue that:\n",
        "     * your value is attainable by some configuration, and\n",
        "     * no configuration can exceed (or go below) it.\n",
        "\n",
        "9. At the end, clearly isolate the answer:\n",
        "   - Provide the final answer as a single number or expression on its own line.\n",
        "   - Do not include any extra words, symbols, or explanation on that final line.\n",
        "\"\"\"\n",
        "\n",
        "# 🎯 What GEPA discovered:\n",
        "# - Domain-specific heuristics for different math areas (geometry, combinatorics, NT)\n",
        "# - Structured problem-solving workflow\n",
        "# - Sanity checks and validation strategies\n",
        "# - Explicit handling of common failure modes (mixing counting models, etc.)\n",
        "#\n",
        "# A human prompt engineer might take hours to discover these strategies!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aime-takeaway",
      "metadata": {},
      "source": [
        "### Key Takeaway\n",
        "\n",
        "By including the model's **reasoning trace** in `side_info`, GEPA can understand *how* the model approaches problems—not just whether it got the answer right. This enables:\n",
        "\n",
        "1. **Targeted improvements**: Fix specific reasoning errors, not random prompt tweaks\n",
        "2. **Domain-specific strategies**: The prompt evolved to include geometry workflows, combinatorics rules, etc.\n",
        "3. **Sanity checks**: GEPA discovered that asking for validation prevents common errors\n",
        "\n",
        "The evolved prompt contains strategies that a human prompt engineer might take hours to discover through manual iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-5",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-6\"></a>\n",
        "# 6. Example 3: Agent Program Evolution — ARC-AGI\n",
        "\n",
        "**Result: GEPA improves GPT-5's performance from 56.5% to 68.0% on ARC-AGI.**\n",
        "\n",
        "This is the most ambitious example: optimizing not just prompts, but **entire agent architectures**—the DSPy program that defines how an LLM reasons about problems.\n",
        "\n",
        "## The Challenge\n",
        "\n",
        "[ARC-AGI](https://arcprize.org/) (Abstraction and Reasoning Corpus) is a benchmark designed to test general intelligence:\n",
        "- Each task shows input-output grid transformation examples\n",
        "- The agent must infer the transformation rule and apply it to test inputs\n",
        "- Tasks require **visual reasoning, pattern recognition, and abstraction**\n",
        "\n",
        "Hand-designing agent architectures for such tasks is extremely difficult.\n",
        "\n",
        "## The Task\n",
        "\n",
        "**Given**: A dataset of ARC-AGI grid transformation tasks\n",
        "**Find**: A DSPy program (agent architecture) that maximizes accuracy\n",
        "\n",
        "**What GEPA optimizes**: The entire DSPy program—signatures, modules, control flow, and prompting strategies.\n",
        "\n",
        "<img src=\"./assets/blog/arc_agi.png\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5ba1793",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set: 200\n",
            "Val set: 200\n",
            "Test set: 400\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import dspy\n",
        "import os\n",
        "\n",
        "from gepa.adapters.dspy_full_program_adapter.full_program_adapter import DspyAdapter\n",
        "from examples.arc_agi.main import metric_fn\n",
        "from examples.arc_agi.data import load_data\n",
        "\n",
        "seed = 0\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "task_lm = dspy.LM(\n",
        "        model=\"openai/gpt-4.1-mini\",\n",
        "        temperature=1.0,\n",
        "        max_tokens=32000,\n",
        "        api_key=api_key,\n",
        "        seed=seed,\n",
        "        cache=False,\n",
        "    )\n",
        "\n",
        "adapter = DspyAdapter(\n",
        "        task_lm=task_lm,\n",
        "        metric_fn=metric_fn,\n",
        "        num_threads=64,\n",
        "        reflection_lm=\"openai/gpt-5\",\n",
        "        rng=random.Random(seed),\n",
        "    )\n",
        "    \n",
        "trainset, valset, testset = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "arc-seed",
      "metadata": {},
      "source": [
        "## The Seed Candidate\n",
        "\n",
        "We start with a minimal Chain-of-Thought agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "arc-seed-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_candidate = {\n",
        "    \"program\": \"\"\"\n",
        "import dspy\n",
        "from typing import List\n",
        "import pydantic\n",
        "\n",
        "MATRIX = List[List[int]]\n",
        "\n",
        "class TrainingExample(pydantic.BaseModel):\n",
        "    input: MATRIX\n",
        "    output: MATRIX\n",
        "\n",
        "class SolveTaskSignature(dspy.Signature):\n",
        "    training_examples: List[TrainingExample] = dspy.InputField(description=\"Input and output examples demonstrating the task to be performed.\")\n",
        "    test_inputs: List[MATRIX] = dspy.InputField(description=\"Input matrices to be solved following the task described in the training examples.\")\n",
        "    test_outputs: List[MATRIX] = dspy.OutputField(description=\"Output matrices corresponding to the test inputs.\")\n",
        "\n",
        "program = dspy.ChainOfThought(SolveTaskSignature)\n",
        "\"\"\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "arc-fitness",
      "metadata": {},
      "source": [
        "## The Fitness Function\n",
        "\n",
        "The fitness function compiles and executes the DSPy program, capturing detailed error information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "arc-fitness-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def fitness_fn(candidate, example):\n",
        "    program = candidate[\"program\"]\n",
        "\n",
        "    try:\n",
        "        evaluation_results = adapter.evaluate(\n",
        "            [example], candidate, capture_traces=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        side_info = {\"input\": example, \"error\": str(e), \"program\": program}\n",
        "        return (0.0, side_info, side_info)\n",
        "\n",
        "    # Program error\n",
        "    if (\n",
        "        not isinstance(evaluation_results.trajectories, list)\n",
        "        or len(evaluation_results.trajectories) == 0\n",
        "    ):\n",
        "        print(\"Error: \")\n",
        "        print(evaluation_results.trajectories)\n",
        "        side_info = {\n",
        "            \"input\": example,\n",
        "            \"error\": f\"All examples failed. Program error: {str(evaluation_results.trajectories)}\",\n",
        "            \"program\": program,\n",
        "        }\n",
        "        return (0.0, side_info, side_info)\n",
        "\n",
        "    # Process evaluations with no program errors\n",
        "    trajectory = evaluation_results.trajectories[0]\n",
        "    metric_result = trajectory.get(\"score\")\n",
        "    score = metric_result.get(\"score\")\n",
        "    feedback = metric_result.get(\"feedback\")\n",
        "    prediction = trajectory.get(\"prediction\")\n",
        "\n",
        "    side_info = {\n",
        "        \"input\": example,\n",
        "        \"reasoning\": prediction.get(\"reasoning\"),\n",
        "        \"feedback\": feedback,\n",
        "        \"output\": prediction.get(\"test_outputs\"),\n",
        "    }\n",
        "\n",
        "    return (score, side_info, side_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "arc-optimize",
      "metadata": {},
      "source": [
        "## Running GEPA Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "arc-optimize-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from examples.arc_agi.prompt import BACKGROUND\n",
        "from gepa.optimize_anything import (\n",
        "    EngineConfig,\n",
        "    GEPAConfig,\n",
        "    ReflectionConfig,\n",
        "    optimize_anything,\n",
        ")\n",
        "\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=trainset,\n",
        "    valset=valset,\n",
        "    objective=\"Optimize the dspy agent program to solve ARC-AGI tasks effectively.\",\n",
        "    background=BACKGROUND,\n",
        "    config=GEPAConfig(\n",
        "        engine=EngineConfig(\n",
        "            max_metric_calls=4000,\n",
        "            track_best_outputs=True,\n",
        "            use_cloudpickle=True,\n",
        "            parallel=True,\n",
        "            max_workers=64,\n",
        "            cache_evaluation=True,\n",
        "        ),\n",
        "        reflection=ReflectionConfig(\n",
        "            reflection_lm=\"openai/gpt-5\",\n",
        "            reflection_minibatch_size=3,\n",
        "        ),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "arc-evolved",
      "metadata": {},
      "source": [
        "## What GEPA Discovered\n",
        "\n",
        "GEPA evolved the simple ChainOfThought into a sophisticated 5-step code synthesis pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "arc-evolved-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evolved by GEPA - A code synthesis agent with self-refinement\n",
        "# This program was DISCOVERED through optimization, not hand-written!\n",
        "\n",
        "evolved_program = '''\n",
        "import dspy\n",
        "from typing import List, Tuple, Optional, Any, Dict\n",
        "import pydantic\n",
        "import re\n",
        "import copy\n",
        "import textwrap\n",
        "from collections import Counter\n",
        "\n",
        "MATRIX = List[List[int]]\n",
        "\n",
        "class TrainingExample(pydantic.BaseModel):\n",
        "    input: MATRIX\n",
        "    output: MATRIX\n",
        "\n",
        "def _strip_code_fences(s: str) -> str:\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = s.strip()\n",
        "    # Remove fenced blocks and language markers\n",
        "    if s.startswith(\"```\"):\n",
        "        s = \"\\n\".join(s.split(\"\\n\")[1:])\n",
        "    if s.endswith(\"```\"):\n",
        "        s = \"\\n\".join(s.split(\"\\n\")[:-1])\n",
        "    s = re.sub(r\"^```[a-zA-Z0-9_+-]*\\s*\", \"\", s.strip())\n",
        "    s = re.sub(r\"\\s*```$\", \"\", s.strip())\n",
        "    return s.strip()\n",
        "\n",
        "def _ensure_transform_code(code: str) -> str:\n",
        "    \"\"\"\n",
        "    Ensure code contains a def transform(grid): function.\n",
        "    If code appears to be just the body, wrap it. Otherwise, return full code as-is.\n",
        "    \"\"\"\n",
        "    code = _strip_code_fences(code)\n",
        "    if \"def transform\" in code:\n",
        "        # Keep helpers that may be defined before/after transform.\n",
        "        return code\n",
        "    # If it's likely just a body (has return or grid indexing), wrap it\n",
        "    lines = code.strip().splitlines()\n",
        "    body = code.strip()\n",
        "    if len(body) == 0:\n",
        "        return \"\"\n",
        "    if \"return\" in body or \"grid\" in body:\n",
        "        wrapped = \"def transform(grid):\\n\" + textwrap.indent(body, \"    \")\n",
        "        return wrapped\n",
        "    return \"\"\n",
        "\n",
        "def _height(grid: MATRIX) -> int:\n",
        "    return len(grid)\n",
        "\n",
        "def _width(grid: MATRIX) -> int:\n",
        "    return len(grid[0]) if grid and isinstance(grid[0], list) else 0\n",
        "\n",
        "def _in_bounds(grid: MATRIX, r: int, c: int) -> bool:\n",
        "    h, w = _height(grid), _width(grid)\n",
        "    return 0 <= r < h and 0 <= c < w\n",
        "\n",
        "def _colors(grid: MATRIX) -> List[int]:\n",
        "    s = set()\n",
        "    for row in grid:\n",
        "        for v in row:\n",
        "            try:\n",
        "                s.add(int(v))\n",
        "            except Exception:\n",
        "                pass\n",
        "    return sorted(list(s))\n",
        "\n",
        "def _color_counts(grid: MATRIX) -> Dict[int, int]:\n",
        "    cnt = Counter()\n",
        "    for row in grid:\n",
        "        for v in row:\n",
        "            try:\n",
        "                cnt[int(v)] += 1\n",
        "            except Exception:\n",
        "                pass\n",
        "    return dict(cnt)\n",
        "\n",
        "def _positions_of(grid: MATRIX, color: int) -> List[Tuple[int, int]]:\n",
        "    pos = []\n",
        "    for r, row in enumerate(grid):\n",
        "        for c, v in enumerate(row):\n",
        "            if v == color:\n",
        "                pos.append((r, c))\n",
        "    return pos\n",
        "\n",
        "def _bbox_of(grid: MATRIX, colors: Optional[List[int]] = None, nonzero: bool = True) -> Optional[Tuple[int, int, int, int]]:\n",
        "    h, w = _height(grid), _width(grid)\n",
        "    rs = []\n",
        "    if colors is not None:\n",
        "        color_set = set(colors)\n",
        "        for r in range(h):\n",
        "            for c in range(w):\n",
        "                if grid[r][c] in color_set:\n",
        "                    rs.append((r, c))\n",
        "    elif nonzero:\n",
        "        for r in range(h):\n",
        "            for c in range(w):\n",
        "                if grid[r][c] != 0:\n",
        "                    rs.append((r, c))\n",
        "    else:\n",
        "        # if not using nonzero and no colors, bbox of entire grid\n",
        "        for r in range(h):\n",
        "            for c in range(w):\n",
        "                rs.append((r, c))\n",
        "    if not rs:\n",
        "        return None\n",
        "    rmin = min(r for r, _ in rs)\n",
        "    rmax = max(r for r, _ in rs)\n",
        "    cmin = min(c for _, c in rs)\n",
        "    cmax = max(c for _, c in rs)\n",
        "    return (rmin, cmin, rmax, cmax)\n",
        "\n",
        "def _crop(grid: MATRIX, bbox: Tuple[int, int, int, int]) -> MATRIX:\n",
        "    rmin, cmin, rmax, cmax = bbox\n",
        "    out = []\n",
        "    for r in range(rmin, rmax + 1):\n",
        "        row = []\n",
        "        for c in range(cmin, cmax + 1):\n",
        "            row.append(int(grid[r][c]))\n",
        "        out.append(row)\n",
        "    return out\n",
        "\n",
        "def _transpose(grid: MATRIX) -> MATRIX:\n",
        "    h, w = _height(grid), _width(grid)\n",
        "    if h == 0 or w == 0:\n",
        "        return []\n",
        "    return [[int(grid[r][c]) for r in range(h)] for c in range(w)]\n",
        "\n",
        "def _new_grid(h: int, w: int, fill: int = 0) -> MATRIX:\n",
        "    return [[int(fill) for _ in range(w)] for _ in range(h)]\n",
        "\n",
        "def _normalize_matrix(mat: MATRIX) -> MATRIX:\n",
        "    out: MATRIX = []\n",
        "    for row in mat:\n",
        "        new_row = []\n",
        "        for v in row:\n",
        "            try:\n",
        "                iv = int(v)\n",
        "            except Exception:\n",
        "                iv = 0\n",
        "            if iv < 0:\n",
        "                iv = 0\n",
        "            if iv > 9:\n",
        "                iv = 9\n",
        "            new_row.append(iv)\n",
        "        out.append(new_row)\n",
        "    return out\n",
        "\n",
        "def _same_shape(a: MATRIX, b: MATRIX) -> bool:\n",
        "    if len(a) != len(b):\n",
        "        return False\n",
        "    return all(len(ra) == len(rb) for ra, rb in zip(a, b))\n",
        "\n",
        "def _compare_matrices(a: MATRIX, b: MATRIX) -> Tuple[bool, List[Tuple[int, int, int, int]]]:\n",
        "    diffs: List[Tuple[int, int, int, int]] = []\n",
        "    if not _same_shape(a, b):\n",
        "        return False, diffs\n",
        "    for i, (ra, rb) in enumerate(zip(a, b)):\n",
        "        for j, (va, vb) in enumerate(zip(ra, rb)):\n",
        "            if va != vb:\n",
        "                diffs.append((i, j, va, vb))\n",
        "    return (len(diffs) == 0), diffs\n",
        "\n",
        "def _verify_on_training(transform_fn, training_examples: List[TrainingExample]) -> Tuple[bool, str]:\n",
        "    reports = []\n",
        "    all_ok = True\n",
        "    for idx, ex in enumerate(training_examples):\n",
        "        try:\n",
        "            out = transform_fn(copy.deepcopy(ex.input))\n",
        "        except Exception as e:\n",
        "            all_ok = False\n",
        "            reports.append(f\"Example {idx}: Exception during transform: {repr(e)}\")\n",
        "            continue\n",
        "        if not isinstance(out, list) or (len(out) > 0 and not isinstance(out[0], list)):\n",
        "            all_ok = False\n",
        "            reports.append(f\"Example {idx}: Output is not a 2D list.\")\n",
        "            continue\n",
        "        out = _normalize_matrix(out)\n",
        "        # For ARC, output shape must match the example's output shape, not necessarily the input shape.\n",
        "        if not _same_shape(out, ex.output):\n",
        "            all_ok = False\n",
        "            reports.append(f\"Example {idx}: Shape mismatch. Got {len(out)}x{len(out[0]) if out else 0}, expected {len(ex.output)}x{len(ex.output[0]) if ex.output else 0}.\")\n",
        "            continue\n",
        "        ok, diffs = _compare_matrices(out, ex.output)\n",
        "        if not ok:\n",
        "            all_ok = False\n",
        "            sample_diffs = diffs[:20]\n",
        "            reports.append(f\"Example {idx}: {len(diffs)} cells differ. Sample diffs (r,c got->exp): \" +\n",
        "                           \", \".join([f\"({r},{c} {gv}->{ev})\" for r, c, gv, ev in sample_diffs]))\n",
        "    return all_ok, \"\\n\".join(reports)\n",
        "\n",
        "def _apply_code_to_tests(code: str, test_inputs: List[MATRIX]) -> List[MATRIX]:\n",
        "    fn = _safe_exec_transform(code)\n",
        "    if not fn:\n",
        "        return []\n",
        "    outputs = []\n",
        "    for idx, ti in enumerate(test_inputs):\n",
        "        try:\n",
        "            out = fn(copy.deepcopy(ti))\n",
        "        except Exception:\n",
        "            return []\n",
        "        if not isinstance(out, list) or (len(out) > 0 and not isinstance(out[0], list)):\n",
        "            return []\n",
        "        out = _normalize_matrix(out)\n",
        "        outputs.append(out)\n",
        "    return outputs\n",
        "\n",
        "def _safe_exec_transform(code: str) -> Optional[Any]:\n",
        "    \"\"\"\n",
        "    Exec the transform function in a restricted namespace with helper utilities.\n",
        "    Returns callable transform or None if failure.\n",
        "    \"\"\"\n",
        "    code = _ensure_transform_code(code)\n",
        "    if not code:\n",
        "        return None\n",
        "\n",
        "    # Safe helper library exposed to the model's code.\n",
        "    sandbox_globals: Dict[str, Any] = {\n",
        "        \"__builtins__\": {\n",
        "            \"range\": range,\n",
        "            \"len\": len,\n",
        "            \"enumerate\": enumerate,\n",
        "            \"min\": min,\n",
        "            \"max\": max,\n",
        "            \"sum\": sum,\n",
        "            \"abs\": abs,\n",
        "            \"all\": all,\n",
        "            \"any\": any,\n",
        "            \"sorted\": sorted,\n",
        "            \"zip\": zip,\n",
        "            \"list\": list,\n",
        "            \"set\": set,\n",
        "            \"tuple\": tuple,\n",
        "            \"int\": int\n",
        "        },\n",
        "        \"deepcopy\": copy.deepcopy,\n",
        "        # helpers\n",
        "        \"height\": _height,\n",
        "        \"width\": _width,\n",
        "        \"in_bounds\": _in_bounds,\n",
        "        \"colors\": _colors,\n",
        "        \"color_counts\": _color_counts,\n",
        "        \"positions_of\": _positions_of,\n",
        "        \"bbox_of\": _bbox_of,\n",
        "        \"crop\": _crop,\n",
        "        \"transpose\": _transpose,\n",
        "        \"new_grid\": _new_grid,\n",
        "    }\n",
        "    sandbox_locals: Dict[str, Any] = {}\n",
        "    try:\n",
        "        exec(code, sandbox_globals, sandbox_locals)\n",
        "        fn = sandbox_locals.get(\"transform\") or sandbox_globals.get(\"transform\")\n",
        "        if callable(fn):\n",
        "            return fn\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "def _matrix_shape(mat: MATRIX) -> Tuple[int, int]:\n",
        "    if not isinstance(mat, list) or (mat and not isinstance(mat[0], list)):\n",
        "        return (0, 0)\n",
        "    return (len(mat), len(mat[0]) if mat else 0)\n",
        "\n",
        "def _stats_for_grid(grid: MATRIX) -> Dict[str, Any]:\n",
        "    h, w = _matrix_shape(grid)\n",
        "    pal = _colors(grid)\n",
        "    cnt = _color_counts(grid)\n",
        "    nonzero_bbox = _bbox_of(grid, colors=None, nonzero=True)\n",
        "    counts_str = \",\".join(f\"{k}:{cnt[k]}\" for k in sorted(cnt.keys()))\n",
        "    return {\n",
        "        \"shape\": [h, w],\n",
        "        \"palette\": pal,\n",
        "        \"counts\": counts_str,\n",
        "        \"nonzero_bbox\": nonzero_bbox\n",
        "    }\n",
        "\n",
        "def _build_training_summary(training_examples: List[TrainingExample]) -> str:\n",
        "    # Construct a concise, structured summary string for the LM.\n",
        "    lines = []\n",
        "    for i, ex in enumerate(training_examples):\n",
        "        in_stats = _stats_for_grid(ex.input)\n",
        "        out_stats = _stats_for_grid(ex.output)\n",
        "        lines.append(\n",
        "            f\"Example {i}: input_shape={in_stats['shape']}, output_shape={out_stats['shape']}, \"\n",
        "            f\"input_palette={in_stats['palette']}, output_palette={out_stats['palette']}, \"\n",
        "            f\"input_counts={in_stats['counts']}, output_counts={out_stats['counts']}, \"\n",
        "            f\"input_nonzero_bbox={in_stats['nonzero_bbox']}, output_nonzero_bbox={out_stats['nonzero_bbox']}\"\n",
        "        )\n",
        "    # Heuristics across examples:\n",
        "    shape_deltas = []\n",
        "    for ex in training_examples:\n",
        "        ih, iw = _matrix_shape(ex.input)\n",
        "        oh, ow = _matrix_shape(ex.output)\n",
        "        shape_deltas.append((oh - ih, ow - iw))\n",
        "    unique_deltas = sorted(set(shape_deltas))\n",
        "    lines.append(f\"Unique shape deltas (oh-ih, ow-iw) across training: {unique_deltas}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "class InduceRuleSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    You are given several training input/output pairs from an ARC task. Infer a single deterministic transformation rule\n",
        "    that maps any input grid to its output grid. Produce:\n",
        "    1) A succinct, generalized rule_summary describing the pattern.\n",
        "    2) A Python function `def transform(grid): ...` implementing the rule.\n",
        "\n",
        "    Execution constraints and helpers:\n",
        "      - grid is a 2D list of ints (0..9). Deterministic logic only (loops, conditionals, list ops).\n",
        "      - Do not import modules or use randomness.\n",
        "      - You MAY change the output shape when the task demands it. Match the training outputs' shapes for their inputs.\n",
        "      - Indices must be in-bounds; handle arbitrary rectangular shapes.\n",
        "      - Mutations: copy if needed; avoid mutating the input in-place unless safe.\n",
        "      - Values must remain ints 0..9.\n",
        "\n",
        "    Common pitfalls to avoid:\n",
        "      - Off-by-one when cropping or expanding; mixing up row/col order.\n",
        "      - Assuming output has same shape as input when training shows otherwise.\n",
        "      - Forgetting to preserve anchor colors or key markers when summarizing patterns.\n",
        "      - Using unavailable libraries (numpy/pandas).\n",
        "\n",
        "    Successful strategies:\n",
        "      - Use bounding boxes of non-zero or target colors to crop or extract regions (bbox_of, crop).\n",
        "      - Build frequency maps to pick dominant or anchor colors (color_counts, colors).\n",
        "      - Compress by selecting rows/cols that contain target colors; preserve ordering.\n",
        "      - When outputs are smaller, they often summarize a motif or mask of specific colors (e.g., 1 and 2).\n",
        "\n",
        "    Helper functions available in the environment:\n",
        "      - height(grid), width(grid), in_bounds(grid,r,c)\n",
        "      - colors(grid) -> sorted list of unique colors\n",
        "      - color_counts(grid) -> dict color->count\n",
        "      - positions_of(grid, color) -> list of (r,c)\n",
        "      - bbox_of(grid, colors=None, nonzero=True) -> (rmin,cmin,rmax,cmax) inclusive or None\n",
        "      - crop(grid, bbox) -> returns subgrid bounded by bbox\n",
        "      - transpose(grid), new_grid(h,w,fill)\n",
        "\n",
        "    Return fields:\n",
        "      - rule_summary: Concise paragraph of the inferred rule.\n",
        "      - python_code: Only the code defining def transform(grid): ...; no extra commentary or fences.\n",
        "\n",
        "    Important: Return only rule_summary and python_code; do not include explanations beyond those fields.\n",
        "    \"\"\"\n",
        "    training_examples: List[TrainingExample] = dspy.InputField(desc=\"Examples showing input->output mappings for the task.\")\n",
        "    training_summary: str = dspy.InputField(desc=\"Structured stats about shapes, palettes, counts, and bboxes across examples.\")\n",
        "    rule_summary: str = dspy.OutputField(desc=\"One concise paragraph describing the inferred rule.\")\n",
        "    python_code: str = dspy.OutputField(desc=\"Only the code defining def transform(grid): ...; no explanations.\")\n",
        "\n",
        "class RefineRuleSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    The previous Python transform did not perfectly match all training examples.\n",
        "    Given the training_examples, training_summary (stats), the prior_code for transform(grid), and a failure_report\n",
        "    describing mismatches, produce a corrected version of transform(grid).\n",
        "\n",
        "    Instructions:\n",
        "    - Keep the same overall approach but fix the logic to satisfy all training pairs.\n",
        "    - You MAY alter the output shape; match each training output's shape for its input.\n",
        "    - Maintain determinism; no external imports; values are ints 0..9.\n",
        "    - Be careful with off-by-one, bounds, and row/col order.\n",
        "    - Return only a complete, standalone function definition: def transform(grid): ...\n",
        "    \"\"\"\n",
        "    training_examples: List[TrainingExample] = dspy.InputField(desc=\"Authoritative input/output examples.\")\n",
        "    training_summary: str = dspy.InputField(desc=\"Structured stats about shapes, palettes, counts, and bboxes across examples.\")\n",
        "    prior_code: str = dspy.InputField(desc=\"Previous attempt's function code.\")\n",
        "    failure_report: str = dspy.InputField(desc=\"Concrete mismatches and diagnostics from verification.\")\n",
        "    python_code: str = dspy.OutputField(desc=\"Revised code: only def transform(grid): ...\")\n",
        "\n",
        "class DirectSolveSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    If code induction fails, directly produce outputs for test_inputs using a consistent rule induced from training.\n",
        "    Requirements:\n",
        "    - Return only a Python literal list of output grids aligned with test_inputs (e.g., [[[...],[...]], [[...], ...]]).\n",
        "    - Do not include any commentary, bullets, or extra text.\n",
        "    - Output ints in 0..9 only. Shapes may differ from inputs; match the pattern demonstrated in training pairs.\n",
        "    - Apply the same inferred rule consistently across all test inputs.\n",
        "    \"\"\"\n",
        "    training_examples: List[TrainingExample] = dspy.InputField(desc=\"Training pairs for the task.\")\n",
        "    test_inputs: List[MATRIX] = dspy.InputField(desc=\"Unseen inputs to solve.\")\n",
        "    test_outputs: List[MATRIX] = dspy.OutputField(desc=\"Predicted outputs for each test input.\")\n",
        "\n",
        "class ARCRuleProgram(dspy.Module):\n",
        "    def __init__(self, max_refinements: int = 4):\n",
        "        super().__init__()\n",
        "        self.induce = dspy.ChainOfThought(InduceRuleSignature)\n",
        "        self.refine = dspy.ChainOfThought(RefineRuleSignature)\n",
        "        self.direct = dspy.ChainOfThought(DirectSolveSignature)\n",
        "        self.max_refinements = max_refinements\n",
        "\n",
        "    def forward(self, training_examples: List[TrainingExample], test_inputs: List[MATRIX]) -> dspy.Prediction:\n",
        "        training_summary = _build_training_summary(training_examples)\n",
        "\n",
        "        # Step 1: Induce rule and code\n",
        "        draft = self.induce(training_examples=training_examples, training_summary=training_summary)\n",
        "        code = draft.python_code or \"\"\n",
        "        code = _strip_code_fences(code)\n",
        "\n",
        "        # Step 2: Verify and refine iteratively\n",
        "        for _ in range(self.max_refinements + 1):\n",
        "            fn = _safe_exec_transform(code)\n",
        "            if fn is None:\n",
        "                failure_report = \"Could not exec transform function. Ensure a valid def transform(grid): ... exists and no imports.\"\n",
        "            else:\n",
        "                ok, report = _verify_on_training(fn, training_examples)\n",
        "                if ok:\n",
        "                    # Step 3: Apply to tests\n",
        "                    outputs = _apply_code_to_tests(code, test_inputs)\n",
        "                    if outputs:\n",
        "                        return dspy.Prediction(test_outputs=outputs)\n",
        "                    failure_report = \"Execution on test inputs failed or returned invalid outputs.\"\n",
        "                else:\n",
        "                    failure_report = report\n",
        "            # Attempt refinement\n",
        "            refined = self.refine(\n",
        "                training_examples=training_examples,\n",
        "                training_summary=training_summary,\n",
        "                prior_code=code,\n",
        "                failure_report=failure_report\n",
        "            )\n",
        "            new_code = _strip_code_fences(refined.python_code or \"\")\n",
        "            if not new_code or new_code.strip() == code.strip():\n",
        "                code = new_code or code\n",
        "                break\n",
        "            code = new_code\n",
        "\n",
        "        # Fallback: direct solve via LM\n",
        "        direct = self.direct(training_examples=training_examples, test_inputs=test_inputs)\n",
        "        safe_outs: List[MATRIX] = []\n",
        "        if isinstance(direct.test_outputs, list):\n",
        "            for mat in direct.test_outputs:\n",
        "                try:\n",
        "                    safe_outs.append(_normalize_matrix(mat))\n",
        "                except Exception:\n",
        "                    # Skip invalid entries\n",
        "                    pass\n",
        "        return dspy.Prediction(test_outputs=safe_outs)\n",
        "\n",
        "class SolveTaskSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    Solve ARC tasks using rule induction with verifiable code and robust fallback.\n",
        "    Inputs:\n",
        "      - training_examples: list of TrainingExample(input, output)\n",
        "      - test_inputs: list of matrices to solve\n",
        "    Output:\n",
        "      - test_outputs: list of predicted output matrices\n",
        "    \"\"\"\n",
        "    training_examples: List[TrainingExample] = dspy.InputField(desc=\"Input/output examples demonstrating the task to be performed.\")\n",
        "    test_inputs: List[MATRIX] = dspy.InputField(desc=\"Input matrices to be solved following the task described in the training examples.\")\n",
        "    test_outputs: List[MATRIX] = dspy.OutputField(desc=\"Output matrices corresponding to the test inputs.\")\n",
        "\n",
        "class SolveARC(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.core = ARCRuleProgram()\n",
        "\n",
        "    def forward(self, training_examples: List[TrainingExample], test_inputs: List[MATRIX]) -> dspy.Prediction:\n",
        "        return self.core(training_examples=training_examples, test_inputs=test_inputs)\n",
        "\n",
        "program = SolveARC()\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "arc-takeaway",
      "metadata": {},
      "source": [
        "### Key Takeaway: Emergent Self-Refinement\n",
        "\n",
        "GEPA discovered **self-refinement**—having the LLM validate and fix its own code before producing outputs. This is remarkable because:\n",
        "\n",
        "1. **Not programmed**: Self-refinement emerged from optimization, not from human design\n",
        "2. **Sophisticated strategy**: The agent now verifies on training before applying to test\n",
        "3. **Multi-attempt recovery**: Up to 5 refinement attempts with targeted feedback\n",
        "4. **Code synthesis**: Instead of direct prediction, the agent writes executable code\n",
        "\n",
        "This demonstrates GEPA's ability to discover **complex reasoning pipelines** that humans might not think to design."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-6",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-7\"></a>\n",
        "# 7. Example 4: Algorithmic Discovery — Circle Packing\n",
        "\n",
        "**Result: GEPA matches or exceeds AlphaEvolve, ShinkaEvolve, and OpenEvolve on circle packing.**\n",
        "\n",
        "This example demonstrates **algorithmic discovery**—evolving code to solve a well-known NP-hard optimization problem.\n",
        "\n",
        "## The Challenge\n",
        "\n",
        "Circle packing is a classic problem with real-world applications (chip layout, material cutting, logistics):\n",
        "- Pack N non-overlapping circles inside a unit square [0,1] × [0,1]\n",
        "- Maximize the sum of all radii\n",
        "- This is **NP-hard**—no known polynomial-time algorithm exists\n",
        "\n",
        "Recent work from DeepMind (AlphaEvolve), and open-source efforts (ShinkaEvolve, OpenEvolve) have used LLMs to evolve packing algorithms.\n",
        "\n",
        "## The Task\n",
        "\n",
        "**Given**: The number of circles N (e.g., N=26)\n",
        "**Find**: Python code that computes optimal circle placements\n",
        "\n",
        "**What GEPA optimizes**: The packing algorithm code—placement strategies, local optimization, constraint handling.\n",
        "\n",
        "<img src=\"./assets/blog/circle_packing_26.png\">\n",
        "\n",
        "<img src=\"./assets/blog/circle_packing_visualization_26.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "circle-setup",
      "metadata": {},
      "source": [
        "## Setting Up the Problem\n",
        "\n",
        "Circle packing is a single-instance optimization problem—no dataset needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "circle-setup-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_circles = 26\n",
        "objective = f\"Optimize circle packing code and refiner prompt to maximize sum of circle radii within a unit square for N={num_circles} circles.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "circle-seed",
      "metadata": {},
      "source": [
        "## The Seed Candidate\n",
        "\n",
        "The same one from ShinkaEvolve. \n",
        "A simple concentric ring layout (1 center circle + 8 inner ring + remaining outer ring)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "circle-seed-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from examples.circle_packing.llms import SEED_REFINEMENT_PROMPT\n",
        "\n",
        "\n",
        "seed_candidate = {\n",
        "    \"code\": '''\n",
        "import numpy as np\n",
        "\n",
        "def main(timeout, current_best_solution):\n",
        "    \"\"\"\n",
        "    Circle packing optimization.\n",
        "\n",
        "    Args:\n",
        "        timeout: Time budget in seconds\n",
        "        current_best_solution: Previous best circles array (n, 3) or None\n",
        "\n",
        "    Returns:\n",
        "        dict with 'circles' (n, 3) array and 'all_scores' list\n",
        "    \"\"\"\n",
        "    n = 26\n",
        "\n",
        "    # Use current_best_solution if provided, otherwise start fresh\n",
        "    if current_best_solution is not None:\n",
        "        circles = current_best_solution.copy()\n",
        "    else:\n",
        "        # Simple initial placement\n",
        "        centers = np.zeros((n, 2))\n",
        "\n",
        "        # Center circle\n",
        "        centers[0] = [0.5, 0.5]\n",
        "\n",
        "        # Ring of 8 around center\n",
        "        for i in range(min(8, n - 1)):\n",
        "            angle = 2 * np.pi * i / 8\n",
        "            centers[i + 1] = [0.5 + 0.3 * np.cos(angle), 0.5 + 0.3 * np.sin(angle)]\n",
        "\n",
        "        # Outer ring for remaining\n",
        "        if n > 9:\n",
        "            remaining = n - 9\n",
        "            for i in range(remaining):\n",
        "                angle = 2 * np.pi * i / remaining\n",
        "                centers[i + 9] = [0.5 + 0.7 * np.cos(angle), 0.5 + 0.7 * np.sin(angle)]\n",
        "\n",
        "        centers = np.clip(centers, 0.01, 0.99)\n",
        "        radii = compute_max_radii(centers)\n",
        "        circles = np.hstack([centers, radii.reshape(-1, 1)])\n",
        "\n",
        "    score = float(np.sum(circles[:, 2]))\n",
        "    return {'circles': circles, 'all_scores': [score]}\n",
        "\n",
        "\n",
        "def compute_max_radii(centers):\n",
        "    \"\"\"Compute maximum radii that don't overlap and stay in unit square.\"\"\"\n",
        "    n = centers.shape[0]\n",
        "    radii = np.ones(n)\n",
        "\n",
        "    # Limit by distance to borders\n",
        "    for i in range(n):\n",
        "        x, y = centers[i]\n",
        "        radii[i] = min(x, y, 1 - x, 1 - y)\n",
        "\n",
        "    # Limit by distance to other circles\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            dist = np.sqrt(np.sum((centers[i] - centers[j]) ** 2))\n",
        "            if radii[i] + radii[j] > dist:\n",
        "                scale = dist / (radii[i] + radii[j])\n",
        "                radii[i] *= scale\n",
        "                radii[j] *= scale\n",
        "\n",
        "    return radii\n",
        "''', \n",
        "    \"refiner_prompt\": SEED_REFINEMENT_PROMPT,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc2b80e",
      "metadata": {},
      "source": [
        "### Refiner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2e06943a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import dspy\n",
        "import os\n",
        "\n",
        "class RefinerSignature(dspy.Signature):\n",
        "    \"\"\"Refine the code based on its evaluation results by fixing the errors and improving the performance.\"\"\"\n",
        "\n",
        "    refiner_prompt = dspy.InputField(desc=\"Instructions for how to refine the code\")\n",
        "    code_to_improve = dspy.InputField(desc=\"Code to improve\")\n",
        "    code_results = dspy.InputField(\n",
        "        desc=\"Evaluation results of the code to improve by fixing the errors and improving the performance\"\n",
        "    )\n",
        "    refined_code = dspy.OutputField(\n",
        "        desc=\"Next iteration of improved code based on the evaluation results\"\n",
        "    )\n",
        "\n",
        "refiner_predictor = dspy.Predict(RefinerSignature)\n",
        "\n",
        "refiner_lm = dspy.LM(\n",
        "    \"openai/gpt-5.1\",\n",
        "    temperature=1.0,\n",
        "    max_tokens=32000,\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        "    cache=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "circle-fitness",
      "metadata": {},
      "source": [
        "## The Fitness Function\n",
        "\n",
        "The fitness function validates constraints and returns detailed violation information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "circle-fitness-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from examples.circle_packing.utils import execute_code\n",
        "from examples.circle_packing.main import refine_code, StateTracker\n",
        "\n",
        "state_tracker = StateTracker()\n",
        "timeout = 600\n",
        "\n",
        "def compute_multiple_metrics(\n",
        "    global_best_score: float, all_scores: list[float]\n",
        ") -> dict[str, float]:\n",
        "    candidate_best_score = max(all_scores)\n",
        "    alpha_fixed = 0.1\n",
        "    ema_fixed = all_scores[0]\n",
        "    for s in all_scores[1:]:\n",
        "        ema_fixed = alpha_fixed * s + (1 - alpha_fixed) * ema_fixed\n",
        "    alpha_adaptive = 2.0 / (len(all_scores) + 1)\n",
        "    ema_adaptive = all_scores[0]\n",
        "    for s in all_scores[1:]:\n",
        "        ema_adaptive = alpha_adaptive * s + (1 - alpha_adaptive) * ema_adaptive\n",
        "\n",
        "    return {\n",
        "        \"max_score\": max(all_scores),\n",
        "        \"mean_score\": sum(all_scores) / len(all_scores),\n",
        "        \"ema_score_fixed\": ema_fixed,\n",
        "        \"ema_score_adaptive\": ema_adaptive,\n",
        "        \"score_improvement_from_previous_best\": candidate_best_score\n",
        "        - global_best_score,\n",
        "    }\n",
        "\n",
        "def fitness_fn(candidate: dict[str, str], *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Evaluate code candidate on batch of problems with optional refinement.\n",
        "    \"\"\"\n",
        "    code_candidate = candidate[\"code\"]\n",
        "\n",
        "    # Code candidate evaluation\n",
        "    global_best_score, global_best_solution = state_tracker.get_best_solution()\n",
        "    cache_key = code_candidate\n",
        "    code_candidate_cache = state_tracker.get(cache_key)\n",
        "\n",
        "    if code_candidate_cache is not None:\n",
        "        code_score, code_side_info = code_candidate_cache\n",
        "    else:\n",
        "        code_result = execute_code(code_candidate, timeout, global_best_solution)\n",
        "        circles = None\n",
        "\n",
        "        if code_result[\"success\"]:\n",
        "            circles = code_result[\"result\"][\"circles\"]\n",
        "            all_scores = code_result[\"result\"][\"all_scores\"]\n",
        "            code_score = code_result[\"result\"][\"validation_details\"][\"sum_radii\"]\n",
        "            code_side_info = {\n",
        "                \"scores\": compute_multiple_metrics(global_best_score, all_scores),\n",
        "                \"Code\": code_candidate,\n",
        "                \"Circles\": circles,\n",
        "                \"Global best circles at the time of evaluation\": global_best_solution,\n",
        "                \"Stdout\": code_result[\"stdout\"],\n",
        "            }\n",
        "        else:\n",
        "            code_score = 0.0\n",
        "            code_side_info = {\n",
        "                \"scores\": {\"sum_radii\": 0.0},\n",
        "                \"Code\": code_candidate,\n",
        "                \"Error\": code_result[\"error\"],\n",
        "                \"Traceback\": code_result.get(\"traceback\", \"\"),\n",
        "                \"Stdout\": code_result[\"stdout\"],\n",
        "                \"Validation Details\": code_result.get(\"validation_details\"),\n",
        "            }\n",
        "\n",
        "        # Cache after computing values\n",
        "        state_tracker.set(\n",
        "            cache_key,\n",
        "            (code_score, code_side_info),\n",
        "            score=code_score,\n",
        "            solution=circles,\n",
        "            artifact={\n",
        "                \"code\": code_candidate,\n",
        "                \"arg_current_best_solution\": global_best_solution,\n",
        "                \"validation details\": code_result.get(\"validation_details\"),\n",
        "            },\n",
        "        )\n",
        "\n",
        "    print(\"Code candidate side info:\")\n",
        "    print(code_side_info)\n",
        "\n",
        "    # Refiner prompt evaluation\n",
        "    # Now that we've got the code's results, we can set a cache key as (prompt, code, best_solution)\n",
        "    # the refiner will receive the code, the\n",
        "    print(\"Refining code...\")\n",
        "\n",
        "    refiner_prompt_candidate = candidate[\"refiner_prompt\"]\n",
        "    global_best_score, global_best_solution = state_tracker.get_best_solution()\n",
        "\n",
        "    # Refine code for this problem\n",
        "    (\n",
        "        refiner_score,\n",
        "        refiner_code,\n",
        "        refiner_side_info,\n",
        "    ) = refine_code(\n",
        "        code=code_candidate,\n",
        "        code_score=code_score,\n",
        "        code_side_info=code_side_info,\n",
        "        refiner_prompt=refiner_prompt_candidate,\n",
        "        refiner_predictor=refiner_predictor,\n",
        "        refiner_lm=refiner_lm,\n",
        "        timeout=timeout,\n",
        "        state_tracker=state_tracker,\n",
        "    )\n",
        "\n",
        "    if refiner_score > code_score:\n",
        "        best_score = refiner_score\n",
        "        best_code = refiner_code\n",
        "        best_circles = refiner_side_info.get(\"Circles\", None)\n",
        "    else:\n",
        "        best_score = code_score\n",
        "        best_code = code_candidate\n",
        "        best_circles = code_side_info.get(\"Circles\", None)\n",
        "\n",
        "    if best_circles is not None:\n",
        "        best_circles = best_circles.tolist()\n",
        "\n",
        "    output = {\n",
        "        \"best_score\": best_score,\n",
        "        \"best_code\": best_code,\n",
        "        \"best_circles\": best_circles,\n",
        "        \"code_candidate\": code_candidate,\n",
        "        \"code_score\": code_score,\n",
        "        \"refiner_prompt\": refiner_prompt_candidate,\n",
        "        \"refiner_code\": refiner_code,\n",
        "        \"refiner_score\": refiner_score,\n",
        "    }\n",
        "\n",
        "    side_info = {\n",
        "        \"scores\": {\n",
        "            \"best_score_from_code_and_refiner\": max(code_score, refiner_score),\n",
        "            \"initial_code\": code_score,\n",
        "            \"refiner_prompt\": refiner_score,\n",
        "        },\n",
        "        \"Input\": {\n",
        "            \"Timeout (s)\": timeout,\n",
        "        },\n",
        "        \"code_specific_info\": code_side_info,\n",
        "        \"refiner_prompt_specific_info\": refiner_side_info,\n",
        "    }\n",
        "\n",
        "    return (best_score, output, side_info)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "circle-optimize",
      "metadata": {},
      "source": [
        "## Running GEPA Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "circle-optimize-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gepa.optimize_anything import (\n",
        "    EngineConfig,\n",
        "    GEPAConfig,\n",
        "    ReflectionConfig,\n",
        "    optimize_anything,\n",
        ")\n",
        "from examples.circle_packing.llms import CIRCLE_PACKING_BACKGROUND, SEED_REFINEMENT_PROMPT\n",
        "\n",
        "\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    objective=objective,\n",
        "    background=CIRCLE_PACKING_BACKGROUND,\n",
        "    config=GEPAConfig(\n",
        "        engine=EngineConfig(\n",
        "            max_metric_calls=200,\n",
        "            track_best_outputs=True,\n",
        "            frontier_type=\"objective\",\n",
        "            cache_evaluation=True,\n",
        "        ),\n",
        "        reflection=ReflectionConfig(\n",
        "            reflection_lm=\"openai/gpt-5\",\n",
        "            reflection_minibatch_size=1,\n",
        "        ),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f9ad7f",
      "metadata": {},
      "source": [
        "## Best Program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba58f0c",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.venv (3.12.3) (Python 3.12.3)' requires the ipykernel package.\n",
            "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/data/lukedhlee/gepa_luke/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def main(timeout, current_best_solution):\n",
        "    \"\"\"\n",
        "    BREAKTHROUGH approach: Large-Neighborhood Sequential Linear Programming (LN-SLP)\n",
        "    with exact LP for radii. We optimize centers by repeatedly solving LPs over\n",
        "    subsets of centers with linearized pairwise constraints and boundary constraints.\n",
        "    Multi-start + global SLP + LNS refinement + dual-informed subset selection.\n",
        "    \"\"\"\n",
        "    n = 26\n",
        "    start_time = time.time()\n",
        "    time_budget = max(1.0, float(timeout) - 0.5)\n",
        "    rng = np.random.default_rng(2026)\n",
        "    eps = 1e-6\n",
        "    r_min = 1e-8\n",
        "    all_scores = []\n",
        "\n",
        "    def time_left():\n",
        "        return time_budget - (time.time() - start_time)\n",
        "\n",
        "    def clamp01(xy):\n",
        "        return np.clip(xy, eps, 1.0 - eps)\n",
        "\n",
        "    def boundary_limits(centers):\n",
        "        x = centers[:, 0]\n",
        "        y = centers[:, 1]\n",
        "        return np.minimum(np.minimum(x, 1 - x), np.minimum(y, 1 - y))\n",
        "\n",
        "    def pairwise_distances(centers):\n",
        "        diff = centers[:, None, :] - centers[None, :, :]\n",
        "        D = np.sqrt(np.maximum(np.sum(diff * diff, axis=2), 0.0))\n",
        "        return D\n",
        "\n",
        "    def solve_radii_lp_fallback(centers, r_min_local):\n",
        "        # Projection-like repair (feasible radii)\n",
        "        b = boundary_limits(centers)\n",
        "        r = np.minimum(b, 0.5).copy()\n",
        "        r = np.maximum(r, r_min_local)\n",
        "        D = pairwise_distances(centers)\n",
        "        max_iter = 1500\n",
        "        tol = 1e-10\n",
        "        for _ in range(max_iter):\n",
        "            r = np.minimum(r, b)\n",
        "            viol = 0.0\n",
        "            for i in range(n):\n",
        "                ri = r[i]\n",
        "                for j in range(i + 1, n):\n",
        "                    dij = D[i, j]\n",
        "                    if dij <= 0:\n",
        "                        r[j] = r_min_local\n",
        "                        continue\n",
        "                    s = ri + r[j]\n",
        "                    if s > dij:\n",
        "                        excess = s - dij\n",
        "                        viol = max(viol, excess)\n",
        "                        total = ri + r[j]\n",
        "                        if total <= 1e-16:\n",
        "                            dec_i = dec_j = 0.5 * excess\n",
        "                        else:\n",
        "                            dec_i = excess * (ri / total)\n",
        "                            dec_j = excess * (r[j] / total)\n",
        "                        ri = max(r_min_local, ri - dec_i)\n",
        "                        r[j] = max(r_min_local, r[j] - dec_j)\n",
        "                r[i] = ri\n",
        "            if viol < tol:\n",
        "                break\n",
        "        r = np.minimum(r, b)\n",
        "        r = np.maximum(r, r_min_local)\n",
        "        return r, True, None\n",
        "\n",
        "    def solve_radii_lp(centers, r_min_local):\n",
        "        try:\n",
        "            from scipy.optimize import linprog\n",
        "            x = centers[:, 0]; y = centers[:, 1]\n",
        "            D = pairwise_distances(centers)\n",
        "            A_rows = []\n",
        "            b_vals = []\n",
        "            # Boundary constraints\n",
        "            for i in range(n):\n",
        "                row = np.zeros(n); row[i] = 1.0; A_rows.append(row); b_vals.append(x[i])\n",
        "                row = np.zeros(n); row[i] = 1.0; A_rows.append(row); b_vals.append(1.0 - x[i])\n",
        "                row = np.zeros(n); row[i] = 1.0; A_rows.append(row); b_vals.append(y[i])\n",
        "                row = np.zeros(n); row[i] = 1.0; A_rows.append(row); b_vals.append(1.0 - y[i])\n",
        "            # Pairwise constraints\n",
        "            for i in range(n):\n",
        "                for j in range(i + 1, n):\n",
        "                    row = np.zeros(n); row[i] = 1.0; row[j] = 1.0\n",
        "                    A_rows.append(row); b_vals.append(D[i, j])\n",
        "            A_ub = np.array(A_rows, dtype=float) if A_rows else None\n",
        "            b_ub = np.array(b_vals, dtype=float) if b_vals else None\n",
        "            bounds = [(r_min_local, None)] * n\n",
        "            c = -np.ones(n, dtype=float)\n",
        "            res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n",
        "            if getattr(res, \"success\", False) and res.x is not None:\n",
        "                r = np.array(res.x, dtype=float)\n",
        "                b = boundary_limits(centers)\n",
        "                r = np.minimum(r, b)\n",
        "                r = np.maximum(r, r_min_local)\n",
        "                # Try to extract duals for guidance\n",
        "                weights = None\n",
        "                try:\n",
        "                    ine = getattr(res, \"ineqlin\", None)\n",
        "                    if ine is not None and hasattr(ine, \"marginals\"):\n",
        "                        y_dual = np.array(ine.marginals, dtype=float)\n",
        "                        weights = -y_dual\n",
        "                        weights = np.maximum(weights, 0.0)\n",
        "                except Exception:\n",
        "                    weights = None\n",
        "                return r, True, weights\n",
        "            else:\n",
        "                return solve_radii_lp_fallback(centers, r_min_local)\n",
        "        except Exception:\n",
        "            return solve_radii_lp_fallback(centers, r_min_local)\n",
        "\n",
        "    def build_slp_subset_lp(centers, subset_idx, trust):\n",
        "        try:\n",
        "            from scipy.optimize import linprog  # noqa: F401\n",
        "        except Exception:\n",
        "            return None\n",
        "        m = len(subset_idx)\n",
        "        var_dim = 2 * m + n\n",
        "        idx_map = {subset_idx[k]: k for k in range(m)}\n",
        "        A_rows = []\n",
        "        b_vals = []\n",
        "\n",
        "        x = centers[:, 0]; y = centers[:, 1]\n",
        "        # Boundary constraints\n",
        "        for i in range(n):\n",
        "            # r_i - dx_i <= x_i\n",
        "            row = np.zeros(var_dim)\n",
        "            row[2 * m + i] = 1.0\n",
        "            if i in idx_map:\n",
        "                k = idx_map[i]\n",
        "                row[2 * k + 0] = -1.0\n",
        "            A_rows.append(row); b_vals.append(x[i])\n",
        "            # r_i + dx_i <= 1 - x_i\n",
        "            row = np.zeros(var_dim)\n",
        "            row[2 * m + i] = 1.0\n",
        "            if i in idx_map:\n",
        "                k = idx_map[i]\n",
        "                row[2 * k + 0] = 1.0\n",
        "            A_rows.append(row); b_vals.append(1.0 - x[i])\n",
        "            # r_i - dy_i <= y_i\n",
        "            row = np.zeros(var_dim)\n",
        "            row[2 * m + i] = 1.0\n",
        "            if i in idx_map:\n",
        "                k = idx_map[i]\n",
        "                row[2 * k + 1] = -1.0\n",
        "            A_rows.append(row); b_vals.append(y[i])\n",
        "            # r_i + dy_i <= 1 - y_i\n",
        "            row = np.zeros(var_dim)\n",
        "            row[2 * m + i] = 1.0\n",
        "            if i in idx_map:\n",
        "                k = idx_map[i]\n",
        "                row[2 * k + 1] = 1.0\n",
        "            A_rows.append(row); b_vals.append(1.0 - y[i])\n",
        "\n",
        "        # Pairwise linearization at current centers\n",
        "        diff = centers[:, None, :] - centers[None, :, :]\n",
        "        Dij = np.sqrt(np.maximum(np.sum(diff * diff, axis=2), 0.0))\n",
        "        for i in range(n):\n",
        "            for j in range(i + 1, n):\n",
        "                dij = Dij[i, j]\n",
        "                if dij <= 1e-12:\n",
        "                    ang = rng.uniform(0, 2 * np.pi)\n",
        "                    ux, uy = np.cos(ang), np.sin(ang)\n",
        "                else:\n",
        "                    u = diff[i, j] / dij\n",
        "                    ux, uy = float(u[0]), float(u[1])\n",
        "                row = np.zeros(var_dim)\n",
        "                if i in idx_map:\n",
        "                    ki = idx_map[i]\n",
        "                    row[2 * ki + 0] += -ux\n",
        "                    row[2 * ki + 1] += -uy\n",
        "                if j in idx_map:\n",
        "                    kj = idx_map[j]\n",
        "                    row[2 * kj + 0] += ux\n",
        "                    row[2 * kj + 1] += uy\n",
        "                row[2 * m + i] += 1.0\n",
        "                row[2 * m + j] += 1.0\n",
        "                A_rows.append(row); b_vals.append(dij)\n",
        "\n",
        "        A_ub = np.array(A_rows, dtype=float) if A_rows else None\n",
        "        b_ub = np.array(b_vals, dtype=float) if b_vals else None\n",
        "\n",
        "        bounds = []\n",
        "        # Displacement bounds (trust region and box)\n",
        "        for k, i in enumerate(subset_idx):\n",
        "            xi, yi = centers[i]\n",
        "            dx_lo = max(-trust, eps - xi)\n",
        "            dx_hi = min(trust, 1.0 - eps - xi)\n",
        "            dy_lo = max(-trust, eps - yi)\n",
        "            dy_hi = min(trust, 1.0 - eps - yi)\n",
        "            bounds.append((dx_lo, dx_hi))\n",
        "            bounds.append((dy_lo, dy_hi))\n",
        "        # Radii bounds\n",
        "        for _ in range(n):\n",
        "            bounds.append((r_min, None))\n",
        "\n",
        "        c = np.zeros(var_dim, dtype=float)\n",
        "        c[2 * m:] = -1.0\n",
        "        return c, A_ub, b_ub, bounds\n",
        "\n",
        "    def slp_subset_step(centers, subset_idx, trust):\n",
        "        try:\n",
        "            from scipy.optimize import linprog\n",
        "        except Exception:\n",
        "            return centers, None, False\n",
        "        built = build_slp_subset_lp(centers, subset_idx, trust)\n",
        "        if built is None:\n",
        "            return centers, None, False\n",
        "        c, A_ub, b_ub, bounds = built\n",
        "        try:\n",
        "            res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n",
        "            if not (getattr(res, \"success\", False) and res.x is not None):\n",
        "                return centers, None, False\n",
        "            m = len(subset_idx)\n",
        "            sol = np.array(res.x, dtype=float)\n",
        "            dxy = sol[:2 * m].reshape(m, 2)\n",
        "            r_lin = sol[2 * m:]\n",
        "            centers_new = centers.copy()\n",
        "            for idx_local, i in enumerate(subset_idx):\n",
        "                centers_new[i] = centers_new[i] + dxy[idx_local]\n",
        "            centers_new = clamp01(centers_new)\n",
        "            return centers_new, r_lin, True\n",
        "        except Exception:\n",
        "            return centers, None, False\n",
        "\n",
        "    def refine_by_slp_iterative(centers_init, max_outer=8, init_trust=0.08):\n",
        "        centers = centers_init.copy()\n",
        "        r, ok, _ = solve_radii_lp(centers, r_min)\n",
        "        if not ok:\n",
        "            return centers, r, False\n",
        "        best_score = float(np.sum(r))\n",
        "        best_centers = centers.copy()\n",
        "        trust = init_trust\n",
        "        for _ in range(max_outer):\n",
        "            if time_left() < 0.15:\n",
        "                break\n",
        "            subset_idx = list(range(n))\n",
        "            centers_cand, _, ok_step = slp_subset_step(centers, subset_idx, trust)\n",
        "            if not ok_step:\n",
        "                trust *= 0.7\n",
        "                if trust < 1e-3:\n",
        "                    break\n",
        "                continue\n",
        "            r_rep, ok2, _ = solve_radii_lp(centers_cand, r_min)\n",
        "            if ok2:\n",
        "                sc = float(np.sum(r_rep))\n",
        "                if sc > best_score + 1e-10:\n",
        "                    centers = centers_cand\n",
        "                    r = r_rep\n",
        "                    best_score = sc\n",
        "                    best_centers = centers.copy()\n",
        "                    trust = min(trust * 1.3, 0.2)\n",
        "                else:\n",
        "                    trust *= 0.6\n",
        "                    if trust < 1e-3:\n",
        "                        break\n",
        "            else:\n",
        "                trust *= 0.6\n",
        "                if trust < 1e-3:\n",
        "                    break\n",
        "        return best_centers, r, True\n",
        "\n",
        "    def dual_inform_weights(centers, r, duals):\n",
        "        w = np.zeros(n, dtype=float)\n",
        "        if duals is not None:\n",
        "            num_boundary = 4 * n\n",
        "            if duals.shape[0] >= num_boundary:\n",
        "                db = duals[:num_boundary].reshape(n, 4)\n",
        "                w += np.sum(db, axis=1)\n",
        "                dp = duals[num_boundary:]\n",
        "                idx = 0\n",
        "                for i in range(n):\n",
        "                    for j in range(i + 1, n):\n",
        "                        if idx >= dp.shape[0]:\n",
        "                            break\n",
        "                        val = dp[idx]\n",
        "                        w[i] += val\n",
        "                        w[j] += val\n",
        "                        idx += 1\n",
        "        with np.errstate(divide='ignore'):\n",
        "            inv_r = 1.0 / np.maximum(r, 1e-6)\n",
        "        w += 0.2 * inv_r\n",
        "        s = np.sum(w)\n",
        "        if s <= 0:\n",
        "            w = np.ones(n) / n\n",
        "        else:\n",
        "            w = w / s\n",
        "        return w\n",
        "\n",
        "    def init_hex_grid():\n",
        "        K = n\n",
        "        cols = int(np.ceil(np.sqrt(K)))\n",
        "        rows = int(np.ceil(K / cols))\n",
        "        xs = np.linspace(0.1, 0.9, cols)\n",
        "        ys = np.linspace(0.1, 0.9, rows)\n",
        "        pts = []\n",
        "        cnt = 0\n",
        "        for r_idx, yy in enumerate(ys):\n",
        "            off = 0.0 if (r_idx % 2 == 0) else (xs[1] - xs[0]) * 0.5 if len(xs) > 1 else 0.0\n",
        "            for xx in xs:\n",
        "                if cnt >= K:\n",
        "                    break\n",
        "                x = np.clip(xx + off, 0.06, 0.94)\n",
        "                x += rng.normal(0, 0.01)\n",
        "                y = yy + rng.normal(0, 0.01)\n",
        "                pts.append([x, y])\n",
        "                cnt += 1\n",
        "            if cnt >= K:\n",
        "                break\n",
        "        pts = np.array(pts, dtype=float)\n",
        "        while pts.shape[0] < K:\n",
        "            pts = np.vstack([pts, rng.uniform(0.15, 0.85, size=(1, 2))])\n",
        "        return clamp01(pts[:n])\n",
        "\n",
        "    def init_edges():\n",
        "        pts = []\n",
        "        e = 1e-3\n",
        "        corners = [[e, e], [1 - e, e], [e, 1 - e], [1 - e, 1 - e]]\n",
        "        pts.extend(corners)\n",
        "        ts = np.linspace(0.15, 0.85, 6)\n",
        "        for t in ts:\n",
        "            pts.append([t, e]); pts.append([t, 1 - e])\n",
        "            pts.append([e, t]); pts.append([1 - e, t])\n",
        "        pts = np.array(pts[:n], dtype=float)\n",
        "        if pts.shape[0] < n:\n",
        "            while pts.shape[0] < n:\n",
        "                edge = rng.integers(0, 4)\n",
        "                t = rng.uniform(0.08, 0.92)\n",
        "                if edge == 0: pts = np.vstack([pts, [t, e]])\n",
        "                elif edge == 1: pts = np.vstack([pts, [t, 1 - e]])\n",
        "                elif edge == 2: pts = np.vstack([pts, [e, t]])\n",
        "                else: pts = np.vstack([pts, [1 - e, t]])\n",
        "        return clamp01(pts[:n])\n",
        "\n",
        "    def init_center_biased():\n",
        "        pts = 0.75 * rng.uniform(0.05, 0.95, size=(n, 2)) + 0.25 * 0.5\n",
        "        pts += rng.normal(0, 0.03, size=(n, 2))\n",
        "        return clamp01(pts)\n",
        "\n",
        "    def init_uniform():\n",
        "        return clamp01(rng.uniform(0.1, 0.9, size=(n, 2)))\n",
        "\n",
        "    def evaluate_centers(centers):\n",
        "        centers = clamp01(centers)\n",
        "        r, ok, duals = solve_radii_lp(centers, r_min)\n",
        "        if not ok or r is None:\n",
        "            b = boundary_limits(centers)\n",
        "            r = np.maximum(np.minimum(b, 0.5), r_min)\n",
        "            duals = None\n",
        "        return np.hstack([centers, r.reshape(-1, 1)]), float(np.sum(r)), duals\n",
        "\n",
        "    # Seed centers\n",
        "    seed_centers_list = []\n",
        "    if isinstance(current_best_solution, np.ndarray):\n",
        "        try:\n",
        "            cb = current_best_solution\n",
        "            if cb.shape[0] == n and cb.shape[1] >= 2:\n",
        "                centers_cb = clamp01(cb[:, :2].astype(float))\n",
        "                seed_centers_list.append(centers_cb)\n",
        "                for _ in range(2):\n",
        "                    seed_centers_list.append(clamp01(centers_cb + rng.normal(0, 0.01, size=centers_cb.shape)))\n",
        "        except Exception:\n",
        "            pass\n",
        "    seed_centers_list.append(init_hex_grid())\n",
        "    seed_centers_list.append(init_edges())\n",
        "    seed_centers_list.append(init_center_biased())\n",
        "    seed_centers_list.append(init_uniform())\n",
        "    seed_centers_list.append(clamp01(init_hex_grid() + rng.normal(0, 0.02, size=(n, 2))))\n",
        "\n",
        "    # Deduplicate seeds with slight jitter on collisions\n",
        "    unique_seeds = []\n",
        "    seen = set()\n",
        "    for c in seed_centers_list:\n",
        "        h = tuple(np.round(c.ravel(), 3))\n",
        "        if h in seen:\n",
        "            c = clamp01(c + rng.normal(0, 0.005, size=c.shape))\n",
        "        unique_seeds.append(c)\n",
        "        seen.add(h)\n",
        "    seed_centers_list = unique_seeds\n",
        "\n",
        "    best_circles = None\n",
        "    best_score = -1e18\n",
        "    best_duals = None\n",
        "\n",
        "    # Initial evaluation of seeds\n",
        "    for centers in seed_centers_list:\n",
        "        circles, score, duals = evaluate_centers(centers)\n",
        "        if score > best_score + 1e-12:\n",
        "            best_score = score\n",
        "            best_circles = circles\n",
        "            best_duals = duals\n",
        "            all_scores.append(best_score)\n",
        "\n",
        "    # Global SLP refine on seeds\n",
        "    per_seed_time = max(0.4, time_left() / max(1, len(seed_centers_list)))\n",
        "    for centers0 in seed_centers_list:\n",
        "        if time_left() < 0.4:\n",
        "            break\n",
        "        t_seed_start = time.time()\n",
        "\n",
        "        def time_left_seed():\n",
        "            return min(time_left(), per_seed_time - (time.time() - t_seed_start))\n",
        "\n",
        "        if time_left_seed() < 0.2:\n",
        "            continue\n",
        "        centers_ref, r_ref, _ = refine_by_slp_iterative(centers0, max_outer=8, init_trust=0.08)\n",
        "        circles, score, duals = evaluate_centers(centers_ref)\n",
        "        if score > best_score + 1e-12:\n",
        "            best_score = score\n",
        "            best_circles = circles\n",
        "            best_duals = duals\n",
        "            all_scores.append(best_score)\n",
        "\n",
        "    # LNS loop\n",
        "    if best_circles is None:\n",
        "        centers = init_edges()\n",
        "        r, _, _ = solve_radii_lp(centers, r_min)\n",
        "        best_circles = np.hstack([centers, r.reshape(-1, 1)])\n",
        "        best_score = float(np.sum(r))\n",
        "        all_scores.append(best_score)\n",
        "\n",
        "    centers = best_circles[:, :2].copy()\n",
        "    r, ok, duals = solve_radii_lp(centers, r_min)\n",
        "    if ok:\n",
        "        best_duals = duals\n",
        "\n",
        "    stagnation = 0\n",
        "    attempts = 0\n",
        "    max_attempts = 1000000\n",
        "    while time_left() > 0.25 and attempts < max_attempts:\n",
        "        attempts += 1\n",
        "        # Determine subset size\n",
        "        base_m = 8\n",
        "        if stagnation > 8:\n",
        "            base_m = 12\n",
        "        elif stagnation > 3:\n",
        "            base_m = 10\n",
        "        m = int(np.clip(base_m + rng.integers(-2, 3), 5, 14))\n",
        "        # Select subset indices\n",
        "        weights = dual_inform_weights(centers, r if r is not None else np.ones(n), best_duals)\n",
        "        try:\n",
        "            subset_idx = rng.choice(n, size=m, replace=False, p=weights)\n",
        "        except Exception:\n",
        "            subset_idx = rng.choice(n, size=m, replace=False)\n",
        "        trust = 0.08 * (0.9 ** (stagnation // 3))\n",
        "        trust = float(np.clip(trust, 0.01, 0.2))\n",
        "\n",
        "        improved_local = False\n",
        "        for _ in range(3):\n",
        "            if time_left() < 0.15:\n",
        "                break\n",
        "            centers_cand, _, ok_step = slp_subset_step(centers, list(subset_idx), trust)\n",
        "            if not ok_step:\n",
        "                trust *= 0.7\n",
        "                continue\n",
        "            r_cand, ok_rep, duals_cand = solve_radii_lp(centers_cand, r_min)\n",
        "            if ok_rep:\n",
        "                sc = float(np.sum(r_cand))\n",
        "                if sc > best_score + 1e-10:\n",
        "                    centers = centers_cand\n",
        "                    r = r_cand\n",
        "                    best_score = sc\n",
        "                    best_circles = np.hstack([centers, r.reshape(-1, 1)])\n",
        "                    best_duals = duals_cand\n",
        "                    all_scores.append(best_score)\n",
        "                    improved_local = True\n",
        "                    stagnation = 0\n",
        "                    trust = min(trust * 1.3, 0.25)\n",
        "                else:\n",
        "                    trust *= 0.8\n",
        "            else:\n",
        "                trust *= 0.7\n",
        "        if not improved_local:\n",
        "            stagnation += 1\n",
        "            # Occasional global SLP to escape\n",
        "            if stagnation % 6 == 0 and time_left() > 0.25:\n",
        "                centers_glob, r_glob, _ = refine_by_slp_iterative(centers, max_outer=5, init_trust=0.06)\n",
        "                circles_glob, sc_glob, duals_glob = evaluate_centers(centers_glob)\n",
        "                if sc_glob > best_score + 1e-10:\n",
        "                    best_score = sc_glob\n",
        "                    best_circles = circles_glob\n",
        "                    centers = circles_glob[:, :2].copy()\n",
        "                    r = circles_glob[:, 2].copy()\n",
        "                    best_duals = duals_glob\n",
        "                    all_scores.append(best_score)\n",
        "                    stagnation = 0\n",
        "            # Occasional random shake of a few smallest-r circles\n",
        "            if stagnation % 4 == 0:\n",
        "                if r is None or not np.all(np.isfinite(r)):\n",
        "                    r = boundary_limits(centers)\n",
        "                kshake = max(3, n // 6)\n",
        "                idx_sorted = np.argsort(r)\n",
        "                picks = idx_sorted[:kshake]\n",
        "                perturb = rng.normal(0, 0.02 * (1 + 0.05 * stagnation), size=(len(picks), 2))\n",
        "                centers_shake = centers.copy()\n",
        "                centers_shake[picks] = clamp01(centers_shake[picks] + perturb)\n",
        "                r_shake, ok_sh, duals_sh = solve_radii_lp(centers_shake, r_min)\n",
        "                if ok_sh:\n",
        "                    sc = float(np.sum(r_shake))\n",
        "                    if sc > best_score + 1e-10:\n",
        "                        centers = centers_shake\n",
        "                        r = r_shake\n",
        "                        best_score = sc\n",
        "                        best_circles = np.hstack([centers, r.reshape(-1, 1)])\n",
        "                        best_duals = duals_sh\n",
        "                        all_scores.append(best_score)\n",
        "                        stagnation = 0\n",
        "\n",
        "        if stagnation > 18:\n",
        "            # Re-diversify by mixing with a new seed\n",
        "            seed = init_uniform() if rng.random() < 0.5 else init_hex_grid()\n",
        "            centers_mix = 0.5 * centers + 0.5 * seed\n",
        "            centers_mix = clamp01(centers_mix)\n",
        "            r_mix, ok_mix, duals_mix = solve_radii_lp(centers_mix, r_min)\n",
        "            if ok_mix:\n",
        "                sc = float(np.sum(r_mix))\n",
        "                if sc > best_score + 1e-10:\n",
        "                    centers = centers_mix\n",
        "                    r = r_mix\n",
        "                    best_score = sc\n",
        "                    best_circles = np.hstack([centers, r.reshape(-1, 1)])\n",
        "                    best_duals = duals_mix\n",
        "                    all_scores.append(best_score)\n",
        "                    stagnation = 0\n",
        "                else:\n",
        "                    stagnation = max(stagnation - 5, 0)\n",
        "\n",
        "    # Final feasibility and polish\n",
        "    try:\n",
        "        centers_final = best_circles[:, :2]\n",
        "        r_final, ok_fin, _ = solve_radii_lp(centers_final, r_min)\n",
        "        if ok_fin and r_final is not None:\n",
        "            best_circles = np.hstack([centers_final, r_final.reshape(-1, 1)])\n",
        "            best_score = float(np.sum(r_final))\n",
        "            if not all_scores or best_score > all_scores[-1] + 1e-9:\n",
        "                all_scores.append(best_score)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return {\n",
        "        'circles': best_circles.astype(float),\n",
        "        'all_scores': [float(s) for s in all_scores] if all_scores else [float(best_score)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "circle-evolved",
      "metadata": {},
      "source": [
        "## What GEPA Discovered\n",
        "\n",
        "GEPA evolved the simple grid-based baseline into a **sophisticated multi-strategy optimizer**. Here's what the evolved code includes:\n",
        "\n",
        "### Strategies Discovered by GEPA\n",
        "\n",
        "| Strategy | Description | How It Helps |\n",
        "|----------|-------------|--------------|\n",
        "| **Halton sequences** | Quasi-random initialization | Better initial coverage than random |\n",
        "| **Zero-vector seeding** | Start from origin | Often near polynomial optima |\n",
        "| **CMA-ES-style evolution** | Covariance matrix adaptation | Adapts search direction to landscape |\n",
        "| **Quadratic surrogate models** | Local function approximation | Efficient local optimization |\n",
        "| **Coordinate descent** | Per-dimension refinement | Fine-tunes individual coordinates |\n",
        "| **Nelder-Mead subspace** | Simplex method in active dimensions | Exploits important variables |\n",
        "| **Ridge-linear probes** | Gradient estimation from archive | Uses history for direction hints |\n",
        "\n",
        "### Results\n",
        "\n",
        "The evolved code achieves packing densities that **match or exceed** published results from:\n",
        "- **AlphaEvolve** (DeepMind)\n",
        "- **ShinkaEvolve**\n",
        "- **OpenEvolve**\n",
        "\n",
        "All without any human optimization expertise—just the problem definition and a baseline!\n",
        "\n",
        "### Key Takeaway\n",
        "\n",
        "GEPA automatically discovered advanced optimization strategies (Halton sequences, CMA-ES, surrogate models) that typically require expert knowledge to implement. The user only needed to:\n",
        "1. Define the problem (pack circles)\n",
        "2. Provide a naive baseline (grid placement)\n",
        "3. Return informative `side_info` (violations, scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85e571da",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# KernelBench\n",
        "\n",
        "This example optimizes CUDA kernels against the KernelBench suite. GEPA evolves two prompts: a kernel generator prompt and a refiner prompt.\n",
        "\n",
        "## The Challenge\n",
        "\n",
        "KernelBench provides PyTorch reference models. The candidate must produce a `ModelNew` with custom CUDA kernels (via `load_inline`) that is correct and faster than the PyTorch baseline.\n",
        "\n",
        "## The Task\n",
        "\n",
        "**Given**: PyTorch reference models + baseline runtimes\n",
        "**Find**: Prompts that generate correct, faster CUDA kernels\n",
        "\n",
        "V100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c921eaf2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import dspy\n",
        "\n",
        "from examples.kernelbench.prompts import BACKGROUND, KERNEL_GEN_PROMPT, REFINER_PROMPT\n",
        "from examples.kernelbench.eval import get_free_gpus, init_gpu_manager, load_dataset, load_or_measure_baselines\n",
        "from examples.kernelbench.main import create_fitness_fn, PromptCache, StateTracker, LLM\n",
        "from gepa.optimize_anything import EngineConfig, GEPAConfig, ReflectionConfig, optimize_anything\n",
        "\n",
        "run_name = time.strftime(\"%y%m%d_%H%M%S\")\n",
        "log_dir = f\"outputs/artifacts/kernelbench/{run_name}\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "dataset = load_dataset()\n",
        "baselines = load_or_measure_baselines(dataset)\n",
        "\n",
        "available_gpus = get_free_gpus()\n",
        "init_gpu_manager(device_list=available_gpus, lock_dir=os.path.join(log_dir, \"gpu_locks\"))\n",
        "\n",
        "lm = dspy.LM(LLM, temperature=1.0, max_tokens=32000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "888fcd1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "objective = \"Generate an LLM prompt that produces fast, correct CUDA kernels outperforming PyTorch baselines.\"\n",
        "\n",
        "seed_candidate = {\n",
        "    \"kernel_gen_prompt\": KERNEL_GEN_PROMPT,\n",
        "    \"refiner_prompt\": REFINER_PROMPT.format(objective=objective),\n",
        "}\n",
        "\n",
        "fitness_fn = create_fitness_fn(\n",
        "    lm,\n",
        "    baselines=baselines,\n",
        "    max_refinements=2,\n",
        ")\n",
        "\n",
        "config = GEPAConfig(\n",
        "    engine=EngineConfig(\n",
        "        run_dir=log_dir,\n",
        "        max_metric_calls=2000,\n",
        "        cache_evaluation=True,\n",
        "        track_best_outputs=True,\n",
        "        parallel=False,\n",
        "        max_workers=1,\n",
        "    ),\n",
        "    reflection=ReflectionConfig(\n",
        "        reflection_minibatch_size=3,\n",
        "        reflection_lm=LLM,\n",
        "    ),\n",
        ")\n",
        "\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=dataset,\n",
        "    config=config,\n",
        "    objective=objective,\n",
        "    background=BACKGROUND,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "979cc0c5",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## The Challenge\n",
        "\n",
        "KernelBench provides PyTorch reference models. The candidate must produce a `ModelNew` with custom CUDA kernels (via `load_inline`) that is correct and faster than the PyTorch baseline.\n",
        "\n",
        "## The Task\n",
        "\n",
        "**Given**: PyTorch reference models + baseline runtimes\n",
        "**Find**: Prompts that generate correct, faster CUDA kernels\n",
        "\n",
        "Results are hardware-dependent; see `outputs/artifacts/kernelbench/` for latest runs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "426b2b01",
      "metadata": {},
      "source": [
        "# 8. Example 5: Systems Optimization — Cloud Infrastructure\n",
        "\n",
        "**Result: GEPA discovers cost-saving strategies for real cloud infrastructure problems.**\n",
        "\n",
        "We demonstrate `optimize_anything` on two challenging systems problems from the [ADRS project](https://adrs-ucb.notion.site/):\n",
        "\n",
        "| Problem | Domain | Baseline Cost | Optimized Cost | Savings |\n",
        "|---------|--------|---------------|----------------|---------|\n",
        "| **Can't Be Late** | Spot Instance Scheduling | 96.48 | 89.86 | **6.9%** |\n",
        "| **Cloudcast** | Multi-Cloud Broadcast | $191 | $120 | **37.3%** |\n",
        "\n",
        "These problems involve optimizing complex algorithms with real-world constraints—exactly where traditional optimization struggles and LLM-based reflection excels."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d5bb9ff",
      "metadata": {},
      "source": [
        "## 8.1 Can't Be Late: Spot Instance Scheduling\n",
        "\n",
        "The [Can't Be Late problem](https://www.usenix.org/conference/nsdi24/presentation/wu-zhanghao) from NSDI'24 challenges us to minimize cloud computing costs while meeting deadlines.\n",
        "\n",
        "### The Challenge\n",
        "\n",
        "A cloud job must complete before a deadline using:\n",
        "- **SPOT instances**: Cheap (~$0.3/hour) but can be preempted at any time\n",
        "- **ON_DEMAND instances**: Expensive (~$1/hour) but guaranteed availability  \n",
        "- **NONE**: Wait without using any instances\n",
        "\n",
        "The strategy must decide dynamically which instance type to use based on:\n",
        "- Time remaining until deadline\n",
        "- Current spot availability\n",
        "- Restart overhead when preempted\n",
        "\n",
        "**Goal**: Minimize cost while ensuring task completion before deadline.\n",
        "\n",
        "### Why This is Hard\n",
        "\n",
        "- Spot preemption is **stochastic** — you can't predict when you'll lose your instance\n",
        "- The optimal strategy depends on **multiple factors**: deadline slack, restart cost, spot availability\n",
        "- Simple heuristics (always use spot, always use on-demand) are suboptimal\n",
        "\n",
        "### Setup\n",
        "\n",
        "```bash\n",
        "# Extract trace data\n",
        "cd examples/adrs/can_be_late/simulator\n",
        "tar -xzf real_traces.tar.gz\n",
        "\n",
        "# Install dependencies\n",
        "pip install -r examples/adrs/can_be_late/simulator/requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23b1475c",
      "metadata": {},
      "source": [
        "### The Seed Candidate\n",
        "\n",
        "We start with a simple baseline strategy that uses spot when available and switches to on-demand only when deadline is tight:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e01ebfc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from sky_spot.strategies.strategy import Strategy\n",
        "from sky_spot.utils import ClusterType\n",
        "\n",
        "class EvolveSingleRegionStrategy(Strategy):\n",
        "    NAME = 'evolve_single_region'\n",
        "    \n",
        "    def __init__(self, args):\n",
        "        super().__init__(args)\n",
        "    \n",
        "    def reset(self, env, task):\n",
        "        super().reset(env, task)\n",
        "    \n",
        "    def _step(self, last_cluster_type: ClusterType, has_spot: bool) -> ClusterType:\n",
        "        env = self.env\n",
        "        \n",
        "        # Task completion check\n",
        "        remaining_task_time = self.task_duration - sum(self.task_done_time)\n",
        "        if remaining_task_time <= 1e-3:\n",
        "            return ClusterType.NONE\n",
        "        \n",
        "        # Calculate remaining time until deadline\n",
        "        remaining_time = self.deadline - env.elapsed_seconds\n",
        "        \n",
        "        # Simple deadline check: if we're running out of time, use ON_DEMAND\n",
        "        # Add restart overhead to account for potential restart\n",
        "        if remaining_task_time + self.restart_overhead >= remaining_time:\n",
        "            # We need ON_DEMAND to guarantee completion\n",
        "            return ClusterType.ON_DEMAND\n",
        "        \n",
        "        # Simple greedy logic: use SPOT if available, wait otherwise\n",
        "        if has_spot:\n",
        "            return ClusterType.SPOT\n",
        "        else:\n",
        "            # Just wait for SPOT to become available\n",
        "            return ClusterType.NONE\n",
        "    \n",
        "    @classmethod\n",
        "    def _from_args(cls, parser):\n",
        "        args, _ = parser.parse_known_args()\n",
        "        return cls(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "833e5bea",
      "metadata": {},
      "source": [
        "### The Fitness Function\n",
        "\n",
        "The fitness function runs simulations on real AWS spot availability traces. The full evaluator is in `examples/adrs/can_be_late/evaluator.py`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94dd3cc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from examples.adrs.can_be_late.evaluator import create_fitness_function\n",
        "from examples.adrs.can_be_late.trace_dataset import load_trace_dataset\n",
        "\n",
        "# Load real AWS spot availability traces\n",
        "dataset_root = \"examples/adrs/can_be_late/simulator/real\"\n",
        "splits = load_trace_dataset(dataset_root=dataset_root)\n",
        "train_set, val_set, test_set = splits[\"train\"], splits[\"val\"], splits[\"test\"]\n",
        "\n",
        "# Create fitness function that runs simulations\n",
        "# Score = -cost (higher is better, lower cost is better)\n",
        "fitness_fn = create_fitness_function(timeout=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cd1036e",
      "metadata": {},
      "source": [
        "### Running GEPA Optimization,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f61a5bf8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gepa.optimize_anything import optimize_anything, GEPAConfig, EngineConfig, ReflectionConfig\n",
        "\n",
        "OPTIMIZATION_OBJECTIVE = \"\"\"Optimize a cloud scheduling strategy for the \"Can't Be Late\" problem.\n",
        "The strategy decides when to use SPOT instances (cheap but can be preempted) vs ON_DEMAND \n",
        "instances (expensive but reliable) to complete a task before its deadline.\"\"\"\n",
        "\n",
        "OPTIMIZATION_BACKGROUND = \"\"\"Key information:\n",
        "- ClusterType.SPOT: Cheap (~$0.3/hour) but can be preempted\n",
        "- ClusterType.ON_DEMAND: Expensive (~$1/hour) but guaranteed\n",
        "- ClusterType.NONE: Wait without using instances\n",
        "- restart_overhead: Time penalty when switching instances\n",
        "- Lower cost is better (scores are negative cost in dollars)\"\"\"\n",
        "\n",
        "gepa_config = GEPAConfig(\n",
        "    engine=EngineConfig(\n",
        "        run_dir=\"runs/cant_be_late\",\n",
        "        max_metric_calls=1000,\n",
        "        track_best_outputs=True,\n",
        "        use_cloudpickle=True,\n",
        "    ),\n",
        "    reflection=ReflectionConfig(\n",
        "        reflection_minibatch_size=3,\n",
        "        reflection_lm=\"openai/gpt-4o\",  # Or use get_reflection_lm() for Gemini\n",
        "    ),\n",
        ")\n",
        "\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=train_set,\n",
        "    valset=val_set,\n",
        "    objective=OPTIMIZATION_OBJECTIVE,\n",
        "    background=OPTIMIZATION_BACKGROUND,\n",
        "    config=gepa_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ae19fa9",
      "metadata": {},
      "source": [
        "### Results\n",
        "\n",
        "![Can't Be Late Optimization](examples/adrs/can_be_late/optimization_trajectory.png)\n",
        "\n",
        "GEPA evolved a strategy that achieves **6.9% cost reduction** compared to the baseline:\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Base Cost | 96.48 |\n",
        "| Optimized Cost | 89.86 |\n",
        "| **Cost Savings** | **6.9%** |\n",
        "\n",
        "The evolved strategy learned nuanced decision boundaries for when to switch between spot and on-demand instances based on deadline slack and restart overhead."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c84190d6",
      "metadata": {},
      "source": [
        "### What GEPA Discovered\n",
        "\n",
        "GEPA evolved a sophisticated strategy with several key improvements over the baseline:\n",
        "1. Overhead-aware switching: Considers restart cost when deciding to switch instance types\n",
        "2. Anti-thrashing logic: Prevents expensive restart loops by \"latching\" to ON_DEMAND\n",
        "3. Dynamic safety buffer: Scales buffer with remaining task size (max(2h, 10% of remaining))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8950a8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from sky_spot.strategies.strategy import Strategy\n",
        "from sky_spot.utils import ClusterType\n",
        "\n",
        "class EvolveSingleRegionStrategy(Strategy):\n",
        "    NAME = 'evolve_single_region'\n",
        "    \n",
        "    def __init__(self, args):\n",
        "        super().__init__(args)\n",
        "    \n",
        "    def reset(self, env, task):\n",
        "        super().reset(env, task)\n",
        "    \n",
        "    def _step(self, last_cluster_type: ClusterType, has_spot: bool) -> ClusterType:\n",
        "        env = self.env\n",
        "        \n",
        "        # Calculate remaining task work\n",
        "        remaining_task_time = self.task_duration - sum(self.task_done_time)\n",
        "        if remaining_task_time <= 1e-3:\n",
        "            return ClusterType.NONE\n",
        "        \n",
        "        # Calculate remaining wall clock time until deadline\n",
        "        remaining_time = self.deadline - env.elapsed_seconds\n",
        "        \n",
        "        # Calculate overhead to switch to (or stay on) ON_DEMAND.\n",
        "        # If we are already on ON_DEMAND, continuing incurs no overhead.\n",
        "        # If we are on SPOT or NONE, switching to ON_DEMAND incurs restart_overhead.\n",
        "        overhead_to_od = self.restart_overhead if last_cluster_type != ClusterType.ON_DEMAND else 0.0\n",
        "        \n",
        "        # 1. Hard Deadline Constraint (Point of No Return)\n",
        "        # If we wait any longer, we won't finish even with guaranteed ON_DEMAND.\n",
        "        # We must verify we have enough time for the work plus any switch overhead.\n",
        "        if remaining_task_time + overhead_to_od >= remaining_time:\n",
        "            return ClusterType.ON_DEMAND\n",
        "        \n",
        "        # 2. Spot Usage Strategy\n",
        "        if has_spot:\n",
        "            # Check if switching to SPOT is safe regarding the deadline.\n",
        "            # Switching to SPOT incurs overhead (if not already on it).\n",
        "            overhead_to_spot = self.restart_overhead if last_cluster_type != ClusterType.SPOT else 0.0\n",
        "            \n",
        "            # If the overhead of switching to SPOT consumes the remaining slack such that \n",
        "            # we can't finish, we must stick with (or switch to) ON_DEMAND.\n",
        "            if remaining_task_time + overhead_to_spot >= remaining_time:\n",
        "                return ClusterType.ON_DEMAND\n",
        "                \n",
        "            return ClusterType.SPOT\n",
        "        \n",
        "        # 3. Unavailable Spot Strategy\n",
        "        # Spot is unavailable. We must decide whether to Wait (NONE) or Run (ON_DEMAND).\n",
        "        \n",
        "        # Calculate current slack: Time margin we can afford to waste.\n",
        "        current_slack = remaining_time - remaining_task_time - overhead_to_od\n",
        "        \n",
        "        # Safety Buffer:\n",
        "        # Maintain a buffer to handle future volatility.\n",
        "        # We use max(2.0h, 10% of remaining task) to scale with task size.\n",
        "        safety_buffer = max(2.0, 0.1 * remaining_task_time)\n",
        "        \n",
        "        # Anti-Thrashing Logic (Latching):\n",
        "        # If we are currently running ON_DEMAND (due to low slack or PNR), we should\n",
        "        # maintain this state until Spot returns. Switching back to NONE (waiting) \n",
        "        # when slack is marginally above the buffer causes expensive restart loops \n",
        "        # (thrashing) as the buffer size dynamically adjusts with task progress.\n",
        "        if last_cluster_type == ClusterType.ON_DEMAND:\n",
        "            return ClusterType.ON_DEMAND\n",
        "        \n",
        "        # If we are currently Waiting (NONE) or were on SPOT, check if slack is critically low.\n",
        "        if current_slack < safety_buffer:\n",
        "            return ClusterType.ON_DEMAND\n",
        "            \n",
        "        # Slack is sufficient; wait for cheaper SPOT instances.\n",
        "        return ClusterType.NONE\n",
        "    \n",
        "    @classmethod\n",
        "    def _from_args(cls, parser):\n",
        "        args, _ = parser.parse_known_args()\n",
        "        return cls(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cdc6083",
      "metadata": {},
      "source": [
        "## 8.2 Cloudcast: Multi-Cloud Broadcast Optimization\n",
        "\n",
        "The Cloudcast problem optimizes data broadcast across multiple cloud providers (AWS, GCP, Azure).\n",
        "\n",
        "### The Challenge\n",
        "\n",
        "Transfer data from a single source to multiple destinations across cloud providers:\n",
        "- **Heterogeneous pricing**: Egress costs vary dramatically between providers\n",
        "- **Bandwidth constraints**: Different regions have different throughput limits\n",
        "- **Partitioning**: Data can be split and sent via different paths\n",
        "\n",
        "**Goal**: Minimize total cost (egress fees + instance costs) while delivering data to all destinations.\n",
        "\n",
        "### Why This is Hard\n",
        "\n",
        "- The search space is **combinatorial** — exponentially many possible routing trees\n",
        "- Costs are **non-uniform** — inter-cloud transfers cost more than intra-cloud\n",
        "- Simple shortest-path algorithms ignore cost structure\n",
        "\n",
        "### Setup\n",
        "\n",
        "```bash\n",
        "pip install networkx pandas numpy\n",
        "pip install -r examples/adrs/cloudcast/requirements.txt\n",
        "```\n",
        "\n",
        "### The Seed Candidate\n",
        "\n",
        "We start with a simple Dijkstra shortest-path baseline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c48317df",
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_candidate = {\"program\": \"\"\"import networkx as nx\n",
        "from typing import Dict, List\n",
        "\n",
        "class BroadCastTopology:\n",
        "    def __init__(self, src: str, dsts: List[str], num_partitions: int = 4, paths=None):\n",
        "        self.src = src\n",
        "        self.dsts = dsts\n",
        "        self.num_partitions = num_partitions\n",
        "        self.paths = paths if paths else {dst: {str(i): None for i in range(num_partitions)} for dst in dsts}\n",
        "\n",
        "    def set_num_partitions(self, num_partitions: int):\n",
        "        self.num_partitions = num_partitions\n",
        "\n",
        "    def append_dst_partition_path(self, dst: str, partition: int, path: List):\n",
        "        partition = str(partition)\n",
        "        if self.paths[dst][partition] is None:\n",
        "            self.paths[dst][partition] = []\n",
        "        self.paths[dst][partition].append(path)\n",
        "\n",
        "\n",
        "def search_algorithm(src, dsts, G, num_partitions):\n",
        "    \\\"\\\"\\\"\n",
        "    Find broadcast paths from source to all destinations.\n",
        "    Uses Dijkstra's shortest path based on cost as edge weight.\n",
        "    \\\"\\\"\\\"\n",
        "    h = G.copy()\n",
        "    h.remove_edges_from(list(h.in_edges(src)) + list(nx.selfloop_edges(h)))\n",
        "    bc_topology = BroadCastTopology(src, dsts, num_partitions)\n",
        "\n",
        "    for dst in dsts:\n",
        "        path = nx.dijkstra_path(h, src, dst, weight=\"cost\")\n",
        "        for i in range(0, len(path) - 1):\n",
        "            s, t = path[i], path[i + 1]\n",
        "            for j in range(bc_topology.num_partitions):\n",
        "                bc_topology.append_dst_partition_path(dst, j, [s, t, G[s][t]])\n",
        "\n",
        "    return bc_topology\n",
        "\"\"\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e8d5150",
      "metadata": {},
      "source": [
        "### The Fitness Function\n",
        "\n",
        "The fitness function simulates the broadcast and computes costs. Full evaluator is in `examples/adrs/cloudcast/evaluator.py`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d80aebb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from examples.adrs.cloudcast.evaluator import create_fitness_function, load_config_dataset\n",
        "\n",
        "# Load broadcast configuration scenarios\n",
        "config_dir = \"examples/adrs/cloudcast/cloudcast/config\"\n",
        "samples = load_config_dataset(config_dir=config_dir)\n",
        "train_set = val_set = test_set = samples  # Use all configs for train/val/test\n",
        "\n",
        "# Create fitness function\n",
        "# Score = 1 / (1 + total_cost) - higher score means lower cost\n",
        "fitness_fn = create_fitness_function(timeout=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b3266cc",
      "metadata": {},
      "source": [
        "### Running GEPA Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dfab99a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gepa.optimize_anything import optimize_anything, GEPAConfig, EngineConfig, ReflectionConfig\n",
        "\n",
        "OPTIMIZATION_OBJECTIVE = \"\"\"Optimize a broadcast routing algorithm for multi-cloud data transfer.\n",
        "The algorithm decides how to route data from a single source to multiple destinations\n",
        "across cloud providers (AWS, GCP, Azure). Minimize total cost (egress + instance costs).\"\"\"\n",
        "\n",
        "OPTIMIZATION_BACKGROUND = \"\"\"Key information:\n",
        "- Nodes are cloud regions (e.g., \"aws:us-east-1\", \"gcp:europe-west1\")\n",
        "- Edges have 'cost' ($/GB egress) and 'throughput' (Gbps) attributes\n",
        "- Data is partitioned and can be routed independently\n",
        "- Total cost = egress costs + instance costs\n",
        "- Score = 1 / (1 + total_cost) - higher is better\"\"\"\n",
        "\n",
        "gepa_config = GEPAConfig(\n",
        "    engine=EngineConfig(\n",
        "        run_dir=\"runs/cloudcast\",\n",
        "        max_metric_calls=1000,\n",
        "        track_best_outputs=True,\n",
        "        use_cloudpickle=True,\n",
        "    ),\n",
        "    reflection=ReflectionConfig(\n",
        "        reflection_minibatch_size=3,\n",
        "        reflection_lm=\"openai/gpt-4o\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=train_set,\n",
        "    valset=val_set,\n",
        "    objective=OPTIMIZATION_OBJECTIVE,\n",
        "    background=OPTIMIZATION_BACKGROUND,\n",
        "    config=gepa_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfeabc0e",
      "metadata": {},
      "source": [
        "### Results\n",
        "\n",
        "![Cloudcast Optimization](examples/adrs/cloudcast/optimization_trajectory.png)\n",
        "\n",
        "GEPA evolved a routing algorithm that achieves **37.3% cost reduction**:\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Base Cost | $191 |\n",
        "| Optimized Cost | $120 |\n",
        "| **Cost Savings** | **37.3%** |\n",
        "\n",
        "The evolved algorithm discovered strategies for:\n",
        "- Preferring intra-cloud paths when possible\n",
        "- Intelligent partition routing to balance load\n",
        "- Cost-aware path selection over naive shortest-path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9482a5f",
      "metadata": {},
      "source": [
        "### What GEPA Discovered\n",
        "\n",
        "GEPA evolved a sophisticated routing algorithm with several key innovations:\n",
        "1. Iterative Marginal-Cost Shortest Path: Replaces Steiner Tree heuristic\n",
        "2. Zero marginal cost for reused edges: Encourages multicast tree building\n",
        "3. Congestion-aware routing: Penalties based on partition-level usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "132ab8da",
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import random\n",
        "from typing import Dict, List\n",
        "\n",
        "class SingleDstPath(Dict):\n",
        "    partition: int\n",
        "    edges: List[List]  # [[src, dst, edge data]]\n",
        "\n",
        "class BroadCastTopology:\n",
        "    def __init__(self, src: str, dsts: List[str], num_partitions: int = 4, paths: Dict[str, 'SingleDstPath'] = None):\n",
        "        self.src = src\n",
        "        self.dsts = dsts\n",
        "        self.num_partitions = num_partitions\n",
        "        if paths is not None:\n",
        "            self.paths = paths\n",
        "        else:\n",
        "            self.paths = {dst: {str(i): None for i in range(num_partitions)} for dst in dsts}\n",
        "\n",
        "    def get_paths(self):\n",
        "        return self.paths\n",
        "\n",
        "    def set_num_partitions(self, num_partitions: int):\n",
        "        self.num_partitions = num_partitions\n",
        "\n",
        "    def set_dst_partition_paths(self, dst: str, partition: int, paths: List[List]):\n",
        "        partition = str(partition)\n",
        "        self.paths[dst][partition] = paths\n",
        "\n",
        "    def append_dst_partition_path(self, dst: str, partition: int, path: List):\n",
        "        partition = str(partition)\n",
        "        if self.paths[dst][partition] is None:\n",
        "            self.paths[dst][partition] = []\n",
        "        self.paths[dst][partition].append(path)\n",
        "\n",
        "def search_algorithm(src, dsts, G, num_partitions):\n",
        "    \"\"\"\n",
        "    Optimized broadcast routing algorithm using Iterative Marginal-Cost Shortest Path.\n",
        "    \n",
        "    Improvements:\n",
        "    1. Replaced complex Steiner Tree heuristic with a destination-iterative Shortest Path approach.\n",
        "       - This prevents the creation of inefficient, deep trees which were hurting transfer times.\n",
        "       - It maintains 'Multicast' cost benefits by setting the cost of edges already used \n",
        "         in the current partition to zero (Marginal Cost logic).\n",
        "    2. Congestion Management:\n",
        "       - Tracks usage at the partition level.\n",
        "       - Penalties are applied to throughput to balance load across partitions.\n",
        "    3. Tuning:\n",
        "       - ALPHA increased to 0.02 to better balance Egress Cost vs Transfer Time.\n",
        "       - Random shuffle of destinations prevents bias in shared path construction.\n",
        "    \"\"\"\n",
        "    \n",
        "    # PARAMETERS\n",
        "    # Balances Cost ($) vs 1/Throughput (Time). \n",
        "    # Increased from 0.005 to 0.02 to penalize low-bandwidth paths more aggressively,\n",
        "    # addressing high transfer times observed in evaluation.\n",
        "    ALPHA = 0.02 \n",
        "    \n",
        "    # Seed for reproducibility\n",
        "    random.seed(42)\n",
        "\n",
        "    bc_topology = BroadCastTopology(src, dsts, num_partitions)\n",
        "    \n",
        "    # Global usage: (u, v) -> count of partitions utilizing this link\n",
        "    # We use this to calculate the 'Time Penalty' for subsequent partitions.\n",
        "    global_edge_usage = {} \n",
        "\n",
        "    # We iterate through each partition to assign paths\n",
        "    for part_id in range(num_partitions):\n",
        "        # Track edges activated in the CURRENT partition.\n",
        "        # In an overlay multicast model, the source pays egress only once per link per partition,\n",
        "        # regardless of how many destinations share that link.\n",
        "        partition_active_edges = set()\n",
        "        \n",
        "        # Shuffle destinations to avoid routing bias (e.g., always optimizing for the first dst)\n",
        "        current_dsts = list(dsts)\n",
        "        random.shuffle(current_dsts)\n",
        "        \n",
        "        # Define a dynamic weight function for Dijkstra\n",
        "        def get_weight(u, v, d):\n",
        "            # 1. Throughput / Congestion Component\n",
        "            throughput = d.get('throughput', 1.0)\n",
        "            if throughput < 1e-6: throughput = 1e-6\n",
        "            \n",
        "            # Usage penalty applies to the 'Time' aspect.\n",
        "            # We assume bandwidth is shared or congested by total active partitions.\n",
        "            usage = global_edge_usage.get((u, v), 0)\n",
        "            time_penalty = (1.0 + usage) / throughput\n",
        "            \n",
        "            # 2. Cost Component\n",
        "            # If the edge is already active in this partition, marginal egress cost is 0.\n",
        "            # This encourages building a shared multicast tree greedily.\n",
        "            if (u, v) in partition_active_edges:\n",
        "                cost = 0.0\n",
        "            else:\n",
        "                cost = d.get('cost', 0.0)\n",
        "            \n",
        "            return cost + (ALPHA * time_penalty)\n",
        "\n",
        "        for dst in current_dsts:\n",
        "            if dst == src:\n",
        "                continue\n",
        "            \n",
        "            path_segments = []\n",
        "            try:\n",
        "                # Find shortest path using the custom marginal-cost weights\n",
        "                # This naturally gravitates towards reusing existing branches of the tree (Steiner-like)\n",
        "                # but will branch out (Unicast) if the shared path is too slow or expensive.\n",
        "                path_nodes = nx.shortest_path(G, source=src, target=dst, weight=get_weight)\n",
        "                \n",
        "                # Reconstruct path data\n",
        "                for i in range(len(path_nodes) - 1):\n",
        "                    u, v = path_nodes[i], path_nodes[i+1]\n",
        "                    path_segments.append([u, v, G[u][v]])\n",
        "                    \n",
        "                    # Mark edge as active for this partition\n",
        "                    partition_active_edges.add((u, v))\n",
        "                    \n",
        "            except nx.NetworkXNoPath:\n",
        "                # Fallback if unreachable\n",
        "                path_segments = []\n",
        "            \n",
        "            bc_topology.set_dst_partition_paths(dst, part_id, path_segments)\n",
        "        \n",
        "        # After routing all destinations for this partition, update global congestion stats\n",
        "        for u, v in partition_active_edges:\n",
        "            global_edge_usage[(u, v)] = global_edge_usage.get((u, v), 0) + 1\n",
        "\n",
        "    return bc_topology"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af8bdac",
      "metadata": {},
      "source": [
        "### Key Takeaways from Systems Optimization\n",
        "\n",
        "1. **Rich Side Information Matters**: Both problems provide detailed simulation output (costs, timings, paths taken). This lets GEPA understand *why* a strategy underperforms—not just that it does.\n",
        "\n",
        "2. **Domain Complexity**: These problems involve stochastic elements (spot preemption), complex constraints (deadlines, bandwidth limits), and non-linear cost structures. Traditional optimizers struggle here.\n",
        "\n",
        "3. **Code as the Optimized Artifact**: Unlike prompt optimization, we're evolving actual *algorithms*—decision functions and routing strategies. GEPA treats code as just another text artifact to optimize.\n",
        "\n",
        "4. **Real-World Impact**: These aren't toy problems. The Cloudcast savings of 37% on a large-scale data transfer could mean thousands of dollars saved per transfer.\n",
        "\n",
        "---\n",
        "\n",
        "The complete code for both examples is available in:\n",
        "- `examples/adrs/can_be_late/` — Spot instance scheduling\n",
        "- `examples/adrs/cloudcast/` — Multi-cloud broadcast"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-7",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-8\"></a>\n",
        "# 9. How It Works Under the Hood\n",
        "\n",
        "GEPA (Generative Evolutionary Prompting with ASI) operates through a loop of **evaluation**, **reflection**, and **proposal**:\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "│                              GEPA LOOP                                       │\n",
        "├─────────────────────────────────────────────────────────────────────────────┤\n",
        "│                                                                             │\n",
        "│   ┌──────────────┐                                                          │\n",
        "│   │  EVALUATE    │  Run fitness_fn on candidates                            │\n",
        "│   │              │  → Collect scores AND side_info                          │\n",
        "│   └──────┬───────┘                                                          │\n",
        "│          │                                                                  │\n",
        "│          ▼                                                                  │\n",
        "│   ┌──────────────┐                                                          │\n",
        "│   │   SELECT     │  Choose candidates for mutation                          │\n",
        "│   │              │  → Pareto selection across objectives/instances          │\n",
        "│   │              │  → Epsilon-greedy exploration                            │\n",
        "│   └──────┬───────┘                                                          │\n",
        "│          │                                                                  │\n",
        "│          ▼                                                                  │\n",
        "│   ┌──────────────┐                                                          │\n",
        "│   │   REFLECT    │  LLM analyzes evaluation results                         │\n",
        "│   │              │  → \"Why did this candidate fail?\"                        │\n",
        "│   │              │  → Uses side_info to understand failure modes            │\n",
        "│   └──────┬───────┘                                                          │\n",
        "│          │                                                                  │\n",
        "│          ▼                                                                  │\n",
        "│   ┌──────────────┐                                                          │\n",
        "│   │   PROPOSE    │  LLM generates improved candidates                       │\n",
        "│   │              │  → Targeted mutations based on reflection                │\n",
        "│   │              │  → Preserves successful behaviors                        │\n",
        "│   └──────┬───────┘                                                          │\n",
        "│          │                                                                  │\n",
        "│          └──────────────────► REPEAT until stopping condition               │\n",
        "│                                                                             │\n",
        "└─────────────────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### 1. Pareto Frontier\n",
        "GEPA maintains a **Pareto frontier** of candidates that are optimal on different subsets of the data:\n",
        "- **Multi-objective**: Some candidates optimize for accuracy, others for speed\n",
        "- **Instance-level**: Some candidates excel on certain problem types\n",
        "- **Diversity**: The frontier preserves diverse strategies for exploration\n",
        "\n",
        "### 2. Reflective Mutation\n",
        "Unlike **random mutation** in traditional evolutionary algorithms, GEPA uses LLMs to make **targeted improvements**:\n",
        "\n",
        "| Traditional EA | GEPA |\n",
        "|---------------|------|\n",
        "| Random bit flips | LLM analyzes failure modes |\n",
        "| Blind crossover | LLM preserves working patterns |\n",
        "| Requires many generations | Sample-efficient |\n",
        "| No domain knowledge | Uses side_info for context |\n",
        "\n",
        "### 3. Side Information Flow\n",
        "The `side_info` returned by your fitness function powers the reflection:\n",
        "\n",
        "```python\n",
        "# What the LLM sees during reflection:\n",
        "\"\"\"\n",
        "Current candidate: {code: \"def solve(x): ...\"}\n",
        "\n",
        "Evaluation results on 3 examples:\n",
        "  Example 1: Score 0.8\n",
        "    Input: \"Pack 26 circles\"\n",
        "    Output: circles array\n",
        "    Error: \"Circles 3 and 7 overlap\"\n",
        "    \n",
        "  Example 2: Score 0.0  \n",
        "    Input: \"Pack 26 circles\"\n",
        "    Error: \"IndexError on line 42\"\n",
        "    \n",
        "  Example 3: Score 1.0\n",
        "    Input: \"Pack 26 circles\"\n",
        "    Output: Valid packing with sum_radii=2.89\n",
        "\n",
        "Propose an improved version that fixes these issues.\n",
        "\"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a23cdcd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "fitness_fn = create_fitness_fn(\n",
        "    lm,\n",
        "    baselines=baselines,\n",
        "    use_rag=True,\n",
        "    max_refinements=5,\n",
        "    tracker=tracker,\n",
        "    kernel_gen_cache=kernel_gen_cache,\n",
        "    refiner_cache=refiner_cache,\n",
        ")\n",
        "\n",
        "config = GEPAConfig(\n",
        "    engine=EngineConfig(\n",
        "        run_dir=log_dir,\n",
        "        max_metric_calls=2000,\n",
        "        cache_evaluation=True,\n",
        "        track_best_outputs=True,\n",
        "        parallel=False,\n",
        "        max_workers=1,\n",
        "    ),\n",
        "    reflection=ReflectionConfig(\n",
        "        reflection_minibatch_size=3,\n",
        "        reflection_lm=LLM,\n",
        "    ),\n",
        ")\n",
        "\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=dataset,\n",
        "    config=config,\n",
        "    objective=objective,\n",
        "    background=BACKGROUND,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-8",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-9\"></a>\n",
        "# 10. Conclusion: From Imperative to Declarative Optimization\n",
        "\n",
        "We are witnessing a **paradigm shift** in optimization—from imperative implementations to declarative specifications:\n",
        "\n",
        "| **Old Paradigm** | **New Paradigm with `optimize_anything`** |\n",
        "|------------------|------------------------------------------|\n",
        "| Imperative: specify *how* to optimize | Declarative: specify *what* to optimize |\n",
        "| Different libraries for different problems | **One API for everything** |\n",
        "| Mathematically-specific algorithms | Language-driven proposal generation |\n",
        "| Scalar fitness only | **Rich diagnostic information (ASI)** |\n",
        "| Random mutations | **Targeted, reflective mutations** |\n",
        "| Expert knowledge required | LLM brings domain knowledge |\n",
        "\n",
        "## The `optimize_anything` Vision\n",
        "\n",
        "**If it can be represented as text, it can be optimized.**\n",
        "\n",
        "| Domain | What You Optimize | Example |\n",
        "|--------|-------------------|---------|\n",
        "| **Code** | Algorithms, implementations | Black-box optimization code |\n",
        "| **Prompts** | Instructions, examples | System prompts for math problems |\n",
        "| **Agent Architectures** | Program structure, control flow | DSPy programs for ARC-AGI |\n",
        "| **Configurations** | Hyperparameters, settings | JSON/YAML configs |\n",
        "| **Data Structures** | Schemas, templates | API specifications |\n",
        "\n",
        "## Why This Matters\n",
        "\n",
        "1. **Democratization**: You don't need a PhD in optimization to solve hard problems\n",
        "2. **Generalization**: One framework, infinite applications\n",
        "3. **Sample Efficiency**: LLM reflection beats random search\n",
        "4. **Emergent Capabilities**: GEPA discovers strategies you wouldn't think of\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "```bash\n",
        "pip install gepa\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "getting-started-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gepa.optimize_anything import optimize_anything, GEPAConfig, EngineConfig, ReflectionConfig\n",
        "\n",
        "# 1. Define your seed candidate (starting point)\n",
        "seed_candidate = {\n",
        "    \"my_param\": \"initial value\"  # Can be code, prompt, config, etc.\n",
        "}\n",
        "\n",
        "# 2. Define your fitness function (how to measure success)\n",
        "def fitness_fn(candidate, example=None):\n",
        "    # Run your system with the candidate\n",
        "    output = run_my_system(candidate[\"my_param\"], example)\n",
        "    \n",
        "    # Compute score (higher is better)\n",
        "    score = compute_score(output, example)\n",
        "    \n",
        "    # Collect rich diagnostic information (ASI)\n",
        "    side_info = {\n",
        "        \"Input\": example,\n",
        "        \"Output\": output,\n",
        "        \"Expected\": example.get(\"answer\") if example else None,\n",
        "        \"Error\": get_error_message(output),\n",
        "        \"Feedback\": analyze_performance(output),\n",
        "    }\n",
        "    \n",
        "    return score, output, side_info\n",
        "\n",
        "# 3. Run optimization\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=my_examples,  # Optional: for multi-instance mode\n",
        "    objective=\"Find a parameter that maximizes performance\",  # Optional: guidance\n",
        "    config=GEPAConfig(\n",
        "        engine=EngineConfig(max_metric_calls=100),\n",
        "        reflection=ReflectionConfig(reflection_lm=\"openai/gpt-4o\"),\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 4. Use the optimized result\n",
        "print(\"Best candidate:\", result.best_candidate)\n",
        "print(\"Best score:\", result.best_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "final-takeaways",
      "metadata": {},
      "source": [
        "## Summary: What We Showed\n",
        "\n",
        "| Example | What We Optimized | Key Insight |\n",
        "|---------|-------------------|-------------|\n",
        "| **Mathematical Optimization** | Python code for black-box optimization | GEPA discovers algorithms automatically |\n",
        "| **Prompt Engineering** | System prompts for math problems | LLM reflection finds domain-specific strategies |\n",
        "| **Agent Evolution** | DSPy programs for ARC-AGI | Self-refinement emerged without being programmed |\n",
        "| **Algorithmic Discovery** | Circle packing algorithms | Matches state-of-the-art (AlphaEvolve, etc.) |\n",
        "| **Systems: Scheduling** | Spot instance strategies | 6.9% cost reduction on real AWS traces |\n",
        "| **Systems: Networking** | Multi-cloud broadcast routing | 37.3% cost reduction on cloud transfers |\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Unified Interface**: One API for prompts, code, configs, and agent architectures\n",
        "\n",
        "2. **Side Information (ASI) is Key**: The more diagnostic information you provide, the better GEPA can reason about improvements\n",
        "\n",
        "3. **Beyond Scalar Optimization**: Traditional optimizers only see scores; GEPA sees error messages, execution traces, and domain-specific feedback\n",
        "\n",
        "4. **Emergent Capabilities**: Sophisticated strategies (like self-refinement in ARC-AGI) emerge without explicit programming\n",
        "\n",
        "5. **The Convex Hull**: `optimize_anything` is designed to cover all text-based optimization problems under one abstraction\n",
        "\n",
        "---\n",
        "\n",
        "## Try It Yourself\n",
        "\n",
        "**If you can express your system's parameters as text and compute a score with diagnostic feedback, GEPA can optimize it.**\n",
        "\n",
        "```python\n",
        "pip install gepa\n",
        "```\n",
        "\n",
        "```python\n",
        "from gepa.optimize_anything import optimize_anything\n",
        "\n",
        "result = optimize_anything(\n",
        "    seed_candidate={\"your_param\": \"your_value\"},\n",
        "    fitness_fn=your_fitness_function,\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "*GEPA is open-source. Star us on [GitHub](https://github.com/stanfordnlp/gepa)!*\n",
        "\n",
        "---\n",
        "\n",
        "## Appendix: Full Code Examples\n",
        "\n",
        "The complete, runnable code for all examples in this post can be found in the `examples/` directory:\n",
        "\n",
        "- `examples/new_polynomial/` — Mathematical optimization (EvalSet)\n",
        "- `examples/math/` — Prompt engineering (AIME 2025)\n",
        "- `examples/arc_agi/` — Agent program evolution (ARC-AGI)\n",
        "- `examples/circle_packing/` — Algorithmic discovery (Circle Packing)\n",
        "- `examples/adrs/can_be_late/` — Spot instance scheduling\n",
        "- `examples/adrs/cloudcast/` — Multi-cloud broadcast optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a689649",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Minimal Working Example: Optimize a Sorting Function\n",
        "\n",
        "Here's a complete, runnable example that optimizes a Python sorting function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2dcd15",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Minimal working example: Optimize a sorting function\n",
        "This evolves Python code that sorts a list of numbers.\n",
        "\"\"\"\n",
        "import time\n",
        "from gepa.optimize_anything import optimize_anything, GEPAConfig, EngineConfig, ReflectionConfig\n",
        "\n",
        "# 1. SEED CANDIDATE: A naive bubble sort implementation\n",
        "seed_candidate = {\n",
        "    \"code\": \"\"\"\n",
        "def sort_list(arr):\n",
        "    '''Sort a list of numbers in ascending order.'''\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "# 2. DATASET: Test cases to optimize on\n",
        "dataset = [\n",
        "    {\"input\": [64, 34, 25, 12, 22, 11, 90], \"expected\": [11, 12, 22, 25, 34, 64, 90]},\n",
        "    {\"input\": [5, 1, 4, 2, 8], \"expected\": [1, 2, 4, 5, 8]},\n",
        "    {\"input\": [3, 3, 1, 2, 1], \"expected\": [1, 1, 2, 3, 3]},\n",
        "    {\"input\": list(range(100, 0, -1)), \"expected\": list(range(1, 101))},  # Worst case\n",
        "]\n",
        "\n",
        "# 3. FITNESS FUNCTION: Measure correctness and speed\n",
        "def fitness_fn(candidate, example):\n",
        "    code = candidate[\"code\"]\n",
        "    \n",
        "    try:\n",
        "        # Execute the code\n",
        "        exec(code, globals())\n",
        "        \n",
        "        # Time the execution\n",
        "        start = time.time()\n",
        "        result = sort_list(example[\"input\"].copy())\n",
        "        elapsed = time.time() - start\n",
        "        \n",
        "        # Check correctness\n",
        "        correct = result == example[\"expected\"]\n",
        "        score = 1.0 if correct else 0.0\n",
        "        \n",
        "        # Bonus for speed (if correct)\n",
        "        if correct and elapsed < 0.001:\n",
        "            score += 0.1\n",
        "        \n",
        "        # Rich side_info for LLM reflection\n",
        "        side_info = {\n",
        "            \"Input\": example[\"input\"],\n",
        "            \"Output\": result,\n",
        "            \"Expected\": example[\"expected\"],\n",
        "            \"Correct\": correct,\n",
        "            \"Time (ms)\": elapsed * 1000,\n",
        "            \"Error\": None,\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        score = 0.0\n",
        "        side_info = {\n",
        "            \"Input\": example[\"input\"],\n",
        "            \"Error\": str(e),\n",
        "            \"Code\": code,\n",
        "        }\n",
        "    \n",
        "    return score, {\"code\": code, \"result\": result if 'result' in dir() else None}, side_info\n",
        "\n",
        "# 4. RUN OPTIMIZATION\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=dataset,\n",
        "    objective=\"Optimize the sorting function for correctness and speed.\",\n",
        "    background=\"Consider algorithms like quicksort, mergesort, or heapsort.\",\n",
        "    config=GEPAConfig(\n",
        "        engine=EngineConfig(max_metric_calls=50),\n",
        "        reflection=ReflectionConfig(reflection_lm=\"openai/gpt-4o-mini\"),\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 5. USE THE RESULT\n",
        "print(\"=\" * 60)\n",
        "print(\"OPTIMIZED CODE:\")\n",
        "print(\"=\" * 60)\n",
        "print(result.best_candidate[\"code\"])\n",
        "print(f\"\\nBest score: {result.best_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04fa1609",
      "metadata": {},
      "source": [
        "### What This Example Demonstrates\n",
        "\n",
        "1. **Seed Candidate**: We start with a naive O(n²) bubble sort\n",
        "2. **Dataset**: Four test cases including a worst-case reversed list\n",
        "3. **Fitness Function**: \n",
        "   - Returns correctness score (0 or 1)\n",
        "   - Returns **rich side_info** including input, output, timing, and errors\n",
        "4. **Optimization**: GEPA will evolve the code to find faster algorithms\n",
        "5. **Result**: Often discovers quicksort or similar O(n log n) algorithms\n",
        "\n",
        "The key is the `side_info` dictionary—it tells GEPA exactly what went wrong so it can make targeted improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "729fb723",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## The Challenge\n",
        "\n",
        "KernelBench provides PyTorch reference models. The candidate must produce a `ModelNew` with custom CUDA kernels (via `load_inline`) that is correct and faster than the PyTorch baseline.\n",
        "\n",
        "## The Task\n",
        "\n",
        "**Given**: PyTorch reference models + baseline runtimes\n",
        "**Find**: Prompts that generate correct, faster CUDA kernels\n",
        "\n",
        "Results are hardware-dependent; see `outputs/artifacts/kernelbench/` for latest runs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9885dc9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "levels = [\"level1\"]\n",
        "run_name = time.strftime(\"%y%m%d_%H%M%S\")\n",
        "log_dir = f\"outputs/artifacts/kernelbench/{run_name}\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "dataset = load_dataset(levels=levels)\n",
        "baselines = load_or_measure_baselines(dataset)\n",
        "\n",
        "available_gpus = get_free_gpus() or list(range(4))\n",
        "init_gpu_manager(device_list=available_gpus, lock_dir=os.path.join(log_dir, \"gpu_locks\"))\n",
        "\n",
        "tracker = StateTracker(log_dir=log_dir, total_problems=len(dataset))\n",
        "kernel_gen_cache = PromptCache(cache_dir=os.path.join(log_dir, \"kernel_gen_cache\"), name=\"kernel_gen\")\n",
        "refiner_cache = PromptCache(cache_dir=os.path.join(log_dir, \"refiner_cache\"), name=\"refiner\")\n",
        "\n",
        "lm = dspy.LM(LLM, temperature=1.0, max_tokens=32000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae63f2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "fitness_fn = create_fitness_fn(\n",
        "    lm,\n",
        "    baselines=baselines,\n",
        "    use_rag=True,\n",
        "    max_refinements=5,\n",
        "    tracker=tracker,\n",
        "    kernel_gen_cache=kernel_gen_cache,\n",
        "    refiner_cache=refiner_cache,\n",
        ")\n",
        "\n",
        "config = GEPAConfig(\n",
        "    engine=EngineConfig(\n",
        "        run_dir=log_dir,\n",
        "        max_metric_calls=2000,\n",
        "        cache_evaluation=True,\n",
        "        track_best_outputs=True,\n",
        "        parallel=False,\n",
        "        max_workers=1,\n",
        "    ),\n",
        "    reflection=ReflectionConfig(\n",
        "        reflection_minibatch_size=3,\n",
        "        reflection_lm=LLM,\n",
        "    ),\n",
        ")\n",
        "\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=dataset,\n",
        "    config=config,\n",
        "    objective=objective,\n",
        "    background=BACKGROUND,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b326af7",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## When to Use `optimize_anything`\n",
        "\n",
        "### Best Use Cases\n",
        "\n",
        "| Problem Type | Example | Why GEPA Excels |\n",
        "|--------------|---------|-----------------|\n",
        "| **Prompt Engineering** | System prompts, few-shot examples | LLM understands language nuances |\n",
        "| **Code Evolution** | Algorithm design, bug fixes | LLM can read and write code |\n",
        "| **Agent Architecture** | DSPy programs, reasoning pipelines | LLM can propose structural changes |\n",
        "| **Configuration Tuning** | JSON/YAML configs | LLM understands parameter relationships |\n",
        "| **Template Optimization** | Email templates, API specs | LLM understands domain context |\n",
        "\n",
        "### When Traditional Methods May Be Better\n",
        "\n",
        "| Problem Type | Better Alternative | Why |\n",
        "|--------------|-------------------|-----|\n",
        "| **Neural Network Training** | PyTorch + SGD | Gradient information is crucial |\n",
        "| **Convex Optimization** | SciPy, CVXPY | Mathematical structure exploitable |\n",
        "| **Combinatorial (small scale)** | OR-Tools, SAT solvers | Exact methods available |\n",
        "\n",
        "### The Rule of Thumb\n",
        "\n",
        "**Use `optimize_anything` when:**\n",
        "1. The artifact being optimized can be meaningfully represented as text\n",
        "2. You can provide informative feedback about why candidates fail\n",
        "3. Domain knowledge would help but isn't easily encoded as math\n",
        "4. The search space is too complex for grid/random search\n",
        "\n",
        "---\n",
        "\n",
        "*Questions? Issues? Contributions welcome at [github.com/stanfordnlp/gepa](https://github.com/stanfordnlp/gepa)*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
