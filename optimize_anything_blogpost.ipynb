{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eefb064",
   "metadata": {},
   "source": [
    "# GEPA: Optimize Anything with LLMs\n",
    "\n",
    "LLM-based optimization algorithms like GEPA, OpenEvolve, ShinkaEvolve, and AlphaEvolve have shown real promise. But their focus has been fragmented: GEPA focused on evolving LLM prompts, while others targeted scientific and algorithmic discovery. Today, we're announcing GEPA's new `optimize_anything` API—an open-source library you can plug into any scenario to optimize text of any kind: prompts, code, agents, etc.\n",
    "\n",
    "With this API, GEPA becomes a general-purpose text evolution engine. Given a target metric, GEPA efficiently searches for the right parameters to improve that metric. It uses LLMs to generate proposals and leverages side information from the optimization environment to guide the search. This means GEPA can optimize essentially *anything* with a textual representation.\n",
    "\n",
    "To show what this looks like in practice, we provide four examples:\n",
    "- Mathematical optimization. GEPA can outperform Optuna on Evalset.\n",
    "- Prompt evolution. We evolve a prompt for GPT-4.1 Mini on AIME 2025, improving accuracy from 46.67% to 53.33%.\n",
    "- Agent program evolution. We evolve the full agent program for GPT-5 on ARC-AGI, boosting scores from 55.6% to 60.5%.\n",
    "- Algorithmic discovery. We tackle a classic combinatorial optimization problem, Circle Packing, achieving 100+% of AlphaEvolve and OpenEvolve's solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52b3cc",
   "metadata": {},
   "source": [
    "## The optimize_anything API\n",
    "\n",
    "At its core, the API is remarkably simple. You provide just two things:\n",
    "\n",
    "1. **A seed candidate** — your starting point, represented as a dictionary mapping parameter names to their values. \n",
    "2. **A fitness function** — tells GEPA how good each candidate is. The fitness function also returns any additional information available from the environment about the evaluated candidate, like compiler error messages, that can guide the optimization.\n",
    "\n",
    "That's it. GEPA handles the rest — selecting candidates, reflecting on failures, proposing improvements, and tracking the optimization trajectory, finally returning the optimized parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b5553",
   "metadata": {},
   "source": [
    "### The Fitness Function: Your Optimization Signal\n",
    "\n",
    "The fitness function is where you define *what* you're optimizing for. It takes a candidate and a data instance, returning scores and diagnostic information:\n",
    "\n",
    "```python\n",
    "def fitness_fn(candidate: dict[str, str], instance: DataInst) -> list[tuple[float, Any, dict]]:\n",
    "    # Run your system with the candidate parameters\n",
    "    output = run_my_system(candidate, instance)\n",
    "    \n",
    "    # Compute a score (higher is better)\n",
    "    score = compute_score(output, instance)\n",
    "    \n",
    "    # Collect diagnostic info for LLM reflection\n",
    "    side_info = {\n",
    "        \"input\": instance,\n",
    "        \"output\": output,\n",
    "        \"expected\": instance[\"expected\"],\n",
    "        \"error_analysis\": analyze_errors(output)\n",
    "    }\n",
    "    return (score, output, side_info)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf16cd",
   "metadata": {},
   "source": [
    "### The Power of Side Information\n",
    "\n",
    "The `side_info` dictionary empowers GEPA. Unlike traditional optimization that only sees a scalar score, GEPA's LLM-based reflection can understand *why* a candidate performed poorly:\n",
    "\n",
    "- **Error messages**: Compiler errors, runtime exceptions, validation failures\n",
    "- **Execution traces**: What the candidate actually did vs. what was expected\n",
    "- **Partial results**: Which sub-tasks succeeded, which failed\n",
    "- **Domain-specific feedback**: Any signal that helps explain performance\n",
    "\n",
    "You have complete control over what to put inside `side_info.` The more informative your `side_info`, the better GEPA can reason about improvements. This enables GEPA to optimize complex artifacts like code and agent architectures — not just tweak numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f143f",
   "metadata": {},
   "source": [
    "## Example 1: Mathematical optimization\n",
    "\n",
    "We first demonstrate GEPA's ability to evolve a search code that minimizes blackbox functions from the [evalset benchmark](https://github.com/sigopt/evalset/tree/main) — a collection of challenging optimization test functions (Ackley, Rosenbrock, Rastrigin, etc.) benchmarked by the [Optuna paper](https://arxiv.org/pdf/1907.10902). \n",
    "\n",
    "**The task**: Given a blackbox function, write code that finds its minimum. The code can use any optimization library (Optuna, scipy, etc.) and returns the best `x`.\n",
    "\n",
    "**What GEPA optimizes**: The Python code itself — its structure, algorithm choice, hyperparameters, and implementation details.\n",
    "\n",
    "The figure below shows GEPA competing with Optuna. Starting from minimal baseline code, GEPA initially underperforms. However, it progressively discovers more effective optimization strategies, eventually finding better solutions than Optuna in later stages. \n",
    "\n",
    "<img src=\"./assets/blog/polynomial_optimization_normalized.png\" width=\"50%\">\n",
    "\n",
    "Here is the code to reproduce this experiment: [link].\n",
    "\n",
    "While Optuna requires users to select sampling algorithms and techniques for advanced use cases, GEPA frees you from such decisions. By simply defining a baseline code template and the fitness function, GEPA automatically evolves the search code—from experimenting with high-level search strategies to fine-tuning hyperparameters. \n",
    "\n",
    "Now, let's walk through a simple example of optimizing code on a single problem from evalset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ae873",
   "metadata": {},
   "source": [
    "### Setting up the dataset\n",
    "\n",
    "Here, the dataset is a single blackbox optimization problem with bounds, dimension, and problem characteristics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76ced13f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'sample_problem' from 'examples.polynomial.evalset' (/Users/lukedhlee/luke_optany/external/gepa-optimize-anything/examples/polynomial/evalset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexamples\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolynomial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevalset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sample_problem\n\u001b[32m      3\u001b[39m dataset = [sample_problem]\n\u001b[32m      4\u001b[39m dataset\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'sample_problem' from 'examples.polynomial.evalset' (/Users/lukedhlee/luke_optany/external/gepa-optimize-anything/examples/polynomial/evalset.py)"
     ]
    }
   ],
   "source": [
    "from examples.polynomial.evalset import sample_problem\n",
    "\n",
    "dataset = [sample_problem]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a91948d",
   "metadata": {},
   "source": [
    "### The seed candidate\n",
    "\n",
    "We start with a trivial baseline that randomly samples a solution. The function signature exposes the problem structure—`dim` and `bounds` define the search space, `total_evaluation_budgets` limits how many times the code can call the objective, and `prev_best_x` provides the best solution found so far (if any). The solver code can use `objective_function` to evaluate and compare candidates before returning its best guess.\n",
    "\n",
    "(is this function too complicated?)\n",
    "- maybe i could specify total_evaluation_budgets in the LLM prompt instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcad3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_candidate = \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def solve(dim, bounds, objective_function, prev_best_x):\n",
    "    bounds_arr = np.array(bounds)\n",
    "    x = np.random.uniform(bounds_arr[:, 0], bounds_arr[:, 1])\n",
    "    y = objective_function(x)\n",
    "    return x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd33611",
   "metadata": {},
   "source": [
    "### The fitness function\n",
    "\n",
    "The fitness function executes the candidate code in a sandboxed environment, captures the result, and returns rich diagnostic information:\n",
    "\n",
    "(update the code below to use our real code. but currently it looks quite complicated. let's modify it. let's abstrsact awway none-gepa-related codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from examples.new_polynomial.evaluator import execute_code, compute_score\n",
    "\n",
    "# TODO: add best_side_info to the fitness function\n",
    "def fitness_fn(candidate: dict[str, str], problem: Any, best_side_info: dict) -> list[tuple[float, Any, dict]]:\n",
    "    code = candidate[\"code\"]\n",
    "    execution = execute_code(code, 300, {\n",
    "        \"dim\": problem[\"dim\"], \n",
    "        \"bounds\": problem[\"bounds\"], \n",
    "        \"objective_function\": problem[\"objective_function\"],\n",
    "        \"prev_best_x\": best_side_info[\"X\"],\n",
    "    })\n",
    "    score = compute_score(execution)\n",
    "    \n",
    "    side_info = {\n",
    "        \"scores\": {\"score\": score},\n",
    "        \"Input\": {\"problem_description\": problem[\"problem_description\"]},\n",
    "        \"code_side_info\": {\n",
    "            \"X\": execution[\"results\"].get(\"x\", \"not found\"),\n",
    "            \"Prints\": execution[\"output\"],       # Captured stdout\n",
    "            \"Logs\": execution[\"logs\"],           # Captured stderr  \n",
    "            \"Error\": execution[\"error\"],         # Any exceptions\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    return score, {\"code\": code, **side_info}, side_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218c04c",
   "metadata": {},
   "source": [
    "Notice how `side_info` captures everything the LLM needs to understand *why* the code failed or succeeded: error messages, print output, and the result found.\n",
    "\n",
    "### Running GEPA optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa.optimize_anything import (\n",
    "    optimize_anything,\n",
    "    GEPAConfig,\n",
    "    EngineConfig,\n",
    "    ReflectionConfig,\n",
    ")\n",
    "\n",
    "result = optimize_anything(\n",
    "    seed_candidate=seed_candidate,\n",
    "    fitness_fn=fitness_fn,\n",
    "    dataset=dataset,\n",
    "    config=GEPAConfig(\n",
    "        engine=EngineConfig(\n",
    "            max_metric_calls=100,  # tweak the number\n",
    "            track_best_outputs=True,\n",
    "            cache_evaluation=True,  # TODO: add\n",
    "        ),\n",
    "        reflection=ReflectionConfig(\n",
    "            reflection_lm=\"openai/gpt-5.1\",\n",
    "            reflection_minibatch_size=1,     # Problems shown per reflection. In this example, we have only one problem, and thus we set it 1.\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Access the optimized code\n",
    "print(result.best_candidate[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be2b18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# EVOLVE-BLOCK-START\n",
      "# MUTATION APPLIED: Added zero-vector warm-start, ridge linear surrogate directional probes, and a budget-limited Nelder–Mead subspace finisher\n",
      "# RATIONALE: Zero seed targets typical polynomial optima near origin; ridge-linear steps cheaply exploit global gradient hints from archive; Nelder–Mead subspace can squeeze extra improvements near the end without heavy modeling\n",
      "\n",
      "import numpy as np\n",
      "import os\n",
      "import json\n",
      "import math\n",
      "import time\n",
      "\n",
      "\n",
      "def solve(dim, total_evaluation_budgets, bounds):\n",
      "    # Bounds and helpers\n",
      "    lb = np.array([b[0] for b in bounds], dtype=float)\n",
      "    ub = np.array([b[1] for b in bounds], dtype=float)\n",
      "    span = ub - lb\n",
      "    mid = (lb + ub) / 2.0\n",
      "\n",
      "    # Anytime-valid best\n",
      "    best_x = mid.copy()\n",
      "    best_y = -np.inf\n",
      "    evals = 0\n",
      "    budgets = int(total_evaluation_budgets)\n",
      "\n",
      "    rng = np.random.default_rng()\n",
      "\n",
      "    # Reflection to bounds (mirror) for arrays of points\n",
      "    def reflect_to_bounds(X):\n",
      "        X = np.asarray(X, dtype=float)\n",
      "        if X.ndim == 1:\n",
      "            Y = X.copy()\n",
      "            for j in range(dim):\n",
      "                if span[j] <= 0:\n",
      "                    Y[j] = lb[j]\n",
      "                    continue\n",
      "                r = (Y[j] - lb[j]) % (2.0 * span[j])\n",
      "                if r <= span[j]:\n",
      "                    Y[j] = lb[j] + r\n",
      "                else:\n",
      "                    Y[j] = ub[j] - (r - span[j])\n",
      "            return Y\n",
      "        else:\n",
      "            Y = X.copy()\n",
      "            for j in range(dim):\n",
      "                if span[j] <= 0:\n",
      "                    Y[:, j] = lb[j]\n",
      "                    continue\n",
      "                r = (Y[:, j] - lb[j]) % (2.0 * span[j])\n",
      "                mask = r <= span[j]\n",
      "                Y[mask, j] = lb[j] + r[mask]\n",
      "                Y[~mask, j] = ub[j] - (r[~mask] - span[j])\n",
      "            return Y\n",
      "\n",
      "    # Archive of evaluated points\n",
      "    archive_X = []\n",
      "    archive_y = []\n",
      "    archive_cap = max(800, 30 * dim)  # larger local history for surrogates\n",
      "\n",
      "    # Safe evaluation wrapper\n",
      "    def evaluate(xcand):\n",
      "        nonlocal evals, best_x, best_y\n",
      "        if evals >= budgets:\n",
      "            return None\n",
      "        xclip = reflect_to_bounds(xcand)  # reflection ensures in-bounds\n",
      "        try:\n",
      "            y = objective_function(xclip)\n",
      "        except Exception:\n",
      "            y = -np.inf\n",
      "        evals += 1\n",
      "        if not np.isfinite(y):\n",
      "            y = -np.inf\n",
      "        # push to archive\n",
      "        archive_X.append(xclip.copy())\n",
      "        archive_y.append(float(y))\n",
      "        if len(archive_X) > archive_cap:\n",
      "            idx = np.argsort(np.array(archive_y))[::-1]\n",
      "            keep_top = idx[: archive_cap // 2]\n",
      "            keep_recent = np.arange(max(0, len(archive_X) - archive_cap // 2), len(archive_X))\n",
      "            keep_idx = np.unique(np.concatenate([keep_top, keep_recent]))\n",
      "            archive_X[:] = [archive_X[i] for i in keep_idx]\n",
      "            archive_y[:] = [archive_y[i] for i in keep_idx]\n",
      "        if y > best_y:\n",
      "            best_y = float(y)\n",
      "            best_x = xclip.copy()\n",
      "            print(f\"Improved: eval {evals}/{budgets}, best_y={best_y:.6f}\")\n",
      "        return y\n",
      "\n",
      "    # Halton sequence for initialization\n",
      "    def halton(n, d):\n",
      "        def next_prime():\n",
      "            primes = []\n",
      "            candidate = 2\n",
      "            while True:\n",
      "                is_p = True\n",
      "                r = int(candidate**0.5) + 1\n",
      "                for p in primes:\n",
      "                    if p > r:\n",
      "                        break\n",
      "                    if candidate % p == 0:\n",
      "                        is_p = False\n",
      "                        break\n",
      "                if is_p:\n",
      "                    primes.append(candidate)\n",
      "                    yield candidate\n",
      "                candidate += 1\n",
      "\n",
      "        def vdc(i, base):\n",
      "            v = 0.0\n",
      "            denom = 1.0\n",
      "            while i > 0:\n",
      "                i, rem = divmod(i, base)\n",
      "                denom *= base\n",
      "                v += rem / denom\n",
      "            return v\n",
      "\n",
      "        g = next_prime()\n",
      "        bases = [next(g) for _ in range(d)]\n",
      "        H = np.zeros((n, d), dtype=float)\n",
      "        for j in range(d):\n",
      "            b = bases[j]\n",
      "            for i in range(n):\n",
      "                H[i, j] = vdc(i + 1, b)\n",
      "        return H\n",
      "\n",
      "    # Simple Latin Hypercube Sampling in [0,1]^d\n",
      "    def lhs(n, d):\n",
      "        P = np.zeros((n, d), dtype=float)\n",
      "        for j in range(d):\n",
      "            perm = rng.permutation(n)\n",
      "            P[:, j] = (perm + rng.random(n)) / n\n",
      "        return P\n",
      "\n",
      "    # Warm-start seeds\n",
      "    seeds = []\n",
      "\n",
      "    env_x = os.environ.get(\"PAST_BEST_X\", \"\").strip()\n",
      "    if env_x:\n",
      "        try:\n",
      "            arr = None\n",
      "            if env_x.startswith(\"[\"):\n",
      "                arr = np.array(json.loads(env_x), dtype=float)\n",
      "            else:\n",
      "                arr = np.array([float(v) for v in env_x.split(\",\")], dtype=float)\n",
      "            if arr.shape == (dim,):\n",
      "                arr = reflect_to_bounds(arr)\n",
      "                seeds.append(arr)\n",
      "                print(\"Warm-start: using PAST_BEST_X from environment\")\n",
      "        except Exception as e:\n",
      "            print(f\"Warning: failed to parse PAST_BEST_X: {e}\")\n",
      "\n",
      "    # Previous trace for the known 11D box\n",
      "    if dim == 11 and np.allclose(lb, -10) and np.allclose(ub, 30):\n",
      "        prev_trace_x = np.array(\n",
      "            [\n",
      "                15.20424723,\n",
      "                29.3365135,\n",
      "                8.96735818,\n",
      "                5.99694284,\n",
      "                17.86117045,\n",
      "                7.47621172,\n",
      "                25.77999103,\n",
      "                -6.53946212,\n",
      "                6.99513464,\n",
      "                1.20286935,\n",
      "                0.21514371,\n",
      "            ],\n",
      "            dtype=float,\n",
      "        )\n",
      "        if prev_trace_x.shape == (dim,):\n",
      "            seeds.append(reflect_to_bounds(prev_trace_x))\n",
      "            print(\"Warm-start: using previous trace candidate for dim=11\")\n",
      "\n",
      "    # Midpoint and zero-vector seeds (zero often near polynomial optima)\n",
      "    seeds.append(mid.copy())\n",
      "    zero_vec = np.zeros(dim, dtype=float)\n",
      "    if np.all(zero_vec >= lb) and np.all(zero_vec <= ub):\n",
      "        seeds.append(zero_vec)\n",
      "        print(\"Seeding: added zero vector\")\n",
      "\n",
      "    # Halton + LHS seeds mapped to bounds\n",
      "    extra_seeds = max(0, min(8, budgets - len(seeds)))\n",
      "    if extra_seeds > 0:\n",
      "        H = halton((extra_seeds + 1) // 2, dim)\n",
      "        L = lhs(extra_seeds - H.shape[0], dim) if extra_seeds > H.shape[0] else np.zeros((0, dim))\n",
      "        S = []\n",
      "        for i in range(H.shape[0]):\n",
      "            S.append(lb + H[i] * span)\n",
      "        for i in range(L.shape[0]):\n",
      "            S.append(lb + L[i] * span)\n",
      "        seeds.extend(S)\n",
      "\n",
      "    # A couple random seeds\n",
      "    n_random_seeds = max(1, min(2, budgets - len(seeds)))\n",
      "    for _ in range(n_random_seeds):\n",
      "        seeds.append(rng.uniform(lb, ub))\n",
      "\n",
      "    # Evaluate seeds\n",
      "    for s in seeds:\n",
      "        if evals >= budgets:\n",
      "            break\n",
      "        evaluate(s)\n",
      "\n",
      "    if evals >= budgets:\n",
      "        print(f\"Finished during seeding. Best y: {best_y:.6f}\")\n",
      "        print(\"y: \", best_y)\n",
      "        return best_x\n",
      "\n",
      "    # ES parameters\n",
      "    def suggest_lambda(rem):\n",
      "        # slightly stronger population\n",
      "        base = int(max(10, 6 + 3.8 * np.log(dim + 5)))\n",
      "        base = int(min(base + rem // max(60, 10 * dim), base + 8))\n",
      "        return max(4, min(base, rem))\n",
      "\n",
      "    sigma = 0.20  # relative to span\n",
      "    sigma_min, sigma_max = 0.004, 0.6  # allow finer final search\n",
      "\n",
      "    m = best_x.copy()\n",
      "    m_y = best_y\n",
      "\n",
      "    stall_gens = 0\n",
      "    max_stall_gens = 10\n",
      "    gen = 0\n",
      "    last_grad_gen = -999\n",
      "\n",
      "    # Local coordinate pattern search with smaller granularity\n",
      "    def local_refine(max_evals):\n",
      "        nonlocal sigma\n",
      "        if max_evals <= 0:\n",
      "            return\n",
      "        s = np.clip(0.5 * sigma * span, 0.001 * span, 0.2 * span)\n",
      "        shrink = 0.5\n",
      "        min_step = np.maximum(1e-5 * span, 1e-8)  # absolute minimal step\n",
      "        while (max_evals > 0) and np.any(s > min_step):\n",
      "            improved_loop = False\n",
      "            order = rng.permutation(dim)\n",
      "            for j in order:\n",
      "                if max_evals <= 0:\n",
      "                    break\n",
      "                if s[j] <= min_step[j]:\n",
      "                    continue\n",
      "                for direction in (+1, -1):\n",
      "                    if max_evals <= 0:\n",
      "                        break\n",
      "                    current = best_x.copy()\n",
      "                    step = s[j]\n",
      "                    candidate = current.copy()\n",
      "                    candidate[j] = candidate[j] + direction * step\n",
      "                    y = evaluate(candidate)\n",
      "                    max_evals -= 1\n",
      "                    if y is None:\n",
      "                        return\n",
      "                    if y > best_y:\n",
      "                        improved_loop = True\n",
      "                        while max_evals > 0:\n",
      "                            current = best_x.copy()\n",
      "                            step *= 1.6\n",
      "                            if step < min_step[j]:\n",
      "                                break\n",
      "                            candidate2 = current.copy()\n",
      "                            candidate2[j] = candidate2[j] + direction * step\n",
      "                            y2 = evaluate(candidate2)\n",
      "                            max_evals -= 1\n",
      "                            if y2 is None or not (y2 > best_y):\n",
      "                                break\n",
      "                if not improved_loop:\n",
      "                    s[j] *= shrink\n",
      "            if not improved_loop:\n",
      "                s *= shrink\n",
      "            sigma = max(sigma_min, min(sigma_max, float(np.median(s / np.maximum(span, 1e-12)))))\n",
      "\n",
      "    # Quadratic surrogate trust-region proposals around best_x using full archive\n",
      "    def quad_trust_propose(max_evals):\n",
      "        if max_evals <= 0 or len(archive_X) < dim + 5:\n",
      "            return 0\n",
      "        X = np.array(archive_X, dtype=float)\n",
      "        y = np.array(archive_y, dtype=float)\n",
      "        maskf = np.isfinite(y)\n",
      "        X = X[maskf]\n",
      "        y = y[maskf]\n",
      "        if len(X) < dim + 5:\n",
      "            return 0\n",
      "        U = (X - best_x) / np.maximum(span, 1e-12)\n",
      "        p = 1 + dim + dim + (dim * (dim - 1)) // 2\n",
      "        target = min(len(U), max(6 * dim, p + 10))\n",
      "        r = max(0.1 * sigma, 0.02)\n",
      "        sel_idx = np.where(np.linalg.norm(U, axis=1) <= r)[0]\n",
      "        attempts = 0\n",
      "        while (len(sel_idx) < min(target, len(U))) and attempts < 5:\n",
      "            r *= 1.7\n",
      "            sel_idx = np.where(np.linalg.norm(U, axis=1) <= r)[0]\n",
      "            attempts += 1\n",
      "        if len(sel_idx) < max(dim + 5, p // 2):\n",
      "            dists = np.linalg.norm(U, axis=1)\n",
      "            sel_idx = np.argsort(dists)[: min(target, len(U))]\n",
      "        Usel = U[sel_idx]\n",
      "        ysel = y[sel_idx]\n",
      "        n = len(Usel)\n",
      "        if n < dim + 5:\n",
      "            return 0\n",
      "\n",
      "        ones = np.ones((n, 1), dtype=float)\n",
      "        A_parts = [ones, Usel, Usel**2]\n",
      "        cross_cols = []\n",
      "        pairs = []\n",
      "        for i in range(dim):\n",
      "            for j in range(i + 1, dim):\n",
      "                cross_cols.append((Usel[:, i] * Usel[:, j])[:, None])\n",
      "                pairs.append((i, j))\n",
      "        if len(cross_cols) > 0:\n",
      "            A_parts.append(np.hstack(cross_cols))\n",
      "        A = np.hstack(A_parts)\n",
      "        lam = max(1e-6, 1e-3 * np.maximum(1.0, np.var(ysel)))\n",
      "        A_reg = np.vstack([A, math.sqrt(lam) * np.eye(A.shape[1])])\n",
      "        y_reg = np.concatenate([ysel, np.zeros(A.shape[1], dtype=float)])\n",
      "        try:\n",
      "            w, *_ = np.linalg.lstsq(A_reg, y_reg, rcond=1e-3)\n",
      "        except Exception:\n",
      "            return 0\n",
      "\n",
      "        idx_const = 0\n",
      "        idx_lin_start = 1\n",
      "        idx_lin_end = 1 + dim\n",
      "        idx_sq_start = idx_lin_end\n",
      "        idx_sq_end = idx_sq_start + dim\n",
      "        idx_cross_start = idx_sq_end\n",
      "        g = w[idx_lin_start:idx_lin_end].copy()\n",
      "        Q = np.zeros((dim, dim), dtype=float)\n",
      "        sq_coeffs = w[idx_sq_start:idx_sq_end]\n",
      "        for i in range(dim):\n",
      "            Q[i, i] = 2.0 * sq_coeffs[i]\n",
      "        offset = idx_cross_start\n",
      "        for k, (i, j) in enumerate(pairs):\n",
      "            Q[i, j] = w[offset + k]\n",
      "            Q[j, i] = w[offset + k]\n",
      "\n",
      "        try:\n",
      "            eig_max = np.linalg.eigvalsh((Q + Q.T) * 0.5).max()\n",
      "        except Exception:\n",
      "            eig_max = np.max(np.diag(Q))\n",
      "        if eig_max >= -1e-8:\n",
      "            Qc = Q - (eig_max + 1e-3) * np.eye(dim)\n",
      "        else:\n",
      "            Qc = Q\n",
      "\n",
      "        proposals = []\n",
      "        try:\n",
      "            u_star = -np.linalg.solve(Qc, g)\n",
      "        except Exception:\n",
      "            diag = np.diag(Qc)\n",
      "            diag = np.where(diag < -1e-8, diag, -1e-3)\n",
      "            u_star = -g / diag\n",
      "\n",
      "        r_tr = min(0.6, max(0.12 * sigma, 0.02))\n",
      "        norm_u = np.linalg.norm(u_star)\n",
      "        if norm_u > r_tr and norm_u > 0:\n",
      "            u_star = u_star * (r_tr / norm_u)\n",
      "        x_star = best_x + u_star * span\n",
      "        proposals.append(x_star)\n",
      "\n",
      "        diag = np.diag(Qc)\n",
      "        diag = np.where(diag < -1e-8, diag, -1e-3)\n",
      "        u_diag = -g / diag\n",
      "        norm_ud = np.linalg.norm(u_diag)\n",
      "        if norm_ud > r_tr and norm_ud > 0:\n",
      "            u_diag = u_diag * (r_tr / norm_ud)\n",
      "        proposals.append(best_x + u_diag * span)\n",
      "\n",
      "        if norm_u > 1e-12:\n",
      "            for alpha in (0.5, 1.5):\n",
      "                u_ls = u_star * alpha\n",
      "                norm_ls = np.linalg.norm(u_ls)\n",
      "                if norm_ls > r_tr and norm_ls > 0:\n",
      "                    u_ls = u_ls * (r_tr / norm_ls)\n",
      "                proposals.append(best_x + u_ls * span)\n",
      "\n",
      "        for _ in range(2):\n",
      "            du = rng.standard_normal(dim)\n",
      "            du /= np.linalg.norm(du) + 1e-12\n",
      "            du *= r_tr * rng.uniform(0.3, 1.0)\n",
      "            proposals.append(x_star + du * span)\n",
      "\n",
      "        used = 0\n",
      "        for p in proposals:\n",
      "            if used >= max_evals or evals >= budgets:\n",
      "                break\n",
      "            evaluate(p)\n",
      "            used += 1\n",
      "        return used\n",
      "\n",
      "    # Subspace (active-dim) quadratic trust-region proposals\n",
      "    def subspace_quad_trust_propose(k_dims, max_evals):\n",
      "        if max_evals <= 0 or len(archive_X) < max(dim // 2, 12):\n",
      "            return 0\n",
      "        X = np.array(archive_X, dtype=float)\n",
      "        y = np.array(archive_y, dtype=float)\n",
      "        maskf = np.isfinite(y)\n",
      "        X = X[maskf]\n",
      "        y = y[maskf]\n",
      "        if len(X) < max(dim // 2, 12):\n",
      "            return 0\n",
      "        U = (X - best_x) / np.maximum(span, 1e-12)\n",
      "        # choose points near best_x\n",
      "        dists = np.linalg.norm(U, axis=1)\n",
      "        sel0 = np.argsort(dists)[: min(len(U), max(10 * k_dims, 5 * k_dims + 10))]\n",
      "        Usel = U[sel0]\n",
      "        ysel = y[sel0]\n",
      "        if len(Usel) < 5 * k_dims + 5:\n",
      "            return 0\n",
      "        # active dims via correlation with y\n",
      "        corrs = []\n",
      "        for j in range(dim):\n",
      "            uj = Usel[:, j]\n",
      "            if np.std(uj) < 1e-12:\n",
      "                corrs.append(0.0)\n",
      "            else:\n",
      "                c = np.corrcoef(uj, ysel)[0, 1]\n",
      "                if not np.isfinite(c):\n",
      "                    c = 0.0\n",
      "                corrs.append(abs(c))\n",
      "        idxs = np.argsort(corrs)[::-1][: min(k_dims, dim)]\n",
      "        k = len(idxs)\n",
      "        if k == 0:\n",
      "            return 0\n",
      "        U_k = Usel[:, idxs]\n",
      "        n = len(U_k)\n",
      "        # Build quadratic in k dims\n",
      "        ones = np.ones((n, 1), dtype=float)\n",
      "        A_parts = [ones, U_k, U_k**2]\n",
      "        cross_cols = []\n",
      "        pairs = []\n",
      "        for i in range(k):\n",
      "            for j in range(i + 1, k):\n",
      "                cross_cols.append((U_k[:, i] * U_k[:, j])[:, None])\n",
      "                pairs.append((i, j))\n",
      "        if len(cross_cols) > 0:\n",
      "            A_parts.append(np.hstack(cross_cols))\n",
      "        A = np.hstack(A_parts)\n",
      "        p = 1 + k + k + (k * (k - 1)) // 2\n",
      "        if n < p + 3:\n",
      "            return 0\n",
      "        lam = max(1e-6, 1e-3 * np.maximum(1.0, np.var(ysel)))\n",
      "        A_reg = np.vstack([A, math.sqrt(lam) * np.eye(A.shape[1])])\n",
      "        y_reg = np.concatenate([ysel, np.zeros(A.shape[1], dtype=float)])\n",
      "        try:\n",
      "            w, *_ = np.linalg.lstsq(A_reg, y_reg, rcond=1e-3)\n",
      "        except Exception:\n",
      "            return 0\n",
      "        idx_const = 0\n",
      "        idx_lin_start = 1\n",
      "        idx_lin_end = 1 + k\n",
      "        idx_sq_start = idx_lin_end\n",
      "        idx_sq_end = idx_sq_start + k\n",
      "        idx_cross_start = idx_sq_end\n",
      "        gk = w[idx_lin_start:idx_lin_end].copy()\n",
      "        Qk = np.zeros((k, k), dtype=float)\n",
      "        sq_coeffs = w[idx_sq_start:idx_sq_end]\n",
      "        for i in range(k):\n",
      "            Qk[i, i] = 2.0 * sq_coeffs[i]\n",
      "        offset = idx_cross_start\n",
      "        for t, (i, j) in enumerate(pairs):\n",
      "            Qk[i, j] = w[offset + t]\n",
      "            Qk[j, i] = w[offset + t]\n",
      "        # make concave\n",
      "        try:\n",
      "            eig_max = np.linalg.eigvalsh((Qk + Qk.T) * 0.5).max()\n",
      "        except Exception:\n",
      "            eig_max = np.max(np.diag(Qk))\n",
      "        if eig_max >= -1e-8:\n",
      "            Qkc = Qk - (eig_max + 1e-3) * np.eye(k)\n",
      "        else:\n",
      "            Qkc = Qk\n",
      "        try:\n",
      "            uk = -np.linalg.solve(Qkc, gk)\n",
      "        except Exception:\n",
      "            diag = np.diag(Qkc)\n",
      "            diag = np.where(diag < -1e-8, diag, -1e-3)\n",
      "            uk = -gk / diag\n",
      "        r_tr = min(0.5, max(0.1 * sigma, 0.02))\n",
      "        nu = np.linalg.norm(uk)\n",
      "        if nu > r_tr and nu > 0:\n",
      "            uk = uk * (r_tr / nu)\n",
      "        # map back to full space\n",
      "        full_u = np.zeros(dim, dtype=float)\n",
      "        full_u[idxs] = uk\n",
      "        props = [best_x + full_u * span]\n",
      "        # a couple of radial variants\n",
      "        for alpha in (0.6, 1.3):\n",
      "            uk2 = uk * alpha\n",
      "            if np.linalg.norm(uk2) > r_tr and np.linalg.norm(uk2) > 0:\n",
      "                uk2 = uk2 * (r_tr / np.linalg.norm(uk2))\n",
      "            fu2 = np.zeros(dim, dtype=float)\n",
      "            fu2[idxs] = uk2\n",
      "            props.append(best_x + fu2 * span)\n",
      "        used = 0\n",
      "        for pnt in props:\n",
      "            if used >= max_evals or evals >= budgets:\n",
      "                break\n",
      "            evaluate(pnt)\n",
      "            used += 1\n",
      "        return used\n",
      "\n",
      "    # Finite-difference gradient trust-region probes near best_x\n",
      "    def gradient_tr_probe(max_evals, gen_idx):\n",
      "        nonlocal last_grad_gen\n",
      "        if max_evals <= 0:\n",
      "            return 0\n",
      "        if gen_idx - last_grad_gen < 4:\n",
      "            return 0\n",
      "        last_grad_gen = gen_idx\n",
      "\n",
      "        per_dim_cost = 2\n",
      "        max_dims = max(1, min(dim, max_evals // per_dim_cost))\n",
      "        if max_dims <= 0:\n",
      "            return 0\n",
      "        idxs = np.arange(dim)\n",
      "        rng.shuffle(idxs)\n",
      "        idxs = idxs[:max_dims]\n",
      "\n",
      "        g = np.zeros(dim, dtype=float)\n",
      "        h_base = np.clip(0.01 * sigma, 0.002, 0.05)\n",
      "        used = 0\n",
      "        for j in idxs:\n",
      "            if evals >= budgets or used + 2 > max_evals:\n",
      "                break\n",
      "            h = max(h_base * span[j], 1e-8)\n",
      "            xp = best_x.copy()\n",
      "            xp[j] += h\n",
      "            xm = best_x.copy()\n",
      "            xm[j] -= h\n",
      "            yp = evaluate(xp)\n",
      "            used += 1\n",
      "            ym = evaluate(xm)\n",
      "            used += 1\n",
      "            if yp is None or ym is None:\n",
      "                continue\n",
      "            g[j] = (yp - ym) / (2.0 * h)\n",
      "\n",
      "        normg = np.linalg.norm(g)\n",
      "        if normg <= 1e-16:\n",
      "            return used\n",
      "\n",
      "        d = g / (normg + 1e-12)\n",
      "        r_tr = min(0.5, max(0.1 * sigma, 0.01))\n",
      "        for alpha in (0.3, 0.8, 1.2):\n",
      "            if evals >= budgets or used >= max_evals:\n",
      "                break\n",
      "            step = r_tr * alpha\n",
      "            cand = best_x + step * d * span\n",
      "            evaluate(cand)\n",
      "            used += 1\n",
      "        return used\n",
      "\n",
      "    # Ridge-linear surrogate directional probe using archive (cheap global gradient hint)\n",
      "    def ridge_linear_probe(max_evals):\n",
      "        if max_evals <= 0 or len(archive_X) < max(20, 3 * dim):\n",
      "            return 0\n",
      "        X = np.array(archive_X, dtype=float)\n",
      "        y = np.array(archive_y, dtype=float)\n",
      "        maskf = np.isfinite(y)\n",
      "        X = X[maskf]\n",
      "        y = y[maskf]\n",
      "        if len(X) < max(20, 3 * dim):\n",
      "            return 0\n",
      "        U = (X - best_x) / np.maximum(span, 1e-12)\n",
      "        dists = np.linalg.norm(U, axis=1)\n",
      "        # select points near best but with variety\n",
      "        sel = np.argsort(dists)[: min(len(U), max(12 * dim, 80))]\n",
      "        Usel = U[sel]\n",
      "        ysel = y[sel]\n",
      "        n = len(Usel)\n",
      "        ones = np.ones((n, 1), dtype=float)\n",
      "        A = np.hstack([ones, Usel])\n",
      "        lam = max(1e-6, 1e-3 * np.maximum(1.0, np.var(ysel)))\n",
      "        A_reg = np.vstack([A, math.sqrt(lam) * np.eye(A.shape[1])])\n",
      "        y_reg = np.concatenate([ysel, np.zeros(A.shape[1], dtype=float)])\n",
      "        try:\n",
      "            w, *_ = np.linalg.lstsq(A_reg, y_reg, rcond=1e-3)\n",
      "        except Exception:\n",
      "            return 0\n",
      "        g = w[1:].copy()\n",
      "        if not np.all(np.isfinite(g)):\n",
      "            return 0\n",
      "        normg = np.linalg.norm(g)\n",
      "        if normg <= 1e-12:\n",
      "            return 0\n",
      "        d = g / normg\n",
      "        used = 0\n",
      "        r_tr = min(0.55, max(0.12 * sigma, 0.02))\n",
      "        for alpha in (0.5, 1.0, 1.6):\n",
      "            if evals >= budgets or used >= max_evals:\n",
      "                break\n",
      "            cand = best_x + (r_tr * alpha) * d * span\n",
      "            evaluate(cand)\n",
      "            used += 1\n",
      "        return used\n",
      "\n",
      "    # Generate approximately orthogonal noise directions (blocks of up to dim vectors)\n",
      "    def orthogonal_noise(lam):\n",
      "        dirs = []\n",
      "        remaining = lam\n",
      "        while remaining > 0:\n",
      "            k = int(min(dim, remaining))\n",
      "            G = rng.standard_normal((dim, k))\n",
      "            try:\n",
      "                Q, _ = np.linalg.qr(G, mode=\"reduced\")\n",
      "                V = Q.T  # shape (k, dim)\n",
      "            except Exception:\n",
      "                V = G.T / (np.linalg.norm(G, axis=0, keepdims=True).T + 1e-12)\n",
      "            scales = rng.standard_normal(k)\n",
      "            block = (V.T * scales).T  # each row is a direction\n",
      "            dirs.append(block)\n",
      "            remaining -= k\n",
      "        Z = np.vstack(dirs)[:lam]\n",
      "        return Z\n",
      "\n",
      "    # Diagonal sampling scales from elite archive (normalized to median=1)\n",
      "    def diagonal_sampling_scales():\n",
      "        if len(archive_X) < max(20, 4 * dim):\n",
      "            return np.ones(dim, dtype=float)\n",
      "        yarr = np.array(archive_y, dtype=float)\n",
      "        mask = np.isfinite(yarr)\n",
      "        if not np.any(mask):\n",
      "            return np.ones(dim, dtype=float)\n",
      "        idx_sorted = np.argsort(yarr[mask])[::-1]\n",
      "        Xarr = np.array(archive_X, dtype=float)[mask][idx_sorted]\n",
      "        topk = Xarr[: min(len(Xarr), max(20, 6 * dim))]\n",
      "        U = (topk - best_x) / np.maximum(span, 1e-12)\n",
      "        stds = np.std(np.abs(U), axis=0) + 1e-12\n",
      "        med = np.median(stds)\n",
      "        if not np.isfinite(med) or med <= 0:\n",
      "            return np.ones(dim, dtype=float)\n",
      "        scales = np.clip(stds / med, 0.6, 1.8)\n",
      "        return scales\n",
      "\n",
      "    # Occasional global exploration burst\n",
      "    def global_explore(max_evals):\n",
      "        used = 0\n",
      "        if max_evals <= 0:\n",
      "            return 0\n",
      "        n = int(min(max_evals, max(6, 2 * dim)))\n",
      "        for _ in range(n):\n",
      "            if evals >= budgets:\n",
      "                break\n",
      "            # Biased towards best with heavy tails, or uniform\n",
      "            if rng.random() < 0.6:\n",
      "                z = rng.standard_cauchy(dim)\n",
      "                z = np.clip(z, -8, 8)\n",
      "                scale = 0.4 if sigma > 0.05 else 0.7  # push more when sigma small\n",
      "                cand = best_x + scale * sigma * z * span\n",
      "            else:\n",
      "                cand = rng.uniform(lb, ub)\n",
      "            evaluate(cand)\n",
      "            used += 1\n",
      "        return used\n",
      "\n",
      "    # Budget-limited Nelder–Mead in a chosen subspace near best_x\n",
      "    def nelder_mead_subspace(k_dims, max_evals):\n",
      "        if max_evals <= 0:\n",
      "            return 0\n",
      "        # choose active dims by archive correlation; fallback random\n",
      "        if len(archive_X) >= max(20, 3 * dim):\n",
      "            X = np.array(archive_X, dtype=float)\n",
      "            y = np.array(archive_y, dtype=float)\n",
      "            mask = np.isfinite(y)\n",
      "            X = X[mask]\n",
      "            y = y[mask]\n",
      "            U = (X - best_x) / np.maximum(span, 1e-12)\n",
      "            dists = np.linalg.norm(U, axis=1)\n",
      "            sel = np.argsort(dists)[: min(len(U), max(10 * k_dims, 5 * k_dims + 10))]\n",
      "            Usel = U[sel]\n",
      "            ysel = y[sel]\n",
      "            corrs = []\n",
      "            for j in range(dim):\n",
      "                uj = Usel[:, j]\n",
      "                if np.std(uj) < 1e-12:\n",
      "                    corrs.append(0.0)\n",
      "                else:\n",
      "                    c = np.corrcoef(uj, ysel)[0, 1]\n",
      "                    if not np.isfinite(c):\n",
      "                        c = 0.0\n",
      "                    corrs.append(abs(c))\n",
      "            idxs = np.argsort(corrs)[::-1][: min(k_dims, dim)]\n",
      "        else:\n",
      "            idxs = rng.choice(dim, size=min(k_dims, dim), replace=False)\n",
      "        k = len(idxs)\n",
      "        if k == 0:\n",
      "            return 0\n",
      "\n",
      "        # initial simplex\n",
      "        step = np.clip(0.12 * sigma * span[idxs], 1e-6, 0.2 * span[idxs])\n",
      "        simplex = [best_x.copy()]\n",
      "        for i in range(k):\n",
      "            p = best_x.copy()\n",
      "            p[idxs[i]] = p[idxs[i]] + step[i]\n",
      "            simplex.append(reflect_to_bounds(p))\n",
      "        ys = []\n",
      "        used = 0\n",
      "        for p in simplex:\n",
      "            if evals >= budgets or used >= max_evals:\n",
      "                break\n",
      "            ys.append(evaluate(p))\n",
      "            used += 1\n",
      "        if used < len(simplex):\n",
      "            return used\n",
      "\n",
      "        alpha, gamma, rho, nm_shrink = 1.0, 2.0, 0.5, 0.5\n",
      "        it_guard = 0\n",
      "        while used < max_evals and evals < budgets and it_guard < 200:\n",
      "            it_guard += 1\n",
      "            order = np.argsort([-np.inf if v is None else v for v in ys])[::-1]  # maximize\n",
      "            simplex = [simplex[i] for i in order]\n",
      "            ys = [ys[i] for i in order]\n",
      "            xh = np.array(simplex[-1])\n",
      "            yh = ys[-1]\n",
      "            xl = np.array(simplex[0])\n",
      "            yl = ys[0]\n",
      "            xc = np.mean(np.array(simplex[:-1]), axis=0)\n",
      "\n",
      "            xr = reflect_to_bounds(xc + alpha * (xc - xh))\n",
      "            if used >= max_evals or evals >= budgets:\n",
      "                break\n",
      "            yr = evaluate(xr)\n",
      "            used += 1\n",
      "            if yr is None:\n",
      "                break\n",
      "\n",
      "            if yr > yl:\n",
      "                xe = reflect_to_bounds(xc + gamma * (xr - xc))\n",
      "                if used >= max_evals or evals >= budgets:\n",
      "                    break\n",
      "                ye = evaluate(xe)\n",
      "                used += 1\n",
      "                if ye is not None and ye > yr:\n",
      "                    simplex[-1] = xe\n",
      "                    ys[-1] = ye\n",
      "                else:\n",
      "                    simplex[-1] = xr\n",
      "                    ys[-1] = yr\n",
      "            elif yr > ys[-2]:\n",
      "                simplex[-1] = xr\n",
      "                ys[-1] = yr\n",
      "            else:\n",
      "                xcand = reflect_to_bounds(xc + rho * (xh - xc))\n",
      "                if used >= max_evals or evals >= budgets:\n",
      "                    break\n",
      "                ycand = evaluate(xcand)\n",
      "                used += 1\n",
      "                if ycand is not None and ycand > yh:\n",
      "                    simplex[-1] = xcand\n",
      "                    ys[-1] = ycand\n",
      "                else:\n",
      "                    # shrink around best\n",
      "                    xbest = np.array(simplex[0])\n",
      "                    new_simplex = [xbest.copy()]\n",
      "                    new_ys = [ys[0]]\n",
      "                    for i in range(1, len(simplex)):\n",
      "                        xi = reflect_to_bounds(xbest + nm_shrink * (simplex[i] - xbest))\n",
      "                        if used >= max_evals or evals >= budgets:\n",
      "                            new_simplex.append(xi)\n",
      "                            new_ys.append(None)\n",
      "                            continue\n",
      "                        yi = evaluate(xi)\n",
      "                        used += 1\n",
      "                        new_simplex.append(xi)\n",
      "                        new_ys.append(yi)\n",
      "                    simplex, ys = new_simplex, new_ys\n",
      "                    continue\n",
      "        return used\n",
      "\n",
      "    start_time = time.time()\n",
      "\n",
      "    while evals < budgets:\n",
      "        gen += 1\n",
      "        remaining = budgets - evals\n",
      "        lam = suggest_lambda(remaining)\n",
      "\n",
      "        # Orthogonalized sampling around m with diagonal scaling and occasional heavy-tail perturbations\n",
      "        Z = orthogonal_noise(lam)\n",
      "        diag_sc = diagonal_sampling_scales()\n",
      "        Z = Z * diag_sc[None, :]\n",
      "        if rng.random() < 0.25:\n",
      "            idx_ht = rng.integers(0, lam)\n",
      "            ht = rng.standard_cauchy(size=dim)\n",
      "            ht = np.clip(ht, -10, 10)\n",
      "            Z[idx_ht] = ht * diag_sc\n",
      "\n",
      "        # add antithetic partners to improve symmetry when possible\n",
      "        if lam % 2 == 1 and remaining >= lam + 1:\n",
      "            # ensure even count by duplicating one and mirroring\n",
      "            Z = np.vstack([Z, -Z[-1:]])\n",
      "            lam = Z.shape[0]\n",
      "        for i in range(0, lam, 2):\n",
      "            if i + 1 < lam:\n",
      "                Z[i] = -Z[i + 1]\n",
      "\n",
      "        candidates = m + Z * (sigma * span)\n",
      "        candidates = reflect_to_bounds(candidates)\n",
      "\n",
      "        ys = []\n",
      "        improvements = 0\n",
      "        for i in range(lam):\n",
      "            if evals >= budgets:\n",
      "                break\n",
      "            y_i = evaluate(candidates[i])\n",
      "            ys.append(y_i)\n",
      "            if y_i is not None and y_i > m_y:\n",
      "                m_y = float(y_i)\n",
      "                m = candidates[i].copy()\n",
      "                improvements += 1\n",
      "\n",
      "        if len(ys) == 0:\n",
      "            break\n",
      "\n",
      "        # Recombination (rank-based)\n",
      "        valid_idx = [i for i, yy in enumerate(ys) if yy is not None and np.isfinite(yy)]\n",
      "        if len(valid_idx) > 0:\n",
      "            sorted_idx = sorted(valid_idx, key=lambda i: ys[i], reverse=True)\n",
      "            mu = max(1, len(sorted_idx) // 2)\n",
      "            ranks = np.arange(1, mu + 1)\n",
      "            weights = np.log(mu + 0.5) - np.log(ranks)\n",
      "            weights /= np.sum(weights)\n",
      "            m_recomb = np.sum(candidates[sorted_idx[:mu]] * weights[:, None], axis=0)\n",
      "            alpha = 0.35\n",
      "            m = reflect_to_bounds((1 - alpha) * m + alpha * m_recomb)\n",
      "\n",
      "        # 1/5th success rule for global step-size\n",
      "        succ_frac = improvements / max(1, lam)\n",
      "        if succ_frac > 0.22:\n",
      "            sigma *= 1.12\n",
      "            stall_gens = 0\n",
      "        else:\n",
      "            sigma *= 0.87\n",
      "            stall_gens += 1\n",
      "        sigma = float(np.clip(sigma, sigma_min, sigma_max))\n",
      "\n",
      "        # Occasional lightweight local probes near the current best\n",
      "        if succ_frac > 0 and evals < budgets:\n",
      "            k = min(3, dim)\n",
      "            idxs = rng.choice(dim, size=k, replace=False)\n",
      "            for j in idxs:\n",
      "                if evals >= budgets:\n",
      "                    break\n",
      "                step = 0.35 * sigma * span[j]\n",
      "                if step <= 0:\n",
      "                    continue\n",
      "                for direction in (+1, -1):\n",
      "                    if evals >= budgets:\n",
      "                        break\n",
      "                    probe = best_x.copy()\n",
      "                    probe[j] = probe[j] + direction * step\n",
      "                    evaluate(probe)\n",
      "\n",
      "        # Cheap ridge-linear surrogate directional steps when stagnating lightly\n",
      "        if (stall_gens >= 1) and evals < budgets:\n",
      "            rem = budgets - evals\n",
      "            budget_r = int(min(max(3, dim // 2), rem // 4 if rem > 40 else rem // 3))\n",
      "            used_r = ridge_linear_probe(budget_r)\n",
      "            if used_r > 0:\n",
      "                m = best_x.copy()\n",
      "                m_y = best_y\n",
      "\n",
      "        # If stagnated for a few generations, try subspace and full quadratic surrogate trust-region proposals\n",
      "        if (stall_gens >= 2) and evals < budgets:\n",
      "            rem = budgets - evals\n",
      "            # First try a couple of active subspaces\n",
      "            budget_s = int(min(rem // 4, max(6 * dim, 18)))\n",
      "            used_s = 0\n",
      "            used_s += subspace_quad_trust_propose(min(4, dim), budget_s // 2)\n",
      "            if evals < budgets and used_s < budget_s:\n",
      "                used_s += subspace_quad_trust_propose(min(6, dim), budget_s - used_s)\n",
      "            if used_s > 0:\n",
      "                m = best_x.copy()\n",
      "                m_y = best_y\n",
      "\n",
      "        if (stall_gens >= 3) and evals < budgets:\n",
      "            rem = budgets - evals\n",
      "            budget_q = int(min(rem // 4, max(8 * dim, 24)))\n",
      "            used_q = quad_trust_propose(budget_q)\n",
      "            if used_q > 0:\n",
      "                m = best_x.copy()\n",
      "                m_y = best_y\n",
      "\n",
      "        # Finite-difference gradient trust-region steps when stagnating mildly\n",
      "        if (stall_gens >= 2) and evals < budgets:\n",
      "            rem = budgets - evals\n",
      "            budget_g = int(min(max(6, 2 * dim), rem // 3 if rem > 30 else rem // 2))\n",
      "            used_g = gradient_tr_probe(budget_g, gen)\n",
      "            if used_g > 0:\n",
      "                m = best_x.copy()\n",
      "                m_y = best_y\n",
      "\n",
      "        # Occasional global exploration burst if long stall\n",
      "        if (stall_gens >= 6) and evals < budgets:\n",
      "            rem = budgets - evals\n",
      "            burst = int(min(rem // 3, max(12, 3 * dim)))\n",
      "            print(f\"Global exploration burst at gen {gen} with budget {burst}\")\n",
      "            global_explore(burst)\n",
      "            m = best_x.copy()\n",
      "            m_y = best_y\n",
      "\n",
      "        print(f\"Gen {gen}: evals {evals}/{budgets}, best_y={best_y:.6f}, sigma={sigma:.3f}, stall_gens={stall_gens}\")\n",
      "\n",
      "        # If stagnated long, perform a local refiner; then maybe restart\n",
      "        if stall_gens >= max_stall_gens and evals < budgets:\n",
      "            rem_before = budgets - evals\n",
      "            refine_budget = int(min(rem_before // 3, max(dim * 8, 24)))\n",
      "            if refine_budget > 0:\n",
      "                print(f\"Local refine at gen {gen} with budget {refine_budget}\")\n",
      "                local_refine(refine_budget)\n",
      "                m = best_x.copy()\n",
      "                m_y = best_y\n",
      "            stall_gens = 0\n",
      "            if evals < budgets:\n",
      "                # soft restart near the best with moderate sigma\n",
      "                m = reflect_to_bounds(best_x + rng.normal(scale=0.12, size=dim) * span)\n",
      "                m_y = -np.inf\n",
      "                sigma = 0.22\n",
      "                print(f\"Restart at gen {gen}: new center near best, sigma={sigma:.3f}\")\n",
      "\n",
      "        # Final-phase: if nearing the end, allocate remaining budget to subspace + local refine + last surrogate pass + NM finisher\n",
      "        remaining = budgets - evals\n",
      "        if remaining <= max(30, dim * 4) and remaining > 0:\n",
      "            used = 0\n",
      "            used += subspace_quad_trust_propose(min(6, dim), max(1, remaining // 4))\n",
      "            remaining = budgets - evals\n",
      "            if remaining > 0:\n",
      "                used += quad_trust_propose(max(1, remaining // 4))\n",
      "            remaining = budgets - evals\n",
      "            if remaining > 0:\n",
      "                nm_budget = max(0, remaining // 3)\n",
      "                if nm_budget > 0:\n",
      "                    print(f\"Final-phase Nelder–Mead subspace with budget {nm_budget}\")\n",
      "                    nelder_mead_subspace(min(6, dim), nm_budget)\n",
      "            remaining = budgets - evals\n",
      "            if remaining > 0:\n",
      "                print(f\"Final-phase local refine with remaining budget {remaining}\")\n",
      "                local_refine(remaining)\n",
      "            break\n",
      "\n",
      "        # Safety: time cap guard (soft)\n",
      "        if time.time() - start_time > 0.9 * 300:\n",
      "            print(\"Time cap nearing, exiting loop.\")\n",
      "            break\n",
      "\n",
      "    print(\"y: \", best_y)\n",
      "    return best_x\n",
      "\n",
      "\n",
      "# EVOLVE-BLOCK-END\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from examples.new_polynomial.best_program import program\n",
    "\n",
    "print(program)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54affe23",
   "metadata": {},
   "source": [
    "**The evolved solution:** GEPA discovered a hybrid optimizer that combines adaptive evolutionary search with surrogate-assisted trust-region methods—automatically escalating from cheap linear models to richer quadratic approximations as the search stalls.\n",
    "\n",
    "**Why it works:** Rather than relying on a fixed algorithm, GEPA learned to dynamically balance exploration (orthogonalized sampling, Cauchy jumps) and exploitation (gradient probes, Nelder-Mead) based on observed progress—a strategy no human specified, but one that outperforms hand-tuned baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774db146",
   "metadata": {},
   "source": [
    "## Example 2: Prompt Optimization\n",
    "\n",
    "In our [GEPA paper](link) at ICLR 2025, we showed that GEPA outperforms the previous state-of-the-art optimizer, MIPROv2, by over 10%—and even beats GRPO using only 2% of the rollouts across four tasks.\n",
    "\n",
    "<!-- <img src=\"./assets/blog/gepa_aime.png\" width=\"70%\"> -->\n",
    "\n",
    "In this tutorial, we evolve a GPT4.1 Mini's prompt by training it on AIME 2022~2024 and test it on AIME 2025. \n",
    "\n",
    "<img src=\"./assets/blog/aime_best_comparison.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f526e920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45 training examples\n",
      "Loaded 45 validation examples\n",
      "Loaded 30 test examples\n"
     ]
    }
   ],
   "source": [
    "from examples.math.dataset import load_math_dataset\n",
    "\n",
    "trainset, valset, testset = load_math_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec163d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "\n",
    "# Use GPT-4.1-mini as the language model to solve the math problems.\n",
    "lm = dspy.LM(\"gpt-4.1-mini\", api_key=os.environ.get(\"OPENAI_API_KEY\"), temperature=1.0, max_tokens=32000)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Define a simple base prompt that we will optimize.\n",
    "SEED_PROMPT = \"\"\"Solve the math problem carefully. Break down the steps and provide the final answer as a single number.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff6ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa.optimize_anything import SideInfo\n",
    "\n",
    "from examples.math.main import run_llm, math_metric\n",
    "\n",
    "def fitness_fn(candidate: dict[str, str], example: Any) -> list[tuple[float, Any, SideInfo]]:\n",
    "    prediction = run_llm(example, candidate[\"prompt\"])\n",
    "    metric_result = math_metric(example, prediction)\n",
    "    score = metric_result.score\n",
    "    feedback = metric_result.feedback\n",
    "\n",
    "    output = {\n",
    "        \"prompt\": candidate[\"prompt\"],\n",
    "        \"answer\": prediction.answer,\n",
    "        \"score\": score,\n",
    "    }\n",
    "\n",
    "    side_info = {\n",
    "        \"Input\": example.input,\n",
    "        \"Output\": prediction.answer,\n",
    "        \"Reasoning\": getattr(prediction, \"reasoning\", \"\"),\n",
    "        \"ExecutionFeedback\": feedback,\n",
    "    }\n",
    "\n",
    "    return (score, output, side_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14431a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa.optimize_anything import (\n",
    "    EngineConfig,\n",
    "    GEPAConfig,\n",
    "    ReflectionConfig,\n",
    "    optimize_anything,\n",
    ")\n",
    "\n",
    "gepa_config = GEPAConfig(\n",
    "    engine=EngineConfig(\n",
    "        max_metric_calls=800,\n",
    "        track_best_outputs=True,\n",
    "    ),\n",
    "    reflection=ReflectionConfig(\n",
    "        reflection_minibatch_size=3,\n",
    "        skip_perfect_score=False,\n",
    "        reflection_lm=\"openai/gpt-5.1\",\n",
    "    )\n",
    ")\n",
    "\n",
    "result = optimize_anything(\n",
    "    seed_candidate={\"prompt\": SEED_PROMPT},\n",
    "    fitness_fn=fitness_fn,\n",
    "    dataset=trainset,\n",
    "    valset=valset,\n",
    "    config=gepa_config,\n",
    ")\n",
    "\n",
    "best_prompt = result.best_candidate[\"prompt\"]\n",
    "best_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.math.main import evaluate_on_dataset\n",
    "\n",
    "# Baseline Evaluation\n",
    "print(\"\\nEvaluating Baseline (Initial Prompt)...\")\n",
    "baseline_score = evaluate_on_dataset(SEED_PROMPT, testset)\n",
    "\n",
    "# Optimized Evaluation\n",
    "print(\"\\nEvaluating Best Optimized Program...\")\n",
    "best_prompt = result.best_candidate[\"prompt\"]\n",
    "print(f\"Best Prompt Found:\\n{best_prompt}\")\n",
    "\n",
    "optimized_score = evaluate_on_dataset(best_prompt, testset)\n",
    "\n",
    "print(f\"Baseline Score: {baseline_score:.2%}\")\n",
    "print(f\"Optimized Score: {optimized_score:.2%}\")\n",
    "print(f\"Improvement: {optimized_score - baseline_score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e40dc9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solve from first principles with explicit checks. Requirements:\n",
      "\n",
      "1) Model precisely:\n",
      "- Define all objects, variables, and constraints algebraically/combinatorially.\n",
      "- Choose one counting model (labeled vs indistinguishable) and stay consistent. For combinatorics, either label and divide at the end OR keep indistinguishable throughout—do not mix.\n",
      "- For number-theory/decimal/ratio problems, state factorizations and gcd/lcm relations explicitly.\n",
      "\n",
      "2) Mapping/Counting rigor:\n",
      "- When mapping elements between sets (e.g., m ↦ m/gcd(m,N)), prove injectivity/surjectivity or otherwise handle overlaps via inclusion–exclusion. Do not assume unions over divisors are disjoint without proof.\n",
      "- When computing a probability, ensure numerator and denominator are counts from the same sample space.\n",
      "- Keep all computations exact (fractions/radicals/modular arithmetic); avoid decimals unless terminating.\n",
      "\n",
      "3) Geometry workflow:\n",
      "- Draw and name a diagram (mentally or on paper). List candidate theorems: power of a point, radical axis, homothety, parallel chords/tangents, similar triangles, right triangles, cyclicity, angle/length chasing.\n",
      "- Identify perpendiculars to tangents through centers; use rectangles formed by distances from a point on a circle to parallel lines; note that for a point on an incircle, distances to each of a pair of parallel sides sum to the diameter; use midpoint/radical-axis facts for intersecting circles and common tangents.\n",
      "- Prefer exact relations (e.g., MP·MQ = (tangent length)^2) over coordinate guesses. If coordinates are used, ensure constraints (parallelism, perpendicularity, tangency) are enforced exactly.\n",
      "\n",
      "4) Sanity checks and diagnostics:\n",
      "- If an assumption yields a contradiction (e.g., negative squared length), discard and rebuild the setup.\n",
      "- For combinatorics/NT counts, validate with a smaller analog (e.g., replace 9999 by 9 or 99) to detect double-counting/missing cases before scaling up.\n",
      "- For expressions of the form m√n, reduce n to be squarefree; then compute the requested function (e.g., m+n).\n",
      "- Perform at least one independent cross-check (alternative derivation, structural identity, modular check, or small-n analog).\n",
      "\n",
      "5) Output:\n",
      "- Extract exactly what is asked (e.g., remainder, perimeter, m+n). Provide the final answer only as a single number with no extra text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from examples.math.best_program import prompt\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd938c",
   "metadata": {},
   "source": [
    "TODO: show the evolved prompt\n",
    "Also run the test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352303a5",
   "metadata": {},
   "source": [
    "## Example 3: Agent Optimization — Evolving DSPy Programs for ARC-AGI\n",
    "\n",
    "Our third example pushes GEPA further: optimizing not just prompts or hyperparameters, but the *entire structure* of an AI agent. We'll evolve a DSPy program to solve ARC-AGI tasks — a challenging benchmark requiring visual reasoning and pattern recognition.\n",
    "\n",
    "**The task**: Given input-output matrix pairs as training examples, produce the correct output for test inputs.\n",
    "\n",
    "**What GEPA optimizes**: The entire DSPy program source code — signatures, modules, control flow, and prompting strategies.\n",
    "\n",
    "**Result**: GEPA improves GPT5's performance from **X% to Y%** by discovering an [elaborate 5-step reasoning pipeline with self-refinement.]\n",
    "\n",
    "<!-- ![ARC AGI Graph](./assets/blog/arc_agi_optimization_progress.png) -->\n",
    "<img src=\"./assets/blog/arc_agi_best_comparison.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58492d",
   "metadata": {},
   "source": [
    "### Setting up the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00eba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 200\n",
      "Val set: 200\n",
      "Test set: 400\n"
     ]
    }
   ],
   "source": [
    "from examples.arc_agi.data import load_arc_agi_dataset\n",
    "\n",
    "train_set, val_set, test_set = load_arc_agi_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e1e27",
   "metadata": {},
   "source": [
    "### The seed candidate\n",
    "\n",
    "We start with a simple Chain-of-Thought program — just a single DSPy module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f3c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_candidate = \"\"\"import dspy\n",
    "from typing import List\n",
    "import pydantic\n",
    "\n",
    "MATRIX = List[List[int]]\n",
    "\n",
    "class TrainingExample(pydantic.BaseModel):\n",
    "    input: MATRIX\n",
    "    output: MATRIX\n",
    "\n",
    "class SolveTaskSignature(dspy.Signature):\n",
    "    training_examples: List[TrainingExample] = dspy.InputField(description=\"Input and output examples demonstrating the task to be performed.\")\n",
    "    test_inputs: List[MATRIX] = dspy.InputField(description=\"Input matrices to be solved following the task described in the training examples.\")\n",
    "    test_outputs: List[MATRIX] = dspy.OutputField(description=\"Output matrices corresponding to the test inputs.\")\n",
    "\n",
    "program = dspy.ChainOfThought(SolveTaskSignature)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193aa56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "\n",
    "from gepa.adapters.dspy_full_program_adapter.full_program_adapter import DspyAdapter\n",
    "from examples.arc_agi.main import metric_fn\n",
    "\n",
    "# Create LMs\n",
    "task_lm = dspy.LM(\n",
    "    model=\"openai/gpt-5\",\n",
    "    temperature=1.0,\n",
    "    max_tokens=32000,\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Create adapter\n",
    "adapter = DspyAdapter(\n",
    "    task_lm=task_lm,\n",
    "    metric_fn=metric_fn,\n",
    "    num_threads=64,\n",
    "    reflection_lm=\"openai/gpt-5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e441e",
   "metadata": {},
   "source": [
    "### The fitness function\n",
    "\n",
    "The fitness function compiles and runs the DSPy program, comparing outputs against ground truth. Crucially, it provides detailed feedback about *what went wrong*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001eb5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_fn(candidate, example):\n",
    "    program = candidate[\"program\"]\n",
    "    print(\"Example: \", type(example))\n",
    "\n",
    "    try:\n",
    "        evaluation_results = adapter.evaluate([example], candidate, capture_traces=True)\n",
    "    except Exception as e:\n",
    "        side_info = {\n",
    "            \"input\": example,\n",
    "            \"error\": str(e),\n",
    "            \"program\": program\n",
    "        }\n",
    "        return (0.0, side_info, side_info)\n",
    "\n",
    "    # Program error\n",
    "    if not isinstance(evaluation_results.trajectories, list) or len(evaluation_results.trajectories) == 0:\n",
    "        print(\"Error: \")\n",
    "        print(evaluation_results.trajectories)\n",
    "        side_info = {\n",
    "            \"input\": example,\n",
    "            \"error\": f\"All examples failed. Program error: {str(evaluation_results.trajectories)}\",\n",
    "            \"program\": program\n",
    "        }\n",
    "        return (0.0, side_info, side_info)\n",
    "\n",
    "    # Process evaluations with no program errors\n",
    "    trajectory = evaluation_results.trajectories[0]\n",
    "    metric_result = trajectory.get(\"score\")\n",
    "    score = metric_result.get(\"score\")\n",
    "    feedback = metric_result.get(\"feedback\")\n",
    "    prediction = trajectory.get(\"prediction\")\n",
    "\n",
    "    side_info = {\n",
    "        \"input\": example,\n",
    "        \"reasoning\": prediction.get(\"reasoning\"),\n",
    "        \"feedback\": feedback,\n",
    "        \"output\": prediction.get(\"test_outputs\"),\n",
    "    }\n",
    "\n",
    "    return (score, side_info, side_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec5232",
   "metadata": {},
   "source": [
    "### Running GEPA optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa.optimize_anything import (\n",
    "    EngineConfig,\n",
    "    GEPAConfig,\n",
    "    ReflectionConfig,\n",
    "    optimize_anything,\n",
    ")\n",
    "from examples.arc_agi.prompt import REFLECTION_PROMPT\n",
    "\n",
    "gepa_config = GEPAConfig(\n",
    "    engine=EngineConfig(\n",
    "        max_metric_calls=4000,\n",
    "        track_best_outputs=True,\n",
    "        parallel=True,\n",
    "        max_workers=64,\n",
    "    ),\n",
    "    reflection=ReflectionConfig(\n",
    "        reflection_minibatch_size=3,\n",
    "        reflection_lm=\"openai/gpt-5\",\n",
    "        reflection_prompt_template=REFLECTION_PROMPT,\n",
    "    )\n",
    ")\n",
    "\n",
    "result = optimize_anything(\n",
    "    seed_candidate={\"program\": seed_candidate},\n",
    "    fitness_fn=fitness_fn,\n",
    "    dataset=train_set,\n",
    "    valset=val_set,\n",
    "    config=gepa_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "368355bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import dspy\n",
      "from typing import List, Optional, Any, Dict, Tuple, Callable\n",
      "import pydantic\n",
      "import re\n",
      "import traceback\n",
      "import copy\n",
      "\n",
      "MATRIX = List[List[int]]\n",
      "\n",
      "class TrainingExample(pydantic.BaseModel):\n",
      "    input: MATRIX\n",
      "    output: MATRIX\n",
      "\n",
      "class SolveTaskSignature(dspy.Signature):\n",
      "    \"\"\"\n",
      "    Solve ARC-style grid transformations by learning a function from examples.\n",
      "\n",
      "    Inputs:\n",
      "    - training_examples: A list of (input, output) grid pairs that demonstrate the task. Grids are integer matrices.\n",
      "    - test_inputs: Grids to transform using the learned task.\n",
      "\n",
      "    Output:\n",
      "    - test_outputs: Exact grids corresponding to each test input.\n",
      "\n",
      "    Approach:\n",
      "    - Induce a general, deterministic transformation as Python code: def transform(grid: List[List[int]]) -> List[List[int]].\n",
      "    - Common patterns:\n",
      "      1) Separator rows/columns: entire rows/cols of a single color often partition the grid; keep separators unchanged.\n",
      "      2) Block-wise aggregation: when grid is partitioned into kxk blocks by separators, fill each block by a statistic (e.g., majority including 0) inferred from examples.\n",
      "      3) Mask algebra: when two subgrids (e.g., split by a row of constant color) define masks, combine with boolean logic (OR/AND/XOR) and map nonzero to a target color.\n",
      "      4) Noise cleanup: replace a minority/noise color based on orthogonal neighbor majority of a primary color; otherwise drop to background.\n",
      "    - Pitfalls:\n",
      "      - Ensure exact equality with all training outputs; no partial credit.\n",
      "      - Handle arbitrary grid sizes consistent with the pattern inferred from training pairs.\n",
      "      - Do not hardcode coordinates or sizes; infer from structure (e.g., positions of separator lines).\n",
      "      - Preserve separator rows/columns exactly when present.\n",
      "    - Constraints:\n",
      "      - Pure Python on lists; no imports; deterministic; O(n*m) to O(n*m*small) time.\n",
      "    \"\"\"\n",
      "    training_examples: List[TrainingExample] = dspy.InputField(desc=\"Input/output example pairs describing the task.\")\n",
      "    test_inputs: List[MATRIX] = dspy.InputField(desc=\"Inputs to transform after learning from examples.\")\n",
      "    test_outputs: List[MATRIX] = dspy.OutputField(desc=\"Outputs for test_inputs produced by the learned transform.\")\n",
      "\n",
      "class SynthesizeTransform(dspy.Signature):\n",
      "    \"\"\"\n",
      "    Write valid, self-contained Python code that defines:\n",
      "        def transform(grid: List[List[int]]) -> List[List[int]]:\n",
      "            ...\n",
      "            return out_grid\n",
      "\n",
      "    Requirements:\n",
      "    - Use only built-in Python (lists/loops/dicts/sets); no imports.\n",
      "    - May define small helper functions above transform.\n",
      "    - Must be general to similar-sized grids and structures; do NOT hardcode absolute indices from training examples.\n",
      "    - Preserve separator rows/columns if present.\n",
      "    - For block aggregation, infer block sizes from constant-color separator lines/columns.\n",
      "    - For mask logic, infer how to combine masks and the output color mapping from examples.\n",
      "    - For noise cleanup, infer primary vs. noise colors and neighbor rules from examples.\n",
      "\n",
      "    Guidance from previous attempt:\n",
      "    {hint}\n",
      "\n",
      "    Return ONLY code text containing the def transform(...) function (and optional helpers), nothing else.\n",
      "    \"\"\"\n",
      "    training_examples: List[TrainingExample] = dspy.InputField(desc=\"Training pairs to infer the rule.\")\n",
      "    hint: str = dspy.InputField(desc=\"Feedback on prior failures and additional guidance.\")\n",
      "    code: str = dspy.OutputField(desc=\"Python code that defines transform(grid) and helper functions.\")\n",
      "\n",
      "def _extract_code_block(s: str) -> str:\n",
      "    if s is None:\n",
      "        return \"\"\n",
      "    # Try to extract triple-backticked python code if present\n",
      "    m = re.findall(r\"```(?:python)?\\s*(.*?)```\", s, flags=re.DOTALL | re.IGNORECASE)\n",
      "    if m:\n",
      "        return m[-1].strip()\n",
      "    return s.strip()\n",
      "\n",
      "def _load_transform_func(code: str) -> Tuple[Optional[Callable[[MATRIX], MATRIX]], Optional[str]]:\n",
      "    try:\n",
      "        # Sanitize and ensure 'def transform(' exists\n",
      "        if \"def transform(\" not in code:\n",
      "            return None, \"No transform(grid) function found.\"\n",
      "        safe_globals: Dict[str, Any] = {\n",
      "            \"__builtins__\": {\n",
      "                \"range\": range,\n",
      "                \"len\": len,\n",
      "                \"min\": min,\n",
      "                \"max\": max,\n",
      "                \"sum\": sum,\n",
      "                \"enumerate\": enumerate,\n",
      "                \"abs\": abs,\n",
      "                \"all\": all,\n",
      "                \"any\": any,\n",
      "                \"sorted\": sorted,\n",
      "                \"zip\": zip,\n",
      "                \"set\": set,\n",
      "                \"list\": list,\n",
      "                \"dict\": dict\n",
      "            }\n",
      "        }\n",
      "        safe_locals: Dict[str, Any] = {}\n",
      "        exec(code, safe_globals, safe_locals)\n",
      "        fn = safe_locals.get(\"transform\") or safe_globals.get(\"transform\")\n",
      "        if not callable(fn):\n",
      "            return None, \"transform is not callable.\"\n",
      "        return fn, None\n",
      "    except Exception as e:\n",
      "        return None, f\"Code exec error: {e}\n",
      "{traceback.format_exc()}\"\n",
      "\n",
      "def _grids_equal(a: MATRIX, b: MATRIX) -> bool:\n",
      "    if len(a) != len(b):\n",
      "        return False\n",
      "    for r in range(len(a)):\n",
      "        if a[r] != b[r]:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "def _summarize_mismatch(gold: MATRIX, pred: MATRIX, max_points: int = 10) -> str:\n",
      "    pts = []\n",
      "    R = min(len(gold), len(pred))\n",
      "    for r in range(R):\n",
      "        C = min(len(gold[r]), len(pred[r]))\n",
      "        for c in range(C):\n",
      "            if gold[r][c] != pred[r][c]:\n",
      "                pts.append(f\"({r},{c}): expected {gold[r][c]}, got {pred[r][c]}\")\n",
      "                if len(pts) >= max_points:\n",
      "                    break\n",
      "        if len(pts) >= max_points:\n",
      "            break\n",
      "    return \"; \".join(pts) if pts else \"shape or structural mismatch\"\n",
      "\n",
      "class CodeSynthesisSolver(dspy.Module):\n",
      "    def __init__(self, attempts: int = 4):\n",
      "        super().__init__()\n",
      "        self.attempts = attempts\n",
      "        self.codegen = dspy.ChainOfThought(SynthesizeTransform)\n",
      "\n",
      "    def _verify_on_training(self, fn: Callable[[MATRIX], MATRIX], training_examples: List[TrainingExample]) -> Tuple[bool, Optional[str]]:\n",
      "        failures = []\n",
      "        for idx, ex in enumerate(training_examples):\n",
      "            try:\n",
      "                pred = fn(copy.deepcopy(ex.input))\n",
      "            except Exception as e:\n",
      "                return False, f\"Runtime error on example {idx}: {e}\"\n",
      "            if not _grids_equal(pred, ex.output):\n",
      "                mm = _summarize_mismatch(ex.output, pred)\n",
      "                failures.append(f\"Ex {idx} mismatch: {mm}\")\n",
      "                if len(failures) >= 3:\n",
      "                    break\n",
      "        if failures:\n",
      "            return False, \" | \".join(failures)\n",
      "        return True, None\n",
      "\n",
      "    def forward(self, training_examples: List[TrainingExample], test_inputs: List[MATRIX]) -> dspy.Prediction:\n",
      "        hint = (\n",
      "            \"Focus on inferring a general rule from ALL training pairs. \"\n",
      "            \"Verify your transform reproduces every training output exactly before finalizing.\"\n",
      "        )\n",
      "        last_error = None\n",
      "        for attempt in range(1, self.attempts + 1):\n",
      "            pred = self.codegen(training_examples=training_examples, hint=hint)\n",
      "            code_text = pred.code if hasattr(pred, \"code\") and pred.code else \"\"\n",
      "            code_text = _extract_code_block(code_text)\n",
      "            fn, load_err = _load_transform_func(code_text)\n",
      "            if fn is None:\n",
      "                last_error = load_err or \"Unknown code loading error.\"\n",
      "                hint = (\n",
      "                    f\"Attempt {attempt} failed to load transform: {last_error}. \"\n",
      "                    \"Return ONLY valid Python code that defines def transform(grid).\"\n",
      "                )\n",
      "                continue\n",
      "            ok, err = self._verify_on_training(fn, training_examples)\n",
      "            if ok:\n",
      "                # Apply to test inputs\n",
      "                outputs: List[MATRIX] = []\n",
      "                for i, g in enumerate(test_inputs):\n",
      "                    try:\n",
      "                        out = fn(copy.deepcopy(g))\n",
      "                    except Exception as e:\n",
      "                        # If test-time error occurs, treat as failure and refine\n",
      "                        last_error = f\"Runtime error on test input {i}: {e}\"\n",
      "                        ok = False\n",
      "                        break\n",
      "                    outputs.append(out)\n",
      "                if ok:\n",
      "                    return dspy.Prediction(test_outputs=outputs)\n",
      "                else:\n",
      "                    hint = (\n",
      "                        f\"Transform passed training but failed on test due to: {last_error}. \"\n",
      "                        \"Make the transform more robust while preserving training behavior.\"\n",
      "                    )\n",
      "                    continue\n",
      "            else:\n",
      "                last_error = err or \"Mismatch without details.\"\n",
      "                hint = (\n",
      "                    f\"Attempt {attempt} produced incorrect outputs on training: {last_error}. \"\n",
      "                    \"Refine the code: ensure exact equality, preserve separators, infer block sizes, \"\n",
      "                    \"and generalize the rule.\"\n",
      "                )\n",
      "\n",
      "        # Fallback: identity transform to ensure a return (last resort)\n",
      "        outputs = [copy.deepcopy(g) for g in test_inputs]\n",
      "        return dspy.Prediction(test_outputs=outputs)\n",
      "\n",
      "# Instantiate the improved program\n",
      "program = CodeSynthesisSolver(attempts=5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from examples.arc_agi.best_program import program\n",
    "\n",
    "print(program)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31781e",
   "metadata": {},
   "source": [
    "### What GEPA discovered\n",
    "The evolved program implements a code synthesis loop with self-repair. Rather than prompting the LLM to directly output grid transformations, it asks the model to write a `transform(grid)` function, then verifies that function against the task's demonstration pairs before applying it to the held-out test input. If verification fails, it feeds specific mismatches back as hints and retries—up to five attempts.\n",
    "\n",
    "A few innovations stand out:\n",
    "\n",
    "- **Verification-driven self-debugging.** The agent doesn't trust its initial output. It executes the generated code on demonstration inputs, diffs against expected outputs, and uses failure diagnostics (e.g., \"expected 3 at (2,4), got 0\") to refine the next attempt. This behavior emerged from evolution, not manual design.\n",
    "\n",
    "- **Domain priors in the prompt.** The synthesis prompt explicitly encodes common ARC patterns: separator detection, block aggregation, mask algebra, noise cleanup. GEPA discovered that enumerating these priors helps the underlying model generalize across tasks.\n",
    "\n",
    "- **Graceful degradation.** If all attempts fail, the program returns an identity transform. It never crashes, even when it cannot solve a task.\n",
    "\n",
    "GEPA arrived at this design through search rather than human's manual engineering intuition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de947640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the evolved program\n",
    "print(result.best_candidate[\"program\"][:2000])  # First 2000 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a86150",
   "metadata": {},
   "source": [
    "## Circle Packing\n",
    "\n",
    "Circle packing is a classic example used by ShinkaEvolve, OpenEvolve, and AlphaEvolve.\n",
    "Here, we also show how GEPA can conduct an algorithmic discovery for circle packing.\n",
    "\n",
    "<img src=\"./assets/blog/circle_packing/circle_packing_26_comparison.png\" width=\"80%\">\n",
    "\n",
    "GEPA finds a world-record-level solution, achieving 99.999% of AlphaEvolve and ShinkaEvolve and 100.064% of OpenEvolve.\n",
    "\n",
    "<!-- <img src=\"./assets/blog/circle_packing/circle_packing_21.png\" width=\"50%\">\n",
    "\n",
    "<img src=\"./assets/blog/circle_packing/circle_packing_26.png\" width=\"50%\">\n",
    "\n",
    "<img src=\"./assets/blog/circle_packing/circle_packing_32.png\" width=\"50%\"> -->\n",
    "\n",
    "<!-- ### Batch mode (Evolving a search code for 13 instances)\n",
    "\n",
    "num_circles = [7, 13, 19, 21, 22, 26, 29, 31, etc.]\n",
    "\n",
    "<img src=\"./assets/blog/circle_packing/gepa_vs_shinka.png\" width=\"50%\">\n",
    "\n",
    "We take Shinka as a baseline and run the same gpt5.1 for a batch mode. \n",
    "We can see that more data instances -> GEPA save computes while Shinka performs a full validation. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a867ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.635977394754397"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best circles\n",
    "best_circles = [\n",
    "    [0.08468730125813197, 0.08468730125813197, 0.08468730125813197],\n",
    "    [0.8889367079091394, 0.1110632920908606, 0.1110632920908606],\n",
    "    [0.08468730125813231, 0.9153126987418675, 0.08468730125813231],\n",
    "    [0.8889367079091401, 0.8889367079091401, 0.11106329209085986],\n",
    "    [0.2735316496509986, 0.10527607855641154, 0.10527607855641154],\n",
    "    [0.48251901284944326, 0.10371709930579165, 0.10371709930579165],\n",
    "    [0.682251639361118, 0.09615849835819791, 0.09615849835819791],\n",
    "    [0.2735316496509994, 0.8947239214435884, 0.10527607855641163],\n",
    "    [0.4825190128494443, 0.8962829006942082, 0.10371709930579176],\n",
    "    [0.6822516393611192, 0.9038415016418019, 0.09615849835819812],\n",
    "    [0.13236009424048484, 0.2964345012443294, 0.13236009424048484],\n",
    "    [0.07826927088831243, 0.49999999999999845, 0.07826927088831243],\n",
    "    [0.13236009424048586, 0.7035654987556688, 0.13236009424048586],\n",
    "    [0.9075441882905443, 0.31372998221417503, 0.09245581170945572],\n",
    "    [0.9061808044177737, 0.5000000000000004, 0.09381919558222629],\n",
    "    [0.907544188290544, 0.686270017785826, 0.09245581170945605],\n",
    "    [0.2697939589756203, 0.500000000000001, 0.11325541719900234],\n",
    "    [0.38112506495598114, 0.7008876686603106, 0.11641928880622215],\n",
    "    [0.38112506495598014, 0.2991123313396901, 0.11641928880622275],\n",
    "    [0.7632491637451031, 0.7594985921321933, 0.06935728482284345],\n",
    "    [0.5965286484003857, 0.7268072190485636, 0.10053814216358756],\n",
    "    [0.742587601069024, 0.40428027755541857, 0.09571972244458213],\n",
    "    [0.7425876010690241, 0.5957197224445819, 0.09571972244458193],\n",
    "    [0.5965286484003852, 0.273192780951437, 0.1005381421635882],\n",
    "    [0.5325196677311786, 0.5000000000000006, 0.1351282835717167],\n",
    "    [0.7632491637451023, 0.24050140786780766, 0.06935728482284145],\n",
    "]\n",
    "\n",
    "best_circles_sum = np.array(best_circles)[:, 2].sum()\n",
    "best_circles_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ebdd3",
   "metadata": {},
   "source": [
    "Within just 150 evaluations, GEPA found the solution 99.9999880668% of AlphaEvolve, 99.9997836004% of ShinkaEvolve, and 100.063963765% of OpenEvolve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238b846",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "The `optimize_anything` API demonstrates GEPA's power as a general-purpose text evolution engine:\n",
    "\n",
    "1. **Unified interface**: Whether you're optimizing prompts, code, or agent architectures, the API is the same — just define your fitness function with rich `side_info`.\n",
    "\n",
    "2. **Side information is key**: The more diagnostic information you provide, the better GEPA's LLM-based reflection can understand failures and propose targeted improvements.\n",
    "\n",
    "3. **Beyond scalar optimization**: Traditional optimizers only see scores. GEPA sees error messages, execution traces, and domain-specific feedback — enabling it to optimize complex artifacts that would be impossible to search blindly.\n",
    "\n",
    "4. **Emergent capabilities**: GEPA can discover sophisticated strategies (like self-refinement in the ARC-AGI example) that weren't explicitly programmed — they emerge from the optimization process itself.\n",
    "\n",
    "Try `optimize_anything` on your own optimization problems. If you can express your system's parameters as text and compute a score with diagnostic feedback, GEPA can optimize it.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
