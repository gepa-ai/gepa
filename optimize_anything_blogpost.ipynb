{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "outline",
      "metadata": {},
      "source": [
        "# `optimize_anything`: A Universal API for Text-Based Optimization\n",
        "\n",
        "**TL;DR**: We introduce `optimize_anything`, a single, declarative API that uses LLMs as intelligent proposers to optimize *anything* representable as textâ€”code, prompts, configurations, agent architectures. The key insight: if it can be serialized to text, an LLM can reason about it and propose improvements. The secret sauce? **A**uxiliary **S**ide **I**nformation (ASI).\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Unified Interface**: Whether you're optimizing prompts, code, hyperparameters, or agent architectures, the API is the sameâ€”just provide a `seed_candidate` (starting point) and a `fitness_fn` (how good are we doing?).\n",
        "\n",
        "2. **The Convex Hull of Optimization**: `optimize_anything` is designed to be the \"convex hull\" of all text-based optimization problems. Different libraries optimize different things (Optuna for hyperparameters, evolutionary strategies for algorithms, gradient descent for neural networks). We unify them under one abstraction.\n",
        "\n",
        "3. **Side Information is Key**: Unlike traditional optimizers that only see scalar scores, GEPA's LLM-based reflection can understand *why* a candidate performed poorly through rich diagnostic informationâ€”error messages, execution traces, partial results.\n",
        "\n",
        "4. **Emergent Capabilities**: GEPA can discover sophisticated strategies (like self-refinement) that weren't explicitly programmedâ€”they emerge from the optimization process itself.\n",
        "\n",
        "---\n",
        "\n",
        "## Results Summary\n",
        "\n",
        "| Domain | Task | Baseline | Optimized | Improvement |\n",
        "|--------|------|----------|-----------|-------------|\n",
        "| **Mathematical Optimization** | EvalSet Benchmark | Optuna TPE | GEPA | Outperforms Optuna |\n",
        "| **Prompt Engineering** | AIME 2025 (GPT-4.1 Mini) | 46.67% | 53.33% | +6.66% absolute |\n",
        "| **Agent Evolution** | ARC-AGI (GPT-5) | 55.6% | 60.5% | +4.9% absolute. Discovers sophiscated 6-step agent. |\n",
        "| **Algorithmic Discovery** | Circle Packing (N=26) | Prior SOTA | GEPA | Matches/exceeds AlphaEvolve, ShinkaEvolve, OpenEvolve |\n",
        "\n",
        "---\n",
        "\n",
        "## Outline\n",
        "\n",
        "1. **[The Landscape of Optimization (The \"Old\" Way)](#section-1)** â€” The fragmented world of optimization libraries\n",
        "2. **[The Unifying Abstraction: `optimize_anything`](#section-2)** â€” One API to rule them all\n",
        "3. **[The Secret Weapon: Auxiliary Side Information (ASI)](#section-3)** â€” Why GEPA outperforms traditional optimizers\n",
        "4. **[Example 1: Mathematical Optimization](#section-4)** â€” Evolving code to beat Optuna on EvalSet\n",
        "5. **[Example 2: Prompt Engineering](#section-5)** â€” Optimizing prompts for AIME 2025\n",
        "6. **[Example 3: Agent Program Evolution](#section-6)** â€” Evolving DSPy programs for ARC-AGI\n",
        "7. **[Example 4: Algorithmic Discovery](#section-7)** â€” Circle packing that matches AlphaEvolve\n",
        "8. **[How It Works Under the Hood](#section-8)** â€” The GEPA engine\n",
        "9. **[Conclusion](#section-9)** â€” From imperative to declarative optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-1",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-1\"></a>\n",
        "# 1. The Landscape of Optimization (The \"Old\" Way)\n",
        "\n",
        "The world of optimization is **fragmented**. Each problem domain has its own specialized library with its own API, paradigm, and learning curve. Let's look at the major categories:\n",
        "\n",
        "### Hyperparameter/Black-Box Optimization (Optuna)\n",
        "\n",
        "For hyperparameter tuning, you use Bayesian optimization or Tree-structured Parzen Estimators (TPE):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "optuna-example",
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
        "    \n",
        "    # Train model and return validation score\n",
        "    model = build_model(lr, n_layers)\n",
        "    return train_and_evaluate(model)\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scipy-intro",
      "metadata": {},
      "source": [
        "### Mathematical Optimization (SciPy)\n",
        "\n",
        "For continuous optimization of mathematical functions, you use classical algorithms like L-BFGS-B or SLSQP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ab70cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "def rosenbrock(x):\n",
        "    return sum(100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
        "\n",
        "result = minimize(\n",
        "    rosenbrock, \n",
        "    x0=[0, 0, 0, 0],\n",
        "    method='L-BFGS-B',\n",
        "    bounds=[(-5, 5)] * 4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1dcea18",
      "metadata": {},
      "source": [
        "### Evolutionary Algorithms (DEAP)\n",
        "\n",
        "For evolving programs or complex structures, you use genetic algorithms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scipy-example",
      "metadata": {},
      "outputs": [],
      "source": [
        "from deap import base, creator, tools, algorithms\n",
        "\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, 100)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"evaluate\", eval_func)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "population = toolbox.population(n=300)\n",
        "algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deap-intro",
      "metadata": {},
      "source": [
        "### The Problem: Fragmentation\n",
        "\n",
        "**A user needs to learn 3 different paradigms to solve 3 different optimization problems.**\n",
        "\n",
        "| Library | Domain | Paradigm | What You Must Know |\n",
        "|---------|--------|----------|-------------------|\n",
        "| Optuna | Hyperparameters | Bayesian/TPE | Samplers, pruners, search space definition |\n",
        "| SciPy | Mathematical Functions | Classical Methods | Algorithm selection (L-BFGS, SLSQP, etc.) |\n",
        "| DEAP | Evolutionary | Genetic Algorithms | Crossover, mutation, selection operators |\n",
        "\n",
        "Each library has:\n",
        "- **Different APIs and abstractions** â€” you can't just swap one for another\n",
        "- **Different optimization strategies** hard-coded into the implementation\n",
        "- **Different assumptions** about what can be optimized (differentiable? discrete? continuous?)\n",
        "\n",
        "### The Insight: Text is the Universal Representation\n",
        "\n",
        "Here's the key insight: **if something can be represented as text, an LLM can reason about it and propose improvements**.\n",
        "\n",
        "- **Code** is text â†’ LLMs can write and improve code\n",
        "- **Prompts** are text â†’ LLMs can refine instructions\n",
        "- **Configurations** are text â†’ LLMs can tune JSON/YAML\n",
        "- **Agent architectures** are text â†’ LLMs can evolve program structure\n",
        "\n",
        "What if we had **one API** that could optimize all of themâ€”by leveraging the LLM's ability to understand and generate text?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fragmentation-problem",
      "metadata": {},
      "source": [
        "<!-- This cell intentionally left empty - placeholder for removal -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-2",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-2\"></a>\n",
        "# 2. The Unifying Abstraction: `optimize_anything`\n",
        "\n",
        "We introduce `optimize_anything`â€”a single entry point for optimizing any text-representable artifact. It's designed to be the **\"Convex Hull\"** of all optimization problems: every point in the space of text-based optimization can be reached through this API.\n",
        "\n",
        "## The API Signature\n",
        "\n",
        "The API is intentionally minimal. You need only two things:\n",
        "1. **A seed candidate** â€” your starting point\n",
        "2. **A fitness function** â€” how to measure success"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "api-signature",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gepa.optimize_anything import optimize_anything, GEPAConfig\n",
        "\n",
        "def optimize_anything(\n",
        "    # === REQUIRED ===\n",
        "    seed_candidate: dict[str, str],           # Your starting point (text parameters to optimize)\n",
        "    fitness_fn: FitnessFn,                    # How to measure success\n",
        "    \n",
        "    # === OPTIONAL: Data ===\n",
        "    dataset: list[DataInst] | None = None,   # Examples to optimize on (for example, multiple related tasks)\n",
        "    valset: list[DataInst] | None = None,    # Held-out set for ensuring generalization if required\n",
        "    \n",
        "    # === OPTIONAL: Natural Language Guidance ===\n",
        "    objective: str | None = None,            # What you're trying to achieve (e.g. \"Find a prompt that maximizes accuracy\")\n",
        "    background: str | None = None,           # Domain knowledge, constraints, strategies (e.g. Domain knowledge about the framework which the candidate is written in)\n",
        "    \n",
        "    # === OPTIONAL: Fine-Grained Control ===\n",
        "    config: GEPAConfig | None = None,        # Engine, reflection, tracking settings\n",
        ") -> GEPAResult:\n",
        "    \"\"\"\n",
        "    Optimize any parameterized system using evolutionary algorithms with LLM-based reflection.\n",
        "    \n",
        "    Returns:\n",
        "        GEPAResult containing best_candidate, optimization history, and metrics.\n",
        "    \"\"\"\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "api-philosophy",
      "metadata": {},
      "source": [
        "## The Philosophy: Declare, Don't Implement\n",
        "\n",
        "With `optimize_anything`, the user **declares** the optimization problem:\n",
        "\n",
        "| You Provide | Example | Purpose |\n",
        "|-------------|---------|---------|\n",
        "| `seed_candidate` | `{\"prompt\": \"Solve this math problem:\"}` | Your starting point |\n",
        "| `fitness_fn` | Returns (score, output, side_info) | How to measure success |\n",
        "| `dataset` (optional) | List of test cases | Multi-instance generalization |\n",
        "| `objective` (optional) | \"Find a prompt that maximizes accuracy\" | Natural language guidance |\n",
        "| `background` (optional) | \"Solutions must handle edge cases\" | Domain knowledge |\n",
        "\n",
        "GEPA handles the **how**: proposing mutations, reflecting on failures, selecting candidates, and tracking the optimization trajectory.\n",
        "\n",
        "## Two Modes of Operation\n",
        "\n",
        "### Per-Instance Mode (with `dataset`)\n",
        "\n",
        "For problems where you want parameters that **generalize** across examples:\n",
        "- **Prompt optimization**: The same prompt should work on many math problems\n",
        "- **Agent architecture search**: The same agent should solve many tasks\n",
        "\n",
        "```python\n",
        "# dataset is a list of examples\n",
        "result = optimize_anything(\n",
        "    seed_candidate={\"prompt\": \"Solve:\"},\n",
        "    fitness_fn=evaluate_prompt,\n",
        "    dataset=math_problems,  # â† Optimize across these\n",
        "    valset=held_out_problems,  # â† Test generalization\n",
        ")\n",
        "```\n",
        "\n",
        "### Single-Instance Mode (without `dataset`)\n",
        "\n",
        "For problems defined by a **single optimization target**:\n",
        "- **Circle packing**: Maximize sum of radii for N circles\n",
        "- **Code evolution**: Minimize a mathematical function\n",
        "\n",
        "```python\n",
        "# dataset=None triggers single-instance mode\n",
        "result = optimize_anything(\n",
        "    seed_candidate={\"code\": \"def solve(): ...\"},\n",
        "    fitness_fn=evaluate_code,\n",
        "    dataset=None,  # â† Single optimization target\n",
        ")\n",
        "```\n",
        "\n",
        "## The Fitness Function: Your Optimization Signal\n",
        "\n",
        "The fitness function is where you define *what* you're optimizing for:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fitness-fn-example",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "def fitness_fn(\n",
        "    candidate: dict[str, str],  # The parameters being optimized\n",
        "    example: Any | None = None  # A single data instance (None for single-instance mode)\n",
        ") -> tuple[float, Any, dict]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        score: Higher is better\n",
        "        output: The actual output produced (for tracking)\n",
        "        side_info: Diagnostic information for LLM reflection\n",
        "    \"\"\"\n",
        "    # Run your system with the candidate parameters\n",
        "    output = run_my_system(candidate, example)\n",
        "    \n",
        "    # Compute a score (higher is better)\n",
        "    score = compute_score(output, example)\n",
        "    \n",
        "    # Collect diagnostic info for LLM reflection\n",
        "    side_info = {\n",
        "        \"Input\": example[\"input\"],\n",
        "        \"Output\": output,\n",
        "        \"Expected\": example[\"expected\"],\n",
        "        \"Error\": get_error_message(output),\n",
        "    }\n",
        "    \n",
        "    return score, output, side_info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "side-info-power",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-3\"></a>\n",
        "# 3. The Secret Weapon: Auxiliary Side Information (ASI)\n",
        "\n",
        "The `side_info` dictionary is GEPA's secret weaponâ€”we call it **ASI** (**A**uxiliary **S**ide **I**nformation). \n",
        "\n",
        "> *While the AI community debates when we'll achieve ASI (Artificial Superintelligence), you can achieve **your** ASI todayâ€”just return rich diagnostic information from your fitness function.*\n",
        "\n",
        "## Why ASI Matters\n",
        "\n",
        "Traditional optimizers only see a **scalar score**:\n",
        "\n",
        "```\n",
        "Candidate A â†’ Score: 0.73  (Why did it fail? No idea.)\n",
        "Candidate B â†’ Score: 0.85  (What made it better? Unknown.)\n",
        "```\n",
        "\n",
        "GEPA's LLM-based reflection can understand **why** a candidate performed the way it did:\n",
        "\n",
        "```\n",
        "Candidate A â†’ Score: 0.73\n",
        "  side_info: {\n",
        "    \"Error\": \"Circle 3 and Circle 7 overlap by 0.02 units\",\n",
        "    \"Boundary violations\": [\"Circle 12 extends past x=1.0\"],\n",
        "    \"Best score achieved\": 2.847\n",
        "  }\n",
        "```\n",
        "\n",
        "Now the LLM knows *exactly* what to fix.\n",
        "\n",
        "## What to Include in ASI\n",
        "\n",
        "| Information Type | Example | Purpose |\n",
        "|-----------------|---------|----------|\n",
        "| **Error messages** | `\"SyntaxError: invalid syntax on line 42\"` | Helps LLM fix code bugs |\n",
        "| **Execution traces** | `\"Called API 3x, timeout on 3rd call\"` | Helps LLM understand behavior |\n",
        "| **Partial results** | `\"3/5 test cases passed\"` | Helps LLM identify failure patterns |\n",
        "| **Expected vs Actual** | `\"Expected: [1,2,3], Got: [1,2,4]\"` | Helps LLM understand what went wrong |\n",
        "| **Domain feedback** | `\"Circles overlap at positions (0.5, 0.3)\"` | Helps LLM make domain-aware improvements |\n",
        "| **Reasoning traces** | `\"Model's chain-of-thought: ...\"` | Helps LLM understand failure modes |\n",
        "\n",
        "## The ASI Design Principle\n",
        "\n",
        "**Be generous with information.** Include anything that would help a human expert understand why the candidate succeeded or failed. The LLM will use this to make targeted, intelligent improvements rather than random mutations.\n",
        "\n",
        "```python\n",
        "# Good ASI\n",
        "side_info = {\n",
        "    \"Input\": problem_description,\n",
        "    \"Output\": model_output,\n",
        "    \"Expected\": correct_answer,\n",
        "    \"Reasoning\": model_reasoning_trace,\n",
        "    \"Error\": \"Division by zero on line 15\",\n",
        "    \"Partial scores\": {\"accuracy\": 0.8, \"efficiency\": 0.3},\n",
        "}\n",
        "\n",
        "# Bad ASI (not enough information)\n",
        "side_info = {\"score\": 0.73}  # LLM can't help with just this!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-3",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-4\"></a>\n",
        "# 4. Example 1: Mathematical Optimization â€” Beating Optuna\n",
        "\n",
        "**Result: GEPA outperforms Optuna on the EvalSet benchmark.**\n",
        "\n",
        "This example demonstrates how `optimize_anything` can evolve **code** that implements optimization algorithmsâ€”essentially using LLMs to discover optimization strategies automatically.\n",
        "\n",
        "## The Challenge\n",
        "\n",
        "Optuna is the industry standard for black-box optimization. But using Optuna effectively requires:\n",
        "- Choosing sampling algorithms (TPE, CMA-ES, Random, etc.)\n",
        "- Defining search spaces manually\n",
        "- Tuning algorithm-specific hyperparameters\n",
        "- Deep knowledge of optimization theory\n",
        "\n",
        "**What if we could just write code that finds minima, and let GEPA evolve the strategy?**\n",
        "\n",
        "## The Task\n",
        "\n",
        "**Given**: A black-box function `objective_function(x) â†’ float` with bounds\n",
        "**Find**: Python code that discovers the minimum\n",
        "\n",
        "**What GEPA optimizes**: The Python code itselfâ€”algorithm choice, implementation, hyperparameters, heuristics.\n",
        "\n",
        "<img src=\"./assets/blog/polynomial_optimization_normalized.png\" width=\"60%\">\n",
        "\n",
        "*GEPA starts below Optuna but progressively discovers better strategies, eventually surpassing it.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poly-setup",
      "metadata": {},
      "source": [
        "## Setting Up the Problem\n",
        "\n",
        "We use the [EvalSet benchmark](https://github.com/sigopt/evalset)â€”a collection of challenging optimization test functions (Ackley, Rosenbrock, Rastrigin, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-dataset",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Each problem is a black-box function with bounds and dimension\n",
        "dataset = [{\n",
        "    \"problem_description\": \"\"\"Blackbox optimization problem.\n",
        "    Minimize a function that takes a numpy array of shape (11,) and returns a scalar.\n",
        "    Bounds: [(-10, 30)] * 11\n",
        "    The function is unknown - you can only call objective_function(x) to evaluate.\n",
        "    \"\"\",\n",
        "    \"dim\": 11,\n",
        "    \"bounds\": [(-10, 30)] * 11,\n",
        "}]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poly-seed",
      "metadata": {},
      "source": [
        "## The Seed Candidate\n",
        "\n",
        "We start with a trivial baselineâ€”random sampling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-seed-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_candidate = {\n",
        "    \"code\": \"\"\"\n",
        "import numpy as np\n",
        "\n",
        "def solve(dim, bounds, objective_function, prev_best_x):\n",
        "    '''Find x that minimizes objective_function within bounds.'''\n",
        "    bounds_arr = np.array(bounds)\n",
        "    \n",
        "    # Trivial baseline: random sampling\n",
        "    x = np.random.uniform(bounds_arr[:, 0], bounds_arr[:, 1])\n",
        "    y = objective_function(x)\n",
        "    print(\"y:\", y)\n",
        "    return x\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    x = solve(dim, bounds, objective_function, prev_best_x)\n",
        "\"\"\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poly-fitness",
      "metadata": {},
      "source": [
        "## The Fitness Function\n",
        "\n",
        "The fitness function executes the code in a sandbox and captures rich diagnostic information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-fitness-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Any\n",
        "\n",
        "def execute_code(code: str, timeout: int, global_vars: dict) -> dict:\n",
        "    \"\"\"Execute code in sandbox with timeout. Returns output, logs, results, error.\"\"\"\n",
        "    # Implementation handles: stdout/stderr capture, timeout, exception handling\n",
        "    ...\n",
        "\n",
        "def fitness_fn(candidate: dict[str, str], problem: Any) -> tuple[float, Any, dict]:\n",
        "    \"\"\"Evaluate optimization code on a black-box problem.\"\"\"\n",
        "    code = candidate[\"code\"]\n",
        "    \n",
        "    # Execute the candidate code\n",
        "    execution = execute_code(\n",
        "        code,\n",
        "        timeout=300,\n",
        "        global_vars={\n",
        "            \"dim\": problem[\"dim\"],\n",
        "            \"bounds\": problem[\"bounds\"],\n",
        "            \"objective_function\": problem[\"objective_function\"],\n",
        "            \"prev_best_x\": None,  # Can pass previous best for warm-starting\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Compute score: negative function value (higher is better, we're minimizing)\n",
        "    if \"x\" in execution[\"results\"] and not execution[\"error\"]:\n",
        "        x = np.array(execution[\"results\"][\"x\"])\n",
        "        score = -problem[\"objective_function\"](x)  # Negate to maximize\n",
        "    else:\n",
        "        score = -1e9  # Penalize failures\n",
        "    \n",
        "    # Rich side_info for LLM reflection\n",
        "    side_info = {\n",
        "        \"scores\": {\"score\": score},\n",
        "        \"Input\": {\"problem_description\": problem[\"problem_description\"]},\n",
        "        \"code_side_info\": {\n",
        "            \"X\": execution[\"results\"].get(\"x\", \"not found\"),\n",
        "            \"Prints\": execution[\"output\"],   # Captured stdout\n",
        "            \"Logs\": execution[\"logs\"],       # Captured stderr  \n",
        "            \"Error\": execution[\"error\"],     # Any exceptions\n",
        "        },\n",
        "    }\n",
        "    \n",
        "    return score, {\"code\": code, **side_info}, side_info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poly-optimize",
      "metadata": {},
      "source": [
        "## Running GEPA Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-optimize-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gepa.optimize_anything import (\n",
        "    optimize_anything,\n",
        "    GEPAConfig,\n",
        "    EngineConfig,\n",
        "    ReflectionConfig,\n",
        ")\n",
        "\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=dataset,\n",
        "    config=GEPAConfig(\n",
        "        engine=EngineConfig(\n",
        "            max_metric_calls=100,\n",
        "            track_best_outputs=True,\n",
        "        ),\n",
        "        reflection=ReflectionConfig(\n",
        "            reflection_lm=\"openai/gpt-5\",\n",
        "            reflection_minibatch_size=1,  # One problem at a time\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Optimized code:\")\n",
        "print(result.best_candidate[\"code\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poly-evolved",
      "metadata": {},
      "source": [
        "## What GEPA Discovered\n",
        "\n",
        "GEPA evolved the trivial random sampler into a sophisticated optimization strategy. Here's a snippet from the evolved code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly-evolved-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evolved by GEPA - combines multiple strategies:\n",
        "\n",
        "evolved_code = \"\"\"\n",
        "import numpy as np\n",
        "\n",
        "def solve(dim, total_evaluation_budgets, bounds):\n",
        "    lb = np.array([b[0] for b in bounds], dtype=float)\n",
        "    ub = np.array([b[1] for b in bounds], dtype=float)\n",
        "    span = ub - lb\n",
        "    mid = (lb + ub) / 2.0\n",
        "    \n",
        "    # 1. Smart seeding: Halton sequence + LHS for diversity\n",
        "    def halton(n, d):\n",
        "        # Van der Corput sequence implementation\n",
        "        ...\n",
        "    \n",
        "    # 2. Zero-vector warm-start (often near polynomial optima)\n",
        "    zero_vec = np.zeros(dim)\n",
        "    if np.all(zero_vec >= lb) and np.all(zero_vec <= ub):\n",
        "        seeds.append(zero_vec)\n",
        "    \n",
        "    # 3. CMA-ES inspired evolution strategy\n",
        "    sigma = 0.20  # Adaptive step size\n",
        "    ...\n",
        "    \n",
        "    # 4. Local refinement with coordinate descent\n",
        "    def local_refine(max_evals):\n",
        "        ...\n",
        "    \n",
        "    return best_x\n",
        "\"\"\"\n",
        "\n",
        "# GEPA discovered: Halton sequences, zero-vector seeding,\n",
        "# CMA-ES-style evolution, and local refinementâ€”all without being told!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poly-takeaway",
      "metadata": {},
      "source": [
        "### Key Takeaway\n",
        "\n",
        "**GEPA vs. Traditional Optimization**:\n",
        "\n",
        "| Aspect | Optuna | GEPA |\n",
        "|--------|--------|------|\n",
        "| Algorithm selection | Manual (TPE, CMA-ES, etc.) | Automatic (evolved) |\n",
        "| Hyperparameter tuning | Required | Evolved |\n",
        "| Domain knowledge needed | High | Low |\n",
        "| What user provides | Search space + sampler config | Baseline code + fitness function |\n",
        "| What gets optimized | Parameter values | The optimization algorithm itself |\n",
        "\n",
        "While Optuna requires users to select algorithms and tune hyperparameters, GEPA automatically **discovers optimization strategies** by evolving code. The user just provides the problem and a baselineâ€”GEPA evolves Halton sequences, surrogate models, local refinement, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-4",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-5\"></a>\n",
        "# 5. Example 2: Prompt Engineering â€” AIME 2025\n",
        "\n",
        "**Result: GEPA improves GPT-4.1 Mini's accuracy from 46.67% to 53.33% on AIME 2025.**\n",
        "\n",
        "This example demonstrates how `optimize_anything` can evolve **prompts**â€”the natural language instructions that guide LLM behavior.\n",
        "\n",
        "## The Challenge\n",
        "\n",
        "Prompt engineering is often done through **trial and error**:\n",
        "1. Write a prompt\n",
        "2. Test on a few examples\n",
        "3. Manually tweak based on intuition\n",
        "4. Repeat until it \"feels right\"\n",
        "\n",
        "This is slow, doesn't scale, and doesn't guarantee you've found the best prompt.\n",
        "\n",
        "## The Task\n",
        "\n",
        "**Given**: A dataset of AIME math competition problems\n",
        "**Find**: A system prompt that maximizes GPT-4.1 Mini's accuracy\n",
        "\n",
        "**What GEPA optimizes**: The instruction promptâ€”what guidance to give the model.\n",
        "\n",
        "<img src=\"./assets/blog/aime_best_comparison.png\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aime-setup",
      "metadata": {},
      "source": [
        "## Setting Up the Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aime-setup-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import dspy\n",
        "import os\n",
        "\n",
        "# Configure the language model\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "lm = dspy.LM(\"gpt-4.1-mini\", api_key=api_key, temperature=1.0, max_tokens=32000)\n",
        "dspy.configure(lm=lm)\n",
        "\n",
        "# Load AIME dataset splits\n",
        "from examples.math.dataset import load_math_dataset\n",
        "trainset, valset, testset = load_math_dataset()\n",
        "\n",
        "print(f\"Training: {len(trainset)} problems\")\n",
        "print(f\"Validation: {len(valset)} problems\")\n",
        "print(f\"Test: {len(testset)} problems\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aime-module",
      "metadata": {},
      "source": [
        "## The DSPy Module\n",
        "\n",
        "We use DSPy's `ChainOfThought` for step-by-step reasoning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aime-module-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MathSolverSignature(dspy.Signature):\n",
        "    \"\"\"Solve a math competition problem.\"\"\"\n",
        "    input = dspy.InputField(desc=\"The math problem to solve.\")\n",
        "    answer = dspy.OutputField(desc=\"The final numerical answer.\")\n",
        "\n",
        "predictor = dspy.ChainOfThought(MathSolverSignature)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aime-seed",
      "metadata": {},
      "source": [
        "## The Seed Candidate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aime-seed-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_candidate = {\n",
        "    \"prompt\": \"\"\"Solve the math problem carefully. Break down the steps and provide the final answer as a single number.\"\"\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aime-fitness",
      "metadata": {},
      "source": [
        "## The Fitness Function\n",
        "\n",
        "The fitness function runs the predictor and collects detailed feedback:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aime-fitness-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def math_metric(example, prediction):\n",
        "    \"\"\"Compute score and detailed feedback.\"\"\"\n",
        "    correct_answer = int(example.answer)\n",
        "    \n",
        "    try:\n",
        "        llm_answer = int(prediction.answer)\n",
        "    except (ValueError, TypeError):\n",
        "        return dspy.Prediction(\n",
        "            score=0.0, \n",
        "            feedback=f\"Could not parse '{prediction.answer}' as integer. Correct: {correct_answer}\"\n",
        "        )\n",
        "    \n",
        "    score = float(correct_answer == llm_answer)\n",
        "    \n",
        "    if score == 1.0:\n",
        "        feedback = f\"Correct! Answer: {correct_answer}\"\n",
        "    else:\n",
        "        feedback = f\"Incorrect. Got {llm_answer}, expected {correct_answer}.\"\n",
        "        if hasattr(example, 'solution'):\n",
        "            feedback += f\"\\n\\nReference solution:\\n{example.solution}\"\n",
        "    \n",
        "    return dspy.Prediction(score=score, feedback=feedback)\n",
        "\n",
        "def fitness_fn(candidate: dict[str, str], example) -> tuple[float, Any, dict]:\n",
        "    \"\"\"Evaluate a prompt candidate on a single math problem.\"\"\"\n",
        "    # Set the prompt\n",
        "    predictor.predict.signature.instructions = candidate[\"prompt\"]\n",
        "    \n",
        "    # Run prediction\n",
        "    prediction = predictor(input=example.input)\n",
        "    metric_result = math_metric(example, prediction)\n",
        "    \n",
        "    output = {\n",
        "        \"prompt\": candidate[\"prompt\"],\n",
        "        \"answer\": prediction.answer,\n",
        "        \"score\": metric_result.score,\n",
        "    }\n",
        "    \n",
        "    # Side info includes reasoning trace for LLM reflection\n",
        "    side_info = {\n",
        "        \"Input\": example.input,\n",
        "        \"Output\": prediction.answer,\n",
        "        \"Reasoning\": getattr(prediction, \"reasoning\", \"\"),  # Chain-of-thought trace\n",
        "        \"ExecutionFeedback\": metric_result.feedback,\n",
        "    }\n",
        "    \n",
        "    return metric_result.score, output, side_info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aime-optimize",
      "metadata": {},
      "source": [
        "## Running GEPA Optimization\n",
        "\n",
        "Note: We use `valset` for generalization testingâ€”GEPA optimizes on `trainset` but tracks performance on held-out `valset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aime-optimize-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gepa.optimize_anything import (\n",
        "    optimize_anything,\n",
        "    GEPAConfig,\n",
        "    EngineConfig,\n",
        "    ReflectionConfig,\n",
        ")\n",
        "\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=trainset,   # Optimize on training set\n",
        "    valset=valset,      # Track generalization on validation set\n",
        "    config=GEPAConfig(\n",
        "        engine=EngineConfig(\n",
        "            max_metric_calls=800,\n",
        "            track_best_outputs=True,\n",
        "            parallel=True,\n",
        "            max_workers=32,\n",
        "        ),\n",
        "        reflection=ReflectionConfig(\n",
        "            reflection_lm=\"openai/gpt-5\",\n",
        "            reflection_minibatch_size=3,  # Show 3 problems per reflection\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"\\nOptimized prompt:\")\n",
        "print(result.best_candidate[\"prompt\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aime-evolved",
      "metadata": {},
      "source": [
        "## The Optimized Prompt\n",
        "\n",
        "GEPA discovered a detailed, structured prompt with domain-specific strategies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aime-evolved-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This prompt was EVOLVED by GEPA, not written by a human!\n",
        "# Starting from a simple \"Solve carefully and provide the answer\" prompt,\n",
        "# GEPA discovered domain-specific strategies through reflection.\n",
        "\n",
        "optimized_prompt = \"\"\"\n",
        "Solve from first principles with explicit checks. Requirements:\n",
        "\n",
        "1) Model precisely:\n",
        "- Define all objects, variables, and constraints algebraically/combinatorially.\n",
        "- Choose one counting model (labeled vs indistinguishable) and stay consistent.\n",
        "  For combinatorics, either label and divide at the end OR keep indistinguishable\n",
        "  throughoutâ€”do not mix.\n",
        "- For number-theory/decimal/ratio problems, state factorizations and gcd/lcm \n",
        "  relations explicitly.\n",
        "\n",
        "2) Mapping/Counting rigor:\n",
        "- When mapping elements between sets (e.g., m â†¦ m/gcd(m,N)), prove \n",
        "  injectivity/surjectivity or handle overlaps via inclusionâ€“exclusion.\n",
        "- When computing probability, ensure numerator and denominator are counts \n",
        "  from the same sample space.\n",
        "- Keep all computations exact (fractions/radicals/modular arithmetic); \n",
        "  avoid decimals unless terminating.\n",
        "\n",
        "3) Geometry workflow:\n",
        "- Draw and name a diagram (mentally or on paper). List candidate theorems: \n",
        "  power of a point, radical axis, homothety, similar triangles, cyclicity.\n",
        "- Identify perpendiculars to tangents through centers; use midpoint/radical-axis \n",
        "  facts for intersecting circles and common tangents.\n",
        "- Prefer exact relations (e.g., MPÂ·MQ = (tangent length)^2) over coordinate guesses.\n",
        "\n",
        "4) Sanity checks and diagnostics:\n",
        "- If an assumption yields a contradiction (e.g., negative squared length), \n",
        "  discard and rebuild the setup.\n",
        "- For combinatorics/NT counts, validate with a smaller analog (e.g., replace \n",
        "  9999 by 9 or 99) to detect double-counting before scaling up.\n",
        "- For expressions of the form mâˆšn, reduce n to be squarefree.\n",
        "- Perform at least one independent cross-check (alternative derivation, \n",
        "  structural identity, modular check, or small-n analog).\n",
        "\n",
        "5) Output:\n",
        "- Extract exactly what is asked (e.g., remainder, perimeter, m+n). \n",
        "- Provide the final answer only as a single number with no extra text.\n",
        "\"\"\"\n",
        "\n",
        "# ðŸŽ¯ What GEPA discovered:\n",
        "# - Domain-specific heuristics for different math areas (geometry, combinatorics, NT)\n",
        "# - Structured problem-solving workflow\n",
        "# - Sanity checks and validation strategies\n",
        "# - Explicit handling of common failure modes (mixing counting models, etc.)\n",
        "#\n",
        "# A human prompt engineer might take hours to discover these strategies!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aime-takeaway",
      "metadata": {},
      "source": [
        "### Key Takeaway\n",
        "\n",
        "By including the model's **reasoning trace** in `side_info`, GEPA can understand *how* the model approaches problemsâ€”not just whether it got the answer right. This enables:\n",
        "\n",
        "1. **Targeted improvements**: Fix specific reasoning errors, not random prompt tweaks\n",
        "2. **Domain-specific strategies**: The prompt evolved to include geometry workflows, combinatorics rules, etc.\n",
        "3. **Sanity checks**: GEPA discovered that asking for validation prevents common errors\n",
        "\n",
        "The evolved prompt contains strategies that a human prompt engineer might take hours to discover through manual iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-5",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-6\"></a>\n",
        "# 6. Example 3: Agent Program Evolution â€” ARC-AGI\n",
        "\n",
        "**Result: GEPA improves GPT-5's performance from 55.6% to 60.5% on ARC-AGI.**\n",
        "\n",
        "This is the most ambitious example: optimizing not just prompts, but **entire agent architectures**â€”the DSPy program that defines how an LLM reasons about problems.\n",
        "\n",
        "## The Challenge\n",
        "\n",
        "[ARC-AGI](https://arcprize.org/) (Abstraction and Reasoning Corpus) is a benchmark designed to test general intelligence:\n",
        "- Each task shows input-output grid transformation examples\n",
        "- The agent must infer the transformation rule and apply it to test inputs\n",
        "- Tasks require **visual reasoning, pattern recognition, and abstraction**\n",
        "\n",
        "Hand-designing agent architectures for such tasks is extremely difficult.\n",
        "\n",
        "## The Task\n",
        "\n",
        "**Given**: A dataset of ARC-AGI grid transformation tasks\n",
        "**Find**: A DSPy program (agent architecture) that maximizes accuracy\n",
        "\n",
        "**What GEPA optimizes**: The entire DSPy programâ€”signatures, modules, control flow, and prompting strategies.\n",
        "\n",
        "<img src=\"./assets/blog/arc_agi_best_comparison.png\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "arc-seed",
      "metadata": {},
      "source": [
        "## The Seed Candidate\n",
        "\n",
        "We start with a minimal Chain-of-Thought agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "arc-seed-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_candidate = {\n",
        "    \"program\": \"\"\"\n",
        "import dspy\n",
        "from typing import List\n",
        "import pydantic\n",
        "\n",
        "MATRIX = List[List[int]]\n",
        "\n",
        "class TrainingExample(pydantic.BaseModel):\n",
        "    input: MATRIX\n",
        "    output: MATRIX\n",
        "\n",
        "class SolveTaskSignature(dspy.Signature):\n",
        "    '''Solve ARC grid transformation tasks.'''\n",
        "    training_examples: List[TrainingExample] = dspy.InputField(\n",
        "        description=\"Input/output pairs demonstrating the transformation.\"\n",
        "    )\n",
        "    test_inputs: List[MATRIX] = dspy.InputField(\n",
        "        description=\"Inputs to transform.\"\n",
        "    )\n",
        "    test_outputs: List[MATRIX] = dspy.OutputField(\n",
        "        description=\"Transformed outputs.\"\n",
        "    )\n",
        "\n",
        "program = dspy.ChainOfThought(SolveTaskSignature)\n",
        "\"\"\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "arc-fitness",
      "metadata": {},
      "source": [
        "## The Fitness Function\n",
        "\n",
        "The fitness function compiles and executes the DSPy program, capturing detailed error information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "arc-fitness-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fitness_fn(candidate: dict[str, str], example) -> tuple[float, Any, dict]:\n",
        "    \"\"\"Evaluate a DSPy program on an ARC-AGI task.\"\"\"\n",
        "    program_code = candidate[\"program\"]\n",
        "    \n",
        "    try:\n",
        "        # Compile and execute the program\n",
        "        program = compile_dspy_program(program_code)\n",
        "        prediction = program(\n",
        "            training_examples=example.training_examples,\n",
        "            test_inputs=example.test_inputs\n",
        "        )\n",
        "        \n",
        "        # Score: exact match on test outputs\n",
        "        score = compute_arc_score(prediction.test_outputs, example.test_outputs)\n",
        "        \n",
        "        side_info = {\n",
        "            \"Input\": example,\n",
        "            \"Output\": prediction.test_outputs,\n",
        "            \"Reasoning\": getattr(prediction, \"reasoning\", \"\"),\n",
        "            \"Feedback\": f\"Score: {score:.2f}\",\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Capture compilation/runtime errors\n",
        "        score = 0.0\n",
        "        side_info = {\n",
        "            \"Input\": example,\n",
        "            \"Error\": str(e),\n",
        "            \"Traceback\": traceback.format_exc(),\n",
        "        }\n",
        "    \n",
        "    return score, {\"program\": program_code, \"score\": score}, side_info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "arc-optimize",
      "metadata": {},
      "source": [
        "## Running GEPA Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "arc-optimize-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=arc_trainset,\n",
        "    valset=arc_valset,\n",
        "    objective=\"Evolve a DSPy program that solves ARC-AGI grid transformation tasks.\",\n",
        "    background=\"\"\"ARC-AGI tasks require visual reasoning and pattern recognition.\n",
        "    Common patterns include: rotations, reflections, color substitutions, \n",
        "    pattern completion, and object manipulation.\"\"\",\n",
        "    config=GEPAConfig(\n",
        "        engine=EngineConfig(\n",
        "            max_metric_calls=500,\n",
        "            track_best_outputs=True,\n",
        "        ),\n",
        "        reflection=ReflectionConfig(\n",
        "            reflection_lm=\"openai/gpt-5\",\n",
        "            reflection_minibatch_size=3,\n",
        "        ),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "arc-evolved",
      "metadata": {},
      "source": [
        "## What GEPA Discovered\n",
        "\n",
        "GEPA evolved the simple ChainOfThought into a sophisticated 5-step code synthesis pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "arc-evolved-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evolved by GEPA - A code synthesis agent with self-refinement\n",
        "# This program was DISCOVERED through optimization, not hand-written!\n",
        "\n",
        "evolved_program = \"\"\"\n",
        "import dspy\n",
        "from typing import List, Optional, Callable\n",
        "import copy\n",
        "import re\n",
        "\n",
        "class SynthesizeTransform(dspy.Signature):\n",
        "    '''\n",
        "    Write valid Python code that defines: def transform(grid) -> grid\n",
        "    \n",
        "    Requirements:\n",
        "    - Use only built-in Python (lists/loops/dicts/sets); no imports.\n",
        "    - Must be general to similar-sized grids; do NOT hardcode indices.\n",
        "    - Preserve separator rows/columns if present.\n",
        "    '''\n",
        "    training_examples: List[TrainingExample] = dspy.InputField()\n",
        "    hint: str = dspy.InputField(desc=\"Feedback on prior failures.\")\n",
        "    code: str = dspy.OutputField(desc=\"Python code defining transform().\")\n",
        "\n",
        "class CodeSynthesisSolver(dspy.Module):\n",
        "    def __init__(self, attempts=5):\n",
        "        self.codegen = dspy.ChainOfThought(SynthesizeTransform)\n",
        "        self.attempts = attempts\n",
        "    \n",
        "    def _verify_on_training(self, fn, training_examples):\n",
        "        '''Validate transform matches ALL training outputs exactly.'''\n",
        "        for idx, ex in enumerate(training_examples):\n",
        "            pred = fn(copy.deepcopy(ex.input))\n",
        "            if pred != ex.output:\n",
        "                return False, f\"Mismatch on example {idx}\"\n",
        "        return True, None\n",
        "    \n",
        "    def forward(self, training_examples, test_inputs):\n",
        "        hint = \"Infer a general rule from ALL training pairs.\"\n",
        "        \n",
        "        for attempt in range(self.attempts):\n",
        "            # Step 1: Generate code hypothesis\n",
        "            pred = self.codegen(training_examples=training_examples, hint=hint)\n",
        "            code = extract_code_block(pred.code)\n",
        "            \n",
        "            # Step 2: Load and compile\n",
        "            fn, load_error = load_transform_func(code)\n",
        "            if fn is None:\n",
        "                hint = f\"Attempt {attempt} failed to compile: {load_error}\"\n",
        "                continue\n",
        "            \n",
        "            # Step 3: Validate on ALL training examples\n",
        "            ok, validation_error = self._verify_on_training(fn, training_examples)\n",
        "            \n",
        "            if ok:\n",
        "                # Step 4: Execute on test inputs\n",
        "                outputs = [fn(copy.deepcopy(g)) for g in test_inputs]\n",
        "                return dspy.Prediction(test_outputs=outputs)\n",
        "            else:\n",
        "                # Step 5: Self-refine based on error feedback\n",
        "                hint = f\"Attempt {attempt} incorrect: {validation_error}. Fix it.\"\n",
        "        \n",
        "        # Fallback: identity transform\n",
        "        return dspy.Prediction(test_outputs=[copy.deepcopy(g) for g in test_inputs])\n",
        "\n",
        "program = CodeSynthesisSolver(attempts=5)\n",
        "\"\"\"\n",
        "\n",
        "# ðŸŽ¯ GEPA discovered SELF-REFINEMENT!\n",
        "# \n",
        "# The evolved agent automatically:\n",
        "# 1. Hypothesizes a transformation rule from examples\n",
        "# 2. Generates Python code implementing it\n",
        "# 3. Validates the code on ALL training examples\n",
        "# 4. Self-refines if validation fails (up to 5 attempts)\n",
        "# 5. Only executes on test inputs after validation passes\n",
        "#\n",
        "# This strategy was NOT programmed - it EMERGED from optimization!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "arc-takeaway",
      "metadata": {},
      "source": [
        "### Key Takeaway: Emergent Self-Refinement\n",
        "\n",
        "GEPA discovered **self-refinement**â€”having the LLM validate and fix its own code before producing outputs. This is remarkable because:\n",
        "\n",
        "1. **Not programmed**: Self-refinement emerged from optimization, not from human design\n",
        "2. **Sophisticated strategy**: The agent now verifies on training before applying to test\n",
        "3. **Multi-attempt recovery**: Up to 5 refinement attempts with targeted feedback\n",
        "4. **Code synthesis**: Instead of direct prediction, the agent writes executable code\n",
        "\n",
        "This demonstrates GEPA's ability to discover **complex reasoning pipelines** that humans might not think to design."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-6",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-7\"></a>\n",
        "# 7. Example 4: Algorithmic Discovery â€” Circle Packing\n",
        "\n",
        "**Result: GEPA matches or exceeds AlphaEvolve, ShinkaEvolve, and OpenEvolve on circle packing.**\n",
        "\n",
        "This example demonstrates **algorithmic discovery**â€”evolving code to solve a well-known NP-hard optimization problem.\n",
        "\n",
        "## The Challenge\n",
        "\n",
        "Circle packing is a classic problem with real-world applications (chip layout, material cutting, logistics):\n",
        "- Pack N non-overlapping circles inside a unit square [0,1] Ã— [0,1]\n",
        "- Maximize the sum of all radii\n",
        "- This is **NP-hard**â€”no known polynomial-time algorithm exists\n",
        "\n",
        "Recent work from DeepMind (AlphaEvolve), and open-source efforts (ShinkaEvolve, OpenEvolve) have used LLMs to evolve packing algorithms.\n",
        "\n",
        "## The Task\n",
        "\n",
        "**Given**: The number of circles N (e.g., N=26)\n",
        "**Find**: Python code that computes optimal circle placements\n",
        "\n",
        "**What GEPA optimizes**: The packing algorithm codeâ€”placement strategies, local optimization, constraint handling.\n",
        "\n",
        "<img src=\"./assets/blog/circle_packing/circle_packing_26.png\" width=\"40%\">\n",
        "<img src=\"./assets/blog/circle_packing/gepa_vs_shinka.png\" width=\"50%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "circle-setup",
      "metadata": {},
      "source": [
        "## Setting Up the Problem\n",
        "\n",
        "Circle packing is a single-instance optimization problemâ€”no dataset needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "circle-setup-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_CIRCLES = 26  # Can be 21, 26, 32, etc.\n",
        "\n",
        "# Single-instance mode: dataset describes the problem\n",
        "dataset = [{\n",
        "    \"problem_description\": f\"\"\"Pack {NUM_CIRCLES} non-overlapping circles inside a unit square [0,1]Ã—[0,1].\n",
        "    Each circle has center (x, y) and radius r.\n",
        "    Constraints:\n",
        "    - Circles must not overlap: distance between any two centers >= sum of radii\n",
        "    - Circles must fit in square: r <= x <= 1-r and r <= y <= 1-r\n",
        "    - All radii must be positive\n",
        "    \n",
        "    Objective: Maximize the sum of all radii.\n",
        "    \"\"\",\n",
        "    \"num_circles\": NUM_CIRCLES,\n",
        "}]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "circle-seed",
      "metadata": {},
      "source": [
        "## The Seed Candidate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "circle-seed-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_candidate = {\n",
        "    \"code\": \"\"\"\n",
        "import numpy as np\n",
        "\n",
        "def pack_circles(n_circles: int) -> np.ndarray:\n",
        "    '''Pack n circles in unit square. Returns array of shape (n, 3) with [x, y, r].'''\n",
        "    circles = []\n",
        "    \n",
        "    # Simple grid placement\n",
        "    grid_size = int(np.sqrt(n_circles)) + 1\n",
        "    spacing = 1.0 / (grid_size + 1)\n",
        "    radius = spacing / 2.5\n",
        "    \n",
        "    for i in range(n_circles):\n",
        "        row = i // grid_size\n",
        "        col = i % grid_size\n",
        "        x = (col + 1) * spacing\n",
        "        y = (row + 1) * spacing\n",
        "        circles.append([x, y, radius])\n",
        "    \n",
        "    return np.array(circles)\n",
        "\"\"\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "circle-fitness",
      "metadata": {},
      "source": [
        "## The Fitness Function\n",
        "\n",
        "The fitness function validates constraints and returns detailed violation information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "circle-fitness-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_circles(circles: np.ndarray, n_circles: int) -> dict:\n",
        "    \"\"\"Validate circle packing constraints.\"\"\"\n",
        "    violations = []\n",
        "    \n",
        "    for i, (x, y, r) in enumerate(circles):\n",
        "        # Check bounds\n",
        "        if x - r < 0 or x + r > 1 or y - r < 0 or y + r > 1:\n",
        "            violations.append(f\"Circle {i} exceeds bounds\")\n",
        "        if r <= 0:\n",
        "            violations.append(f\"Circle {i} has non-positive radius\")\n",
        "    \n",
        "    # Check overlaps\n",
        "    for i in range(n_circles):\n",
        "        for j in range(i + 1, n_circles):\n",
        "            dist = np.sqrt((circles[i, 0] - circles[j, 0])**2 + \n",
        "                          (circles[i, 1] - circles[j, 1])**2)\n",
        "            if dist < circles[i, 2] + circles[j, 2] - 1e-9:\n",
        "                violations.append(f\"Circles {i} and {j} overlap\")\n",
        "    \n",
        "    return {\n",
        "        \"valid\": len(violations) == 0,\n",
        "        \"violations\": violations,\n",
        "        \"sum_radii\": float(circles[:, 2].sum()) if len(violations) == 0 else 0.0,\n",
        "    }\n",
        "\n",
        "def fitness_fn(candidate: dict[str, str], example) -> tuple[float, Any, dict]:\n",
        "    \"\"\"Evaluate circle packing code.\"\"\"\n",
        "    code = candidate[\"code\"]\n",
        "    n_circles = example[\"num_circles\"]\n",
        "    \n",
        "    try:\n",
        "        # Execute the packing code\n",
        "        result = execute_code(code, timeout=600, n_circles=n_circles)\n",
        "        circles = result[\"circles\"]\n",
        "        \n",
        "        # Validate constraints\n",
        "        validation = validate_circles(circles, n_circles)\n",
        "        score = validation[\"sum_radii\"]\n",
        "        \n",
        "        side_info = {\n",
        "            \"scores\": {\"sum_radii\": score},\n",
        "            \"Code\": code,\n",
        "            \"Circles\": circles.tolist(),\n",
        "            \"Violations\": validation[\"violations\"],\n",
        "            \"Stdout\": result.get(\"stdout\", \"\"),\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        score = 0.0\n",
        "        side_info = {\n",
        "            \"scores\": {\"sum_radii\": 0.0},\n",
        "            \"Code\": code,\n",
        "            \"Error\": str(e),\n",
        "            \"Traceback\": traceback.format_exc(),\n",
        "        }\n",
        "    \n",
        "    return score, {\"code\": code, \"score\": score}, side_info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "circle-optimize",
      "metadata": {},
      "source": [
        "## Running GEPA Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "circle-optimize-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=dataset,\n",
        "    objective=\"Maximize the sum of radii of N circles packed in a unit square.\",\n",
        "    background=\"\"\"Circle packing is NP-hard. Effective approaches include:\n",
        "    - Greedy placement with local optimization\n",
        "    - Simulated annealing\n",
        "    - Gradient-based methods treating circles as soft constraints\n",
        "    - Physics simulations with repulsion forces\n",
        "    \"\"\",\n",
        "    config=GEPAConfig(\n",
        "        engine=EngineConfig(\n",
        "            max_metric_calls=200,\n",
        "            track_best_outputs=True,\n",
        "            frontier_type=\"objective\",  # Single objective optimization\n",
        "        ),\n",
        "        reflection=ReflectionConfig(\n",
        "            reflection_lm=\"openai/gpt-5\",\n",
        "            reflection_minibatch_size=1,\n",
        "        ),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "circle-evolved",
      "metadata": {},
      "source": [
        "## What GEPA Discovered\n",
        "\n",
        "GEPA evolved the simple grid-based baseline into a **sophisticated multi-strategy optimizer**. Here's what the evolved code includes:\n",
        "\n",
        "### Strategies Discovered by GEPA\n",
        "\n",
        "| Strategy | Description | How It Helps |\n",
        "|----------|-------------|--------------|\n",
        "| **Halton sequences** | Quasi-random initialization | Better initial coverage than random |\n",
        "| **Zero-vector seeding** | Start from origin | Often near polynomial optima |\n",
        "| **CMA-ES-style evolution** | Covariance matrix adaptation | Adapts search direction to landscape |\n",
        "| **Quadratic surrogate models** | Local function approximation | Efficient local optimization |\n",
        "| **Coordinate descent** | Per-dimension refinement | Fine-tunes individual coordinates |\n",
        "| **Nelder-Mead subspace** | Simplex method in active dimensions | Exploits important variables |\n",
        "| **Ridge-linear probes** | Gradient estimation from archive | Uses history for direction hints |\n",
        "\n",
        "### Results\n",
        "\n",
        "The evolved code achieves packing densities that **match or exceed** published results from:\n",
        "- **AlphaEvolve** (DeepMind)\n",
        "- **ShinkaEvolve**\n",
        "- **OpenEvolve**\n",
        "\n",
        "All without any human optimization expertiseâ€”just the problem definition and a baseline!\n",
        "\n",
        "### Key Takeaway\n",
        "\n",
        "GEPA automatically discovered advanced optimization strategies (Halton sequences, CMA-ES, surrogate models) that typically require expert knowledge to implement. The user only needed to:\n",
        "1. Define the problem (pack circles)\n",
        "2. Provide a naive baseline (grid placement)\n",
        "3. Return informative `side_info` (violations, scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-7",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-8\"></a>\n",
        "# 8. How It Works Under the Hood\n",
        "\n",
        "GEPA (Generative Evolutionary Prompting with ASI) operates through a loop of **evaluation**, **reflection**, and **proposal**:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                              GEPA LOOP                                       â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                             â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                          â”‚\n",
        "â”‚   â”‚  EVALUATE    â”‚  Run fitness_fn on candidates                            â”‚\n",
        "â”‚   â”‚              â”‚  â†’ Collect scores AND side_info                          â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                          â”‚\n",
        "â”‚          â”‚                                                                  â”‚\n",
        "â”‚          â–¼                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                          â”‚\n",
        "â”‚   â”‚   SELECT     â”‚  Choose candidates for mutation                          â”‚\n",
        "â”‚   â”‚              â”‚  â†’ Pareto selection across objectives/instances          â”‚\n",
        "â”‚   â”‚              â”‚  â†’ Epsilon-greedy exploration                            â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                          â”‚\n",
        "â”‚          â”‚                                                                  â”‚\n",
        "â”‚          â–¼                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                          â”‚\n",
        "â”‚   â”‚   REFLECT    â”‚  LLM analyzes evaluation results                         â”‚\n",
        "â”‚   â”‚              â”‚  â†’ \"Why did this candidate fail?\"                        â”‚\n",
        "â”‚   â”‚              â”‚  â†’ Uses side_info to understand failure modes            â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                          â”‚\n",
        "â”‚          â”‚                                                                  â”‚\n",
        "â”‚          â–¼                                                                  â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                          â”‚\n",
        "â”‚   â”‚   PROPOSE    â”‚  LLM generates improved candidates                       â”‚\n",
        "â”‚   â”‚              â”‚  â†’ Targeted mutations based on reflection                â”‚\n",
        "â”‚   â”‚              â”‚  â†’ Preserves successful behaviors                        â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                          â”‚\n",
        "â”‚          â”‚                                                                  â”‚\n",
        "â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º REPEAT until stopping condition               â”‚\n",
        "â”‚                                                                             â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### 1. Pareto Frontier\n",
        "GEPA maintains a **Pareto frontier** of candidates that are optimal on different subsets of the data:\n",
        "- **Multi-objective**: Some candidates optimize for accuracy, others for speed\n",
        "- **Instance-level**: Some candidates excel on certain problem types\n",
        "- **Diversity**: The frontier preserves diverse strategies for exploration\n",
        "\n",
        "### 2. Reflective Mutation\n",
        "Unlike **random mutation** in traditional evolutionary algorithms, GEPA uses LLMs to make **targeted improvements**:\n",
        "\n",
        "| Traditional EA | GEPA |\n",
        "|---------------|------|\n",
        "| Random bit flips | LLM analyzes failure modes |\n",
        "| Blind crossover | LLM preserves working patterns |\n",
        "| Requires many generations | Sample-efficient |\n",
        "| No domain knowledge | Uses side_info for context |\n",
        "\n",
        "### 3. Side Information Flow\n",
        "The `side_info` returned by your fitness function powers the reflection:\n",
        "\n",
        "```python\n",
        "# What the LLM sees during reflection:\n",
        "\"\"\"\n",
        "Current candidate: {code: \"def solve(x): ...\"}\n",
        "\n",
        "Evaluation results on 3 examples:\n",
        "  Example 1: Score 0.8\n",
        "    Input: \"Pack 26 circles\"\n",
        "    Output: circles array\n",
        "    Error: \"Circles 3 and 7 overlap\"\n",
        "    \n",
        "  Example 2: Score 0.0  \n",
        "    Input: \"Pack 26 circles\"\n",
        "    Error: \"IndexError on line 42\"\n",
        "    \n",
        "  Example 3: Score 1.0\n",
        "    Input: \"Pack 26 circles\"\n",
        "    Output: Valid packing with sum_radii=2.89\n",
        "\n",
        "Propose an improved version that fixes these issues.\n",
        "\"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-8",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"section-9\"></a>\n",
        "# 9. Conclusion: From Imperative to Declarative Optimization\n",
        "\n",
        "We are witnessing a **paradigm shift** in optimizationâ€”from imperative implementations to declarative specifications:\n",
        "\n",
        "| **Old Paradigm** | **New Paradigm with `optimize_anything`** |\n",
        "|------------------|------------------------------------------|\n",
        "| Imperative: specify *how* to optimize | Declarative: specify *what* to optimize |\n",
        "| Different libraries for different problems | **One API for everything** |\n",
        "| Mathematically-specific algorithms | Language-driven proposal generation |\n",
        "| Scalar fitness only | **Rich diagnostic information (ASI)** |\n",
        "| Random mutations | **Targeted, reflective mutations** |\n",
        "| Expert knowledge required | LLM brings domain knowledge |\n",
        "\n",
        "## The `optimize_anything` Vision\n",
        "\n",
        "**If it can be represented as text, it can be optimized.**\n",
        "\n",
        "| Domain | What You Optimize | Example |\n",
        "|--------|-------------------|---------|\n",
        "| **Code** | Algorithms, implementations | Black-box optimization code |\n",
        "| **Prompts** | Instructions, examples | System prompts for math problems |\n",
        "| **Agent Architectures** | Program structure, control flow | DSPy programs for ARC-AGI |\n",
        "| **Configurations** | Hyperparameters, settings | JSON/YAML configs |\n",
        "| **Data Structures** | Schemas, templates | API specifications |\n",
        "\n",
        "## Why This Matters\n",
        "\n",
        "1. **Democratization**: You don't need a PhD in optimization to solve hard problems\n",
        "2. **Generalization**: One framework, infinite applications\n",
        "3. **Sample Efficiency**: LLM reflection beats random search\n",
        "4. **Emergent Capabilities**: GEPA discovers strategies you wouldn't think of\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "```bash\n",
        "pip install gepa\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "getting-started-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gepa.optimize_anything import optimize_anything, GEPAConfig, EngineConfig, ReflectionConfig\n",
        "\n",
        "# 1. Define your seed candidate (starting point)\n",
        "seed_candidate = {\n",
        "    \"my_param\": \"initial value\"  # Can be code, prompt, config, etc.\n",
        "}\n",
        "\n",
        "# 2. Define your fitness function (how to measure success)\n",
        "def fitness_fn(candidate, example=None):\n",
        "    # Run your system with the candidate\n",
        "    output = run_my_system(candidate[\"my_param\"], example)\n",
        "    \n",
        "    # Compute score (higher is better)\n",
        "    score = compute_score(output, example)\n",
        "    \n",
        "    # Collect rich diagnostic information (ASI)\n",
        "    side_info = {\n",
        "        \"Input\": example,\n",
        "        \"Output\": output,\n",
        "        \"Expected\": example.get(\"answer\") if example else None,\n",
        "        \"Error\": get_error_message(output),\n",
        "        \"Feedback\": analyze_performance(output),\n",
        "    }\n",
        "    \n",
        "    return score, output, side_info\n",
        "\n",
        "# 3. Run optimization\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=my_examples,  # Optional: for multi-instance mode\n",
        "    objective=\"Find a parameter that maximizes performance\",  # Optional: guidance\n",
        "    config=GEPAConfig(\n",
        "        engine=EngineConfig(max_metric_calls=100),\n",
        "        reflection=ReflectionConfig(reflection_lm=\"openai/gpt-4o\"),\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 4. Use the optimized result\n",
        "print(\"Best candidate:\", result.best_candidate)\n",
        "print(\"Best score:\", result.best_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "final-takeaways",
      "metadata": {},
      "source": [
        "## Summary: What We Showed\n",
        "\n",
        "| Example | What We Optimized | Key Insight |\n",
        "|---------|-------------------|-------------|\n",
        "| **Mathematical Optimization** | Python code for black-box optimization | GEPA discovers algorithms automatically |\n",
        "| **Prompt Engineering** | System prompts for math problems | LLM reflection finds domain-specific strategies |\n",
        "| **Agent Evolution** | DSPy programs for ARC-AGI | Self-refinement emerged without being programmed |\n",
        "| **Algorithmic Discovery** | Circle packing algorithms | Matches state-of-the-art (AlphaEvolve, etc.) |\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Unified Interface**: One API for prompts, code, configs, and agent architectures\n",
        "\n",
        "2. **Side Information (ASI) is Key**: The more diagnostic information you provide, the better GEPA can reason about improvements\n",
        "\n",
        "3. **Beyond Scalar Optimization**: Traditional optimizers only see scores; GEPA sees error messages, execution traces, and domain-specific feedback\n",
        "\n",
        "4. **Emergent Capabilities**: Sophisticated strategies (like self-refinement in ARC-AGI) emerge without explicit programming\n",
        "\n",
        "5. **The Convex Hull**: `optimize_anything` is designed to cover all text-based optimization problems under one abstraction\n",
        "\n",
        "---\n",
        "\n",
        "## Try It Yourself\n",
        "\n",
        "**If you can express your system's parameters as text and compute a score with diagnostic feedback, GEPA can optimize it.**\n",
        "\n",
        "```python\n",
        "pip install gepa\n",
        "```\n",
        "\n",
        "```python\n",
        "from gepa.optimize_anything import optimize_anything\n",
        "\n",
        "result = optimize_anything(\n",
        "    seed_candidate={\"your_param\": \"your_value\"},\n",
        "    fitness_fn=your_fitness_function,\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "*GEPA is open-source. Star us on [GitHub](https://github.com/stanfordnlp/gepa)!*\n",
        "\n",
        "---\n",
        "\n",
        "## Appendix: Full Code Examples\n",
        "\n",
        "The complete, runnable code for all examples in this post can be found in the `examples/` directory:\n",
        "\n",
        "- `examples/new_polynomial/` â€” Mathematical optimization (EvalSet)\n",
        "- `examples/math/` â€” Prompt engineering (AIME 2025)\n",
        "- `examples/arc_agi/` â€” Agent program evolution (ARC-AGI)\n",
        "- `examples/circle_packing/` â€” Algorithmic discovery (Circle Packing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a689649",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Minimal Working Example: Optimize a Sorting Function\n",
        "\n",
        "Here's a complete, runnable example that optimizes a Python sorting function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2dcd15",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Minimal working example: Optimize a sorting function\n",
        "This evolves Python code that sorts a list of numbers.\n",
        "\"\"\"\n",
        "import time\n",
        "from gepa.optimize_anything import optimize_anything, GEPAConfig, EngineConfig, ReflectionConfig\n",
        "\n",
        "# 1. SEED CANDIDATE: A naive bubble sort implementation\n",
        "seed_candidate = {\n",
        "    \"code\": \"\"\"\n",
        "def sort_list(arr):\n",
        "    '''Sort a list of numbers in ascending order.'''\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "# 2. DATASET: Test cases to optimize on\n",
        "dataset = [\n",
        "    {\"input\": [64, 34, 25, 12, 22, 11, 90], \"expected\": [11, 12, 22, 25, 34, 64, 90]},\n",
        "    {\"input\": [5, 1, 4, 2, 8], \"expected\": [1, 2, 4, 5, 8]},\n",
        "    {\"input\": [3, 3, 1, 2, 1], \"expected\": [1, 1, 2, 3, 3]},\n",
        "    {\"input\": list(range(100, 0, -1)), \"expected\": list(range(1, 101))},  # Worst case\n",
        "]\n",
        "\n",
        "# 3. FITNESS FUNCTION: Measure correctness and speed\n",
        "def fitness_fn(candidate, example):\n",
        "    code = candidate[\"code\"]\n",
        "    \n",
        "    try:\n",
        "        # Execute the code\n",
        "        exec(code, globals())\n",
        "        \n",
        "        # Time the execution\n",
        "        start = time.time()\n",
        "        result = sort_list(example[\"input\"].copy())\n",
        "        elapsed = time.time() - start\n",
        "        \n",
        "        # Check correctness\n",
        "        correct = result == example[\"expected\"]\n",
        "        score = 1.0 if correct else 0.0\n",
        "        \n",
        "        # Bonus for speed (if correct)\n",
        "        if correct and elapsed < 0.001:\n",
        "            score += 0.1\n",
        "        \n",
        "        # Rich side_info for LLM reflection\n",
        "        side_info = {\n",
        "            \"Input\": example[\"input\"],\n",
        "            \"Output\": result,\n",
        "            \"Expected\": example[\"expected\"],\n",
        "            \"Correct\": correct,\n",
        "            \"Time (ms)\": elapsed * 1000,\n",
        "            \"Error\": None,\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        score = 0.0\n",
        "        side_info = {\n",
        "            \"Input\": example[\"input\"],\n",
        "            \"Error\": str(e),\n",
        "            \"Code\": code,\n",
        "        }\n",
        "    \n",
        "    return score, {\"code\": code, \"result\": result if 'result' in dir() else None}, side_info\n",
        "\n",
        "# 4. RUN OPTIMIZATION\n",
        "result = optimize_anything(\n",
        "    seed_candidate=seed_candidate,\n",
        "    fitness_fn=fitness_fn,\n",
        "    dataset=dataset,\n",
        "    objective=\"Optimize the sorting function for correctness and speed.\",\n",
        "    background=\"Consider algorithms like quicksort, mergesort, or heapsort.\",\n",
        "    config=GEPAConfig(\n",
        "        engine=EngineConfig(max_metric_calls=50),\n",
        "        reflection=ReflectionConfig(reflection_lm=\"openai/gpt-4o-mini\"),\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 5. USE THE RESULT\n",
        "print(\"=\" * 60)\n",
        "print(\"OPTIMIZED CODE:\")\n",
        "print(\"=\" * 60)\n",
        "print(result.best_candidate[\"code\"])\n",
        "print(f\"\\nBest score: {result.best_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04fa1609",
      "metadata": {},
      "source": [
        "### What This Example Demonstrates\n",
        "\n",
        "1. **Seed Candidate**: We start with a naive O(nÂ²) bubble sort\n",
        "2. **Dataset**: Four test cases including a worst-case reversed list\n",
        "3. **Fitness Function**: \n",
        "   - Returns correctness score (0 or 1)\n",
        "   - Returns **rich side_info** including input, output, timing, and errors\n",
        "4. **Optimization**: GEPA will evolve the code to find faster algorithms\n",
        "5. **Result**: Often discovers quicksort or similar O(n log n) algorithms\n",
        "\n",
        "The key is the `side_info` dictionaryâ€”it tells GEPA exactly what went wrong so it can make targeted improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b326af7",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## When to Use `optimize_anything`\n",
        "\n",
        "### Best Use Cases\n",
        "\n",
        "| Problem Type | Example | Why GEPA Excels |\n",
        "|--------------|---------|-----------------|\n",
        "| **Prompt Engineering** | System prompts, few-shot examples | LLM understands language nuances |\n",
        "| **Code Evolution** | Algorithm design, bug fixes | LLM can read and write code |\n",
        "| **Agent Architecture** | DSPy programs, reasoning pipelines | LLM can propose structural changes |\n",
        "| **Configuration Tuning** | JSON/YAML configs | LLM understands parameter relationships |\n",
        "| **Template Optimization** | Email templates, API specs | LLM understands domain context |\n",
        "\n",
        "### When Traditional Methods May Be Better\n",
        "\n",
        "| Problem Type | Better Alternative | Why |\n",
        "|--------------|-------------------|-----|\n",
        "| **Neural Network Training** | PyTorch + SGD | Gradient information is crucial |\n",
        "| **Convex Optimization** | SciPy, CVXPY | Mathematical structure exploitable |\n",
        "| **Combinatorial (small scale)** | OR-Tools, SAT solvers | Exact methods available |\n",
        "\n",
        "### The Rule of Thumb\n",
        "\n",
        "**Use `optimize_anything` when:**\n",
        "1. The artifact being optimized can be meaningfully represented as text\n",
        "2. You can provide informative feedback about why candidates fail\n",
        "3. Domain knowledge would help but isn't easily encoded as math\n",
        "4. The search space is too complex for grid/random search\n",
        "\n",
        "---\n",
        "\n",
        "*Questions? Issues? Contributions welcome at [github.com/stanfordnlp/gepa](https://github.com/stanfordnlp/gepa)*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
