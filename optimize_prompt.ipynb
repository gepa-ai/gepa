{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eefb064",
   "metadata": {},
   "source": [
    "# GEPA: Optimize Anything using ASI (additional side information)\n",
    "\n",
    "GEPA is a text evolution engine: Given a target metric, GEPA can efficiently search for the right parameters (including numerical, textual and code) to improve that metric. This way, GEPA can optimize essentially represent _anything_ that has a textual representation. In this post, we leverage this insight to present GEPA's optimize-anything API, which leverages the reflective capabilities of LLMs, to optimize anything representable as text. Crucially, GEPA can leverage any additional information available from the optimization environment simply by serializing into text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eaa077",
   "metadata": {},
   "source": [
    "## Prompt optimization on AIME 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4319f39d",
   "metadata": {},
   "source": [
    "\n",
    "### Set up a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ebc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45 training examples\n",
      "Loaded 45 validation examples\n",
      "Loaded 30 test examples\n"
     ]
    }
   ],
   "source": [
    "from examples.math.dataset import load_math_dataset\n",
    "\n",
    "trainset, valset, testset = load_math_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a1a41",
   "metadata": {},
   "source": [
    "### Seed candidate\n",
    "\n",
    "We will use the dspy Signature to easily feed a problem and parse the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25f3e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "\n",
    "# Define the language model\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "lm = dspy.LM(\"gpt-4.1-mini\", api_key=api_key, temperature=1.0, max_tokens=32000)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Let's optimize the prompt of the following dspy reasoning module.\n",
    "class MathSolverSignature(dspy.Signature):\n",
    "    input = dspy.InputField(desc=\"The math problem to solve.\")\n",
    "    answer = dspy.OutputField(desc=\"The final numerical answer.\")\n",
    "\n",
    "predictor = dspy.ChainOfThought(MathSolverSignature)\n",
    "\n",
    "# This is the initial prompt that we will optimize.\n",
    "SEED_PROMPT = \"\"\"Solve the math problem carefully. Break down the steps and provide the final answer as a single number.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c0f193",
   "metadata": {},
   "source": [
    "### Fitness function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed00708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Sequence\n",
    "from examples.math.main import math_metric\n",
    "\n",
    "def fitness_fn(candidate: dict[str, str], batch: Sequence[Any]) -> list[tuple[float, Any, dict]]:\n",
    "    predictor.predict.signature.instructions = candidate[\"prompt\"]\n",
    "\n",
    "    evaluator = dspy.Evaluate(\n",
    "        devset=list(batch),\n",
    "        metric=math_metric,\n",
    "        num_threads=16,\n",
    "        display_progress=True,\n",
    "    )\n",
    "    eval_result = evaluator(predictor)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for example, prediction, metric_result in eval_result.results:\n",
    "        score = metric_result.score\n",
    "        feedback = metric_result.feedback\n",
    "\n",
    "        artifact = {\n",
    "            \"prompt\": candidate[\"prompt\"],\n",
    "            \"answer\": prediction.answer,\n",
    "        }\n",
    "\n",
    "        side_info = {\n",
    "            \"Input\": example.input,\n",
    "            \"Output\": prediction.answer,\n",
    "            \"Reasoning\": prediction.reasoning,\n",
    "            \"ExecutionFeedback\": feedback,\n",
    "        }\n",
    "\n",
    "        results.append((score, artifact, side_info))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d2df4",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18641a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.00 / 45 (46.7%): 100%|██████████| 45/45 [00:00<00:00, 5713.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/01 21:22:28 INFO dspy.evaluate.evaluate: Average Metric: 21.0 / 45 (46.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 0: Base program full valset score: 0.4666666666666667 over 45 / 45 examples\n"
     ]
    }
   ],
   "source": [
    "from gepa.optimize_anything import (\n",
    "    EngineConfig,\n",
    "    GEPAConfig,\n",
    "    ReflectionConfig,\n",
    "    optimize_anything,\n",
    ")\n",
    "\n",
    "gepa_config = GEPAConfig(\n",
    "    engine=EngineConfig(\n",
    "        max_metric_calls=800,\n",
    "        track_best_outputs=True,\n",
    "    ),\n",
    "    reflection=ReflectionConfig(\n",
    "        reflection_minibatch_size=3,\n",
    "        skip_perfect_score=False,\n",
    "        reflection_lm=\"openai/gpt-5\",\n",
    "    )\n",
    ")\n",
    "\n",
    "result = optimize_anything(\n",
    "    seed_candidate={\"prompt\": SEED_PROMPT},\n",
    "    fitness_fn=fitness_fn,\n",
    "    dataset=trainset,\n",
    "    valset=valset,\n",
    "    config=gepa_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c83e4",
   "metadata": {},
   "source": [
    "### Check the improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c3ddcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Baseline (Initial Prompt)...\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.00 / 30 (50.0%): 100%|██████████| 30/30 [00:00<00:00, 637.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/01 16:51:24 INFO dspy.evaluate.evaluate: Average Metric: 15.0 / 30 (50.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluating Best Optimized Program...\n",
      "Best Prompt Found:\n",
      "Solve the math problem carefully. Break down the steps and provide the final answer as a single number.\n",
      "Average Metric: 15.00 / 30 (50.0%): 100%|██████████| 30/30 [00:00<00:00, 5119.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/01 16:51:24 INFO dspy.evaluate.evaluate: Average Metric: 15.0 / 30 (50.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Score: 50.00%\n",
      "Optimized Score: 50.00%\n",
      "Improvement: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from examples.math.main import evaluate_on_dataset\n",
    "\n",
    "# Baseline Evaluation\n",
    "print(\"\\nEvaluating Baseline (Initial Prompt)...\")\n",
    "predictor.predict.signature.instructions = SEED_PROMPT\n",
    "baseline_score = evaluate_on_dataset(predictor, testset)\n",
    "\n",
    "# Optimized Evaluation\n",
    "print(\"\\nEvaluating Best Optimized Program...\")\n",
    "best_prompt = result.best_candidate[\"prompt\"]\n",
    "print(f\"Best Prompt Found:\\n{best_prompt}\")\n",
    "\n",
    "predictor.predict.signature.instructions = best_prompt\n",
    "optimized_score = evaluate_on_dataset(predictor, testset)\n",
    "\n",
    "print(f\"Baseline Score: {baseline_score:.2%}\")\n",
    "print(f\"Optimized Score: {optimized_score:.2%}\")\n",
    "print(f\"Improvement: {optimized_score - baseline_score:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
