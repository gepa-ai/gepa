{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eefb064",
   "metadata": {},
   "source": [
    "# GEPA: Optimize Anything using ASI (additional side information)\n",
    "\n",
    "GEPA is a text evolution engine: Given a target metric, GEPA can efficiently search for the right parameters (including numerical, textual and code) to improve that metric. This way, GEPA can optimize essentially represent _anything_ that has a textual representation. In this post, we leverage this insight to present GEPA's optimize-anything API, which leverages the reflective capabilities of LLMs, to optimize anything representable as text. Crucially, GEPA can leverage any additional information available from the optimization environment simply by serializing into text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352303a5",
   "metadata": {},
   "source": [
    "Agent Optimization — Evolving DSPy Programs for ARC-AGI\n",
    "\n",
    "Our second example pushes GEPA further: optimizing not just prompts or hyperparameters, but the *entire structure* of an AI agent. We'll evolve a DSPy program to solve ARC-AGI tasks — a challenging benchmark requiring visual reasoning and pattern recognition.\n",
    "\n",
    "**The task**: Given input-output matrix pairs as training examples, produce the correct output for test inputs.\n",
    "\n",
    "**What GEPA optimizes**: The entire DSPy program source code — signatures, modules, control flow, and prompting strategies.\n",
    "\n",
    "**Result**: GEPA improves Gemini-2.5-Pro's performance from **44% to 49.5%** by discovering an elaborate 5-step reasoning pipeline with self-refinement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58492d",
   "metadata": {},
   "source": [
    "### Setting up the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e00eba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 200\n",
      "Val set: 200\n",
      "Test set: 400\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import dspy\n",
    "\n",
    "ds = load_dataset(\"dataartist/arc-agi\")\n",
    "\n",
    "def format_dataset(data):\n",
    "    return [\n",
    "        dspy.Example(\n",
    "            training_examples=ex[\"train\"],\n",
    "            test_inputs=[x[\"input\"] for x in ex[\"test\"]],\n",
    "            test_outputs=[x[\"output\"] for x in ex[\"test\"]],\n",
    "        ).with_inputs(\"training_examples\", \"test_inputs\")\n",
    "        for ex in data\n",
    "    ]\n",
    "\n",
    "full_train = format_dataset(ds[\"training\"])\n",
    "test_set = format_dataset(ds[\"evaluation\"])\n",
    "\n",
    "random.Random(0).shuffle(full_train)\n",
    "split_idx = len(full_train) // 2\n",
    "train_set, val_set = full_train[:split_idx], full_train[split_idx:]\n",
    "\n",
    "print(f\"Train set: {len(train_set)}\")\n",
    "print(f\"Val set: {len(val_set)}\")\n",
    "print(f\"Test set: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56104b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set[:20]\n",
    "val_set = val_set[:20]\n",
    "test_set = test_set[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e1e27",
   "metadata": {},
   "source": [
    "### The seed candidate\n",
    "\n",
    "We start with a simple Chain-of-Thought program — just a single DSPy module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77f3c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_candidate = \"\"\"import dspy\n",
    "from typing import List\n",
    "import pydantic\n",
    "\n",
    "MATRIX = List[List[int]]\n",
    "\n",
    "class TrainingExample(pydantic.BaseModel):\n",
    "    input: MATRIX\n",
    "    output: MATRIX\n",
    "\n",
    "class SolveTaskSignature(dspy.Signature):\n",
    "    training_examples: List[TrainingExample] = dspy.InputField(description=\"Input and output examples demonstrating the task to be performed.\")\n",
    "    test_inputs: List[MATRIX] = dspy.InputField(description=\"Input matrices to be solved following the task described in the training examples.\")\n",
    "    test_outputs: List[MATRIX] = dspy.OutputField(description=\"Output matrices corresponding to the test inputs.\")\n",
    "\n",
    "program = dspy.ChainOfThought(SolveTaskSignature)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e441e",
   "metadata": {},
   "source": [
    "### Dspy adaptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "001eb5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from gepa.adapters.dspy_full_program_adapter.full_program_adapter import DspyAdapter\n",
    "from examples.arc_agi.main import metric_fn\n",
    "\n",
    "# Create LMs\n",
    "task_lm = dspy.LM(\n",
    "    model=\"openai/gpt-5\",\n",
    "    temperature=1.0,\n",
    "    max_tokens=32000,\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Create adapter\n",
    "adapter = DspyAdapter(\n",
    "    task_lm=task_lm,\n",
    "    metric_fn=metric_fn,\n",
    "    num_threads=64,\n",
    "    reflection_lm=\"openai/gpt-5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf676af",
   "metadata": {},
   "source": [
    "### Fitness function\n",
    "\n",
    "The fitness function compiles and runs the DSPy program, comparing outputs against ground truth. Crucially, it provides detailed feedback about *what went wrong*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9cbac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_error_results(program, batch, error_msg):\n",
    "    \"\"\"Create error results for all examples in batch.\"\"\"\n",
    "    results = []\n",
    "    for example in batch:\n",
    "        log = {\"error\": error_msg, \"program\": program}\n",
    "        side_info = {\n",
    "            \"input\": example,\n",
    "            \"error\": error_msg,\n",
    "        }\n",
    "        results.append((0.0, log, side_info))\n",
    "    return results\n",
    "\n",
    "def format_results(program, trajectories):\n",
    "    results = [] \n",
    "    for traj in trajectories:\n",
    "        \"\"\"Create a single result item from trajectory.\"\"\"\n",
    "        metric_result = traj.get(\"score\")\n",
    "        score = metric_result.get(\"score\")\n",
    "        feedback = metric_result.get(\"feedback\")\n",
    "        prediction = traj.get(\"prediction\")\n",
    "        model_answer = prediction.get(\"test_outputs\")\n",
    "\n",
    "        log = {\n",
    "            \"program\": program,\n",
    "            \"model_answer\": model_answer,\n",
    "            \"score\": score,\n",
    "        }\n",
    "\n",
    "        side_info = {\n",
    "            \"input\": traj.get(\"example\"),\n",
    "            \"reasoning\": prediction.get(\"reasoning\"),\n",
    "            \"feedback\": feedback,\n",
    "            \"output\": model_answer,\n",
    "        }\n",
    "\n",
    "        results.append((score, log, side_info))\n",
    "\n",
    "    return results\n",
    "\n",
    "def fitness_fn(candidate, batch):\n",
    "    \"\"\"Evaluate candidate program on batch and return results.\"\"\"\n",
    "    program = candidate[\"program\"]\n",
    "\n",
    "    try:\n",
    "        eval_batch = adapter.evaluate(batch, candidate, capture_traces=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating candidate: {e}\")\n",
    "        return format_error_results(program, batch, str(e))\n",
    "\n",
    "    # Program error\n",
    "    if not isinstance(eval_batch.trajectories, list):\n",
    "        error_msg = f\"All examples failed. Program error: {str(eval_batch.trajectories)}\"\n",
    "        return format_error_results(program, batch, error_msg)\n",
    "\n",
    "    # Process evaluations with no errors\n",
    "    return format_results(program, eval_batch.trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec5232",
   "metadata": {},
   "source": [
    "### Running GEPA optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b5fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.00 / 8 (100.0%):  80%|████████  | 8/10 [01:22<00:18,  9.14s/it] "
     ]
    }
   ],
   "source": [
    "from gepa.optimize_anything import (\n",
    "    EngineConfig,\n",
    "    GEPAConfig,\n",
    "    ReflectionConfig,\n",
    "    optimize_anything,\n",
    ")\n",
    "\n",
    "gepa_config = GEPAConfig(\n",
    "    engine=EngineConfig(\n",
    "        max_metric_calls=100,\n",
    "        track_best_outputs=True,\n",
    "    ),\n",
    "    reflection=ReflectionConfig(\n",
    "        reflection_minibatch_size=3,\n",
    "        skip_perfect_score=False,\n",
    "        reflection_lm=\"openai/gpt-5\",\n",
    "    )\n",
    ")\n",
    "\n",
    "result = optimize_anything(\n",
    "    seed_candidate={\"program\": seed_candidate},\n",
    "    fitness_fn=fitness_fn,\n",
    "    dataset=train_set,\n",
    "    valset=val_set,\n",
    "    config=gepa_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31781e",
   "metadata": {},
   "source": [
    "### What GEPA discovered\n",
    "\n",
    "After optimization, GEPA evolved the simple ChainOfThought into an elaborate 5-step pipeline:\n",
    "\n",
    "1. **Hypothesize Rule**: Ask LLM to deduce a natural language transformation rule from training examples\n",
    "2. **Generate Code**: Ask LLM to implement the rule as a Python function\n",
    "3. **Validate on Training**: Run the code on all training examples, collecting feedback on failures\n",
    "4. **Refine if Needed**: If validation fails, ask LLM to fix the code using gathered feedback\n",
    "5. **Execute on Test**: Run the refined code on test inputs\n",
    "\n",
    "Remarkably, **GEPA discovered reflective self-refinement** — having the LLM check and fix its own code before producing final outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de947640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the evolved program\n",
    "print(result.best_candidate[\"program\"][:2000])  # First 2000 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238b846",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "The `optimize_anything` API demonstrates GEPA's power as a general-purpose text evolution engine:\n",
    "\n",
    "1. **Unified interface**: Whether you're optimizing prompts, code, or agent architectures, the API is the same — just define your fitness function with rich `side_info`.\n",
    "\n",
    "2. **Side information is key**: The more diagnostic information you provide, the better GEPA's LLM-based reflection can understand failures and propose targeted improvements.\n",
    "\n",
    "3. **Beyond scalar optimization**: Traditional optimizers only see scores. GEPA sees error messages, execution traces, and domain-specific feedback — enabling it to optimize complex artifacts that would be impossible to search blindly.\n",
    "\n",
    "4. **Emergent capabilities**: GEPA can discover sophisticated strategies (like self-refinement in the ARC-AGI example) that weren't explicitly programmed — they emerge from the optimization process itself.\n",
    "\n",
    "Try `optimize_anything` on your own optimization problems. If you can express your system's parameters as text and compute a score with diagnostic feedback, GEPA can optimize it.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
