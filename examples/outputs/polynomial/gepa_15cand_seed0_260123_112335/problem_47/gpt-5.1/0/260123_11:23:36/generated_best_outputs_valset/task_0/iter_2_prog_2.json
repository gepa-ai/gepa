{
    "score": 2.9528045053022576e-07,
    "Input": "Sargan",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budget-aware blackbox minimizer using:\n    - Global quasi-random sampling (scrambled progression)\n    - Optional Sobol-like low-discrepancy pattern via permutations\n    - Local adaptive hill-climbing with step-size control\n    - Optional warm-start from prev_best_x\n    \"\"\"\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    # Degenerate cases: return center of bounds\n    if dim <= 0 or budget <= 0:\n        return np.mean(bounds, axis=1)\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Clamp helper\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # ---------- Warm start ----------\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape == (dim,):\n            x0 = clip_to_bounds(x0)\n            y0 = objective_function(x0)\n            evals_used += 1\n            best_x, best_y = x0, y0\n\n    # If no valid warm start, sample one random point\n    if best_x is None and evals_used < budget:\n        x0 = np.random.uniform(low, high)\n        y0 = objective_function(x0)\n        evals_used += 1\n        best_x, best_y = x0, y0\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n\n    # Adaptive allocation: more global search for small budgets\n    if remaining < 20:\n        global_evals = remaining\n        local_evals = 0\n    else:\n        global_evals = max(5, int(0.65 * remaining))\n        local_evals = remaining - global_evals\n\n    # ---------- Global Search ----------\n    # Use simple low-discrepancy-like progression with per-dimension shifts\n    # to encourage coverage without clustering\n    dim_shifts = np.random.rand(dim)\n    # Random per-dimension permutations of indices to avoid alignment\n    if global_evals > 0:\n        perms = [np.random.permutation(global_evals) for _ in range(dim)]\n    else:\n        perms = []\n\n    for i in range(global_evals):\n        # base scalar progression\n        t = (i + 0.5) / global_evals\n        # construct quasi-random vector\n        u = np.empty(dim)\n        for d in range(dim):\n            # mix scalar progression, random shift, and permuted index\n            idx = perms[d][i]\n            u[d] = np.mod(t + dim_shifts[d] + (idx + 0.5) / global_evals, 1.0)\n        x = low + span * u\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x\n\n        if evals_used >= budget:\n            return best_x\n\n    if local_evals <= 0:\n        return best_x\n\n    # ---------- Local Search (adaptive hill-climbing) ----------\n    x_center = best_x.copy()\n    # Initial step ~ 15% of span (slightly smaller than previous 20%)\n    base_step = 0.15 * span\n    base_step[base_step == 0.0] = 1.0  # handle zero-width dims\n\n    # We adapt step size based on success rate\n    step = base_step.copy()\n    success_count = 0\n    window = max(5, local_evals // 10)\n\n    for k in range(local_evals):\n        # Occasional larger exploratory jump to avoid premature stagnation\n        if k > 0 and k % max(10, local_evals // 5) == 0:\n            jump_scale = 2.0\n        else:\n            jump_scale = 1.0\n\n        perturb = np.random.normal(loc=0.0, scale=0.5, size=dim) * step * jump_scale\n        x_candidate = clip_to_bounds(x_center + perturb)\n        y_candidate = objective_function(x_candidate)\n        evals_used += 1\n\n        if y_candidate < best_y:\n            best_y = y_candidate\n            best_x = x_candidate\n            x_center = x_candidate\n            success_count += 1\n\n        # Adapt step every \"window\" evaluations based on recent success rate\n        if (k + 1) % window == 0:\n            success_rate = success_count / float(window)\n            # If many successes, increase step modestly\n            if success_rate > 0.3:\n                step *= 1.2\n            # If very few successes, reduce step\n            elif success_rate < 0.1:\n                step *= 0.5\n            # Clamp step to reasonable bounds: not too small vs domain\n            step = np.clip(step, 1e-8 * (span + 1.0), 0.5 * (span + 1.0))\n            success_count = 0\n\n        if evals_used >= budget:\n            break\n\n    return best_x",
    "X": "0.00022270181324960796 0.00023646420748270797"
}