{
    "score": 0.015828767422237426,
    "Input": "Sargan",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Simple budget-aware blackbox minimizer using:\n    - Sobol-like low-discrepancy random sampling for global search\n    - Local random search around best found point for exploitation\n    - Optional warm-start from prev_best_x\n    \"\"\"\n    bounds = np.array(config['bounds'], dtype=float)\n    dim = int(config['dim'])\n    budget = int(config['budget'])\n\n    if dim <= 0 or budget <= 0:\n        # Degenerate cases: return center of bounds\n        return np.mean(bounds, axis=1)\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Clamp helper\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    # Initialize best solution\n    best_x = None\n    best_y = np.inf\n\n    # Warm-start if available and within bounds\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape == (dim,):\n            x0 = clip_to_bounds(x0)\n            y0 = objective_function(x0)\n            evals_used += 1\n            best_x, best_y = x0, y0\n\n    # If no valid warm start, sample one random point\n    if best_x is None and evals_used < budget:\n        x0 = np.random.uniform(low, high)\n        y0 = objective_function(x0)\n        evals_used += 1\n        best_x, best_y = x0, y0\n\n    # If budget is exhausted by warm start(s)\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n\n    # Allocate roughly half for global search, half for local refinement\n    global_evals = max(1, int(0.6 * remaining))\n    local_evals = remaining - global_evals\n\n    # ---------- Global Search (quasi-random sampling) ----------\n    # Use simple scrambled uniform sampling\n    for i in range(global_evals):\n        # Try to spread samples using a simple progression\n        t = (i + 0.5) / global_evals\n        # Generate a base random vector and blend with t to reduce clustering\n        r = np.random.rand(dim)\n        x = low + span * np.mod(r + t, 1.0)\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x\n\n    # If no room for local search, return best found\n    if evals_used >= budget or local_evals <= 0:\n        return best_x\n\n    # ---------- Local Search (stochastic hill-climbing) ----------\n    x_center = best_x.copy()\n    # Initial step size as fraction of domain width\n    base_step = 0.2 * span\n    base_step[base_step == 0.0] = 1.0  # handle zero-width dims\n\n    # Geometric decay of step size over local iterations\n    for k in range(local_evals):\n        decay = 0.9 ** (k / max(1, local_evals - 1))\n        step = base_step * decay\n\n        # Propose a local perturbation (Gaussian)\n        perturb = np.random.normal(loc=0.0, scale=0.5, size=dim) * step\n        x_candidate = clip_to_bounds(x_center + perturb)\n\n        y_candidate = objective_function(x_candidate)\n        evals_used += 1\n\n        if y_candidate < best_y:\n            best_y = y_candidate\n            best_x = x_candidate\n            x_center = x_candidate  # move center to improved point\n\n        if evals_used >= budget:\n            break\n\n    return best_x",
    "X": "0.09587956743599015 -0.05222103357319417"
}