{
    "score": -4.999999999998858,
    "Input": "McCourt14",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Derivative-free optimizer combining:\n    - Space-filling global search (Sobol-like LHS)\n    - Warm start from prev_best_x if provided\n    - Multi-scale local search with adaptive step size\n\n    The algorithm:\n    1. Use ~40% of the budget for global exploration.\n    2. Use remaining evaluations for increasingly local refinement.\n    3. Always honor the evaluation budget and return the best point seen.\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n\n    # Handle degenerate or trivial budget\n    if budget <= 0:\n        return np.clip(lower + rng.rand(dim) * span, lower, upper)\n\n    def project(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n\n    # Initialize incumbent solution: prefer warm start if available\n    if prev_best_x is not None:\n        try:\n            x_best = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            x_best = project(x_best)\n        except Exception:\n            x_best = lower + rng.rand(dim) * span\n    else:\n        x_best = lower + rng.rand(dim) * span\n\n    y_best = objective_function(x_best)\n    evals_used += 1\n\n    if budget == 1:\n        return x_best\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # --- Global search: Latin-hypercube-like sampling ---\n    # Use a bit less than half the budget globally to keep room for local search\n    global_budget = max(1, int(0.4 * remaining))\n    global_budget = min(global_budget, budget - evals_used)\n    if global_budget > 0:\n        # Stratified sampling per dimension\n        # For each dimension, sample one point in each global_budget stratum then shuffle\n        u = (np.arange(global_budget) + rng.rand(global_budget)) / global_budget\n        global_points = np.empty((global_budget, dim), dtype=float)\n        for d in range(dim):\n            rng.shuffle(u)\n            global_points[:, d] = lower[d] + u * span[d]\n\n        for i in range(global_budget):\n            if evals_used >= budget:\n                break\n            x = global_points[i]\n            y = objective_function(x)\n            evals_used += 1\n            if y < y_best:\n                x_best, y_best = x, y\n\n    # Update remaining budget for local search\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # --- Multi-scale local search (Gaussian mutations) ---\n    # Initial step ~ 15% of box; then we progressively shrink the scale.\n    base_scale = 0.15\n    step_scale = base_scale * np.where(span > 0, span, 1.0)\n\n    # Minimal step to avoid total stagnation; scale with box size if nonzero\n    min_scale = 1e-6 * np.where(span > 0, span, 1.0)\n\n    # Number of offspring per iteration; adjust with dimension and budget\n    offspring_per_iter = max(3, min(10, remaining // 3))\n    if offspring_per_iter <= 0:\n        return x_best\n\n    max_iters = max(1, remaining // offspring_per_iter)\n\n    for it in range(max_iters):\n        if evals_used >= budget:\n            break\n\n        improved = False\n        # Mildly decrease scale over time to encourage convergence\n        decay_factor = 0.99\n        step_scale = np.maximum(min_scale, step_scale * decay_factor)\n\n        for _ in range(offspring_per_iter):\n            if evals_used >= budget:\n                break\n\n            noise = rng.randn(dim) * step_scale\n            x_new = project(x_best + noise)\n            y_new = objective_function(x_new)\n            evals_used += 1\n\n            if y_new < y_best:\n                x_best, y_best = x_new, y_new\n                improved = True\n\n        # Simple 1D step-size control: enlarge on improvement, shrink on failure\n        if improved:\n            step_scale = np.maximum(min_scale, step_scale * 1.2)\n        else:\n            step_scale = np.maximum(min_scale, step_scale * 0.5)\n\n    return x_best",
    "X": "0.10000011928258017 0.7999999528660137 0.29999982907259376"
}