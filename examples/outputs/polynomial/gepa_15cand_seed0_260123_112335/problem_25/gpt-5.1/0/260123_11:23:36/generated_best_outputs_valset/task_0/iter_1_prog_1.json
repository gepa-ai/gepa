{
    "score": -4.999999999997624,
    "Input": "McCourt14",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Basic derivative-free optimizer combining global random search\n    with a local refinement around the incumbent best point.\n\n    Strategy:\n    1. Use a fraction of the budget for global random search in the box.\n    2. Use the remaining evaluations for local mutations around current best.\n    3. Warm start from prev_best_x if provided and inside bounds.\n\n    This is intentionally simple and robust for arbitrary blackbox objectives.\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n\n    # Handle degenerate budget\n    if budget <= 0:\n        # Just return a random feasible point\n        return np.clip(lower + rng.rand(dim) * span, lower, upper)\n\n    # Helper to project into bounds\n    def project(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n\n    # Initialize incumbent solution\n    if prev_best_x is not None:\n        x_best = np.asarray(prev_best_x, dtype=float).reshape(dim)\n        x_best = project(x_best)\n    else:\n        x_best = lower + rng.rand(dim) * span\n\n    y_best = objective_function(x_best)\n    evals_used += 1\n\n    # If only one eval allowed, return the (warm-start) point\n    if budget == 1:\n        return x_best\n\n    # Split budget: portion for global search, rest for local refinement\n    # Ensure at least a few global samples and a few local steps when possible\n    global_frac = 0.5\n    global_budget = max(1, int(global_frac * (budget - evals_used)))\n    local_budget = max(0, budget - evals_used - global_budget)\n\n    # --- Global random search ---\n    for _ in range(global_budget):\n        x = lower + rng.rand(dim) * span\n        y = objective_function(x)\n        evals_used += 1\n        if y < y_best:\n            x_best, y_best = x, y\n\n    # Recompute remaining budget for local search\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # --- Local search: adaptive Gaussian mutations around best ---\n    # Start with a step size proportional to box size\n    base_scale = 0.2\n    step_scale = base_scale * span\n\n    # Minimal step relative to span to avoid stagnation in tiny boxes\n    min_scale = 1e-6 * np.maximum(span, 1.0)\n\n    # Use a simple (1+lambda) ES style: several offspring per iteration\n    offspring_per_iter = 4\n    # Number of iterations bounded by remaining budget\n    max_iters = max(1, remaining // offspring_per_iter)\n\n    for _ in range(max_iters):\n        improved = False\n        for _ in range(offspring_per_iter):\n            if evals_used >= budget:\n                break\n\n            noise = rng.randn(dim) * step_scale\n            x_new = project(x_best + noise)\n            y_new = objective_function(x_new)\n            evals_used += 1\n\n            if y_new < y_best:\n                x_best, y_best = x_new, y_new\n                improved = True\n\n        # Simple step-size adaptation: shrink if no improvement, grow slightly otherwise\n        if improved:\n            step_scale = np.maximum(min_scale, step_scale * 1.05)\n        else:\n            step_scale = np.maximum(min_scale, step_scale * 0.5)\n\n        if evals_used >= budget:\n            break\n\n    return x_best",
    "X": "0.09999979087710749 0.7999998686703759 0.30000018452725"
}