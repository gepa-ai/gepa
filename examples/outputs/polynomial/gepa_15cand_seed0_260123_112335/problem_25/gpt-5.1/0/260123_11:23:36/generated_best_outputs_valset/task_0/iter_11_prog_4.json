{
    "score": -5.0,
    "Input": "McCourt14",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Derivative-free optimizer combining:\n    - Space-filling global search (LHS-style)\n    - Warm start from prev_best_x if provided\n    - Three-phase search: global exploration + basin-hopping + local refinement\n    - Adaptive step size and covariance for local search\n\n    Respects the evaluation budget and returns the best point seen.\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n    eff_span = np.where(span > 0, span, 1.0)\n\n    def project(x):\n        return np.clip(x, lower, upper)\n\n    # Degenerate budget: return any valid point\n    if budget <= 0:\n        return project(lower + rng.rand(dim) * eff_span)\n\n    evals_used = 0\n\n    # --- Initialization with warm start and random backup ---\n    if prev_best_x is not None:\n        try:\n            x_best = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            x_best = project(x_best)\n        except Exception:\n            x_best = lower + rng.rand(dim) * eff_span\n    else:\n        x_best = lower + rng.rand(dim) * eff_span\n\n    y_best = objective_function(x_best)\n    evals_used += 1\n    if evals_used >= budget:\n        return x_best\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # --- Global exploration (LHS-like) ---\n    # Allocate ~40% of remaining to global sampling\n    global_budget = int(0.4 * remaining)\n    global_budget = max(1, min(global_budget, budget - evals_used))\n\n    if global_budget > 0:\n        u_base = (np.arange(global_budget) + rng.rand(global_budget)) / global_budget\n        global_points = np.empty((global_budget, dim), dtype=float)\n\n        for d in range(dim):\n            if span[d] <= 0:\n                # Fixed coordinate\n                global_points[:, d] = lower[d]\n            else:\n                rng.shuffle(u_base)\n                global_points[:, d] = lower[d] + u_base * span[d]\n\n        for i in range(global_budget):\n            if evals_used >= budget:\n                break\n            x = global_points[i]\n            y = objective_function(x)\n            evals_used += 1\n            if y < y_best:\n                x_best, y_best = x, y\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # --- Basin-hopping style medium-scale search ---\n    # Coarse perturbations to escape local minima before fine local search\n    basin_budget = int(0.2 * remaining)\n    basin_budget = max(0, min(basin_budget, budget - evals_used))\n\n    if basin_budget > 0:\n        # Step size: relatively large fraction of box\n        basin_sigma = (0.3 / np.sqrt(max(1, dim))) * eff_span\n        basin_sigma = np.maximum(basin_sigma, 1e-6 * eff_span)\n\n        for _ in range(basin_budget):\n            if evals_used >= budget:\n                break\n            step = rng.randn(dim) * basin_sigma\n            x_new = project(x_best + step)\n            y_new = objective_function(x_new)\n            evals_used += 1\n            if y_new < y_best:\n                x_best, y_best = x_new, y_new\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # --- Local search: adaptive Gaussian with covariance-like adaptation ---\n    # Initial step: moderate fraction of box; smaller in high dim\n    base_scale = 0.15 / np.sqrt(max(1, dim))\n    sigma = (base_scale * eff_span).astype(float)\n\n    # Bounds on step sizes\n    min_sigma = 1e-8 * eff_span\n    max_sigma = eff_span\n\n    # Offspring per iteration\n    # More conservative to avoid overspending budget in high dim\n    max_offspring = max(4, remaining // 3)\n    offspring_per_iter = 4 + dim // 4\n    offspring_per_iter = int(np.clip(offspring_per_iter, 3, min(20, max_offspring)))\n    if offspring_per_iter <= 0:\n        return x_best\n\n    max_iters = max(1, remaining // offspring_per_iter)\n\n    # Per-dimension success statistics\n    success_history = np.zeros(dim, dtype=float) + 1e-8\n    attempt_history = np.zeros(dim, dtype=float) + 1e-8\n\n    no_improve_iters = 0\n\n    for _ in range(max_iters):\n        if evals_used >= budget:\n            break\n\n        improved_any = False\n\n        for _ in range(offspring_per_iter):\n            if evals_used >= budget:\n                break\n\n            z = rng.randn(dim)\n            step = z * sigma\n            x_new = project(x_best + step)\n            y_new = objective_function(x_new)\n            evals_used += 1\n\n            mask = np.abs(step) > 0\n            attempt_history[mask] += 1.0\n\n            if y_new < y_best:\n                improved_any = True\n                success_history[mask] += 1.0\n                x_best, y_best = x_new, y_new\n\n        # Adapt step sizes based on success rate per dimension (target ~0.2)\n        if np.any(attempt_history > 0):\n            success_rate = success_history / attempt_history\n            factor = np.ones(dim, dtype=float)\n            factor[success_rate > 0.25] = 1.25\n            factor[success_rate < 0.15] = 0.75\n            sigma *= factor\n            sigma = np.clip(sigma, min_sigma, max_sigma)\n            # Forget past to stay adaptive\n            success_history *= 0.6\n            attempt_history *= 0.6\n\n        if improved_any:\n            no_improve_iters = 0\n        else:\n            no_improve_iters += 1\n            # If stagnating, shrink globally and occasionally try a larger jump\n            sigma *= 0.5\n            sigma = np.clip(sigma, min_sigma, max_sigma)\n            if no_improve_iters % 5 == 0:\n                # Occasional exploratory jump from current best\n                if evals_used < budget:\n                    jump_sigma = np.minimum(0.5 * eff_span, np.maximum(5 * sigma, 1e-4 * eff_span))\n                    step = rng.randn(dim) * jump_sigma\n                    x_new = project(x_best + step)\n                    y_new = objective_function(x_new)\n                    evals_used += 1\n                    if y_new < y_best:\n                        x_best, y_best = x_new, y_new\n                        no_improve_iters = 0\n\n    return x_best",
    "X": "0.09999999951845737 0.8000000004091748 0.30000000299012214"
}