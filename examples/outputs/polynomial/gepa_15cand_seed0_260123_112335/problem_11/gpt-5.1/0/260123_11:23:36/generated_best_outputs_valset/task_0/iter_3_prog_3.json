{
    "score": -3.8627553778379737,
    "Input": "Hartmann3",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budgeted evaluations.\n\n    Strategy (hybrid global + local search):\n    - Respect config['budget'] function evaluations.\n    - Warm-start from prev_best_x if available (ensuring it is evaluated exactly once).\n    - Global exploration with LHS-style stratified sampling plus random jitter.\n    - Maintain an elite set of best points.\n    - Local derivative-free search (adaptive coordinate pattern search)\n      starting from the current best and from elites.\n    \"\"\"\n\n    rng = np.random.default_rng()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    if dim <= 0 or budget <= 0:\n        return np.zeros(dim, dtype=float)\n\n    # Helper: project into bounds\n    def project(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # Maintain an archive of elite points for potential local restarts\n    elite_size = min(8, max(2, budget // max(8, 2 * dim)))\n    elite_x = []\n    elite_y = []\n\n    def update_elites(x, y):\n        nonlocal elite_x, elite_y\n        if not np.isfinite(y):\n            return\n        if len(elite_x) < elite_size:\n            elite_x.append(x.copy())\n            elite_y.append(float(y))\n        else:\n            worst_idx = int(np.argmax(elite_y))\n            if y < elite_y[worst_idx]:\n                elite_x[worst_idx] = x.copy()\n                elite_y[worst_idx] = float(y)\n\n    # Evaluate a candidate if budget allows\n    def eval_candidate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        y = objective_function(x)\n        evals_used += 1\n        if np.isfinite(y) and y < best_y:\n            best_y = float(y)\n            best_x = x.copy()\n        update_elites(x, y)\n        return y\n\n    # 1) Warm start from prev_best_x if provided and valid\n    if prev_best_x is not None and evals_used < budget:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            x0 = project(x0)\n            eval_candidate(x0)\n        except Exception:\n            pass\n\n    if evals_used >= budget:\n        if best_x is not None:\n            return best_x\n        # Fallback: center of box\n        return project(low + 0.5 * span)\n\n    # 2) Global exploration with stratified sampling\n    remaining_budget = budget - evals_used\n    if remaining_budget <= 0:\n        return best_x if best_x is not None else project(low + 0.5 * span)\n\n    global_frac = 0.6\n    # Allocate global evaluations based on total budget, but cap to remaining\n    n_global_target = int(global_frac * budget)\n    n_global = max(dim * 5, n_global_target)  # a bit denser than before\n    n_global = min(n_global, remaining_budget)\n\n    if n_global > 0:\n        # Latin-hypercube-like stratified sampling\n        strata = (np.arange(n_global) + rng.random(n_global)) / n_global\n        xs_unit = np.zeros((n_global, dim))\n        for d in range(dim):\n            perm = rng.permutation(n_global)\n            xs_unit[:, d] = strata[perm]\n\n        xs = low + xs_unit * span\n\n        # Add modest jitter while keeping inside bounds\n        noise_scale = 0.05\n        if np.any(span > 0):\n            xs += rng.normal(0.0, noise_scale, size=(n_global, dim)) * span\n\n        for i in range(n_global):\n            if evals_used >= budget:\n                break\n            x = project(xs[i])\n            eval_candidate(x)\n\n    if best_x is None:\n        best_x = project(low + 0.5 * span)\n\n    remaining_budget = budget - evals_used\n    if remaining_budget <= 0:\n        return best_x\n\n    # 3) Local exploitation: coordinate pattern search with adaptive step sizes\n    # Initial step: moderate fraction of span, but avoid being too small\n    base_span = np.where(span > 0, span, 1.0)\n    step = 0.2 * base_span\n    min_step = 1e-6 * base_span\n    shrink = 0.5\n    grow = 1.3\n\n    def coordinate_search(x_start):\n        nonlocal step, evals_used, best_x, best_y\n        x_center = x_start.copy()\n        improved_any = False\n\n        # One full sweep over all coordinates\n        for d in range(dim):\n            if evals_used >= budget:\n                break\n\n            improved_local = False\n\n            for direction in (+1.0, -1.0):\n                if evals_used >= budget:\n                    break\n                if step[d] <= 0:\n                    continue\n                x_try = x_center.copy()\n                x_try[d] += direction * step[d]\n                x_try = project(x_try)\n                y_new = eval_candidate(x_try)\n                if y_new is None:\n                    break\n                if np.isfinite(y_new) and y_new < best_y - 1e-12:\n                    x_center = x_try\n                    improved_any = True\n                    improved_local = True\n\n            # Per-dimension step adaptation\n            if improved_local:\n                step[d] = min(step[d] * grow, base_span[d])\n            else:\n                step[d] *= shrink\n\n        return x_center, improved_any\n\n    # Use remaining budget for repeated local refinement with elite restarts\n    # Mix between best_x (exploitation) and elites (diversification)\n    while evals_used < budget and np.any(step > min_step):\n        if elite_x and rng.random() < 0.4:\n            # Pick the best elite most of the time, else random elite\n            if rng.random() < 0.7:\n                idx = int(np.argmin(elite_y))\n            else:\n                idx = int(rng.integers(len(elite_x)))\n            x_start = elite_x[idx].copy()\n        else:\n            x_start = best_x.copy()\n\n        x_new, improved = coordinate_search(x_start)\n\n        if improved and best_x is not None:\n            # best_x already updated inside eval_candidate; keep consistency\n            best_x = best_x.copy()\n        else:\n            # No improvement in this sweep: globally shrink step\n            step *= shrink\n\n        if evals_used >= budget:\n            break\n\n    return best_x",
    "X": "0.11330594039006371 0.5549800806901442 0.8522077249063987"
}