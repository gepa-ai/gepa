{
    "score": -3.862779383081765,
    "Input": "Hartmann3",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budgeted evaluations.\n\n    Strategy (hybrid global + local search, tuned for small budgets):\n\n    - Respect config['budget'] evaluations.\n    - Warm-start from prev_best_x if available (evaluated at most once).\n    - Global exploration via low-discrepancy (Sobol-like) sampling with jitter.\n    - Adaptive elite archive of good points.\n    - Local derivative-free search (coordinate pattern search) from best + elites.\n    \"\"\"\n\n    rng = np.random.default_rng()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    if dim <= 0 or budget <= 0:\n        return np.zeros(dim, dtype=float)\n\n    # Helper: project into bounds\n    def project(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # Elite archive size tuned for small budgets\n    # Ensure at least a few elites, but don't overspend memory / selection\n    elite_size = max(3, min(12, budget // max(4, dim)))\n    elite_x = []\n    elite_y = []\n\n    def update_elites(x, y):\n        nonlocal elite_x, elite_y\n        if not np.isfinite(y):\n            return\n        if len(elite_x) < elite_size:\n            elite_x.append(x.copy())\n            elite_y.append(float(y))\n        else:\n            worst_idx = int(np.argmax(elite_y))\n            if y < elite_y[worst_idx]:\n                elite_x[worst_idx] = x.copy()\n                elite_y[worst_idx] = float(y)\n\n    # Evaluate a candidate if budget allows\n    def eval_candidate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        y = objective_function(x)\n        evals_used += 1\n        if np.isfinite(y) and y < best_y:\n            best_y = float(y)\n            best_x = x.copy()\n        update_elites(x, y)\n        return y\n\n    # 1) Warm start from prev_best_x if provided and valid\n    if prev_best_x is not None and evals_used < budget:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            x0 = project(x0)\n            eval_candidate(x0)\n        except Exception:\n            # Ignore malformed warm starts\n            pass\n\n    if evals_used >= budget:\n        if best_x is not None:\n            return best_x\n        return project(low + 0.5 * span)\n\n    # 2) Global exploration: low-discrepancy sampling + jitter\n    remaining_budget = budget - evals_used\n    if remaining_budget <= 0:\n        return best_x if best_x is not None else project(low + 0.5 * span)\n\n    # Allocate global vs local depending on budget and dimension\n    # For tiny budgets, lean more on global exploration.\n    if budget < 20:\n        global_frac = 0.8\n    elif budget < 60:\n        global_frac = 0.65\n    else:\n        global_frac = 0.55\n\n    n_global_target = int(global_frac * budget)\n    # Ensure at least a few samples per dimension if possible\n    n_global = max(dim * 4, n_global_target)\n    n_global = min(n_global, remaining_budget)\n\n    def sobol_like_samples(n, d):\n        \"\"\"\n        Lightweight Sobol-like sequence using randomized van der Corput in each dim\n        with different bases. This avoids heavy dependencies and provides better\n        coverage than plain random for small n.\n        \"\"\"\n        # Use first few primes as bases; cycle if dim > len(primes)\n        primes = [2, 3, 5, 7, 11, 13, 17]\n        bases = [primes[i % len(primes)] for i in range(d)]\n\n        # generate n 1D van der Corput values per dimension\n        ks = np.arange(1, n + 1)\n        xs_unit = np.empty((n, d), dtype=float)\n\n        for j in range(d):\n            base = bases[j]\n            k = ks.copy()\n            # random scramble by shuffling indices per dim\n            rng.shuffle(k)\n            vdc = np.zeros(n, dtype=float)\n            denom = 1.0\n            while np.any(k):\n                denom *= base\n                k, remainder = divmod(k, base)\n                vdc += remainder / denom\n            xs_unit[:, j] = vdc\n\n        return xs_unit\n\n    if n_global > 0:\n        xs_unit = sobol_like_samples(n_global, dim)\n        xs = low + xs_unit * span\n\n        # Moderate jitter to avoid alignment with boundaries\n        noise_scale = 0.03\n        if np.any(span > 0):\n            xs += rng.normal(0.0, noise_scale, size=(n_global, dim)) * span\n\n        for i in range(n_global):\n            if evals_used >= budget:\n                break\n            x = project(xs[i])\n            eval_candidate(x)\n\n    if best_x is None:\n        best_x = project(low + 0.5 * span)\n\n    remaining_budget = budget - evals_used\n    if remaining_budget <= 0:\n        return best_x\n\n    # 3) Local exploitation: coordinate pattern search with adaptive step sizes\n    base_span = np.where(span > 0, span, 1.0)\n\n    # Initial step size tuned for bounded small-dim problems (e.g., Hartmann)\n    step = 0.15 * base_span\n    min_step = 1e-6 * base_span\n    shrink = 0.5\n    grow = 1.4\n\n    def coordinate_search(x_start):\n        nonlocal step, evals_used, best_x, best_y\n        x_center = x_start.copy()\n        improved_any = False\n\n        # One sweep over all coordinates\n        for d in range(dim):\n            if evals_used >= budget:\n                break\n\n            improved_local = False\n            current_step = step[d]\n            if current_step <= 0:\n                continue\n\n            for direction in (+1.0, -1.0):\n                if evals_used >= budget:\n                    break\n                x_try = x_center.copy()\n                x_try[d] += direction * current_step\n                x_try = project(x_try)\n                y_new = eval_candidate(x_try)\n                if y_new is None:\n                    break\n                if np.isfinite(y_new) and y_new < best_y - 1e-12:\n                    x_center = x_try\n                    improved_any = True\n                    improved_local = True\n\n            # Per-dimension step adaptation\n            if improved_local:\n                step[d] = min(step[d] * grow, base_span[d])\n            else:\n                step[d] *= shrink\n\n        return x_center, improved_any\n\n    # 4) Use remaining budget for repeated local refinement with elite restarts\n    # Bias strongly to best_x but occasionally restart from diverse elites.\n    while evals_used < budget and np.any(step > min_step):\n        if elite_x and rng.random() < 0.35:\n            # Mostly pick the top elite, occasionally a random one\n            if rng.random() < 0.75:\n                idx = int(np.argmin(elite_y))\n            else:\n                idx = int(rng.integers(len(elite_x)))\n            x_start = elite_x[idx].copy()\n        else:\n            x_start = best_x.copy()\n\n        x_new, improved = coordinate_search(x_start)\n\n        if not improved:\n            # If no improvement from this starting point, shrink globally\n            step *= shrink\n\n        if evals_used >= budget:\n            break\n\n    return best_x",
    "X": "0.11564969039006372 0.5555660181901442 0.8524022744669456"
}