{
    "score": -6.890759148510282,
    "Input": "Michalewicz",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    # --- Basic config parsing ---\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config.get(\"budget\", 1))\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    span_safe = span.copy()\n    span_safe[span_safe == 0.0] = 1.0\n    avg_span = float(np.mean(span_safe)) if np.all(np.isfinite(span_safe)) else 1.0\n    if not (np.isfinite(avg_span) and avg_span > 0.0):\n        avg_span = 1.0\n\n    def clamp(x):\n        return np.clip(x, low, high)\n\n    # --- Evaluation bookkeeping ---\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return np.inf\n        x = np.asarray(x, dtype=float).reshape(-1)\n        try:\n            fx = float(objective_function(x))\n        except Exception:\n            fx = np.inf\n        evals_used += 1\n        if fx < best_y:\n            best_x, best_y = x.copy(), fx\n        return fx\n\n    # --- Initialization / warm start ---\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size >= dim:\n            x0 = x0[:dim]\n        if x0.shape == (dim,):\n            x0 = clamp(x0)\n            eval_point(x0)\n\n    # Center and random point if nothing yet\n    if best_x is None and evals_used < budget:\n        center = clamp((low + high) * 0.5)\n        eval_point(center)\n\n    if best_x is None and evals_used < budget:\n        x0 = np.random.uniform(low, high)\n        eval_point(x0)\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        if best_x is None:\n            best_x = clamp((low + high) * 0.5)\n        return best_x\n\n    # --- Very small budget: focused stochastic local search around current best ---\n    if remaining <= 5:\n        base_step = 0.15 * span_safe\n        for _ in range(remaining):\n            scale = base_step * np.exp(np.random.uniform(-1.0, 0.5, size=dim))\n            perturb = np.random.standard_cauchy(size=dim) * scale\n            candidate = clamp(best_x + perturb)\n            eval_point(candidate)\n        if best_x is None:\n            best_x = clamp((low + high) * 0.5)\n        return best_x\n\n    # --- Global seeding: improved space coverage with random + directed probes ---\n    remaining = budget - evals_used\n    # Use ~35% of remaining for seeding, bounded\n    n_seed = max(20, min(80, int(0.35 * remaining)))\n    n_seed = min(n_seed, remaining)\n\n    # Low-discrepancy-like sampling mixed with local around warm start\n    grid_levels = int(max(1, np.round(np.log2(dim + 1))))\n    for i in range(n_seed):\n        if evals_used >= budget:\n            break\n        if best_x is not None and i % 5 == 0:\n            # local probe around current best\n            step = 0.3 * span_safe\n            scale = step * np.exp(np.random.uniform(-1.2, 0.3, size=dim))\n            perturb = np.random.randn(dim) * scale\n            x = clamp(best_x + perturb)\n        else:\n            # jittered grid / low-discrepancy-like sampling\n            level = (i % grid_levels) + 1\n            div = 2 ** level\n            base = (i // grid_levels) % div\n            u = (base + 0.5 + np.random.rand(dim) * 0.5) / div\n            scramble = np.random.rand(dim)\n            u = (u + scramble) % 1.0\n            x = low + u * span\n        eval_point(x)\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        if best_x is None:\n            best_x = clamp((low + high) * 0.5)\n        return best_x\n\n    # --- Ensure we have a valid starting center ---\n    if best_x is None:\n        mean = np.random.uniform(low, high)\n        eval_point(mean)\n    else:\n        mean = best_x.copy()\n\n    # --- Adaptive Evolution Strategy (diagonal covariance) with restarts ---\n    sigma = 0.25 * avg_span\n    sigma = float(np.clip(sigma, 1e-3 * avg_span, 0.5 * avg_span))\n    C_diag = (0.35 * span_safe) ** 2\n    C_diag = np.maximum(C_diag, 1e-20)\n\n    base_lam = max(10, int(4 + 3 * np.log(dim + 1)))\n    restart_factor = 1.5\n    max_lam = 10 * max(10, int(4 + 3 * np.log(dim + 1)))\n\n    def local_refinement(steps, use_cauchy=True):\n        nonlocal best_x\n        if best_x is None:\n            return\n        base_step = 0.06 * span_safe\n        for _ in range(steps):\n            if evals_used >= budget:\n                break\n            scale = base_step * np.exp(np.random.uniform(-1.3, 0.3, size=dim))\n            if use_cauchy:\n                perturb = np.random.standard_cauchy(size=dim) * scale\n            else:\n                perturb = np.random.randn(dim) * scale\n            candidate = clamp(best_x + perturb)\n            eval_point(candidate)\n\n    # --- Main ES loop with multiple restarts ---\n    while evals_used < budget:\n        remaining = budget - evals_used\n        if remaining <= 0:\n            break\n\n        # Population size scales with remaining budget\n        lam = min(max(base_lam, 10), max(10, remaining // 3))\n        if remaining < 2 * lam:\n            # Not enough for full generation: do local refinement only\n            local_refinement(remaining, use_cauchy=True)\n            break\n\n        mu = max(4, lam // 2)\n        ranks = np.arange(1, mu + 1)\n        weights = np.log(mu + 0.5) - np.log(ranks)\n        weights /= np.sum(weights)\n\n        no_improve = 0\n        max_generations = max(1, remaining // lam)\n\n        for _ in range(max_generations):\n            if evals_used >= budget:\n                break\n\n            std = np.sqrt(np.maximum(C_diag, 1e-30))\n\n            # Antithetic sampling for variance reduction\n            half_lam = lam // 2\n            z_half = np.random.randn(half_lam, dim) * std\n            z = np.vstack([z_half, -z_half])\n            if z.shape[0] < lam:\n                extra = np.random.randn(lam - z.shape[0], dim) * std\n                z = np.vstack([z, extra])\n\n            pop = clamp(mean + sigma * z[:lam])\n\n            fitness = np.full(lam, np.inf, dtype=float)\n            for i in range(lam):\n                if evals_used >= budget:\n                    break\n                fitness[i] = eval_point(pop[i])\n\n            valid_mask = np.isfinite(fitness)\n            if not np.any(valid_mask):\n                break\n\n            idx_sorted = np.argsort(fitness[valid_mask])\n            valid_pop = pop[valid_mask][idx_sorted]\n            valid_fit = fitness[valid_mask][idx_sorted]\n\n            n_parents = min(mu, valid_pop.shape[0])\n            parents = valid_pop[:n_parents]\n            w = weights[:n_parents]\n            w /= np.sum(w)\n\n            old_mean = mean.copy()\n            mean = np.sum(parents * w[:, None], axis=0)\n            mean = clamp(mean)\n\n            # Diagonal covariance update\n            steps = parents - old_mean\n            step_sq = np.sum(w[:, None] * (steps ** 2), axis=0)\n            C_diag = 0.85 * C_diag + 0.15 * np.maximum(step_sq, 1e-20)\n\n            best_gen = valid_fit[0]\n            if best_gen + 1e-12 < best_y:\n                sigma *= 1.12\n                no_improve = 0\n            else:\n                sigma *= 0.9\n                no_improve += 1\n\n            sigma = float(np.clip(sigma, 5e-5 * avg_span, 0.5 * avg_span))\n\n            # Occasionally add small local search around global best\n            if no_improve == 3 and evals_used < budget:\n                local_refinement(min(4, budget - evals_used), use_cauchy=False)\n\n            if no_improve >= 7:\n                break\n\n        # Restart: recenter around global best and adjust population scale\n        base_lam = int(max(base_lam, lam) * restart_factor)\n        base_lam = min(base_lam, max_lam)\n\n        if best_x is not None:\n            mean = best_x.copy()\n        else:\n            mean = np.random.uniform(low, high)\n            eval_point(mean)\n\n        sigma = max(0.5 * sigma, 5e-5 * avg_span)\n        C_diag = np.maximum(C_diag, 1e-20)\n\n        remaining = budget - evals_used\n        if remaining <= 0:\n            break\n\n        # Short Gaussian burst after restart for quick local adjustment\n        burst_steps = min(6, remaining)\n        local_refinement(burst_steps, use_cauchy=False)\n\n    if best_x is None:\n        best_x = clamp((low + high) * 0.5)\n    return best_x",
    "X": "2.209812075450044 1.5704903035757465 1.2850511335597674 1.1113083803480321 1.7207736044039428 0.9089950476593305 2.518290153961277 2.07769365906141"
}