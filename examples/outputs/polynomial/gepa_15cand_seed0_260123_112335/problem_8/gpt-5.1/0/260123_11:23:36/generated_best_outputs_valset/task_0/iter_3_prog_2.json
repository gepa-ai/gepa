{
    "score": 2.774141493899734,
    "Input": "Easom",
    "Prints": "",
    "Logs": "",
    "Error": "assignment destination is read-only\nTraceback (most recent call last):\n  File \"/data/lukedhlee/gepa_luke/external/gepa-optimize-anything/src/gepa/utils/code_execution.py\", line 312, in _execute_in_process\n    result = context[entry_point](*entry_point_args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<string>\", line 183, in solve\nValueError: assignment destination is read-only\n\nCode did not return a valid numpy array.",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization using a hybrid of:\n    - global exploration via quasi-random (Sobol-like) sampling\n    - adaptive local search around best point (CMA-ES style covariance adaptation)\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config[\"budget\"])\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n    span[span <= 0] = 1.0  # safety for degenerate bounds\n\n    def clip(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n\n    # --- Initialization (use prev_best_x if available) ---\n    best_x = None\n    best_y = None\n\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = clip(x0)\n            try:\n                y0 = objective_function(x0)\n                evals_used += 1\n                best_x, best_y = x0, y0\n            except Exception:\n                best_x, best_y = None, None\n\n    # If we still have no incumbent, take one random sample\n    if best_x is None and evals_used < budget:\n        x0 = rng.uniform(lower, upper)\n        y0 = objective_function(x0)\n        evals_used += 1\n        best_x, best_y = x0, y0\n\n    if evals_used >= budget:\n        return best_x\n\n    # -------------- Global exploration phase --------------\n    # Allocate ~60% of remaining budget to global exploration\n    remaining = budget - evals_used\n    global_budget = max(1, int(0.6 * remaining))\n\n    # Sobol-like low-discrepancy sampling (simple Van der Corput in each dim)\n    def van_der_corput(n, base=2):\n        vdc = 0.0\n        denom = 1.0\n        while n:\n            n, remainder = divmod(n, base)\n            denom *= base\n            vdc += remainder / denom\n        return vdc\n\n    for i in range(global_budget):\n        # build a quasi-random point in [0, 1]^dim\n        # scramble base per dimension for decorrelation\n        u = np.empty(dim)\n        for d in range(dim):\n            base = 2 + (d % 3)  # small set of bases: 2,3,4\n            u[d] = van_der_corput(i + 1 + d * 131, base=base)\n        x = lower + u * span\n        # small random perturbation for additional diversity\n        perturb = rng.normal(scale=0.05, size=dim) * span\n        x = clip(x + perturb)\n\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = x, y\n\n        if evals_used >= budget:\n            return best_x\n\n    # -------------- Local exploitation phase --------------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Simple CMA-ES\u2013like covariance-adaptive local search around best_x\n    center = best_x.copy()\n\n    # Population size: small for low medium budget, scales with dim\n    lam = max(4, int(4 + 3 * np.log(dim + 1)))\n    lam = min(lam, max(4, remaining // 2))  # ensure at least 2 generations\n\n    # Initialize isotropic covariance in normalized space\n    sigma_start = 0.3  # relative to span\n    sigma_end = 0.02\n    C = np.eye(dim)\n\n    # Precompute evolution path & learning rates (simplified)\n    # These are loose CMA-like parameters; tuned for robustness, not speed.\n    c_c = 2 / (dim + 2)\n    c_cov = 2 / (dim ** 2 + 6)\n    p_c = np.zeros(dim)\n\n    gen = 0\n    while evals_used < budget:\n        remaining = budget - evals_used\n        if remaining <= 0:\n            break\n\n        # Adapt sigma linearly over remaining evaluations\n        frac = gen / max(1, remaining + gen)\n        sigma = (1 - frac) * sigma_start + frac * sigma_end\n\n        # Sample offspring in normalized space, then scale by span\n        # x = center + span * N(0, sigma^2 * C)\n        try:\n            A = np.linalg.cholesky(C)\n        except np.linalg.LinAlgError:\n            # fallback to diagonal if numerical issues\n            C = np.diag(np.maximum(1e-8, np.diag(C)))\n            A = np.sqrt(C)\n\n        n_offspring = min(lam, remaining)\n        z = rng.randn(n_offspring, dim)\n        steps = (z @ A.T) * sigma\n        X = center + steps * span\n\n        # Clip to bounds\n        X = clip(X)\n\n        # Evaluate offspring\n        Ys = []\n        for j in range(n_offspring):\n            if evals_used >= budget:\n                break\n            y = objective_function(X[j])\n            Ys.append(y)\n            evals_used += 1\n\n        if not Ys:\n            break\n\n        Ys = np.array(Ys)\n        idx = np.argsort(Ys)\n        X = X[idx]\n        Ys = Ys[idx]\n\n        # Update global best\n        if Ys[0] < best_y:\n            best_y = Ys[0]\n            best_x = X[0].copy()\n            center = best_x.copy()\n\n        # Recompute center as weighted mean of top half offspring\n        mu = max(1, n_offspring // 2)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        elite = X[:mu]\n        new_center = np.sum(elite * weights[:, None], axis=0)\n\n        # Update evolution path and covariance in normalized space\n        # Normalize steps relative to span\n        norm_steps = ((elite - center) / (span + 1e-12)) / max(1e-12, sigma)\n        mean_step = np.sum(norm_steps * weights[:, None], axis=0)\n\n        p_c = (1 - c_c) * p_c + np.sqrt(c_c * (2 - c_c)) * mean_step\n        C = (1 - c_cov) * C + c_cov * (\n            np.outer(p_c, p_c) +\n            np.sum(\n                [w * np.outer(s, s) for w, s in zip(weights, norm_steps)],\n                axis=0,\n            )\n        )\n\n        # Ensure covariance stays symmetric positive\n        C = 0.5 * (C + C.T)\n        diag = np.diag(C)\n        diag[diag <= 1e-10] = 1e-10\n        C[np.diag_indices(dim)] = diag\n\n        center = new_center\n        gen += 1\n\n    return best_x",
    "X": "not found"
}