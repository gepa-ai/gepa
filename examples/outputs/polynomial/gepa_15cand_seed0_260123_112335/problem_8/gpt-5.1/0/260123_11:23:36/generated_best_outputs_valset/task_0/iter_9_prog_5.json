{
    "score": 1.0731775613765007,
    "Input": "Easom",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization with:\n    - robust initialization (incl. warm start)\n    - global quasi-random exploration with stronger diversification\n    - adaptive local search (CMA-ES\u2013like) with more aggressive exploitation on multimodal functions\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    bounds = np.array(bounds, dtype=float, copy=True)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config[\"budget\"])\n\n    # Ensure proper shape for bounds\n    if bounds.ndim != 2 or bounds.shape[0] != dim or bounds.shape[1] != 2:\n        bounds = np.reshape(bounds, (dim, 2))\n\n    lower = bounds[:, 0].astype(float).copy()\n    upper = bounds[:, 1].astype(float).copy()\n    span = (upper - lower).astype(float)\n    span = np.where(span <= 0, 1.0, span)\n\n    def clip(x):\n        x = np.asarray(x, dtype=float)\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n\n    # ----------------- Initialization -----------------\n    best_x = None\n    best_y = np.inf\n\n    # Use prev_best_x if available and valid\n    if prev_best_x is not None and evals_used < budget:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = clip(x0)\n            try:\n                y0 = float(objective_function(x0))\n                evals_used += 1\n                if np.isfinite(y0) and y0 < best_y:\n                    best_x, best_y = x0.copy(), y0\n            except Exception:\n                evals_used += 1  # still counts toward budget\n\n    # Fallback: random start if we don't have a valid incumbent\n    if (best_x is None or not np.isfinite(best_y)) and evals_used < budget:\n        x0 = rng.uniform(lower, upper)\n        try:\n            y0 = float(objective_function(x0))\n        except Exception:\n            evals_used += 1\n            # Return any feasible point if even this fails\n            return np.asarray(clip(x0), dtype=float).reshape(dim)\n        evals_used += 1\n        best_x, best_y = x0.copy(), y0\n\n    # Safety: if we somehow still lack a finite best_y\n    if not np.isfinite(best_y):\n        return clip(lower + 0.5 * span).reshape(dim)\n\n    if evals_used >= budget:\n        return np.asarray(best_x, dtype=float).reshape(dim)\n\n    # ----------------- Global exploration -----------------\n    remaining = budget - evals_used\n    # Slightly less global budget to emphasize local on difficult functions like Easom\n    global_budget = max(1, int(0.5 * remaining))\n\n    def van_der_corput(n, base=2):\n        vdc = 0.0\n        denom = 1.0\n        while n:\n            n, remainder = divmod(n, base)\n            denom *= base\n            vdc += remainder / denom\n        return vdc\n\n    # Scale for perturbations: mix of span-based and absolute to help huge ranges\n    base_scale_lowdim = 0.1\n    base_scale_highdim = 0.03\n\n    for i in range(global_budget):\n        if evals_used >= budget:\n            break\n\n        u = np.empty(dim, dtype=float)\n        for d in range(dim):\n            base = 2 + (d % 3)\n            u[d] = van_der_corput(i + 1 + d * 131, base=base)\n        x = lower + u * span\n\n        # Dimension-dependent perturbation\n        if dim <= 5:\n            base_scale = base_scale_lowdim\n        else:\n            base_scale = base_scale_highdim / np.sqrt(max(1, dim))\n        perturb = rng.normal(scale=base_scale, size=dim) * span\n\n        # Occasionally do a large jump to escape local traps (help Easom-like)\n        if rng.rand() < 0.15:\n            large_scale = 0.5\n            perturb += rng.normal(scale=large_scale, size=dim) * span\n\n        x = clip(x + perturb)\n\n        try:\n            y = float(objective_function(x))\n        except Exception:\n            evals_used += 1\n            continue\n\n        evals_used += 1\n        if np.isfinite(y) and y < best_y:\n            best_x, best_y = x.copy(), y\n\n    if evals_used >= budget:\n        return np.asarray(best_x, dtype=float).reshape(dim)\n\n    # ----------------- Local exploitation (CMA-ES\u2013like) -----------------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return np.asarray(best_x, dtype=float).reshape(dim)\n\n    center = np.asarray(best_x, dtype=float).copy()\n\n    # Population size: scale with dim but cap by remaining budget\n    lam = max(6, int(4 + 3 * np.log(dim + 1)))\n    lam = min(lam, max(4, remaining))  # ensure at least one generation\n\n    # Start with moderate to somewhat aggressive local scale\n    sigma_start = 0.35\n    sigma_end = 0.03\n    C = np.eye(dim, dtype=float)\n\n    # CMA-like parameters (slightly faster adaptation)\n    c_c = 2.0 / (dim + 2.0)\n    c_cov = 2.0 / (dim ** 1.5 + 6.0)\n    p_c = np.zeros(dim, dtype=float)\n\n    max_gens = max(1, remaining // max(1, lam))\n    gen = 0\n\n    # Compute a characteristic length scale for step limiting\n    span_scale = np.maximum(span, 1e-12)\n\n    # small regularization to keep covariance healthy\n    min_eig = 1e-10\n\n    while evals_used < budget and gen < max_gens:\n        remaining = budget - evals_used\n        if remaining <= 0:\n            break\n\n        # Decrease sigma over generations rather than raw eval count\n        if max_gens > 1:\n            frac = gen / (max_gens - 1)\n        else:\n            frac = 1.0\n        frac = np.clip(frac, 0.0, 1.0)\n        sigma = (1.0 - frac) * sigma_start + frac * sigma_end\n\n        # Cholesky or diagonal fallback\n        try:\n            A = np.linalg.cholesky(C)\n        except np.linalg.LinAlgError:\n            # Eigen decomposition for robust repair\n            try:\n                eigvals, eigvecs = np.linalg.eigh(C)\n                eigvals = np.maximum(eigvals, min_eig)\n                C = (eigvecs * eigvals) @ eigvecs.T\n            except Exception:\n                diag = np.maximum(min_eig, np.diag(C))\n                C = np.diag(diag)\n            A = np.linalg.cholesky(C + min_eig * np.eye(dim))\n\n        n_offspring = int(min(lam, remaining))\n        if n_offspring <= 0:\n            break\n\n        z = rng.randn(n_offspring, dim)\n        steps = (z @ A.T) * sigma\n\n        # Limit step size (in normalized space) to keep moves reasonable\n        step_norms = np.linalg.norm(steps, axis=1)\n        max_step = 4.0 if dim <= 5 else 3.0\n        mask = step_norms > max_step\n        if np.any(mask):\n            steps[mask] *= (max_step / step_norms[mask])[:, None]\n\n        X = center + steps * span_scale\n        X = clip(X)\n\n        Ys = []\n        valid_indices = []\n        for j in range(n_offspring):\n            if evals_used >= budget:\n                break\n            try:\n                y = float(objective_function(X[j]))\n            except Exception:\n                evals_used += 1\n                continue\n            evals_used += 1\n            if not np.isfinite(y):\n                continue\n            Ys.append(y)\n            valid_indices.append(j)\n\n        if not Ys:\n            gen += 1\n            continue\n\n        Ys = np.asarray(Ys, dtype=float)\n        X = X[np.asarray(valid_indices, dtype=int)]\n\n        idx = np.argsort(Ys)\n        X = X[idx]\n        Ys = Ys[idx]\n\n        # Update global best\n        if Ys[0] < best_y:\n            best_y = Ys[0]\n            best_x = X[0].copy()\n            center = best_x.copy()\n\n        # Recompute center using weighted mean of top half\n        mu = max(1, X.shape[0] // 2)\n        elite = X[:mu]\n        ranks = np.arange(1, mu + 1)\n        weights = np.log(mu + 0.5) - np.log(ranks)\n        weights = np.maximum(weights, 0.0)\n        if np.sum(weights) == 0.0:\n            weights = np.ones_like(weights)\n        weights /= np.sum(weights)\n\n        old_center = center.copy()\n        new_center = np.sum(elite * weights[:, None], axis=0)\n\n        # Normalized steps from old center for covariance update\n        norm_steps = ((elite - old_center) / span_scale) / max(1e-12, sigma)\n        mean_step = np.sum(norm_steps * weights[:, None], axis=0)\n\n        p_c = (1.0 - c_c) * p_c + np.sqrt(c_c * (2.0 - c_c)) * mean_step\n\n        # Rank-one + rank-mu covariance update\n        rank_mu = np.zeros((dim, dim), dtype=float)\n        for w, s in zip(weights, norm_steps):\n            rank_mu += w * np.outer(s, s)\n\n        C = (1.0 - c_cov) * C + c_cov * (np.outer(p_c, p_c) + rank_mu)\n\n        # Symmetrize and ensure positive diagonal\n        C = 0.5 * (C + C.T)\n        diag = np.diag(C).copy()\n        diag[diag <= min_eig] = min_eig\n        C[np.diag_indices(dim)] = diag\n\n        center = new_center\n        gen += 1\n\n    return np.asarray(best_x, dtype=float).reshape(dim)",
    "X": "-0.16669892976689954 -0.014023365179493874"
}