{
    "score": 0.47089199034310614,
    "Input": "McCourt17",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budget-aware blackbox minimizer with warm start and hybrid global/local search.\n\n    This version focuses on:\n      - More robust use of warm-start (prev_best_x) with an immediate intensification\n      - Simpler, more aggressive global search early (Sobol-like + random + local jitter)\n      - Leaner local improvement logic (ES-like in low dim, coordinate search in high dim)\n      - Stricter budget accounting and graceful handling of invalid evaluations\n    \"\"\"\n    rng = np.random\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low, high = bounds[:, 0], bounds[:, 1]\n    span = high - low\n    span[span == 0] = 1.0  # avoid zero-length for scaling\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    # --------------------\n    # Initialize best (using warm start if available)\n    # --------------------\n    best_x = None\n    best_y = np.inf\n\n    # Evaluate a warm start if valid\n    if prev_best_x is not None and budget > 0:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.shape[0] == dim:\n                x0 = clip(x0)\n                try:\n                    y0 = objective_function(x0)\n                except Exception:\n                    evals_used += 1\n                    y0 = np.inf\n                else:\n                    evals_used += 1\n                if np.isfinite(y0):\n                    best_x, best_y = x0, y0\n        except Exception:\n            pass\n\n    # Fallback: small random batch to establish an initial best\n    if (best_x is None or not np.isfinite(best_y)) and evals_used < budget:\n        batch = min(6, budget - evals_used)\n        xs = rng.uniform(low, high, size=(batch, dim))\n        for i in range(batch):\n            if evals_used >= budget:\n                break\n            x0 = xs[i]\n            try:\n                y0 = objective_function(x0)\n            except Exception:\n                evals_used += 1\n                continue\n            evals_used += 1\n            if not np.isfinite(y0):\n                continue\n            if y0 < best_y:\n                best_y = y0\n                best_x = x0.copy()\n\n    # Still no finite value: try once more, else return any random point\n    if best_x is None:\n        if evals_used < budget:\n            x0 = rng.uniform(low, high, size=dim)\n            try:\n                y0 = objective_function(x0)\n            except Exception:\n                evals_used += 1\n                return clip(np.asarray(x0, dtype=float))\n            evals_used += 1\n            if np.isfinite(y0):\n                return clip(x0)\n        x0 = rng.uniform(low, high, size=dim)\n        return clip(x0)\n\n    if evals_used >= budget:\n        return clip(np.asarray(best_x, dtype=float))\n\n    # --------------------\n    # Quick intensification around current best/warm start\n    # --------------------\n    remaining_for_quick = min(8, budget - evals_used)\n    if remaining_for_quick > 0:\n        local_sigma = 0.05 * span\n        local_sigma[local_sigma == 0] = 1.0\n        x_center = best_x.copy()\n        for _ in range(remaining_for_quick):\n            if evals_used >= budget:\n                break\n            step = rng.normal(scale=local_sigma, size=dim)\n            cand = clip(x_center + step)\n            try:\n                y = objective_function(cand)\n            except Exception:\n                evals_used += 1\n                continue\n            evals_used += 1\n            if not np.isfinite(y):\n                continue\n            if y < best_y:\n                best_y = y\n                best_x = cand.copy()\n                x_center = cand.copy()\n\n    if evals_used >= budget:\n        return clip(np.asarray(best_x, dtype=float))\n\n    # --------------------\n    # Decide global vs local budget split\n    # --------------------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return clip(np.asarray(best_x, dtype=float))\n\n    # Use more global search for higher dim and small budgets\n    if remaining <= 15:\n        global_frac = 0.7\n    else:\n        if dim <= 5:\n            base_global = 0.4\n        elif dim <= 20:\n            base_global = 0.55\n        else:\n            base_global = 0.65\n        global_frac = base_global\n\n    n_global = max(1, int(round(remaining * global_frac)))\n    n_global = min(n_global, remaining - 1) if remaining > 1 else remaining\n    n_local = max(0, remaining - n_global)\n\n    if n_local == 0 and remaining >= 10:\n        n_local = 3\n        n_global = max(1, remaining - n_local)\n\n    # --------------------\n    # Sobol-like low discrepancy sampler (simple Gray-code variant)\n    # --------------------\n    def sobol_like(size, dim, seed):\n        rs = np.random.RandomState(seed)\n        max_bits = 30\n        directions = rs.rand(max_bits, dim)\n        seq = np.empty((size, dim), dtype=float)\n        x = np.zeros(dim)\n        for i in range(size):\n            g = i ^ (i >> 1)  # Gray code\n            bit = 0\n            v = np.zeros(dim)\n            t = g\n            while t and bit < max_bits:\n                if t & 1:\n                    v += directions[bit]\n                t >>= 1\n                bit += 1\n            x = (x + v) % 1.0\n            seq[i] = x\n        return seq\n\n    # --------------------\n    # Global search\n    # --------------------\n    if n_global > 0 and evals_used < budget:\n        if dim <= 10:\n            rand_frac = 0.3\n        elif dim <= 40:\n            rand_frac = 0.45\n        else:\n            rand_frac = 0.6\n        n_rand = max(1, int(round(rand_frac * n_global)))\n        n_qmc = max(0, n_global - n_rand)\n\n        xs_list = []\n\n        if n_qmc > 0:\n            xs_unit = sobol_like(\n                n_qmc, dim, seed=int(rng.randint(0, 1_000_000))\n            )\n            xs_list.append(low + xs_unit * span)\n\n        if n_rand > 0:\n            xs_rand = rng.uniform(low, high, size=(n_rand, dim))\n            xs_list.append(xs_rand)\n\n        xs = np.vstack(xs_list) if xs_list else np.empty((0, dim))\n\n        # Inject jittered points around current best\n        if best_x is not None and np.isfinite(best_y) and xs.shape[0] > 0:\n            k = max(1, xs.shape[0] // 5)\n            noise = rng.normal(scale=0.12 * span, size=(k, dim))\n            xs[:k] = clip(best_x + noise)\n\n        # Also inject jittered points around previous best from other runs\n        if prev_best_x is not None and xs.shape[0] > 0:\n            try:\n                pb = np.asarray(prev_best_x, dtype=float).reshape(-1)\n                if pb.shape[0] == dim:\n                    pb = clip(pb)\n                    k2 = max(1, xs.shape[0] // 8)\n                    noise = rng.normal(scale=0.08 * span, size=(k2, dim))\n                    xs[-k2:] = clip(pb + noise)\n            except Exception:\n                pass\n\n        if xs.shape[0] > 1:\n            idx = rng.permutation(xs.shape[0])\n            xs = xs[idx]\n\n        for x in xs:\n            if evals_used >= budget:\n                break\n            try:\n                y = objective_function(x)\n            except Exception:\n                evals_used += 1\n                continue\n            evals_used += 1\n            if not np.isfinite(y):\n                continue\n            if y < best_y:\n                best_y = y\n                best_x = x.copy()\n\n    if evals_used >= budget or n_local <= 0:\n        return clip(np.asarray(best_x, dtype=float))\n\n    # --------------------\n    # Local search\n    # --------------------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return clip(np.asarray(best_x, dtype=float))\n\n    # If best_y is not finite (should be rare now), fallback to random search\n    if not np.isfinite(best_y):\n        while evals_used < budget:\n            x = rng.uniform(low, high, size=dim)\n            try:\n                y = objective_function(x)\n            except Exception:\n                evals_used += 1\n                continue\n            evals_used += 1\n            if not np.isfinite(y):\n                continue\n            if y < best_y:\n                best_y = y\n                best_x = x.copy()\n        return clip(np.asarray(best_x, dtype=float))\n\n    current_x = best_x.copy()\n    current_y = best_y\n\n    # Base step size scaled by span and dimension\n    if dim <= 10:\n        base_sigma = span * 0.12\n    else:\n        base_sigma = span * (0.16 if dim <= 40 else 0.22)\n    base_sigma[base_sigma == 0] = 1.0\n\n    iters = min(n_local, 60 if dim <= 5 else 50 if dim <= 10 else 40 if dim <= 40 else 32)\n    if iters <= 0:\n        return clip(np.asarray(best_x, dtype=float))\n\n    evals_per_iter = max(1, n_local // iters)\n    iters = min(iters, (n_local + evals_per_iter - 1) // evals_per_iter)\n\n    # --------------------\n    # Low-dimensional: simple ES-like adaptation with diagonal variance\n    # --------------------\n    if dim <= 12:\n        cov = (base_sigma ** 2)\n        cov[cov <= 1e-32] = 1e-32\n\n        learning_rate = 0.22\n        shrink = 0.6\n        expand = 1.3\n\n        success_count = 0\n        total_tries = 0\n        no_improve_iters = 0\n\n        for it in range(iters):\n            if evals_used >= budget:\n                break\n            iter_evals = min(evals_per_iter, budget - evals_used)\n            if iter_evals <= 0:\n                break\n\n            iter_scale = 0.6 ** (it / max(1, iters - 1))\n            step_std = np.sqrt(cov) * iter_scale\n            step_std[step_std <= 1e-16] = 1e-16\n\n            improved = False\n            best_iter_x = current_x\n            best_iter_y = current_y\n\n            for _ in range(iter_evals):\n                if evals_used >= budget:\n                    break\n                step = rng.normal(scale=step_std, size=dim)\n                cand = clip(current_x + step)\n                try:\n                    y = objective_function(cand)\n                except Exception:\n                    evals_used += 1\n                    continue\n                evals_used += 1\n                total_tries += 1\n\n                if not np.isfinite(y):\n                    continue\n\n                if y < best_y:\n                    best_y = y\n                    best_x = cand.copy()\n                if y < best_iter_y:\n                    best_iter_y = y\n                    best_iter_x = cand.copy()\n                    improved = True\n                    success_count += 1\n\n            if improved:\n                diff = best_iter_x - current_x\n                cov = (1.0 - learning_rate) * cov + learning_rate * (diff ** 2 + 1e-32)\n                current_x, current_y = best_iter_x, best_iter_y\n                cov *= expand\n                no_improve_iters = 0\n            else:\n                cov *= shrink\n                no_improve_iters += 1\n\n            if total_tries > 0:\n                success_rate = success_count / float(total_tries)\n                if success_rate < 0.18:\n                    cov *= 0.8\n                elif success_rate > 0.42:\n                    cov *= 1.12\n\n            cov = np.clip(cov, 1e-32, (span ** 2))\n\n            # Small rescue global moves on stagnation\n            if no_improve_iters >= 4 and evals_used < budget:\n                rescue_evals = min(3, budget - evals_used)\n                for _ in range(rescue_evals):\n                    if evals_used >= budget:\n                        break\n                    cand = rng.uniform(low, high, size=dim)\n                    try:\n                        y = objective_function(cand)\n                    except Exception:\n                        evals_used += 1\n                        continue\n                    evals_used += 1\n                    if not np.isfinite(y):\n                        continue\n                    if y < best_y:\n                        best_y = y\n                        best_x = cand.copy()\n                        current_x, current_y = cand.copy(), y\n                        no_improve_iters = 0\n                if evals_used >= budget:\n                    break\n\n    # --------------------\n    # Higher-dimensional: coordinate-wise stochastic hill-climb\n    # --------------------\n    else:\n        sigma = base_sigma.copy()\n        sigma = np.clip(sigma, span * 1e-3, span)\n        success_count = 0\n        total_tries = 0\n        no_improve_iters = 0\n\n        for it in range(iters):\n            if evals_used >= budget:\n                break\n            iter_evals = min(evals_per_iter, budget - evals_used)\n            if iter_evals <= 0:\n                break\n\n            iter_scale = 0.7 ** (it / max(1, iters - 1))\n            step_std = sigma * iter_scale\n            step_std[step_std <= 1e-16] = 1e-16\n\n            improved = False\n            best_iter_x = current_x\n            best_iter_y = current_y\n\n            for _ in range(iter_evals):\n                if evals_used >= budget:\n                    break\n\n                # Sparse perturbations: 1 or 2 coordinates\n                if rng.rand() < 0.3 and dim > 1:\n                    idxs = rng.choice(dim, size=2, replace=False)\n                    step = np.zeros(dim, dtype=float)\n                    step[idxs] = rng.normal(scale=step_std[idxs])\n                else:\n                    idx = rng.randint(0, dim)\n                    step = np.zeros(dim, dtype=float)\n                    step[idx] = rng.normal(scale=step_std[idx])\n\n                cand = clip(current_x + step)\n                try:\n                    y = objective_function(cand)\n                except Exception:\n                    evals_used += 1\n                    continue\n                evals_used += 1\n                total_tries += 1\n\n                if not np.isfinite(y):\n                    continue\n\n                if y < best_y:\n                    best_y = y\n                    best_x = cand.copy()\n                if y < best_iter_y:\n                    best_iter_y = y\n                    best_iter_x = cand.copy()\n                    improved = True\n                    success_count += 1\n\n            if improved:\n                current_x, current_y = best_iter_x, best_iter_y\n                sigma *= 1.15\n                no_improve_iters = 0\n            else:\n                sigma *= 0.65\n                no_improve_iters += 1\n\n            if total_tries > 0:\n                success_rate = success_count / float(total_tries)\n                if success_rate < 0.16:\n                    sigma *= 0.8\n                elif success_rate > 0.44:\n                    sigma *= 1.1\n\n            sigma = np.clip(sigma, span * 1e-4, span)\n\n            # Rescue: random re-starts on heavy stagnation\n            if no_improve_iters >= 6 and evals_used < budget:\n                rescue_evals = min(4, budget - evals_used)\n                for _ in range(rescue_evals):\n                    if evals_used >= budget:\n                        break\n                    cand = rng.uniform(low, high, size=dim)\n                    try:\n                        y = objective_function(cand)\n                    except Exception:\n                        evals_used += 1\n                        continue\n                    evals_used += 1\n                    if not np.isfinite(y):\n                        continue\n                    if y < best_y:\n                        best_y = y\n                        best_x = cand.copy()\n                        current_x, current_y = cand.copy(), y\n                        no_improve_iters = 0\n                if evals_used >= budget:\n                    break\n\n    return clip(np.asarray(best_x, dtype=float))",
    "X": "0.3125093288834936 0.9166193490652871 0.31250972439296704 0.7062098841188673 0.03963634726482391 0.9270306890580299 0.5978976169509393"
}