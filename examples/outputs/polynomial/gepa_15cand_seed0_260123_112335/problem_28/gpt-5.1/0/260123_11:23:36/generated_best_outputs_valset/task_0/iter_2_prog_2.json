{
    "score": 0.501573412611362,
    "Input": "McCourt17",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budget-aware blackbox minimizer using:\n      - Warm start (if provided)\n      - Space-filling global sampling (Sobol-like via scrambled Halton)\n      - Adaptive local search (CMA-ES style covariance adaptation in small dim,\n        Gaussian hill-climb in higher dim)\n\n    Designed to use the full evaluation budget and handle generic blackbox tasks.\n    \"\"\"\n    rng = np.random\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low, high = bounds[:, 0], bounds[:, 1]\n    span = high - low\n    span[span == 0] = 1.0  # safeguard: zero-width bounds\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    # Initialize best\n    best_x = None\n    best_y = np.inf\n\n    # Warm start if possible\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.shape[0] == dim:\n                x0 = clip(x0)\n                y0 = objective_function(x0)\n                evals_used += 1\n                best_x, best_y = x0, y0\n        except Exception:\n            # If warm start fails for any reason, ignore it\n            best_x, best_y = None, np.inf\n\n    # If still no best, take one random point\n    if best_x is None and evals_used < budget:\n        x0 = rng.uniform(low, high)\n        y0 = objective_function(x0)\n        evals_used += 1\n        best_x, best_y = x0, y0\n\n    if evals_used >= budget:\n        return np.asarray(best_x, dtype=float)\n\n    remaining = budget - evals_used\n\n    # Split budget between global (space-filling) and local search.\n    # For small budgets, lean more into global; for larger, emphasize local.\n    if remaining <= 10:\n        global_frac = 0.7\n    elif remaining <= 50:\n        global_frac = 0.55\n    else:\n        global_frac = 0.4\n    n_global = max(1, int(remaining * global_frac))\n    n_local = max(0, remaining - n_global)\n\n    # --------------------\n    # Global search: scrambled Halton-like low-discrepancy sampling\n    # --------------------\n    def halton_sequence(size, dim, base_seed=0):\n        \"\"\"Generate a simple scrambled Halton-like sequence in [0, 1]^dim.\"\"\"\n        # Use the first `dim` primes as bases\n        # Precomputed small primes, enough for moderate dim\n        primes = [\n            2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n            31, 37, 41, 43, 47, 53, 59, 61, 67, 71,\n            73, 79, 83, 89, 97, 101, 103, 107, 109, 113\n        ]\n        bases = primes[:dim]\n\n        # Simple digit scramble using rng permutations per dimension.\n        # We can't reseed global RNG (might affect users),\n        # so we build local RandomState.\n        rs = np.random.RandomState(base_seed)\n\n        # Pre-generate permutations for digits 0..max_base-1\n        max_base = max(bases)\n        perms = rs.permutation(max_base)\n\n        def radical_inverse(n, base, perm_offset):\n            inv = 0.0\n            f = 1.0 / base\n            x = n\n            while x > 0:\n                x, digit = divmod(x, base)\n                # Scramble digit via permuting modulo base\n                d = perms[(digit + perm_offset) % max_base] % base\n                inv += d * f\n                f /= base\n            return inv\n\n        seq = np.empty((size, dim), dtype=float)\n        # offset index so different calls yield different locations\n        start_index = rs.randint(0, 10_000_000)\n        for j, b in enumerate(bases):\n            perm_offset = rs.randint(0, max_base)\n            for i in range(size):\n                n = start_index + i + 1\n                seq[i, j] = radical_inverse(n, b, perm_offset)\n        return seq\n\n    if n_global > 0 and evals_used < budget:\n        # Generate Halton-like samples then map to bounds\n        xs_unit = halton_sequence(n_global, dim, base_seed=int(rng.randint(0, 1_000_000)))\n        xs = low + xs_unit * span\n\n        for i in range(n_global):\n            if evals_used >= budget:\n                break\n            x = xs[i]\n            y = objective_function(x)\n            evals_used += 1\n            if y < best_y:\n                best_y = y\n                best_x = x.copy()\n\n    if evals_used >= budget or n_local <= 0:\n        return np.asarray(best_x, dtype=float)\n\n    # --------------------\n    # Local search\n    # Use a small CMA-like adaptation in low dims, otherwise Gaussian hill climb.\n    # --------------------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return np.asarray(best_x, dtype=float)\n\n    current_x = best_x.copy()\n    current_y = best_y\n\n    # Initial local step size (fraction of span)\n    base_sigma_global = span * 0.25\n    base_sigma_global[base_sigma_global == 0] = 1.0\n\n    # Allocate iterations\n    iters = min(20, remaining)\n    evals_per_iter = max(1, remaining // iters)\n\n    if dim <= 10:\n        # Lightweight CMA-ES style: diagonal covariance adaptation\n        sigma = base_sigma_global.copy()\n        cov = (sigma ** 2)\n        cov[cov <= 1e-32] = 1e-32\n        learning_rate = 0.3\n        shrink = 0.9\n        expand = 1.2\n\n        for it in range(iters):\n            if evals_used >= budget:\n                break\n            # Decay global scale over iterations\n            iter_scale = 0.5 ** (it / max(1, iters - 1))\n            step_std = np.sqrt(cov) * iter_scale\n            step_std[step_std <= 1e-16] = 1e-16\n\n            improved = False\n            for _ in range(evals_per_iter):\n                if evals_used >= budget:\n                    break\n                step = rng.normal(scale=step_std, size=dim)\n                cand = clip(current_x + step)\n                y = objective_function(cand)\n                evals_used += 1\n                if y < best_y:\n                    best_y = y\n                    best_x = cand.copy()\n                    current_x, current_y = best_x, best_y\n                    # Update covariance in direction of improvement\n                    diff = step\n                    cov = (1 - learning_rate) * cov + learning_rate * (diff ** 2 + 1e-32)\n                    improved = True\n\n            # Adapt global scale depending on improvement\n            if improved:\n                cov *= expand\n            else:\n                cov *= shrink\n            cov = np.clip(cov, 1e-32, (span ** 2))\n\n    else:\n        # High-dimensional case: simpler Gaussian hill-climb with coordinate-wise decay\n        sigma = base_sigma_global.copy()\n        for it in range(iters):\n            if evals_used >= budget:\n                break\n            iter_scale = 0.5 ** (it / max(1, iters - 1))\n            step_std = sigma * iter_scale\n            step_std[step_std <= 1e-16] = 1e-16\n\n            improved = False\n            for _ in range(evals_per_iter):\n                if evals_used >= budget:\n                    break\n                step = rng.normal(scale=step_std, size=dim)\n                cand = clip(current_x + step)\n                y = objective_function(cand)\n                evals_used += 1\n                if y < best_y:\n                    best_y = y\n                    best_x = cand.copy()\n                    current_x, current_y = best_x, best_y\n                    improved = True\n\n            # Coarse step-size adaptation\n            if improved:\n                sigma *= 1.1\n            else:\n                sigma *= 0.7\n            sigma = np.clip(sigma, span * 1e-4, span)\n\n    return np.asarray(best_x, dtype=float)",
    "X": "0.3294061744033815 0.9251145120929382 0.37365568756921724 0.6897648146154994 0.0 0.9760120815929174 0.6053766511358796"
}