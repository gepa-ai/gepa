{
    "score": 0.0005093569568533859,
    "Input": "Sphere",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budget-aware hybrid optimizer:\n      - Robust warm start from prev_best_x\n      - Global sampling (quasi-random + random)\n      - Focused restart around best found region\n      - Local search with adaptive Gaussian / coordinate steps\n      - Dimension- and budget-aware allocation\n\n    Returns: best x found as np.ndarray of shape (dim,)\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Degenerate config: return box center\n    if dim <= 0 or budget <= 0:\n        center = (low + high) / 2.0\n        return center.astype(float).reshape(dim)\n\n    # Ensure span is non-zero; track degenerate dims (collapsed bounds)\n    safe_span = np.where(span > 0, span, 1.0)\n    free_dims = span > 0\n\n    # ---- Phase allocation: global / restart-refine / local ----\n    # Slightly more local search than previous version; better exploitation\n    if budget < 15:\n        global_frac = 0.75\n        restart_frac = 0.0\n    elif budget < 60:\n        global_frac = 0.6 if dim <= 10 else 0.7\n        restart_frac = 0.15\n    elif budget < 200:\n        global_frac = 0.5 if dim <= 10 else 0.6\n        restart_frac = 0.2\n    else:\n        global_frac = 0.45 if dim <= 10 else 0.55\n        restart_frac = 0.25\n\n    local_frac = max(0.0, 1.0 - global_frac - restart_frac)\n\n    global_budget = max(1, int(budget * global_frac))\n    restart_budget = max(0, int(budget * restart_frac))\n    local_budget = max(0, budget - global_budget - restart_budget)\n\n    # Fix rounding\n    total_alloc = global_budget + restart_budget + local_budget\n    if total_alloc < budget:\n        global_budget += budget - total_alloc\n    elif total_alloc > budget:\n        global_budget = max(0, global_budget - (total_alloc - budget))\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # -------- Helpers --------\n    def quasi_random_samples(n_samples, base_seed=None):\n        \"\"\"Simple van der Corput-based low-discrepancy sampler in [low, high].\"\"\"\n        if n_samples <= 0:\n            return np.empty((0, dim), dtype=float)\n\n        # Small prime bases; repeat if dim is large\n        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23]\n        bases = [primes[i % len(primes)] for i in range(dim)]\n\n        if base_seed is None:\n            start = np.random.randint(0, 10000)\n        else:\n            start = int(base_seed)\n\n        def vdc(n, base):\n            v = 0.0\n            denom = 1.0\n            while n:\n                n, r = divmod(n, base)\n                denom *= base\n                v += r / denom\n            return v\n\n        xs = np.empty((n_samples, dim), dtype=float)\n        for i in range(n_samples):\n            idx = start + i\n            for d in range(dim):\n                xs[i, d] = vdc(idx, bases[d])\n\n        # Randomly permute each dimension to reduce structure\n        for d in range(dim):\n            perm = np.random.permutation(n_samples)\n            xs[:, d] = xs[perm, d]\n\n        xs = low + span * xs\n        return xs\n\n    def try_eval(x):\n        \"\"\"Safe evaluation with budget check and best update.\"\"\"\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = np.asarray(x, dtype=float).reshape(dim)\n        x = np.clip(x, low, high)\n        x[~free_dims] = low[~free_dims]\n        try:\n            y = objective_function(x)\n        except Exception:\n            return None\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x\n        return y\n\n    # ---------- Warm start handling ----------\n    if prev_best_x is not None and budget > 0:\n        try:\n            prev_arr = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            prev_arr = np.clip(prev_arr, low, high)\n            prev_arr[~free_dims] = low[~free_dims]\n            y = try_eval(prev_arr)\n            if y is None:\n                best_x, best_y = None, np.inf\n        except Exception:\n            best_x, best_y = None, np.inf\n\n    # Adjust allocation for any warm-start evaluation already used\n    if evals_used > 0:\n        if global_budget > 0:\n            global_budget = max(0, global_budget - 1)\n        elif restart_budget > 0:\n            restart_budget = max(0, restart_budget - 1)\n        elif local_budget > 0:\n            local_budget = max(0, local_budget - 1)\n\n    # ---------- Tiny budgets: simplified logic ----------\n    if budget <= 2:\n        if best_x is not None:\n            return np.asarray(best_x, dtype=float).reshape(dim)\n\n        center = (low + high) / 2.0\n        try_eval(center)\n        if evals_used < budget:\n            xr = low + span * np.random.rand(dim)\n            try_eval(xr)\n        if best_x is None:\n            best_x = center\n        return np.asarray(best_x, dtype=float).reshape(dim)\n\n    # ---------- Phase 1: Global search ----------\n    if global_budget > 0 and evals_used < budget:\n        # Mix quasi-random and random; in high dim rely more on pure random\n        if dim <= 15:\n            n_qr = int(0.6 * global_budget)\n        else:\n            n_qr = int(0.4 * global_budget)\n        n_qr = max(0, min(global_budget, n_qr))\n        n_rand = global_budget - n_qr\n\n        if n_qr > 0:\n            samples = quasi_random_samples(n_qr)\n            for i in range(n_qr):\n                if evals_used >= budget:\n                    break\n                try_eval(samples[i])\n\n        for _ in range(n_rand):\n            if evals_used >= budget:\n                break\n            x = low + span * np.random.rand(dim)\n            try_eval(x)\n\n    # If nothing evaluated successfully, fall back to center\n    if best_x is None:\n        center = (low + high) / 2.0\n        try_eval(center)\n        if best_x is None:\n            best_x = center\n            best_y = np.inf\n\n    # ---------- Phase 2: restart-refine around best ----------\n    # Instead of only Gaussian jitter, we do several \"mini-restarts\" with shrinking radius\n    if restart_budget > 0 and evals_used < budget:\n        # Start radius: fairly large fraction of the box (dimension-scaled)\n        base_radius = 0.35 * safe_span / np.sqrt(max(dim, 1))\n        min_radius = 1e-3 * safe_span\n        # Batches of samples with geometrically decreasing radius\n        n_batches = min(4, max(1, restart_budget // max(2, dim // 2)))\n        per_batch = max(1, restart_budget // n_batches)\n\n        for b in range(n_batches):\n            if evals_used >= budget:\n                break\n            # Geometric shrink across batches\n            t_batch = b / max(1, n_batches - 1)\n            radius = base_radius * (0.3 ** t_batch)\n            radius = np.maximum(radius, min_radius)\n\n            for _ in range(per_batch):\n                if evals_used >= budget:\n                    break\n                # Directionally-normalized noise\n                noise = np.random.randn(dim)\n                nrm = np.linalg.norm(noise)\n                if nrm == 0.0:\n                    noise = np.ones(dim) / np.sqrt(dim)\n                else:\n                    noise /= nrm\n                step = noise * radius\n                cand_x = best_x + step\n                cand_x = np.clip(cand_x, low, high)\n                cand_x[~free_dims] = low[~free_dims]\n                try_eval(cand_x)\n\n    # ---------- Phase 3: Local search ----------\n    if local_budget > 0 and evals_used < budget:\n        # Step scales\n        base_step = 0.35 * safe_span / np.sqrt(max(dim, 1))\n        step_floor = 1e-4 * safe_span\n\n        # Diagonal covariance for Gaussian proposals\n        cov_diag = (0.15 * safe_span / np.sqrt(max(dim, 1))) ** 2\n        cov_min = (1e-6 * safe_span) ** 2\n        cov_max = (0.25 * safe_span) ** 2\n\n        success_counter = 0\n        total_counter = 0\n\n        # Track a \"current point\" that may lag behind best for short-term exploitation\n        cur_x = best_x.copy()\n        cur_y = best_y\n\n        for i in range(local_budget):\n            if evals_used >= budget:\n                break\n\n            if local_budget > 1:\n                t = i / (local_budget - 1)\n            else:\n                t = 0.0\n            # Stronger decay for later steps -> finer search near end\n            decay = 0.2 ** t\n            step = np.maximum(base_step * decay, step_floor)\n\n            mode = i % 6\n            if mode == 0:\n                # Full-dimensional Gaussian around current point\n                z = np.random.randn(dim) * np.sqrt(cov_diag)\n                cand_x = cur_x + z\n            elif mode == 1:\n                # Gaussian directly around global best for safety\n                z = np.random.randn(dim) * (0.6 * np.sqrt(cov_diag))\n                cand_x = best_x + z\n            elif mode == 2:\n                # Positive coordinate step on random coordinate\n                coord = np.random.randint(dim)\n                cand_x = cur_x.copy()\n                cand_x[coord] += step[coord]\n            elif mode == 3:\n                # Negative coordinate step\n                coord = np.random.randint(dim)\n                cand_x = cur_x.copy()\n                cand_x[coord] -= step[coord]\n            elif mode == 4:\n                # Uniform small box around current point\n                u = (np.random.rand(dim) * 2.0 - 1.0) * step\n                cand_x = cur_x + u\n            else:\n                # Narrow Gaussian around best_x with very small variance\n                z = np.random.randn(dim) * (0.4 * step)\n                cand_x = best_x + z\n\n            cand_x = np.clip(cand_x, low, high)\n            cand_x[~free_dims] = low[~free_dims]\n\n            prev_best_y = best_y\n            y = try_eval(cand_x)\n\n            if y is None:\n                continue\n\n            # Update current point if it improved local or global best\n            if y < cur_y:\n                cur_x, cur_y = cand_x, y\n\n            total_counter += 1\n            if best_y < prev_best_y - 1e-12:\n                success_counter += 1\n                # Move covariance toward squared step from best to candidate\n                diff = best_x - cand_x\n                cov_diag = 0.9 * cov_diag + 0.1 * (diff * diff + 1e-12)\n\n            # Periodic covariance adaptation\n            if total_counter >= 10:\n                rate = success_counter / total_counter\n                if rate > 0.35:\n                    cov_diag *= 1.35\n                elif rate < 0.15:\n                    cov_diag *= 0.55\n                cov_diag = np.clip(cov_diag, cov_min, cov_max)\n                success_counter = 0\n                total_counter = 0\n\n    best_x = np.asarray(best_x, dtype=float).reshape(dim)\n    return best_x",
    "X": "0.0018672669932929557 -0.003133209814233798 0.0140263135890509 0.008027930382356732 0.007294494924373398 0.004539845440649329 0.012690479704884268"
}