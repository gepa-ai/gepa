{
    "score": 0.00017842053099555608,
    "Input": "Sphere",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budget-aware hybrid optimizer:\n      - Robust warm start from prev_best_x\n      - Global sampling (quasi-random + random)\n      - Adaptive population-based refinement (ES-style)\n      - Local search with adaptive Gaussian / coordinate steps\n      - Dimension- and budget-aware allocation\n\n    Returns: best x found as np.ndarray of shape (dim,)\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Degenerate config: return box center\n    if dim <= 0 or budget <= 0:\n        center = (low + high) / 2.0\n        return center.astype(float).reshape(dim)\n\n    # Ensure span is non-zero; track degenerate dims (collapsed bounds)\n    safe_span = np.where(span > 0, span, 1.0)\n    free_dims = span > 0\n\n    # ---- Phase allocation: global / ES-refine / local ----\n    # Slight tilt towards more structured refinement (ES) for medium/large budgets\n    if budget < 15:\n        global_frac = 0.75\n        refine_frac = 0.0\n    elif budget < 60:\n        global_frac = 0.6 if dim <= 10 else 0.7\n        refine_frac = 0.15\n    elif budget < 200:\n        global_frac = 0.5 if dim <= 10 else 0.6\n        refine_frac = 0.25\n    else:\n        global_frac = 0.4 if dim <= 10 else 0.55\n        refine_frac = 0.3\n\n    local_frac = max(0.0, 1.0 - global_frac - refine_frac)\n\n    global_budget = max(1, int(budget * global_frac))\n    refine_budget = max(0, int(budget * refine_frac))\n    local_budget = max(0, budget - global_budget - refine_budget)\n\n    # Fix rounding\n    total_alloc = global_budget + refine_budget + local_budget\n    if total_alloc < budget:\n        global_budget += budget - total_alloc\n    elif total_alloc > budget:\n        global_budget = max(0, global_budget - (total_alloc - budget))\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # -------- Helpers --------\n    def quasi_random_samples(n_samples, base_seed=None):\n        \"\"\"Simple van der Corput-based low-discrepancy sampler in [low, high].\"\"\"\n        if n_samples <= 0:\n            return np.empty((0, dim), dtype=float)\n\n        # Small prime bases; repeat if dim is large\n        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23]\n        bases = [primes[i % len(primes)] for i in range(dim)]\n\n        if base_seed is None:\n            start = np.random.randint(0, 10000)\n        else:\n            start = int(base_seed)\n\n        def vdc(n, base):\n            v = 0.0\n            denom = 1.0\n            while n:\n                n, r = divmod(n, base)\n                denom *= base\n                v += r / denom\n            return v\n\n        xs = np.empty((n_samples, dim), dtype=float)\n        for i in range(n_samples):\n            idx = start + i\n            for d in range(dim):\n                xs[i, d] = vdc(idx, bases[d])\n\n        # Randomly permute each dimension to reduce structure\n        for d in range(dim):\n            perm = np.random.permutation(n_samples)\n            xs[:, d] = xs[perm, d]\n\n        xs = low + span * xs\n        return xs\n\n    def try_eval(x):\n        \"\"\"Safe evaluation with budget check and best update.\"\"\"\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = np.asarray(x, dtype=float).reshape(dim)\n        x = np.clip(x, low, high)\n        x[~free_dims] = low[~free_dims]\n        try:\n            y = objective_function(x)\n        except Exception:\n            return None\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x\n        return y\n\n    # ---------- Warm start handling ----------\n    if prev_best_x is not None and budget > 0:\n        try:\n            prev_arr = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            prev_arr = np.clip(prev_arr, low, high)\n            prev_arr[~free_dims] = low[~free_dims]\n            y = try_eval(prev_arr)\n            if y is None:\n                best_x, best_y = None, np.inf\n        except Exception:\n            best_x, best_y = None, np.inf\n\n    # Adjust allocation for any warm-start evaluation already used\n    if evals_used > 0:\n        if global_budget > 0:\n            global_budget = max(0, global_budget - 1)\n        elif refine_budget > 0:\n            refine_budget = max(0, refine_budget - 1)\n        elif local_budget > 0:\n            local_budget = max(0, local_budget - 1)\n\n    # ---------- Tiny budgets: simplified logic ----------\n    if budget <= 2:\n        if best_x is not None:\n            return np.asarray(best_x, dtype=float).reshape(dim)\n\n        center = (low + high) / 2.0\n        try_eval(center)\n        if evals_used < budget:\n            xr = low + span * np.random.rand(dim)\n            try_eval(xr)\n        if best_x is None:\n            best_x = center\n        return np.asarray(best_x, dtype=float).reshape(dim)\n\n    # ---------- Phase 1: Global search ----------\n    if global_budget > 0 and evals_used < budget:\n        # Mix quasi-random and random; in high dim rely more on pure random\n        if dim <= 15:\n            n_qr = int(0.7 * global_budget)\n        else:\n            n_qr = int(0.5 * global_budget)\n        n_qr = max(0, min(global_budget, n_qr))\n        n_rand = global_budget - n_qr\n\n        if n_qr > 0:\n            samples = quasi_random_samples(n_qr)\n            for i in range(n_qr):\n                if evals_used >= budget:\n                    break\n                try_eval(samples[i])\n\n        for _ in range(n_rand):\n            if evals_used >= budget:\n                break\n            x = low + span * np.random.rand(dim)\n            try_eval(x)\n\n    # If nothing evaluated successfully, fall back to center\n    if best_x is None:\n        center = (low + high) / 2.0\n        try_eval(center)\n        if best_x is None:\n            best_x = center\n            best_y = np.inf\n\n    # ---------- Phase 2: ES-style population refinement around best ----------\n    # Use a simple (\u03bc, \u03bb)-ES with adaptive step size, well-suited for smooth functions (e.g., Sphere)\n    if refine_budget > 0 and evals_used < budget:\n        # Choose population size proportional to dimension but controlled by budget\n        # \u03bb: offspring, \u03bc: parents\n        max_generations = max(1, refine_budget // max(4, 2 * dim))\n        if max_generations < 1:\n            max_generations = 1\n\n        # Allocate offspring per generation\n        # Ensure at least a few individuals per generation\n        lam = max(4, refine_budget // max_generations)\n        lam = min(lam, max(6 * dim, 40))  # avoid too large \u03bb\n        mu = max(2, lam // 4)\n\n        # Initial mean and step size\n        mean = best_x.copy()\n        # Start with moderate radius relative to box, shrinks via adaptation\n        sigma = 0.25 * np.linalg.norm(safe_span) / np.sqrt(max(dim, 1))\n        if sigma <= 0:\n            sigma = 1.0\n\n        # Element-wise min/max step to avoid exploding / vanishing\n        sigma_min = 1e-6 * np.max(safe_span)\n        sigma_max = 0.5 * np.max(safe_span)\n\n        # Evolution path / success tracking (simple 1/5th rule variant)\n        success_rate_target = 0.2\n        gen_evals = 0\n\n        for gen in range(max_generations):\n            if evals_used >= budget or gen_evals >= refine_budget:\n                break\n\n            # Number of offspring this generation (respect remaining budget)\n            remaining = min(refine_budget - gen_evals, budget - evals_used)\n            if remaining <= 0:\n                break\n            k = min(lam, remaining)\n\n            # Generate offspring\n            noise = np.random.randn(k, dim)\n            offspring = mean + sigma * noise\n            offspring = np.clip(offspring, low, high)\n            offspring[:, ~free_dims] = low[~free_dims]\n\n            ys = np.empty(k, dtype=float)\n            for i in range(k):\n                if evals_used >= budget:\n                    ys[i] = np.inf\n                    continue\n                y = try_eval(offspring[i])\n                if y is None:\n                    ys[i] = np.inf\n                else:\n                    ys[i] = y\n            gen_evals += k\n\n            # If all evaluations failed, skip this generation\n            if not np.isfinite(ys).any():\n                continue\n\n            # Sort by fitness\n            idx = np.argsort(ys)\n            ys_sorted = ys[idx]\n            xs_sorted = offspring[idx]\n\n            # Select \u03bc best as parents (ignore entries with inf)\n            finite_mask = np.isfinite(ys_sorted)\n            if not finite_mask.any():\n                continue\n            finite_indices = np.where(finite_mask)[0]\n            use_mu = min(mu, len(finite_indices))\n            parents = xs_sorted[finite_indices[:use_mu]]\n\n            # Update mean (recombination)\n            new_mean = np.mean(parents, axis=0)\n\n            # Success = improvement over previous best_y\n            prev_best = best_y\n            if ys_sorted[finite_indices[0]] < prev_best - 1e-12:\n                success = 1.0\n            else:\n                success = 0.0\n\n            # Simple step-size adaptation (1/5th success rule style)\n            adjust = np.exp((success - success_rate_target) * 0.6 / max(dim, 1))\n            sigma *= adjust\n            sigma = float(np.clip(sigma, sigma_min, sigma_max))\n\n            mean = new_mean\n\n    # ---------- Phase 3: Local search ----------\n    if local_budget > 0 and evals_used < budget:\n        # Step scales\n        base_step = 0.35 * safe_span / np.sqrt(max(dim, 1))\n        step_floor = 1e-4 * safe_span\n\n        # Diagonal covariance for Gaussian proposals\n        cov_diag = (0.15 * safe_span / np.sqrt(max(dim, 1))) ** 2\n        cov_min = (1e-6 * safe_span) ** 2\n        cov_max = (0.25 * safe_span) ** 2\n\n        success_counter = 0\n        total_counter = 0\n\n        # Track a \"current point\" that may lag behind best for short-term exploitation\n        cur_x = best_x.copy()\n        cur_y = best_y\n\n        for i in range(local_budget):\n            if evals_used >= budget:\n                break\n\n            if local_budget > 1:\n                t = i / (local_budget - 1)\n            else:\n                t = 0.0\n            # Stronger decay for later steps -> finer search near end\n            decay = 0.2 ** t\n            step = np.maximum(base_step * decay, step_floor)\n\n            mode = i % 6\n            if mode == 0:\n                # Full-dimensional Gaussian around current point\n                z = np.random.randn(dim) * np.sqrt(cov_diag)\n                cand_x = cur_x + z\n            elif mode == 1:\n                # Gaussian directly around global best for safety\n                z = np.random.randn(dim) * (0.6 * np.sqrt(cov_diag))\n                cand_x = best_x + z\n            elif mode == 2:\n                # Positive coordinate step on random coordinate\n                coord = np.random.randint(dim)\n                cand_x = cur_x.copy()\n                cand_x[coord] += step[coord]\n            elif mode == 3:\n                # Negative coordinate step\n                coord = np.random.randint(dim)\n                cand_x = cur_x.copy()\n                cand_x[coord] -= step[coord]\n            elif mode == 4:\n                # Uniform small box around current point\n                u = (np.random.rand(dim) * 2.0 - 1.0) * step\n                cand_x = cur_x + u\n            else:\n                # Narrow Gaussian around best_x with very small variance\n                z = np.random.randn(dim) * (0.4 * step)\n                cand_x = best_x + z\n\n            cand_x = np.clip(cand_x, low, high)\n            cand_x[~free_dims] = low[~free_dims]\n\n            prev_best_y = best_y\n            y = try_eval(cand_x)\n\n            if y is None:\n                continue\n\n            # Update current point if it improved local or global best\n            if y < cur_y:\n                cur_x, cur_y = cand_x, y\n\n            total_counter += 1\n            if best_y < prev_best_y - 1e-12:\n                success_counter += 1\n                # Move covariance toward squared step from best to candidate\n                diff = best_x - cand_x\n                cov_diag = 0.9 * cov_diag + 0.1 * (diff * diff + 1e-12)\n\n            # Periodic covariance adaptation\n            if total_counter >= 10:\n                rate = success_counter / total_counter\n                if rate > 0.35:\n                    cov_diag *= 1.35\n                elif rate < 0.15:\n                    cov_diag *= 0.55\n                cov_diag = np.clip(cov_diag, cov_min, cov_max)\n                success_counter = 0\n                total_counter = 0\n\n    best_x = np.asarray(best_x, dtype=float).reshape(dim)\n    return best_x",
    "X": "-0.0073742611333054995 -0.002822965316058473 0.005835792745603445 0.005408777249880111 0.00692878605227256 0.002122350456368167 -0.0004978700949399271"
}