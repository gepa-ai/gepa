{
    "score": -3.4509760383677888,
    "Input": "McCourt08",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization using a hybrid global + local search.\n\n    Strategy:\n    1. Use prev_best_x as a starting point if available and inside bounds.\n    2. Use ~40% of the budget for global exploration via quasi-random (Sobol-like) sampling.\n    3. Maintain and refine multiple elites instead of a single point.\n    4. Use the remaining budget for local Gaussian search around the elites with\n       adaptive step sizes and occasional larger jumps.\n    \"\"\"\n    rng = np.random.RandomState()\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n    span[span <= 0] = 1.0  # prevent degenerate ranges\n\n    def project(x):\n        return np.minimum(upper, np.maximum(lower, x))\n\n    def sobol_like(n, d, rng_local):\n        \"\"\"Lightweight low-discrepancy-like sampler using scrambled Halton-style bases.\"\"\"\n        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n        bases = primes[:d]\n        start = rng_local.randint(0, 10000)\n        pts = np.empty((n, d), dtype=float)\n        for j, b in enumerate(bases):\n            i = np.arange(start, start + n)\n            f = 1.0\n            x = np.zeros(n, dtype=float)\n            while True:\n                f /= b\n                i, r = divmod(i, b)\n                x += f * r\n                if not i.any():\n                    break\n            pts[:, j] = x\n        if d > len(bases):\n            # fill remaining dims with uniform noise\n            pts[:, len(bases):] = rng_local.rand(n, d - len(bases))\n        return pts\n\n    evals_used = 0\n\n    # Initialize elite set\n    # Start with prev_best_x if valid; otherwise random\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = project(x0)\n        else:\n            x0 = rng.uniform(lower, upper)\n    else:\n        x0 = rng.uniform(lower, upper)\n\n    best_x = x0.copy()\n    best_y = objective_function(best_x)\n    evals_used += 1\n    if evals_used >= budget:\n        return best_x\n\n    # Global exploration budget\n    global_frac = 0.4\n    remaining_budget = budget - evals_used\n    global_budget = max(1, int(budget * global_frac))\n    global_budget = min(global_budget, remaining_budget)\n\n    # Use a small elite set to capture multiple good regions\n    elite_size = max(3, min(10, global_budget // 5 if global_budget >= 5 else 1))\n    elites_x = [best_x.copy()]\n    elites_y = [best_y]\n\n    if global_budget > 0:\n        # Sobol-like sampling in [0, 1]^dim then scale to bounds\n        u = sobol_like(global_budget, dim, rng)\n        samples = lower + u * span\n\n        for i in range(global_budget):\n            x = samples[i]\n            y = objective_function(x)\n            evals_used += 1\n\n            # Update global best\n            if y < best_y:\n                best_y = y\n                best_x = x\n\n            # Maintain elite set\n            if len(elites_x) < elite_size:\n                elites_x.append(x)\n                elites_y.append(y)\n            else:\n                worst_idx = int(np.argmax(elites_y))\n                if y < elites_y[worst_idx]:\n                    elites_x[worst_idx] = x\n                    elites_y[worst_idx] = y\n\n            if evals_used >= budget:\n                return best_x\n\n    # Local search phase\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    elites_x = [project(e) for e in elites_x]\n    elites_y = [float(v) for v in elites_y]\n    # Sort elites by quality\n    order = np.argsort(elites_y)\n    elites_x = [elites_x[i] for i in order]\n    elites_y = [elites_y[i] for i in order]\n\n    # Base step size: smaller than before to encourage finer local search\n    base_sigma = 0.15 * span\n    base_sigma[base_sigma == 0] = 0.1\n\n    # Local search loop\n    for i in range(remaining):\n        # Exponential decay but with a floor to avoid freezing\n        t = i / max(1.0, remaining - 1)\n        decay = 0.4 ** t  # slower decay\n        sigma = base_sigma * decay\n        sigma = np.maximum(sigma, 0.02 * span)\n\n        # Occasionally reset to exploratory step to escape local minima\n        if (i % 15) == 0:\n            sigma = base_sigma\n\n        # Choose elite to perturb: bias towards best but not exclusively\n        if len(elites_x) > 1:\n            # Geometric distribution over ranks\n            probs = np.array([0.6 ** k for k in range(len(elites_x))], dtype=float)\n            probs /= probs.sum()\n            idx = rng.choice(len(elites_x), p=probs)\n        else:\n            idx = 0\n\n        center = elites_x[idx]\n        candidate = center + rng.normal(0.0, sigma, size=dim)\n        candidate = project(candidate)\n        y = objective_function(candidate)\n        evals_used += 1\n\n        # Update global best\n        if y < best_y:\n            best_y = y\n            best_x = candidate\n\n        # Update elites\n        worst_idx = int(np.argmax(elites_y))\n        if y < elites_y[worst_idx]:\n            elites_x[worst_idx] = candidate\n            elites_y[worst_idx] = y\n            # Keep elites ordered roughly best-first\n            order = np.argsort(elites_y)\n            elites_x = [elites_x[j] for j in order]\n            elites_y = [elites_y[j] for j in order]\n\n        if evals_used >= budget:\n            break\n\n    return best_x",
    "X": "0.5317841920403381 1.0 0.5863264866549037 0.09307399357533414"
}