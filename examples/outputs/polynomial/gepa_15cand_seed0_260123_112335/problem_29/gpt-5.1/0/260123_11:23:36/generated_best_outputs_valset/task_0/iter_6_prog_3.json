{
    "score": -1.4282269180670293,
    "Input": "McCourt18",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef _project_bounds(x, bounds):\n    return np.clip(x, bounds[:, 0], bounds[:, 1])\n\n\ndef _random_in_bounds(bounds, n=1, rng=None):\n    if rng is None:\n        rng = np.random\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    return rng.uniform(low, high, size=(n, bounds.shape[0]))\n\n\ndef _lhs_samples(bounds, n, rng):\n    \"\"\"Latin-hypercube-like sampling within bounds.\"\"\"\n    dim = bounds.shape[0]\n    if n <= 0:\n        return np.empty((0, dim), dtype=float)\n    m = n\n    base = (np.arange(m) + rng.rand(m)) / m\n    X_unit = np.empty((m, dim), dtype=float)\n    for j in range(dim):\n        rng.shuffle(base)\n        X_unit[:, j] = base\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    return low + (high - low) * X_unit\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid global-local derivative-free optimizer.\n\n    Strategy:\n    - Mixed initialization: prev_best_x, center, random / LHS.\n    - Global exploration (LHS/random) with low-discrepancy coverage.\n    - Adaptive local search (CMA-like) with restart / escape when stagnating.\n    \"\"\"\n    # Use a seeded RNG if provided (for potential reproducibility),\n    # otherwise fall back to high-entropy default.\n    seed = config.get(\"seed\", None)\n    rng = np.random.RandomState(seed) if seed is not None else np.random.RandomState()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    if bounds.shape[0] != dim:\n        bounds = bounds.reshape(dim, 2)\n\n    if budget <= 0:\n        # Still return something valid\n        return _random_in_bounds(bounds, n=1, rng=rng)[0]\n\n    widths = bounds[:, 1] - bounds[:, 0]\n    widths[widths == 0] = 1.0\n    center = (bounds[:, 0] + bounds[:, 1]) * 0.5\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # --- Helper to safely evaluate with budget tracking ---\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # --- Initialization / warm start ---\n    # Build a small set of distinct starting points\n    init_points = []\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n        x0 = _project_bounds(x0, bounds)\n        init_points.append(x0)\n\n    # Always include center of bounds\n    init_points.append(center.copy())\n\n    # One purely random point\n    init_points.append(_random_in_bounds(bounds, n=1, rng=rng)[0])\n\n    # Make them unique-ish\n    unique_init = []\n    for x in init_points:\n        if not unique_init:\n            unique_init.append(x)\n        else:\n            if np.all(np.linalg.norm(np.asarray(unique_init) - x, axis=1) > 1e-9):\n                unique_init.append(x)\n\n    for x in unique_init:\n        if evals_used >= budget:\n            break\n        eval_point(x)\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Global exploration phase ---\n    # Allocate ~50% of remaining budget to global search (but at least dim)\n    global_evals = int(0.5 * budget)\n    global_evals = max(dim, global_evals)\n    global_evals = min(global_evals, remaining)\n\n    if global_evals > 0:\n        # Use LHS if dimension is modest, otherwise fallback to random\n        if dim <= 50:\n            X_global = _lhs_samples(bounds, global_evals, rng)\n        else:\n            X_global = _random_in_bounds(bounds, n=global_evals, rng=rng)\n\n        for i in range(global_evals):\n            if evals_used >= budget:\n                break\n            x = X_global[i]\n            eval_point(x)\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Local evolutionary / CMA-like search phase with stagnation detection ---\n    # Base step size relative to domain width\n    base_sigma = widths / 5.0\n\n    # Population parameters tuned by dimension and remaining budget\n    # Slightly larger population for robustness, capped for efficiency\n    pop_size = max(6, min(30, 6 + dim // 2))\n    elite_frac = 0.35\n    n_elite = max(2, int(elite_frac * pop_size))\n\n    # How many total generations we can afford\n    max_gens = max(1, remaining // pop_size)\n\n    # Allow a limited number of \"soft restarts\" if stuck\n    max_restarts = 2\n    restart_count = 0\n\n    # Initialize search around current best\n    mean = best_x.copy()\n    sigma = base_sigma.copy()\n\n    # Stagnation tracking\n    no_improve_gens = 0\n    max_no_improve = max(3, min(10, dim // 2 + 3))\n\n    gen = 0\n    while gen < max_gens and evals_used < budget:\n        # Anneal global scale of sigma over total generations\n        if max_gens > 1:\n            frac = 1.0 - gen / (max_gens - 1)\n            scale = 0.25 + 0.75 * frac  # start larger, end smaller\n        else:\n            scale = 1.0\n        gen_sigma = sigma * scale\n\n        # Sample population around current mean\n        pop = mean + rng.randn(pop_size, dim) * gen_sigma\n        pop = _project_bounds(pop, bounds)\n\n        ys = np.empty(pop_size, dtype=float)\n        any_improve = False\n\n        for i in range(pop_size):\n            if evals_used >= budget:\n                ys[i] = np.inf\n                continue\n            y = eval_point(pop[i])\n            ys[i] = y\n            if y <= best_y + 1e-12:\n                any_improve = True\n\n        if evals_used >= budget:\n            break\n\n        # Sort by fitness\n        elite_idx = np.argsort(ys)[:n_elite]\n        elites = pop[elite_idx]\n        elite_ys = ys[elite_idx]\n\n        # Update mean with elites (weighted slightly toward best)\n        weights = np.linspace(1.0, 0.2, n_elite)\n        weights /= weights.sum()\n        new_mean = np.sum(elites * weights[:, None], axis=0)\n\n        # Safeguard: keep mean tied to global best to avoid drifting away\n        mean = 0.6 * mean + 0.4 * new_mean\n        mean = 0.7 * mean + 0.3 * best_x  # pull toward best found so far\n\n        # Adapt sigma using spread of elites (coordinate-wise)\n        spread = np.std(elites, axis=0)\n        target_sigma = np.maximum(spread * 2.0, widths * 1e-4)\n        sigma = 0.6 * sigma + 0.4 * target_sigma\n\n        # Bound sigma so we don't explode or collapse too much\n        sigma = np.minimum(sigma, widths)\n        sigma = np.maximum(sigma, widths * 1e-4)\n\n        # Stagnation detection\n        if any_improve:\n            no_improve_gens = 0\n        else:\n            no_improve_gens += 1\n\n        # Soft restart if stagnating: re-center around global best_x,\n        # increase exploration radius moderately\n        if no_improve_gens >= max_no_improve and restart_count < max_restarts:\n            restart_count += 1\n            no_improve_gens = 0\n            mean = best_x.copy()\n            # Increase sigma to escape local basin\n            sigma = np.minimum(widths / 3.0, np.maximum(widths * 0.02, sigma * 1.8))\n\n        gen += 1\n\n    # --- Final local refinement around best_x (small random search) ---\n    remaining = budget - evals_used\n    if remaining > 0:\n        # Use a small, anisotropic step based on last sigma and domain size\n        local_sigma = np.minimum(widths * 0.05, np.maximum(widths * 1e-4, sigma * 0.5))\n        for _ in range(remaining):\n            step = rng.randn(dim) * local_sigma\n            x_candidate = _project_bounds(best_x + step, bounds)\n            eval_point(x_candidate)\n            if evals_used >= budget:\n                break\n\n    return best_x",
    "X": "0.26608009566525775 0.8611602634163995 0.2613561158481016 0.6654306470302798 0.13609066726081548 0.9601488681119685 0.05578349132342603 0.2957170804678391"
}