{
    "score": 0.0046106510682078945,
    "Input": "HelicalValley",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid black-box optimizer with warm start support and budget-aware tuning.\n\n    Strategy:\n    - Robust initialization using warm start (if valid) plus diversified sampling.\n    - Global search using quasi-random (low-discrepancy style) sampling.\n    - Local search using adaptive Gaussian steps and coordinate-wise perturbations.\n    - Dynamic budget split and simple restart logic for robustness, especially on\n      narrow valleys and curved manifolds (e.g. HelicalValley).\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # No/negative budget: return center of box\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    # Utility: clip into bounds\n    def clip_to_bounds(x):\n        return np.minimum(high, np.maximum(low, x))\n\n    rng = np.random.default_rng()\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    # --- Warm start if available and valid ---\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.size == dim:\n                x0 = clip_to_bounds(x0)\n                y0 = objective_function(x0)\n                evals_used += 1\n                best_x, best_y = x0, y0\n        except Exception:\n            best_x, best_y = None, None\n\n    # --- Always include at least one random initialization for robustness ---\n    if evals_used < budget:\n        x_rand = rng.uniform(low, high)\n        y_rand = objective_function(x_rand)\n        evals_used += 1\n        if best_x is None or y_rand < best_y:\n            best_x, best_y = x_rand, y_rand\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Initial global exploration: diversified sampling ---\n    # Use more diverse samples for higher dimensions, but cap cost.\n    init_global = min(remaining, max(4, min(20, 3 * dim)))\n    for _ in range(init_global):\n        x = rng.uniform(low, high)\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = x, y\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Decide initial budget split between global and local ---\n    # For small budgets, rely more on global; for larger, favor local refinement.\n    if remaining < 30:\n        global_budget = max(1, int(0.7 * remaining))\n    elif remaining < 100:\n        global_budget = int(0.55 * remaining)\n    else:\n        global_budget = int(0.45 * remaining)\n    global_budget = max(1, min(global_budget, remaining - 1))\n    local_budget = remaining - global_budget\n\n    # --- Global search: quasi-random (low-discrepancy style) sampling ---\n    base = rng.random(dim)\n    step = (np.sqrt(5.0) - 1.0) / 2.0  # golden ratio conjugate\n    # Use scrambled direction to avoid alignment in high dimension\n    step_vec = (step * (rng.permutation(dim) + 1)) % 1.0\n\n    global_improvements = 0\n    g_evals = 0\n    last_global_best = best_y\n    while g_evals < global_budget and evals_used < budget:\n        u = (base + g_evals * step_vec) % 1.0\n        x = low + u * span\n        y = objective_function(x)\n        evals_used += 1\n        g_evals += 1\n        if y < best_y:\n            best_x, best_y = x, y\n            global_improvements += 1\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Reallocate based on success of global search\n    if global_improvements == 0:\n        # Global did not help: focus on local search but keep tiny global budget for safety\n        extra_global = min(remaining // 10, 2)\n    else:\n        extra_global = min(remaining // 5, global_improvements * 2)\n\n    local_budget = max(1, remaining - extra_global)\n    global_tail_budget = remaining - local_budget\n\n    # --- Local search parameters ---\n    # Slightly larger base sigma to better traverse curved valleys (e.g., HelicalValley)\n    base_sigma = 0.3 * span\n    # Avoid degenerate zero-span dimensions\n    base_sigma[base_sigma == 0] = 1.0\n\n    # Phases of decay for sigma\n    phases = min(6, max(2, int(np.log2(local_budget + 4))))\n    steps_per_phase = max(1, local_budget // phases)\n    # Geometric decay with a bit slower rate for better exploration\n    sigmas = [base_sigma * (0.6 ** p) for p in range(phases)]\n\n    no_improve_streak = 0\n    local_improvements = 0\n\n    # --- Local search loop ---\n    for phase in range(phases):\n        if evals_used >= budget:\n            break\n        sigma = sigmas[phase]\n        phase_steps = min(steps_per_phase, budget - evals_used)\n        for _ in range(phase_steps):\n            if evals_used >= budget:\n                break\n\n            # Mixture of full-dimensional, coordinate, and two-coordinate perturbations\n            r = rng.random()\n            if r < 0.55:\n                noise = rng.normal(0.0, sigma, size=dim)\n            elif r < 0.85:\n                noise = np.zeros(dim)\n                idx = rng.integers(0, dim)\n                noise[idx] = rng.normal(0.0, sigma[idx])\n            else:\n                noise = np.zeros(dim)\n                idxs = rng.choice(dim, size=min(2, dim), replace=False)\n                noise[idxs] = rng.normal(0.0, sigma[idxs])\n\n            x = clip_to_bounds(best_x + noise)\n            y = objective_function(x)\n            evals_used += 1\n\n            if y < best_y:\n                best_x, best_y = x, y\n                no_improve_streak = 0\n                local_improvements += 1\n            else:\n                no_improve_streak += 1\n\n            # Adaptive restart: if stuck, perform a short global restart near best and random\n            if no_improve_streak >= 12 and evals_used < budget:\n                # One random jump\n                x_jump = rng.uniform(low, high)\n                y_jump = objective_function(x_jump)\n                evals_used += 1\n                if y_jump < best_y:\n                    best_x, best_y = x_jump, y_jump\n                    local_improvements += 1\n                    no_improve_streak = 0\n                    continue\n\n                # One jump around current best with larger sigma\n                big_sigma = np.maximum(sigma * 2.0, 0.2 * span)\n                noise_big = rng.normal(0.0, big_sigma, size=dim)\n                x_jump2 = clip_to_bounds(best_x + noise_big)\n                y_jump2 = objective_function(x_jump2)\n                evals_used += 1\n                if y_jump2 < best_y:\n                    best_x, best_y = x_jump2, y_jump2\n                    local_improvements += 1\n                no_improve_streak = 0\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Tail global search if budget remains and was allocated ---\n    tail_budget = min(global_tail_budget, remaining)\n    if tail_budget > 0:\n        base_tail = rng.random(dim)\n        for i in range(tail_budget):\n            if evals_used >= budget:\n                break\n            u = (base_tail + i * step_vec) % 1.0\n            x = low + u * span\n            y = objective_function(x)\n            evals_used += 1\n            if y < best_y:\n                best_x, best_y = x, y\n\n    return best_x",
    "X": "0.99488381031282 0.030367335432858092 0.04703732756765519"
}