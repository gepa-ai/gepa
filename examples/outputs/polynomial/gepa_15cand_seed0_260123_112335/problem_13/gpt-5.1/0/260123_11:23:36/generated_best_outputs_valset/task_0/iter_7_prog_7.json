{
    "score": 0.0015272020504006247,
    "Input": "HelicalValley",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid black-box optimizer with warm start support and budget-aware tuning.\n\n    Strategy (slightly simplified and stabilized vs previous version):\n    - Careful warm start handling and always at least one random sample.\n    - Initial diversified global sampling (random + around-best).\n    - Quasi-random global exploration (low-discrepancy, golden-ratio based).\n    - Stabilized local search:\n        * Anisotropic Gaussian steps with curvature heuristic.\n        * Simpler stagnation handling and restarts.\n        * Step sizes adapt with dimension and budget.\n    - Tail global search with any leftover budget.\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # No/negative budget: return center of box\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    # Utility: clip into bounds\n    def clip_to_bounds(x):\n        return np.minimum(high, np.maximum(low, x))\n\n    rng = np.random.default_rng()\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    # --- Warm start if available and valid ---\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.size == dim:\n                x0 = clip_to_bounds(x0)\n                y0 = objective_function(x0)\n                evals_used += 1\n                best_x, best_y = x0, y0\n        except Exception:\n            best_x, best_y = None, None\n\n    # --- Always include at least one random initialization for robustness ---\n    if evals_used < budget:\n        x_rand = rng.uniform(low, high)\n        y_rand = objective_function(x_rand)\n        evals_used += 1\n        if best_x is None or y_rand < best_y:\n            best_x, best_y = x_rand, y_rand\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Initial diversified sampling (global exploration) ---\n    # Slightly stronger than pure single random, but keep cost modest.\n    # Scale with dimension and budget but add hard caps.\n    init_global = min(remaining, max(4, min(10 * dim, 40, remaining // 2)))\n\n    # Track simple curvature/sensitivity signal per coordinate\n    coord_activity = np.ones(dim)\n\n    for _ in range(init_global):\n        if evals_used >= budget:\n            break\n        # Mix pure random and biased sampling around current best\n        if rng.random() < 0.35:\n            # Around best, scaled by coord_activity (more in \"flatter\" dirs)\n            local_scale = 0.25\n            step = rng.normal(0.0, local_scale, size=dim) * (span + 1e-9) / coord_activity\n            x = clip_to_bounds(best_x + step)\n        else:\n            x = rng.uniform(low, high)\n\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            delta = np.abs(x - best_x) / (span + 1e-12)\n            # Bigger normalized delta -> likely flatter direction -> lower \"activity\"\n            coord_activity *= (1.0 + 0.25 * delta)\n            coord_activity = np.clip(coord_activity, 0.3, 4.0)\n            best_x, best_y = x, y\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Decide global vs local budget for the main phase ---\n    # For small budgets, emphasize global; for larger, emphasize local.\n    if remaining < 30:\n        global_main_budget = max(1, int(0.6 * remaining))\n    elif remaining < 100:\n        global_main_budget = int(0.45 * remaining)\n    else:\n        global_main_budget = int(0.35 * remaining)\n\n    # Ensure at least minimal local phase if budget allows\n    global_main_budget = max(1, min(global_main_budget, remaining - 1))\n    local_budget = remaining - global_main_budget\n\n    # --- Global main search: low-discrepancy style sampling ---\n    base = rng.random(dim)\n    # Golden ratio conjugate\n    phi_conj = (np.sqrt(5.0) - 1.0) / 2.0\n    # Use a relatively irrational direction, scaled per permuted dims\n    step_vec = (phi_conj * (rng.permutation(dim) + 1)) % 1.0\n\n    global_improvements = 0\n    g_evals = 0\n    while g_evals < global_main_budget and evals_used < budget:\n        u = (base + g_evals * step_vec) % 1.0\n        x = low + u * span\n        y = objective_function(x)\n        evals_used += 1\n        g_evals += 1\n        if y < best_y:\n            delta = np.abs(x - best_x) / (span + 1e-12)\n            coord_activity *= (1.0 + 0.2 * delta)\n            coord_activity = np.clip(coord_activity, 0.3, 4.5)\n            best_x, best_y = x, y\n            global_improvements += 1\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Local search setup ---\n    # Base scale: conservative, scaled with dimension.\n    # For higher dim, reduce relative step size a bit.\n    dim_factor = 1.0 / np.sqrt(max(1, dim))\n    base_sigma = (0.25 * dim_factor) * span\n    base_sigma[base_sigma == 0] = 1.0  # avoid zero span dims\n\n    # Adaptive diagonal scaling (approximate inverse curvature)\n    diag_scale = 1.0 / coord_activity\n    diag_scale = np.clip(diag_scale, 0.5, 3.0)\n\n    local_budget = min(local_budget, remaining)\n    if local_budget <= 0:\n        # Fallback: use remaining budget for extra global search\n        extra_global_budget = remaining\n        for i in range(extra_global_budget):\n            if evals_used >= budget:\n                break\n            u = (base + (g_evals + i) * step_vec) % 1.0\n            x = low + u * span\n            y = objective_function(x)\n            evals_used += 1\n            if y < best_y:\n                best_x, best_y = x, y\n        return best_x\n\n    # Number of stages determined by log of budget, but bounded\n    stages = min(8, max(2, int(np.log2(local_budget + 4))))\n    steps_per_stage = max(1, local_budget // stages)\n    no_improve_streak = 0\n    local_improvements = 0\n\n    for stage in range(stages):\n        if evals_used >= budget:\n            break\n\n        # Progressive shrinking of sigma (gentler factor)\n        stage_sigma = base_sigma * (0.7 ** stage) * diag_scale\n        # Guarantee some minimal step size relative to span\n        stage_sigma = np.maximum(stage_sigma, 1e-7 * (span + 1e-7))\n\n        # If local is working well, allow a mild dynamic boost\n        dynamic_steps = steps_per_stage\n        if stage >= 1 and local_improvements > 0 and (budget - evals_used) > dynamic_steps:\n            dynamic_steps = int(dynamic_steps * 1.1)\n\n        stage_steps = min(dynamic_steps, budget - evals_used)\n\n        for _ in range(stage_steps):\n            if evals_used >= budget:\n                break\n\n            # Mix of perturbation types with curvature bias\n            r = rng.random()\n            if r < 0.45:\n                # Full-dimensional step, scaled by curvature\n                noise = rng.normal(0.0, stage_sigma, size=dim)\n            elif r < 0.8:\n                # Single-coordinate step biased towards flatter directions\n                weights = 1.0 / coord_activity\n                wsum = np.sum(weights)\n                if not np.isfinite(wsum) or wsum <= 0:\n                    idx = rng.integers(0, dim)\n                else:\n                    probs = weights / wsum\n                    probs = np.nan_to_num(probs, nan=1.0 / dim)\n                    probs_sum = probs.sum()\n                    if probs_sum <= 0:\n                        probs = np.full(dim, 1.0 / dim)\n                    else:\n                        probs /= probs_sum\n                    idx = rng.choice(dim, p=probs)\n                noise = np.zeros(dim)\n                noise[idx] = rng.normal(0.0, stage_sigma[idx])\n            else:\n                # Two-coordinate coupled step, also curvature-biased\n                k = min(2, dim)\n                weights = 1.0 / coord_activity\n                wsum = np.sum(weights)\n                if not np.isfinite(wsum) or wsum <= 0:\n                    idxs = rng.choice(dim, size=k, replace=False)\n                else:\n                    probs = weights / wsum\n                    probs = np.nan_to_num(probs, nan=1.0 / dim)\n                    probs_sum = probs.sum()\n                    if probs_sum <= 0:\n                        probs = np.full(dim, 1.0 / dim)\n                    else:\n                        probs /= probs_sum\n                    # If dim == 1, fallback to single index\n                    if dim == 1:\n                        idxs = np.array([0])\n                    else:\n                        idxs = rng.choice(dim, size=k, replace=False, p=probs)\n                noise = np.zeros(dim)\n                noise[idxs] = rng.normal(0.0, stage_sigma[idxs])\n\n            x = clip_to_bounds(best_x + noise)\n            y = objective_function(x)\n            evals_used += 1\n\n            if y < best_y:\n                step_vec_used = x - best_x\n                best_x, best_y = x, y\n                no_improve_streak = 0\n                local_improvements += 1\n\n                # Update curvature estimates: larger normalized step -> flatter\n                abs_step = np.abs(step_vec_used) / (span + 1e-12)\n                coord_activity *= (1.0 + 0.25 * abs_step)\n                coord_activity = np.clip(coord_activity, 0.3, 5.0)\n\n                # Update diagonal scale to move more along flatter directions\n                diag_scale = 1.0 / coord_activity\n                diag_scale = np.clip(diag_scale, 0.4, 3.5)\n            else:\n                no_improve_streak += 1\n\n            # Stagnation handling: modest shrink and occasional restart\n            if no_improve_streak >= 15:\n                base_sigma *= 0.7\n                base_sigma = np.maximum(base_sigma, 1e-7 * (span + 1e-7))\n                no_improve_streak = 0\n\n                # Cheap restart: nearby random point\n                if evals_used < budget:\n                    near_sigma = np.maximum(stage_sigma * 1.5, 0.05 * span)\n                    x_jump = clip_to_bounds(best_x + rng.normal(0.0, near_sigma, size=dim))\n                    y_jump = objective_function(x_jump)\n                    evals_used += 1\n                    if y_jump < best_y:\n                        best_x, best_y = x_jump, y_jump\n                        local_improvements += 1\n\n        remaining = budget - evals_used\n        if remaining <= 0:\n            return best_x\n\n    # --- Tail global search with any remaining budget ---\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Use a new base but same step_vec for continuity\n    base_tail = rng.random(dim)\n    for i in range(remaining):\n        if evals_used >= budget:\n            break\n        u = (base_tail + i * step_vec) % 1.0\n        x = low + u * span\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = x, y\n\n    return best_x",
    "X": "0.9992741639746071 0.024369156691108073 0.03884181343652941"
}