{
    "score": 0.0018474188745198847,
    "Input": "HelicalValley",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid black-box optimizer with warm start support, budget-aware tuning,\n    and curvature-adaptive local search.\n\n    Design highlights:\n    - Robust budget handling: never exceeds budget, uses essentially all calls.\n    - Warm start: prev_best_x is evaluated first (if valid) and used as anchor.\n    - Multi-phase search:\n        * Initial diversified sampling (global).\n        * Quasi-random global exploration (low discrepancy).\n        * Curvature-aware local search (anisotropic Gaussian steps).\n        * Tail global search with any leftover budget.\n    - Local search tweaks vs previous version:\n        * Smaller initial step scale (more cautious, better for narrow valleys).\n        * Curvature heuristic: estimate axis-wise sensitivity from sampled steps\n          and bias perturbations along \"flatter\" directions.\n        * Adaptive exploitation ramp: if local search finds improvements early,\n          it steals some evaluations from global tail to continue exploiting.\n        * Softer stagnation thresholds and cheaper restarts.\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # No/negative budget: return center of box\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    # Utility: clip into bounds\n    def clip_to_bounds(x):\n        return np.minimum(high, np.maximum(low, x))\n\n    rng = np.random.default_rng()\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    # --- Warm start if available and valid ---\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.size == dim:\n                x0 = clip_to_bounds(x0)\n                y0 = objective_function(x0)\n                evals_used += 1\n                best_x, best_y = x0, y0\n        except Exception:\n            best_x, best_y = None, None\n\n    # --- Always include at least one random initialization for robustness ---\n    if evals_used < budget:\n        x_rand = rng.uniform(low, high)\n        y_rand = objective_function(x_rand)\n        evals_used += 1\n        if best_x is None or y_rand < best_y:\n            best_x, best_y = x_rand, y_rand\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Initial diversified sampling (global exploration) ---\n    # Slightly stronger than pure single random, but keep cost modest.\n    init_global = min(remaining, max(6, min(25, 3 * dim)))\n    # Track simple curvature signal: relative change per coordinate\n    coord_activity = np.ones(dim)\n\n    for _ in range(init_global):\n        if evals_used >= budget:\n            break\n        # Mix pure random and biased sampling around current best\n        if rng.random() < 0.3:\n            # Around best, scaled by coord_activity\n            step = rng.normal(0.0, 0.3, size=dim) * (span + 1e-9) / coord_activity\n            x = clip_to_bounds(best_x + step)\n        else:\n            x = rng.uniform(low, high)\n\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            # Update coord_activity to reflect where progress happened\n            delta = np.abs(x - best_x) / (span + 1e-12)\n            # Bigger delta -> likely flatter direction -> lower \"activity\"\n            coord_activity *= (1.0 + 0.3 * delta)\n            coord_activity = np.clip(coord_activity, 0.3, 3.0)\n\n            best_x, best_y = x, y\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Decide global vs local budget for the main phase ---\n    # For small budgets, emphasize global; for larger, emphasize local.\n    if remaining < 30:\n        global_main_budget = max(1, int(0.6 * remaining))\n    elif remaining < 100:\n        global_main_budget = int(0.45 * remaining)\n    else:\n        global_main_budget = int(0.35 * remaining)\n\n    global_main_budget = max(1, min(global_main_budget, remaining - 1))\n    local_budget = remaining - global_main_budget\n\n    # --- Global main search: low-discrepancy style sampling ---\n    base = rng.random(dim)\n    # Golden ratio conjugate\n    phi_conj = (np.sqrt(5.0) - 1.0) / 2.0\n    step_vec = (phi_conj * (rng.permutation(dim) + 1)) % 1.0\n\n    global_improvements = 0\n    g_evals = 0\n    while g_evals < global_main_budget and evals_used < budget:\n        u = (base + g_evals * step_vec) % 1.0\n        x = low + u * span\n        y = objective_function(x)\n        evals_used += 1\n        g_evals += 1\n        if y < best_y:\n            delta = np.abs(x - best_x) / (span + 1e-12)\n            coord_activity *= (1.0 + 0.25 * delta)\n            coord_activity = np.clip(coord_activity, 0.3, 3.0)\n\n            best_x, best_y = x, y\n            global_improvements += 1\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Local search setup ---\n    # Base scale: conservative to better handle narrow valleys\n    base_sigma = 0.15 * span\n    base_sigma[base_sigma == 0] = 1.0  # avoid zero span dims\n\n    # Adaptive diagonal scaling (approximate covariance / curvature inversion)\n    # Use inverse of coord_activity to prioritize \"flatter\" directions.\n    diag_scale = 1.0 / coord_activity\n    diag_scale = np.clip(diag_scale, 0.5, 3.0)\n\n    # Divide local budget into stages with geometric decay\n    local_budget = min(local_budget, remaining)\n    if local_budget <= 0:\n        # Fallback: use remaining budget for extra global search\n        extra_global_budget = remaining\n        for i in range(extra_global_budget):\n            if evals_used >= budget:\n                break\n            u = (base + (g_evals + i) * step_vec) % 1.0\n            x = low + u * span\n            y = objective_function(x)\n            evals_used += 1\n            if y < best_y:\n                best_x, best_y = x, y\n        return best_x\n\n    # Number of stages determined by log of budget, but bounded\n    stages = min(8, max(2, int(np.log2(local_budget + 4))))\n    steps_per_stage = max(1, local_budget // stages)\n    no_improve_streak = 0\n    local_improvements = 0\n\n    for stage in range(stages):\n        if evals_used >= budget:\n            break\n\n        # Progressive shrinking of sigma\n        stage_sigma = base_sigma * (0.6 ** stage) * diag_scale\n        # Guarantee some minimal step size relative to span\n        stage_sigma = np.maximum(stage_sigma, 1e-6 * (span + 1e-6))\n\n        # If local is clearly working, dynamically borrow a bit of tail budget\n        dynamic_steps = steps_per_stage\n        if stage >= 1 and local_improvements > 0 and (budget - evals_used) > dynamic_steps:\n            dynamic_steps = int(dynamic_steps * 1.2)\n\n        stage_steps = min(dynamic_steps, budget - evals_used)\n\n        for _ in range(stage_steps):\n            if evals_used >= budget:\n                break\n\n            # Mix of perturbation types with curvature bias\n            r = rng.random()\n            if r < 0.45:\n                # Full-dimensional step, scaled by curvature\n                noise = rng.normal(0.0, stage_sigma, size=dim)\n            elif r < 0.8:\n                # Coordinate-wise step: choose coordinate with probability\n                # inversely proportional to coord_activity (flatter first).\n                weights = 1.0 / coord_activity\n                wsum = np.sum(weights)\n                if wsum <= 0:\n                    idx = rng.integers(0, dim)\n                else:\n                    probs = weights / wsum\n                    idx = rng.choice(dim, p=probs)\n                noise = np.zeros(dim)\n                noise[idx] = rng.normal(0.0, stage_sigma[idx])\n            else:\n                # Two coordinates chosen with same curvature-based weights\n                k = min(2, dim)\n                weights = 1.0 / coord_activity\n                wsum = np.sum(weights)\n                if wsum <= 0:\n                    idxs = rng.choice(dim, size=k, replace=False)\n                else:\n                    probs = weights / wsum\n                    idxs = rng.choice(dim, size=k, replace=False, p=probs)\n                noise = np.zeros(dim)\n                noise[idxs] = rng.normal(0.0, stage_sigma[idxs])\n\n            x = clip_to_bounds(best_x + noise)\n            y = objective_function(x)\n            evals_used += 1\n\n            if y < best_y:\n                step_vec_used = x - best_x\n                best_x, best_y = x, y\n                no_improve_streak = 0\n                local_improvements += 1\n\n                # Update curvature estimates: larger normalized step -> flatter\n                abs_step = np.abs(step_vec_used) / (span + 1e-12)\n                coord_activity *= (1.0 + 0.3 * abs_step)\n                coord_activity = np.clip(coord_activity, 0.3, 5.0)\n\n                # Update diagonal scale to move more along flatter directions\n                diag_scale = 1.0 / coord_activity\n                diag_scale = np.clip(diag_scale, 0.4, 3.5)\n            else:\n                no_improve_streak += 1\n\n            # Stagnation handling: gentle shrink followed by cheap restart\n            if no_improve_streak >= 12:\n                base_sigma *= 0.7\n                base_sigma = np.maximum(base_sigma, 1e-6 * (span + 1e-6))\n\n            if no_improve_streak >= 18 and evals_used < budget:\n                # Single cheap restart: random nearby plus one global sample\n                # Nearby random\n                near_sigma = np.maximum(stage_sigma * 1.8, 0.1 * span)\n                x_jump = clip_to_bounds(best_x + rng.normal(0.0, near_sigma, size=dim))\n                y_jump = objective_function(x_jump)\n                evals_used += 1\n                if y_jump < best_y:\n                    best_x, best_y = x_jump, y_jump\n                    local_improvements += 1\n                    no_improve_streak = 0\n                    continue\n\n                if evals_used < budget:\n                    # One more global jump via quasi-random sequence\n                    t = g_evals\n                    u = (base + t * step_vec) % 1.0\n                    x_jump2 = low + u * span\n                    y_jump2 = objective_function(x_jump2)\n                    evals_used += 1\n                    g_evals += 1\n                    if y_jump2 < best_y:\n                        best_x, best_y = x_jump2, y_jump2\n                        local_improvements += 1\n                    no_improve_streak = 0\n\n        remaining = budget - evals_used\n        if remaining <= 0:\n            return best_x\n\n    # --- Tail global search with any remaining budget ---\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Use a new base but same step_vec for continuity\n    base_tail = rng.random(dim)\n    for i in range(remaining):\n        if evals_used >= budget:\n            break\n        u = (base_tail + i * step_vec) % 1.0\n        x = low + u * span\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = x, y\n\n    return best_x",
    "X": "0.9996678995925874 0.027052854635331806 0.042971140472999554"
}