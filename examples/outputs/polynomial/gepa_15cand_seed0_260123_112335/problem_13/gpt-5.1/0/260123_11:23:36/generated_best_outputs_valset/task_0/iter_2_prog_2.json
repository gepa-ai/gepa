{
    "score": 0.021628246854361977,
    "Input": "HelicalValley",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid black-box optimizer:\n    - Uses warm start if available.\n    - Global search via low-discrepancy (Sobol-like) random sampling.\n    - Local search via adaptive Gaussian search around incumbent.\n    - Dynamically balances global vs local based on improvements.\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # No/negative budget: return center of box\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    # Utility: clip into bounds\n    def clip_to_bounds(x):\n        return np.minimum(high, np.maximum(low, x))\n\n    evals_used = 0\n\n    # --- Initialization with optional warm start ---\n    best_x = None\n    best_y = None\n\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = clip_to_bounds(x0)\n            best_x = x0\n            best_y = objective_function(best_x)\n            evals_used += 1\n\n    # If no warm start or invalid dim, start from random point\n    if best_x is None:\n        best_x = np.random.uniform(low, high)\n        best_y = objective_function(best_x)\n        evals_used += 1\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Budget split: keep slightly more for local refinement ---\n    # Empirically, 50/50 often works well, but we bias mildly to local\n    global_budget = max(1, int(0.45 * remaining))\n    local_budget = remaining - global_budget\n\n    # --- Global search: stratified random sampling ---\n    # Instead of pure iid uniform, we use a simple low-discrepancy style:\n    # generate a random base and additive increments.\n    rng = np.random.default_rng()\n    base = rng.random(dim)\n    step = (np.sqrt(5) - 1.0) / 2.0  # golden ratio conjugate\n    step_vec = (step * np.arange(1, dim + 1)) % 1.0\n\n    for i in range(global_budget):\n        # quasi-random point in [0,1]^dim\n        u = (base + i * step_vec) % 1.0\n        x = low + u * span\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = x, y\n\n    remaining = budget - evals_used\n    if remaining <= 0 or local_budget <= 0:\n        return best_x\n\n    # --- Local search around best_x: adaptive Gaussian neighborhood ---\n    base_sigma = 0.15 * span  # slightly smaller initial scale than before\n    # Fallback for zero-span dimensions\n    base_sigma[base_sigma == 0] = 1.0\n\n    # Use an adaptive schedule over local_budget evaluations\n    # Split into phases; each phase shrinks the neighborhood.\n    phases = min(4, max(1, int(np.log2(local_budget + 1))))\n    steps_per_phase = max(1, local_budget // phases)\n\n    # Sigmas decay geometrically\n    sigmas = [base_sigma * (0.5 ** p) for p in range(phases)]\n\n    no_improve_streak = 0\n    for phase in range(phases):\n        if evals_used >= budget:\n            break\n        sigma = sigmas[phase]\n        phase_steps = min(steps_per_phase, budget - evals_used)\n        for _ in range(phase_steps):\n            if evals_used >= budget:\n                break\n            # Propose candidate around best_x\n            noise = rng.normal(0.0, sigma, size=dim)\n            x = clip_to_bounds(best_x + noise)\n            y = objective_function(x)\n            evals_used += 1\n            if y < best_y:\n                best_x, best_y = x, y\n                no_improve_streak = 0\n            else:\n                no_improve_streak += 1\n\n            # If stuck for a while, inject a global-style jump\n            if no_improve_streak >= 10 and evals_used < budget:\n                u = rng.random(dim)\n                x_jump = low + u * span\n                y_jump = objective_function(x_jump)\n                evals_used += 1\n                if y_jump < best_y:\n                    best_x, best_y = x_jump, y_jump\n                    no_improve_streak = 0\n                else:\n                    no_improve_streak = 0  # reset to avoid too frequent jumps\n\n    return best_x",
    "X": "0.9946151637924188 0.030428310914573896 0.03527219382834503"
}