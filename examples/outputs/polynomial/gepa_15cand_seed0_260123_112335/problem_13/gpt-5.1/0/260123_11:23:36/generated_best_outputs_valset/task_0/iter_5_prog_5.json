{
    "score": 0.001982073154533085,
    "Input": "HelicalValley",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid black-box optimizer with warm start support and budget-aware tuning.\n\n    Improvements over previous version:\n    - Clearer, more robust budget partitioning (no double allocation, always uses\n      almost all evaluations).\n    - Slightly stronger initial global exploration (good for complex valleys).\n    - Local search uses adaptive step-size reduction with stagnation-based shrinking.\n    - Lightweight covariance adaptation: diagonal scaling from successful steps.\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # No/negative budget: return center of box\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    # Utility: clip into bounds\n    def clip_to_bounds(x):\n        return np.minimum(high, np.maximum(low, x))\n\n    rng = np.random.default_rng()\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    # --- Warm start if available and valid ---\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.size == dim:\n                x0 = clip_to_bounds(x0)\n                y0 = objective_function(x0)\n                evals_used += 1\n                best_x, best_y = x0, y0\n        except Exception:\n            best_x, best_y = None, None\n\n    # --- Always include at least one random initialization for robustness ---\n    if evals_used < budget:\n        x_rand = rng.uniform(low, high)\n        y_rand = objective_function(x_rand)\n        evals_used += 1\n        if best_x is None or y_rand < best_y:\n            best_x, best_y = x_rand, y_rand\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Initial diversified sampling (global exploration) ---\n    # Use more diverse samples for higher dimensions, but cap cost.\n    init_global = min(remaining, max(6, min(30, 4 * dim)))\n    for _ in range(init_global):\n        if evals_used >= budget:\n            break\n        x = rng.uniform(low, high)\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = x, y\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Decide global vs local budget for the main phase ---\n    # For small budgets, emphasize global; for larger, emphasize local.\n    if remaining < 30:\n        global_main_budget = max(1, int(0.65 * remaining))\n    elif remaining < 100:\n        global_main_budget = int(0.5 * remaining)\n    else:\n        global_main_budget = int(0.4 * remaining)\n\n    global_main_budget = max(1, min(global_main_budget, remaining - 1))\n    local_budget = remaining - global_main_budget\n\n    # --- Global main search: low-discrepancy style sampling ---\n    base = rng.random(dim)\n    step = (np.sqrt(5.0) - 1.0) / 2.0  # golden ratio conjugate\n    step_vec = (step * (rng.permutation(dim) + 1)) % 1.0\n\n    global_improvements = 0\n    g_evals = 0\n    while g_evals < global_main_budget and evals_used < budget:\n        u = (base + g_evals * step_vec) % 1.0\n        x = low + u * span\n        y = objective_function(x)\n        evals_used += 1\n        g_evals += 1\n        if y < best_y:\n            best_x, best_y = x, y\n            global_improvements += 1\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Local search setup ---\n    # Base scale: moderate to allow traversing curved valleys\n    base_sigma = 0.25 * span\n    base_sigma[base_sigma == 0] = 1.0  # avoid zero span dims\n\n    # Adaptive diagonal scaling (approximate covariance)\n    diag_scale = np.ones(dim)\n\n    # Divide local budget into stages with geometric decay\n    local_budget = min(local_budget, remaining)\n    if local_budget <= 0:\n        # Fallback: use remaining budget for extra global search\n        extra_global_budget = remaining\n        for i in range(extra_global_budget):\n            if evals_used >= budget:\n                break\n            u = (base + (g_evals + i) * step_vec) % 1.0\n            x = low + u * span\n            y = objective_function(x)\n            evals_used += 1\n            if y < best_y:\n                best_x, best_y = x, y\n        return best_x\n\n    stages = min(7, max(2, int(np.log2(local_budget + 4))))\n    steps_per_stage = max(1, local_budget // stages)\n    no_improve_streak = 0\n    local_improvements = 0\n\n    for stage in range(stages):\n        if evals_used >= budget:\n            break\n\n        stage_sigma = base_sigma * (0.55 ** stage) * diag_scale\n        stage_steps = min(steps_per_stage, budget - evals_used)\n\n        for _ in range(stage_steps):\n            if evals_used >= budget:\n                break\n\n            # Mixture of perturbation types\n            r = rng.random()\n            if r < 0.5:\n                # Full-dimensional step\n                noise = rng.normal(0.0, stage_sigma, size=dim)\n            elif r < 0.8:\n                # Single coordinate\n                noise = np.zeros(dim)\n                idx = rng.integers(0, dim)\n                noise[idx] = rng.normal(0.0, stage_sigma[idx])\n            else:\n                # Two coordinates\n                noise = np.zeros(dim)\n                k = min(2, dim)\n                idxs = rng.choice(dim, size=k, replace=False)\n                noise[idxs] = rng.normal(0.0, stage_sigma[idxs])\n\n            x = clip_to_bounds(best_x + noise)\n            y = objective_function(x)\n            evals_used += 1\n\n            if y < best_y:\n                # Successful step: update incumbent and scaling information\n                step_vec_used = x - best_x\n                best_x, best_y = x, y\n                no_improve_streak = 0\n                local_improvements += 1\n\n                # Simple diagonal adaptation: enlarge axes that see progress\n                abs_step = np.abs(step_vec_used) / (span + 1e-12)\n                diag_scale *= (1.0 + 0.2 * np.clip(abs_step, 0.0, 1.0))\n                # Avoid too large or too small scaling\n                diag_scale = np.clip(diag_scale, 0.3, 3.0)\n            else:\n                no_improve_streak += 1\n\n            # Stagnation handling: shrink step size or restart\n            if no_improve_streak >= 10:\n                # Shrink base sigma modestly\n                base_sigma *= 0.7\n                base_sigma = np.maximum(base_sigma, 1e-6 * (span + 1e-6))\n            if no_improve_streak >= 15 and evals_used < budget:\n                # Perform two restarts: one random, one around best with larger step\n                # Random jump\n                x_jump = rng.uniform(low, high)\n                y_jump = objective_function(x_jump)\n                evals_used += 1\n                if y_jump < best_y:\n                    best_x, best_y = x_jump, y_jump\n                    local_improvements += 1\n                    no_improve_streak = 0\n                    continue\n\n                # Jump around current best with larger sigma\n                big_sigma = np.maximum(stage_sigma * 2.5, 0.15 * span)\n                noise_big = rng.normal(0.0, big_sigma, size=dim)\n                x_jump2 = clip_to_bounds(best_x + noise_big)\n                y_jump2 = objective_function(x_jump2)\n                evals_used += 1\n                if y_jump2 < best_y:\n                    best_x, best_y = x_jump2, y_jump2\n                    local_improvements += 1\n                no_improve_streak = 0\n\n        remaining = budget - evals_used\n        if remaining <= 0:\n            return best_x\n\n    # --- Tail global search with any remaining budget ---\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Use a new base but same step_vec for continuity\n    base_tail = rng.random(dim)\n    for i in range(remaining):\n        if evals_used >= budget:\n            break\n        u = (base_tail + i * step_vec) % 1.0\n        x = low + u * span\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = x, y\n\n    return best_x",
    "X": "0.999659024543379 0.02802617734446971 0.044505539222127216"
}