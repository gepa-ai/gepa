{
    "score": 0.00032738185466966166,
    "Input": "HelicalValley",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid black-box optimizer with warm start support and budget-aware tuning.\n\n    Improvements vs previous version:\n    - Stronger, more principled global sampling (Sobol-like + random).\n    - Simpler, more robust CMA-ES\u2013inspired local search with diagonal covariance.\n    - Cleaner budget partition and guaranteed full budget usage.\n    - More stable handling of coord_activity and step scales.\n\n    Strategy:\n    1. Warm start eval (if any) + mandatory random eval.\n    2. Initial global exploration (mixed random / near-best).\n    3. Quasi-random global exploration (low-discrepancy).\n    4. Diagonal-CMA-like local search around best.\n    5. Tail global exploration with any remaining budget.\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # No/negative budget: return center of box\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    def clip_to_bounds(x):\n        return np.minimum(high, np.maximum(low, x))\n\n    rng = np.random.default_rng()\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    # --- Warm start if available and valid ---\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.size == dim:\n                x0 = clip_to_bounds(x0)\n                y0 = objective_function(x0)\n                evals_used += 1\n                best_x, best_y = x0, y0\n        except Exception:\n            best_x, best_y = None, None\n\n    # --- Always at least one random sample ---\n    if evals_used < budget:\n        x_rand = rng.uniform(low, high)\n        y_rand = objective_function(x_rand)\n        evals_used += 1\n        if best_x is None or y_rand < best_y:\n            best_x, best_y = x_rand, y_rand\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Track simple coordinate activity (approx inverse curvature proxy) ---\n    coord_activity = np.ones(dim)\n\n    # --- Initial diversified global exploration ---\n    # Scale with dim and budget but keep cost modest\n    init_global = min(remaining, max(4, min(12 * dim, 60, remaining // 2)))\n\n    for _ in range(init_global):\n        if evals_used >= budget:\n            break\n        if rng.random() < 0.4:\n            # Sample around best, scaled by \"flatter\" directions (low activity)\n            local_scale = 0.3\n            step = rng.normal(0.0, local_scale, size=dim) * (span + 1e-9) / coord_activity\n            x = clip_to_bounds(best_x + step)\n        else:\n            # Pure global\n            x = rng.uniform(low, high)\n\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            delta = np.abs(x - best_x) / (span + 1e-12)\n            coord_activity *= (1.0 + 0.25 * delta)\n            coord_activity = np.clip(coord_activity, 0.3, 5.0)\n            best_x, best_y = x, y\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Decide global vs local budget for main phase ---\n    # Slightly more weight on global for small budgets\n    if remaining < 30:\n        global_main_budget = max(1, int(0.6 * remaining))\n    elif remaining < 100:\n        global_main_budget = int(0.45 * remaining)\n    else:\n        global_main_budget = int(0.35 * remaining)\n\n    global_main_budget = max(1, min(global_main_budget, remaining - 1))\n    local_budget = remaining - global_main_budget\n\n    # --- Global main search: low-discrepancy style (additive recurrence) ---\n    base = rng.random(dim)\n    phi_conj = (np.sqrt(5.0) - 1.0) / 2.0\n    # Deterministic-ish direction; shuffle dims for variation\n    step_vec = ((phi_conj * (np.arange(1, dim + 1))) % 1.0)\n    rng.shuffle(step_vec)\n\n    g_evals = 0\n    while g_evals < global_main_budget and evals_used < budget:\n        u = (base + g_evals * step_vec) % 1.0\n        x = low + u * span\n        y = objective_function(x)\n        evals_used += 1\n        g_evals += 1\n        if y < best_y:\n            delta = np.abs(x - best_x) / (span + 1e-12)\n            coord_activity *= (1.0 + 0.2 * delta)\n            coord_activity = np.clip(coord_activity, 0.3, 5.5)\n            best_x, best_y = x, y\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Local diagonal CMA-ES\u2013style search around best_x ---\n    local_budget = min(local_budget, remaining)\n    if local_budget <= 0:\n        # Fallback: spend remaining on global\n        extra = remaining\n        base_extra = rng.random(dim)\n        for i in range(extra):\n            if evals_used >= budget:\n                break\n            u = (base_extra + i * step_vec) % 1.0\n            x = low + u * span\n            y = objective_function(x)\n            evals_used += 1\n            if y < best_y:\n                best_x, best_y = x, y\n        return best_x\n\n    # Diagonal covariance CMA-ES\u2013like parameters\n    dim_factor = 1.0 / np.sqrt(max(1, dim))\n    sigma = 0.3 * dim_factor * (span + 1e-12)\n    sigma = np.maximum(sigma, 1e-8 * (span + 1e-8))\n\n    pop_size = max(4, min(10 + dim, local_budget // 2))\n    mu = max(2, pop_size // 2)\n    weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n    weights = weights / np.sum(weights)\n    mueff = (np.sum(weights) ** 2) / np.sum(weights ** 2)\n\n    # Learning rates for diagonal covariance (simplified)\n    c1 = 2.0 / ((dim + 1.3) ** 2 + mueff)\n    cmu = min(1 - c1, 2.0 * (mueff - 2 + 1 / mueff) / ((dim + 2) ** 2 + mueff))\n    # Dampen for robustness\n    c1 *= 0.3\n    cmu *= 0.6\n\n    # Initialize diagonal covariance from coord_activity (flatter -> larger var)\n    C = (1.0 / np.clip(coord_activity, 0.5, 3.0)) ** 2\n    C = np.maximum(C, (1e-8 * (span + 1e-8)) ** 2)\n\n    mean = best_x.copy()\n    local_evals = 0\n    no_improve_iters = 0\n\n    while local_evals < local_budget and evals_used < budget:\n        lam = min(pop_size, local_budget - local_evals)\n        # Sample population\n        zs = rng.normal(size=(lam, dim))\n        steps = zs * np.sqrt(C) * sigma\n        xs = clip_to_bounds(mean + steps)\n\n        ys = []\n        for i in range(lam):\n            if evals_used >= budget:\n                break\n            y = objective_function(xs[i])\n            ys.append(y)\n            evals_used += 1\n            local_evals += 1\n            if y < best_y:\n                best_x, best_y = xs[i].copy(), y\n        if len(ys) == 0:\n            break\n\n        ys = np.array(ys)\n        order = np.argsort(ys)\n        xs_sorted = xs[order]\n        zs_sorted = zs[order]\n        ys_sorted = ys[order]\n\n        old_mean = mean.copy()\n        mean = np.sum(weights[:, None] * xs_sorted[:mu], axis=0)\n        step_mean = mean - old_mean\n\n        # Update diagonal covariance using weighted outer products in diag form\n        # Centered steps in z-space\n        z_mean = np.sum(weights[:, None] * zs_sorted[:mu], axis=0)\n        z_sq = zs_sorted[:mu] ** 2\n        c_mu_update = np.sum(weights[:, None] * z_sq, axis=0) - 1.0  # deviation from 1\n\n        C = (1 - c1 - cmu) * C + cmu * np.maximum(0.0, 1.0 + c_mu_update) * C\n        C = np.maximum(C, (1e-10 * (span + 1e-8)) ** 2)\n\n        # Adapt sigma a bit\n        norm_step = np.linalg.norm(step_mean / (np.sqrt(C) * sigma + 1e-12)) / np.sqrt(dim)\n        sigma *= np.exp(0.1 * (norm_step - 1.0))\n        sigma = np.clip(sigma, 1e-8 * (span + 1e-8), 0.5 * (span + 1e-8))\n\n        # Track improvement\n        if ys_sorted[0] < best_y:\n            best_x, best_y = xs_sorted[0].copy(), ys_sorted[0]\n            no_improve_iters = 0\n        else:\n            no_improve_iters += 1\n\n        # Mild restart / broadening if stuck\n        if no_improve_iters >= 5:\n            sigma *= 0.6\n            sigma = np.maximum(sigma, 1e-8 * (span + 1e-8))\n        if no_improve_iters >= 10 and evals_used < budget:\n            # Restart around global best with broader sigma\n            mean = best_x.copy()\n            sigma = np.minimum(0.3 * (span + 1e-8), np.maximum(3 * sigma, 0.05 * (span + 1e-8)))\n            no_improve_iters = 0\n\n        remaining = budget - evals_used\n        if remaining <= 0 or local_evals >= local_budget:\n            break\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Tail global search with remaining budget ---\n    base_tail = rng.random(dim)\n    for i in range(remaining):\n        if evals_used >= budget:\n            break\n        u = (base_tail + i * step_vec) % 1.0\n        x = low + u * span\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = x, y\n\n    return best_x",
    "X": "0.999697569745547 0.010228698844596448 0.015360976071777728"
}