{
    "score": -6.589547802184405,
    "Input": "McCourt20",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization with a hybrid global-local search.\n    Uses quasi-random (Sobol-like) sampling + CMA-ES-style local search,\n    with optional warm start from prev_best_x.\n    \"\"\"\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # ---- Initialization / warm start ----\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape == (dim,):\n            x0 = clip(x0)\n            eval_point(x0)\n\n    if best_x is None:\n        x0 = np.random.uniform(low, high)\n        eval_point(x0)\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # ---- Budget split: global vs local ----\n    # Slightly more global exploration for robustness\n    global_budget = max(1, int(0.7 * remaining))\n    local_budget = remaining - global_budget\n\n    # ---- Global search: quasi-random + localized samples ----\n    # Use a simple low-discrepancy sequence (Halton-like) for better coverage.\n    num_global = global_budget\n    num_uniform = num_global // 2\n    num_localized = num_global - num_uniform\n\n    def halton_sequence(index, base):\n        \"\"\"Generate a scalar in [0,1) from a Halton sequence for a given index and base.\"\"\"\n        f = 1.0\n        r = 0.0\n        i = index\n        while i > 0:\n            f /= base\n            r += f * (i % base)\n            i //= base\n        return r\n\n    # Predefined prime bases for first dimensions; reuse if dim > len(primes)\n    primes = [2, 3, 5, 7, 11, 13, 17, 19]\n    if dim > len(primes):\n        # Extend with odd numbers; for our use, exact primeness is not critical\n        last = primes[-1]\n        while len(primes) < dim:\n            last += 2\n            primes.append(last)\n\n    # Quasi-random uniform sampling over the box\n    for i in range(num_uniform):\n        if evals_used >= budget:\n            break\n        # Halton-like point in [0,1]^dim\n        u = np.empty(dim)\n        idx = i + 1  # Halton sequences are usually 1-based\n        for d in range(dim):\n            u[d] = halton_sequence(idx, primes[d])\n        x = low + u * span\n        eval_point(x)\n\n    # Localized random sampling around current best with shrinking radius\n    for i in range(num_localized):\n        if evals_used >= budget:\n            break\n        frac = (i + 1) / max(1, num_localized)\n        # Shrink radius from full span to 10% of span\n        radius = (1.0 - 0.9 * frac)\n        step = np.random.randn(dim) * (0.3 * span * radius)\n        x = clip(best_x + step)\n        eval_point(x)\n\n    if local_budget <= 0 or evals_used >= budget:\n        return best_x\n\n    # ---- Local CMA-ES-style search around best_x ----\n    # Small population sizes work better for tight budgets.\n    lamb = max(4, 4 + int(3 * np.log(dim + 1)))\n    mu = max(2, lamb // 2)\n\n    # Expected evaluations per generation \u2248 lamb\n    max_generations = max(1, local_budget // lamb)\n\n    # Initial step size proportional to box size but modest\n    sigma = 0.2 * np.mean(span)\n    if sigma <= 0:\n        return best_x\n\n    mean = best_x.copy()\n    cov = np.eye(dim)\n\n    # Ensure numerical stability\n    min_sigma = 1e-9 * np.mean(span + (span == 0))\n\n    for _ in range(max_generations):\n        if evals_used >= budget:\n            break\n\n        # Sample population\n        zs = np.random.randn(lamb, dim)\n        xs = mean + sigma * zs @ np.linalg.cholesky(cov).T\n        xs = clip(xs)\n\n        ys = []\n        for i in range(lamb):\n            if evals_used >= budget:\n                break\n            y = eval_point(xs[i])\n            if y is not None:\n                ys.append((y, xs[i], zs[i]))\n        if not ys:\n            break\n\n        ys.sort(key=lambda t: t[0])\n        top = ys[:mu]\n\n        # Compute new mean in parameter space\n        weights = np.linspace(mu, 1, mu)\n        weights /= weights.sum()\n        new_mean = np.sum([w * x for w, (_, x, _) in zip(weights, top)], axis=0)\n\n        # Adapt covariance using weighted ranked samples\n        dz = np.array([z for (_, _, z) in top])\n        dz_mean = np.average(dz, axis=0, weights=weights)\n        centered = dz - dz_mean\n        C_new = np.zeros((dim, dim))\n        for w, c in zip(weights, centered):\n            C_new += w * np.outer(c, c)\n        # Dampen covariance update\n        cov = 0.7 * cov + 0.3 * (C_new + 1e-8 * np.eye(dim))\n\n        # Update mean\n        mean = new_mean\n\n        # Step-size control: compare best offspring to current best\n        best_offspring_y = top[0][0]\n        if best_offspring_y < best_y:\n            sigma *= 1.2\n        else:\n            sigma *= 0.8\n        sigma = max(sigma, min_sigma)\n\n        # Early stop if step size is extremely small\n        if sigma <= min_sigma:\n            break\n\n    return best_x",
    "X": "0.6999985586613272 0.09975476189999447"
}