{
    "score": -6.595799343628335,
    "Input": "McCourt20",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization with a robust hybrid global-local search.\n    Uses low-discrepancy-like sampling + adaptive local search,\n    with optional warm start from prev_best_x.\n    \"\"\"\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    if budget <= 0 or dim <= 0:\n        return (low + high) / 2.0\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    rng = np.random.default_rng()\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    def eval_point(x):\n        \"\"\"Evaluate x, update global best, respect budget. Returns y or None if budget exhausted.\"\"\"\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        # Ensure correct shape\n        x = np.asarray(x, dtype=float).reshape(dim,)\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # ---------------- Initialization / warm start ----------------\n    # Use prev_best_x if valid; do not consume too much budget here.\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape == (dim,):\n            x0 = clip(x0)\n            eval_point(x0)\n\n    # Fallback: evaluate center with slight perturbation to break symmetry\n    if best_x is None and evals_used < budget:\n        center = (low + high) / 2.0\n        perturb_scale = 0.05\n        perturb = rng.uniform(-perturb_scale, perturb_scale, size=dim) * span\n        x0 = clip(center + perturb)\n        eval_point(x0)\n\n    # If still nothing evaluated (degenerate budget), return center\n    if best_x is None:\n        return (low + high) / 2.0\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # ---------------- Budget split: global vs local ----------------\n    # Slightly more global exploration at small budgets, more local for larger budgets.\n    if remaining < 30:\n        global_ratio = 0.7\n    elif remaining < 100:\n        global_ratio = 0.55\n    else:\n        global_ratio = 0.45\n\n    global_budget = max(1, int(global_ratio * remaining))\n    local_budget = remaining - global_budget\n\n    # Ensure at least a few local evaluations when budget allows\n    if remaining >= 25 and local_budget < 10:\n        deficit = 10 - local_budget\n        steal = min(deficit, max(0, global_budget - 5))\n        if steal > 0:\n            global_budget -= steal\n            local_budget += steal\n\n    # ---------------- Global search: low-discrepancy + localized ----------------\n    if global_budget <= 0 or evals_used >= budget:\n        return best_x\n\n    num_global = global_budget\n    num_uniform = int(0.6 * num_global)\n    num_localized = num_global - num_uniform\n\n    # Halton-like sequence generator (scalar)\n    def halton_scalar(index, base):\n        f = 1.0\n        r = 0.0\n        i = index\n        while i > 0:\n            f /= base\n            r += f * (i % base)\n            i //= base\n        return r\n\n    # Precomputed small prime list; extend with odd numbers if needed.\n    primes = [2, 3, 5, 7, 11, 13, 17, 19]\n    if dim > len(primes):\n        last = primes[-1]\n        # Fast extension: use consecutive odds, primality is not crucial for our purpose\n        while len(primes) < dim:\n            last += 2\n            primes.append(last)\n\n    # Uniform quasi-random sampling\n    for i in range(num_uniform):\n        if evals_used >= budget:\n            break\n        idx = i + 1  # Halton sequence is 1-based\n        u = np.empty(dim, dtype=float)\n        for d in range(dim):\n            u[d] = halton_scalar(idx, primes[d])\n        x = low + u * span\n        eval_point(x)\n\n    # Localized sampling around best_x with shrinking radius\n    if best_x is None:\n        best_x = clip((low + high) / 2.0)\n\n    for i in range(num_localized):\n        if evals_used >= budget:\n            break\n        if num_localized > 1:\n            frac = (i + 1) / num_localized\n        else:\n            frac = 1.0\n        # Radius shrinks from 40% to 5% of span\n        radius = 0.4 - 0.35 * frac\n        step = rng.normal(0.0, 1.0, size=dim) * (span * radius)\n        x = clip(best_x + step)\n        eval_point(x)\n\n    if local_budget <= 0 or evals_used >= budget:\n        return best_x\n\n    # ---------------- Local search: simplified derivative-free ES ----------------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Choose a moderate population size; independent of expensive covariance matrix.\n    lamb = max(8, 4 + int(2 * np.log(dim + 1)))\n    mu = max(2, lamb // 2)\n\n    max_generations = max(1, remaining // lamb)\n    if max_generations <= 0:\n        return best_x\n\n    # Initial step size: scaled with box size and dimension.\n    if np.any(span > 0):\n        base_scale = float(np.mean(span[span > 0]))\n    else:\n        base_scale = 1.0\n    sigma = 0.25 * base_scale / np.sqrt(max(dim, 1))\n\n    if sigma <= 0:\n        return best_x\n\n    mean = best_x.copy()\n\n    # Simple step-size adaptation path\n    path_sigma = np.zeros(dim, dtype=float)\n    c_sigma = 0.3\n    d_sigma = 1.0\n    min_sigma = 1e-12 * base_scale\n    max_sigma = base_scale\n\n    # Precompute recombination weights\n    weights = np.linspace(mu, 1, mu, dtype=float)\n    weights /= weights.sum()\n\n    for _ in range(max_generations):\n        if evals_used >= budget:\n            break\n\n        zs = rng.normal(0.0, 1.0, size=(lamb, dim))\n        xs = mean + sigma * zs\n        xs = clip(xs)\n\n        scored = []\n        for i in range(lamb):\n            if evals_used >= budget:\n                break\n            y = eval_point(xs[i])\n            if y is not None:\n                scored.append((y, xs[i], zs[i]))\n        if not scored:\n            break\n\n        scored.sort(key=lambda t: t[0])\n        elite = scored[:mu]\n\n        # Recombination: update mean in parameter space\n        new_mean = np.zeros(dim, dtype=float)\n        for w, (_, x_i, _) in zip(weights, elite):\n            new_mean += w * x_i\n\n        # Step-size control via evolution path in normalized space\n        dz = np.zeros(dim, dtype=float)\n        for w, (_, _, z_i) in zip(weights, elite):\n            dz += w * z_i\n\n        path_sigma = (1 - c_sigma) * path_sigma + np.sqrt(\n            c_sigma * (2.0 - c_sigma)\n        ) * dz\n        path_norm = np.linalg.norm(path_sigma) / np.sqrt(max(dim, 1))\n        sigma *= float(np.exp((path_norm - 1.0) * 0.25 / d_sigma))\n        sigma = float(np.clip(sigma, min_sigma, max_sigma))\n\n        mean = new_mean\n\n        # Early stop if step-size is tiny or mean is not moving\n        if sigma <= min_sigma or np.allclose(mean, best_x, atol=1e-12 * base_scale):\n            break\n\n    return best_x",
    "X": "0.7000368747783227 0.09998269717385407"
}