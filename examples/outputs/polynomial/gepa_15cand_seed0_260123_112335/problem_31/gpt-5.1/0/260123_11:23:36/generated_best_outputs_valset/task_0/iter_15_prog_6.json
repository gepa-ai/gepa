{
    "score": -6.597614597918929,
    "Input": "McCourt20",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization with a robust hybrid global-local search.\n    Uses low-discrepancy-like sampling + adaptive local search,\n    with optional warm start from prev_best_x.\n    Improved variant: slightly stronger global search, more robust ES\n    for small budgets and high dimensions, and better warm-start usage.\n    \"\"\"\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    if budget <= 0 or dim <= 0:\n        return (low + high) / 2.0\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # Seeded RNG for reproducibility across calls if desired\n    seed = None\n    if \"seed\" in config and config[\"seed\"] is not None:\n        seed = int(config[\"seed\"])\n    rng = np.random.default_rng(seed)\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    def eval_point(x):\n        \"\"\"Evaluate x, update global best, respect budget. Returns y or None if budget exhausted.\"\"\"\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = np.asarray(x, dtype=float).reshape(dim,)\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # ---------------- Initialization / warm start ----------------\n    # Use prev_best_x if valid\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape == (dim,):\n            x0 = clip(x0)\n            eval_point(x0)\n\n    # Also evaluate the center (often good for symmetric problems)\n    if evals_used < budget:\n        center = (low + high) / 2.0\n        eval_point(center)\n\n    # If still nothing evaluated (degenerate budget), return center\n    if best_x is None:\n        return (low + high) / 2.0\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # ---------------- Budget split: global vs local ----------------\n    # Make global search stronger for multimodal functions and high dimensions.\n    if remaining < 30:\n        global_ratio = 0.8\n    elif remaining < 100:\n        global_ratio = 0.65\n    else:\n        global_ratio = 0.55\n\n    # Slightly more global search for high-d problems\n    if dim > 15:\n        global_ratio = min(0.85, global_ratio + 0.1)\n\n    global_budget = max(1, int(global_ratio * remaining))\n    local_budget = remaining - global_budget\n\n    # Ensure at least a minimal local search when budget allows\n    if remaining >= 40 and local_budget < 10:\n        deficit = 10 - local_budget\n        steal = min(deficit, max(0, global_budget - 10))\n        if steal > 0:\n            global_budget -= steal\n            local_budget += steal\n\n    # ---------------- Global search: low-discrepancy + localized + best-of-random-walk ----------------\n    if global_budget <= 0 or evals_used >= budget:\n        return best_x\n\n    num_global = global_budget\n    # Use a mix: 45% quasi-random, 35% localized, 20% random-walk refinement\n    num_qr = int(0.45 * num_global)\n    num_localized = int(0.35 * num_global)\n    num_rw = num_global - num_qr - num_localized\n    if num_qr < 1:\n        num_qr = 1\n    if num_localized < 0:\n        num_localized = 0\n    if num_rw < 0:\n        num_rw = 0\n\n    # Halton-like sequence generator (scalar)\n    def halton_scalar(index, base):\n        f = 1.0\n        r = 0.0\n        i = index\n        while i > 0:\n            f /= base\n            r += f * (i % base)\n            i //= base\n        return r\n\n    # Precomputed small prime list; extend with odd numbers if needed (no heavy primality tests)\n    primes = [2, 3, 5, 7, 11, 13, 17, 19]\n    if dim > len(primes):\n        last = primes[-1]\n        while len(primes) < dim:\n            last += 2\n            primes.append(last)\n\n    # Quasi-random sampling over the whole domain\n    for i in range(num_qr):\n        if evals_used >= budget:\n            break\n        idx = i + 1  # Halton sequence is 1-based\n        u = np.empty(dim, dtype=float)\n        for d in range(dim):\n            u[d] = halton_scalar(idx, primes[d])\n        x = low + u * span\n        eval_point(x)\n\n    # Localized sampling around current best with shrinking radius\n    if best_x is None:\n        best_x = clip((low + high) / 2.0)\n\n    for i in range(num_localized):\n        if evals_used >= budget:\n            break\n        if num_localized > 1:\n            frac = (i + 1) / num_localized\n        else:\n            frac = 1.0\n        # Radius shrinks from 35% to 5% of span\n        radius = 0.35 - 0.30 * frac\n        step = rng.normal(0.0, 1.0, size=dim) * (span * radius)\n        x = clip(best_x + step)\n        eval_point(x)\n\n    # Short random-walk hill-climb from best_x to quickly exploit good basins\n    if num_rw > 0 and evals_used < budget:\n        if np.any(span > 0):\n            base_scale = float(np.mean(span[span > 0]))\n        else:\n            base_scale = 1.0\n        # Slightly smaller initial step than before for more stability\n        rw_sigma = 0.12 * base_scale / np.sqrt(max(dim, 1))\n        current_x = best_x.copy()\n        current_y = best_y\n        for i in range(num_rw):\n            if evals_used >= budget:\n                break\n            factor = 0.5 ** (i / max(1, num_rw - 1))\n            step = rng.normal(0.0, 1.0, size=dim) * (rw_sigma * factor)\n            cand = clip(current_x + step)\n            y = eval_point(cand)\n            if y is None:\n                break\n            if y < current_y:\n                current_x, current_y = cand, y\n\n    if local_budget <= 0 or evals_used >= budget:\n        return best_x\n\n    # ---------------- Local search: simplified derivative-free ES ----------------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Population size scaled with dimension but capped for small budgets\n    # Slightly reduced for small budgets/high dim to avoid overspending per generation\n    lamb = max(5, 4 + int(1.5 * np.log(dim + 1)))\n    lamb = min(lamb, max(4, remaining))  # never exceed remaining evaluations\n    mu = max(2, lamb // 2)\n\n    max_generations = max(1, remaining // lamb)\n    if max_generations <= 0:\n        return best_x\n\n    # Initial step size: scaled with box size and dimension.\n    if np.any(span > 0):\n        base_scale = float(np.mean(span[span > 0]))\n    else:\n        base_scale = 1.0\n\n    # Slightly smaller sigma to avoid too-aggressive moves near good points\n    sigma = 0.15 * base_scale / np.sqrt(max(dim, 1))\n    if sigma <= 0:\n        return best_x\n\n    mean = best_x.copy()\n\n    # Simple step-size adaptation path\n    path_sigma = np.zeros(dim, dtype=float)\n    c_sigma = 0.25\n    d_sigma = 1.1\n    min_sigma = 1e-12 * base_scale\n    max_sigma = 0.5 * base_scale\n\n    # Precompute recombination weights (logarithmic-like via linear but reversed)\n    raw = np.linspace(mu, 1, mu, dtype=float)\n    weights = raw / raw.sum()\n\n    no_improve_gen = 0\n    last_best_y = best_y\n\n    for _ in range(max_generations):\n        if evals_used >= budget:\n            break\n\n        # Sample around current mean; if sigma is tiny, break\n        if sigma <= min_sigma:\n            break\n\n        zs = rng.normal(0.0, 1.0, size=(lamb, dim))\n        xs = mean + sigma * zs\n        xs = clip(xs)\n\n        scored = []\n        for i in range(lamb):\n            if evals_used >= budget:\n                break\n            y = eval_point(xs[i])\n            if y is not None:\n                scored.append((y, xs[i], zs[i]))\n        if not scored:\n            break\n\n        scored.sort(key=lambda t: t[0])\n        elite = scored[:mu]\n\n        # Recombination: update mean in parameter space\n        new_mean = np.zeros(dim, dtype=float)\n        dz = np.zeros(dim, dtype=float)\n        for w, (val, x_i, z_i) in zip(weights, elite):\n            new_mean += w * x_i\n            dz += w * z_i\n\n        # Step-size control via evolution path in normalized space\n        path_sigma = (1 - c_sigma) * path_sigma + np.sqrt(\n            c_sigma * (2.0 - c_sigma)\n        ) * dz\n        path_norm = np.linalg.norm(path_sigma) / np.sqrt(max(dim, 1))\n        sigma *= float(np.exp((path_norm - 1.0) * 0.25 / d_sigma))\n        sigma = float(np.clip(sigma, min_sigma, max_sigma))\n\n        mean = new_mean\n\n        # Early stop if mean is effectively not moving\n        if np.allclose(mean, best_x, atol=1e-12 * base_scale):\n            # Try a small perturbation restart near best_x if some budget remains\n            if budget - evals_used > dim:\n                perturb = rng.normal(0.0, 0.05 * base_scale, size=dim)\n                mean = clip(best_x + perturb)\n                sigma = max(0.1 * base_scale / np.sqrt(max(dim, 1)), min_sigma)\n            else:\n                break\n\n        # Simple stagnation detection: if global best not improved, reduce sigma\n        if best_y < last_best_y - 1e-12:\n            no_improve_gen = 0\n            last_best_y = best_y\n        else:\n            no_improve_gen += 1\n            if no_improve_gen >= 5:\n                sigma *= 0.5\n                sigma = max(sigma, min_sigma)\n                no_improve_gen = 0\n                if sigma <= min_sigma:\n                    break\n\n    return best_x",
    "X": "0.7000003970469661 0.0999997451229606"
}