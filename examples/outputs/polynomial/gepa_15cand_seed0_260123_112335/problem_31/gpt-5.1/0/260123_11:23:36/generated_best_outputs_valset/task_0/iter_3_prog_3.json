{
    "score": -6.59408725364463,
    "Input": "McCourt20",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization with a hybrid global-local search.\n    Uses quasi-random (Halton-like) sampling + CMA-ES-style local search,\n    with optional warm start from prev_best_x.\n    \"\"\"\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    def eval_point(x):\n        \"\"\"Evaluate x, update global best, respect budget. Returns y or None if budget exhausted.\"\"\"\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # ---- Initialization / warm start ----\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape == (dim,):\n            x0 = clip(x0)\n            eval_point(x0)\n\n    # If no warm-start or it did not improve anything, start from center + random\n    if best_x is None:\n        center = (low + high) / 2.0\n        # Small random perturbation around center to break symmetry\n        perturb = np.random.uniform(-0.1, 0.1, size=dim) * span\n        x0 = clip(center + perturb)\n        eval_point(x0)\n\n    # If still no valid evaluation (degenerate budget), return center\n    if best_x is None:\n        return (low + high) / 2.0\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # ---- Budget split: global vs local ----\n    # Slightly more exploration for robustness, but ensure at least a few local iterations.\n    global_budget = max(1, int(0.6 * remaining))\n    local_budget = remaining - global_budget\n    # Guarantee minimum local search effort when budget is sufficient\n    if remaining >= 20 and local_budget < 8:\n        # Steal some from global if needed\n        deficit = 8 - local_budget\n        steal = min(deficit, global_budget - 4)\n        if steal > 0:\n            global_budget -= steal\n            local_budget += steal\n\n    # ---- Global search: quasi-random + localized samples ----\n    num_global = global_budget\n    if num_global <= 0 or evals_used >= budget:\n        return best_x\n\n    # Split: more uniform coverage, some local refinement around best\n    num_uniform = int(0.7 * num_global)\n    num_localized = num_global - num_uniform\n\n    def halton_sequence(index, base):\n        \"\"\"Generate a scalar in [0,1) from a Halton sequence for a given index and base.\"\"\"\n        f = 1.0\n        r = 0.0\n        i = index\n        while i > 0:\n            f /= base\n            r += f * (i % base)\n            i //= base\n        return r\n\n    # Predefined (pseudo-)prime bases for first dimensions; extend if dim > len(primes)\n    primes = [2, 3, 5, 7, 11, 13, 17, 19]\n    if dim > len(primes):\n        last = primes[-1]\n        while len(primes) < dim:\n            last += 2\n            primes.append(last)\n\n    # --- Quasi-random uniform sampling over the box ---\n    for i in range(num_uniform):\n        if evals_used >= budget:\n            break\n        # Halton-like point in [0,1]^dim\n        u = np.empty(dim)\n        idx = i + 1  # Halton sequences are usually 1-based\n        for d in range(dim):\n            u[d] = halton_sequence(idx, primes[d])\n        x = low + u * span\n        eval_point(x)\n\n    # --- Localized random sampling around current best with shrinking radius ---\n    if best_x is None:\n        # Fallback, should not happen but keep safe\n        best_x = clip((low + high) / 2.0)\n\n    for i in range(num_localized):\n        if evals_used >= budget:\n            break\n        if num_localized > 1:\n            frac = (i + 1) / num_localized\n        else:\n            frac = 1.0\n        # Shrink radius from 50% of span to 5% of span\n        radius = 0.5 - 0.45 * frac\n        step = np.random.randn(dim) * (span * radius)\n        x = clip(best_x + step)\n        eval_point(x)\n\n    if local_budget <= 0 or evals_used >= budget:\n        return best_x\n\n    # ---- Local CMA-ES-style search around best_x ----\n    # Population sizes tuned for efficiency across dims\n    lamb = max(6, 4 + int(3 * np.log(dim + 1)))\n    mu = max(2, lamb // 2)\n\n    # Expected evaluations per generation \u2248 lamb\n    max_generations = max(1, local_budget // lamb)\n    if max_generations <= 0 or lamb <= 0:\n        return best_x\n\n    # Initial step size proportional to box size but adaptive with dim\n    base_scale = np.mean(span[span > 0]) if np.any(span > 0) else 1.0\n    sigma = 0.3 * base_scale / np.sqrt(dim if dim > 0 else 1)\n\n    if sigma <= 0:\n        return best_x\n\n    mean = best_x.copy()\n    cov = np.eye(dim)\n\n    # Ensure numerical stability\n    min_sigma = 1e-12 * (base_scale if base_scale > 0 else 1.0)\n\n    # Precompute small ridge for covariance\n    ridge = 1e-8\n\n    # Simple evolution path for step-size control (not full CMA, but helps)\n    path_sigma = np.zeros(dim)\n    c_sigma = 0.3\n    d_sigma = 1.0\n\n    try:\n        chol = np.linalg.cholesky(cov)\n    except np.linalg.LinAlgError:\n        chol = np.eye(dim)\n\n    for _ in range(max_generations):\n        if evals_used >= budget:\n            break\n\n        # Recompute Cholesky occasionally for robustness\n        try:\n            chol = np.linalg.cholesky(cov + ridge * np.eye(dim))\n        except np.linalg.LinAlgError:\n            cov = np.eye(dim)\n            chol = np.eye(dim)\n\n        zs = np.random.randn(lamb, dim)\n        xs = mean + sigma * zs @ chol.T\n        xs = clip(xs)\n\n        ys = []\n        for i in range(lamb):\n            if evals_used >= budget:\n                break\n            y = eval_point(xs[i])\n            if y is not None:\n                ys.append((y, xs[i], zs[i]))\n        if not ys:\n            break\n\n        ys.sort(key=lambda t: t[0])\n        top = ys[:mu]\n\n        # Compute new mean in parameter space (weighted recombination)\n        weights = np.linspace(mu, 1, mu, dtype=float)\n        weights /= weights.sum()\n        new_mean = np.zeros(dim)\n        for w, (_, x_i, _) in zip(weights, top):\n            new_mean += w * x_i\n\n        # Adapt covariance using weighted ranked samples in z-space\n        dz = np.array([z for (_, _, z) in top])\n        dz_mean = np.average(dz, axis=0, weights=weights)\n        centered = dz - dz_mean\n        C_new = np.zeros((dim, dim))\n        for w, c in zip(weights, centered):\n            C_new += w * np.outer(c, c)\n\n        # Dampen covariance update for stability\n        cov = 0.6 * cov + 0.4 * (C_new + ridge * np.eye(dim))\n\n        # Step-size control using simple path\n        mean_step = (new_mean - mean) / max(sigma, 1e-16)\n        path_sigma = (1 - c_sigma) * path_sigma + np.sqrt(\n            c_sigma * (2 - c_sigma)\n        ) * mean_step\n        # Increase sigma if path length is large, decrease if small\n        path_norm = np.linalg.norm(path_sigma) / np.sqrt(dim if dim > 0 else 1)\n        # Reference length ~1 for normal distribution\n        sigma *= np.exp((path_norm - 1.0) * 0.2 / d_sigma)\n        sigma = max(sigma, min_sigma)\n\n        mean = new_mean\n\n        # Early stop if step size is extremely small relative to search space\n        if sigma <= min_sigma or np.allclose(mean, best_x, atol=1e-12 * base_scale):\n            break\n\n    return best_x",
    "X": "0.7000037088867267 0.1000945703919742"
}