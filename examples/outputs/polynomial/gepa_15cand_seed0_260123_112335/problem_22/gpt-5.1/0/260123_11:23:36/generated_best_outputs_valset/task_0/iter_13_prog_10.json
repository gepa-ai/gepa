{
    "score": -0.3892686968678052,
    "Input": "McCourt11",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization with hybrid global/local search + CMA-like refinement.\n\n    This revision:\n    - Keeps the overall hybrid structure (warm start + global + local + CMA-like)\n    - Makes the budget use more predictable and denser (fewer \"wasted\" tiny phases)\n    - Slightly simplifies global/local heuristics to reduce pathological behaviors\n    - Adds gentle restart / diversification if CMA stagnates badly\n    \"\"\"\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    if dim <= 0 or budget <= 0 or bounds.size == 0:\n        if bounds.size == 0:\n            return np.zeros(dim, dtype=float)\n        low = bounds[:, 0]\n        high = bounds[:, 1]\n        return (low + high) / 2.0\n\n    seed = config.get(\"seed\", None)\n    rng = np.random.RandomState(seed) if seed is not None else np.random\n\n    low = bounds[:, 0].astype(float)\n    high = bounds[:, 1].astype(float)\n    span = high - low\n    # Avoid division / multiplication issues\n    zero_mask = span == 0.0\n    span_safe = span.copy()\n    span_safe[zero_mask] = 1.0\n\n    def clip(x):\n        return np.minimum(np.maximum(x, low), high)\n\n    best_x = None\n    best_y = np.inf\n    evals = 0\n\n    def safe_eval(x):\n        nonlocal evals, best_x, best_y\n        if evals >= budget:\n            return np.inf\n        try:\n            y = objective_function(x)\n        except Exception:\n            y = np.inf\n        evals += 1\n        if np.isfinite(y) and y < best_y:\n            best_x, best_y = x.copy(), y\n        return y\n\n    # ----------------- Warm start from prev_best_x -----------------\n    if prev_best_x is not None and evals < budget:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.size == dim and np.all(np.isfinite(x0)):\n                x0 = clip(x0)\n                y0 = safe_eval(x0)\n                if np.isfinite(y0) and evals < budget:\n                    # Jitter around previous best, scale with domain and budget\n                    jitter_scale = 0.05 * span_safe\n                    jitter_scale[zero_mask] = 0.1\n                    # More jitters when budget allows, capped to avoid overspending here\n                    n_jitter = min(max(dim, 4), max(1, (budget // 20)))\n                    n_jitter = min(n_jitter, budget - evals)\n                    for _ in range(n_jitter):\n                        if evals >= budget:\n                            break\n                        cand = x0 + rng.uniform(-jitter_scale, jitter_scale)\n                        cand = clip(cand)\n                        safe_eval(cand)\n        except Exception:\n            pass\n\n    # Fallback initial point if needed\n    if (best_x is None or not np.isfinite(best_y)) and evals < budget:\n        x0 = low + span_safe * rng.rand(dim)\n        y0 = safe_eval(x0)\n        if not np.isfinite(y0):\n            best_x, best_y = x0, np.inf\n\n    if best_x is None or evals >= budget:\n        return np.asarray(\n            best_x if best_x is not None else (low + high) / 2.0, dtype=float\n        )\n\n    remaining_total = budget - evals\n    if remaining_total <= 0:\n        return np.asarray(best_x, dtype=float)\n\n    # ----------------- Low-discrepancy style warmup -----------------\n    # Provide some diverse points early in the budget\n    if remaining_total >= max(5, 2 * dim):\n        warmup_evals = min(max(4, 3 * dim), remaining_total // 3)\n        remaining_total -= warmup_evals\n\n        # Simple Halton sequence generator\n        def halton_index(i, base):\n            f = 1.0\n            r = 0.0\n            while i > 0:\n                f *= 1.0 / base\n                r += f * (i % base)\n                i //= base\n            return r\n\n        bases = [2, 3, 5, 7, 11, 13, 17, 19]\n        while len(bases) < dim:\n            bases.append(bases[-1] + 2)\n\n        for i in range(1, warmup_evals + 1):\n            if evals >= budget:\n                break\n            u = np.empty(dim)\n            for d in range(dim):\n                # random offset for robustness\n                u[d] = halton_index(i + rng.randint(0, 1024), bases[d])\n            u = np.mod(u, 1.0)\n            x = low + span_safe * u\n            # Mild bias toward current best\n            if rng.rand() < 0.4 and best_x is not None:\n                center = (low + high) * 0.5\n                scale = 0.25\n                x = best_x + (x - center) * scale\n                x = clip(x)\n            safe_eval(x)\n\n    remaining_total = budget - evals\n    if remaining_total <= 0 or best_x is None:\n        return np.asarray(\n            best_x if best_x is not None else (low + high) / 2.0, dtype=float\n        )\n\n    # ----------------- Budget split: global / local / CMA-like -----------------\n    # dimension dependent exploration emphasis\n    if dim <= 3:\n        base_global_frac = 0.4\n    elif dim <= 8:\n        base_global_frac = 0.55\n    elif dim <= 20:\n        base_global_frac = 0.65\n    else:\n        base_global_frac = 0.7\n\n    if remaining_total <= 20:\n        # For tiny budgets, focus on global-ish moves\n        global_frac = min(0.85, base_global_frac + 0.2)\n        cma_evals = 0\n    elif remaining_total <= 80:\n        global_frac = base_global_frac\n        cma_evals = max(4, remaining_total // 10)\n    else:\n        global_frac = base_global_frac\n        cma_evals = max(10, remaining_total // 6)\n\n    # Limit CMA to a portion of remaining_total to keep global+local viable\n    cma_evals = min(cma_evals, remaining_total // 2)\n    cma_evals = max(0, cma_evals)\n    remaining = remaining_total - cma_evals\n\n    n_global = max(1, int(remaining * global_frac))\n    n_local = max(0, remaining - n_global)\n\n    # Ensure we have at least some local search when budget is enough\n    if n_local == 0 and remaining >= max(6, 2 * dim):\n        reserve = min(max(4, dim), remaining // 4)\n        n_global = max(1, remaining - reserve)\n        n_local = remaining - n_global\n\n    # ----------------- Global search -----------------\n    if n_global > 0 and evals < budget:\n        if n_global >= dim + 2:\n            base_linspace = np.linspace(0.0, 1.0, n_global, endpoint=False)\n            jitter = rng.rand(n_global) / n_global\n            strata = base_linspace + jitter\n            perms = [rng.permutation(n_global) for _ in range(dim)]\n\n            for i in range(n_global):\n                if evals >= budget:\n                    break\n\n                # mixture of neighborhood and stratified global sampling\n                use_neighborhood = best_x is not None and rng.rand() < 0.5\n                if use_neighborhood:\n                    progress = evals / float(budget)\n                    scale = 0.3 * (1.0 - 0.5 * progress)\n                    scale = float(np.clip(scale, 0.08, 0.35))\n                    step = scale * span_safe\n                    cand = best_x + rng.uniform(-step, step)\n                    x = clip(cand)\n                else:\n                    u = np.empty(dim)\n                    for d in range(dim):\n                        u[d] = strata[perms[d][i]]\n                    x = low + span_safe * u\n\n                y = safe_eval(x)\n\n                # Opposite sampling occasionally for added diversity\n                if evals < budget and rng.rand() < 0.15 and np.isfinite(y):\n                    x_opp = low + high - x\n                    x_opp = clip(x_opp)\n                    if not np.allclose(x_opp, x, atol=1e-12):\n                        safe_eval(x_opp)\n\n        else:\n            # Small-budget global phase: mixture of random and heavy-tailed local jumps\n            for _ in range(n_global):\n                if evals >= budget:\n                    break\n\n                if best_x is not None and rng.rand() < 0.6:\n                    step = 0.3 * span_safe\n                    u = rng.rand(dim)\n                    # symmetric heavy-tail but clipped to domain\n                    perturb = step * np.tan(np.pi * (u - 0.5))\n                    max_step = 1.5 * span_safe\n                    perturb = np.clip(perturb, -max_step, max_step)\n                    cand = best_x + perturb\n                    x = clip(cand)\n                else:\n                    x = low + span_safe * rng.rand(dim)\n\n                safe_eval(x)\n\n    if best_x is None or evals >= budget:\n        return np.asarray(\n            best_x if best_x is not None else (low + high) / 2.0, dtype=float\n        )\n\n    remaining_after_global = budget - evals\n    if remaining_after_global <= 0:\n        return np.asarray(best_x, dtype=float)\n\n    # ----------------- Local search -----------------\n    # Recompute possible cma_budget based on still-available evaluations\n    cma_budget = min(cma_evals, max(0, remaining_after_global - n_local))\n    n_local = min(n_local, max(0, budget - evals - cma_budget))\n\n    if n_local > 0 and evals < budget:\n        base_step = 0.12 * span_safe\n        base_step[zero_mask] = 1.0\n\n        # ensure at least 1 pass\n        passes = min(20, max(1, n_local // max(1, dim)))\n        evals_per_pass = max(1, n_local // passes)\n\n        current_x = best_x.copy()\n        current_y = best_y\n\n        for p in range(passes):\n            if evals >= budget:\n                break\n\n            # exponential step size decay\n            step_scale = max(0.01, 0.65 ** (p + 1))\n            step = base_step * step_scale\n\n            for _ in range(evals_per_pass):\n                if evals >= budget:\n                    break\n\n                if dim <= 3:\n                    k = 1\n                elif dim <= 10:\n                    k = rng.randint(1, 3)\n                else:\n                    k = rng.randint(1, max(2, dim // 4))\n\n                idx = rng.choice(dim, size=k, replace=False)\n\n                if rng.rand() < 0.25:\n                    local_step = step * 2.0\n                else:\n                    local_step = step\n\n                candidate = current_x.copy()\n                perturb = rng.uniform(-local_step[idx], local_step[idx])\n                candidate[idx] += perturb\n                candidate = clip(candidate)\n\n                y = safe_eval(candidate)\n                if not np.isfinite(y):\n                    continue\n\n                if y < current_y:\n                    current_x, current_y = candidate, y\n                else:\n                    # occasional uphill move for escaping small local traps\n                    if rng.rand() < 0.02:\n                        current_x, current_y = candidate, y\n\n        # Synchronize with global best\n        if current_y < best_y:\n            best_x, best_y = current_x.copy(), current_y\n\n    if evals >= budget:\n        return np.asarray(\n            best_x if best_x is not None else (low + high) / 2.0, dtype=float\n        )\n\n    # ----------------- CMA-like refinement -----------------\n    remaining_for_cma = min(cma_budget, budget - evals)\n    if remaining_for_cma <= 0 or best_x is None:\n        return np.asarray(best_x, dtype=float)\n\n    mean = best_x.copy()\n    sigma = 0.08 * span_safe\n    sigma[zero_mask] = 0.5\n\n    lam = max(4, 2 * dim)\n    lam = min(lam, max(4, remaining_for_cma))\n    iterations = max(1, remaining_for_cma // lam)\n\n    if iterations <= 0 or lam <= 0:\n        return np.asarray(best_x, dtype=float)\n\n    stagnation_counter = 0\n    for _ in range(iterations):\n        if evals >= budget:\n            break\n\n        pop = mean + sigma * rng.randn(lam, dim)\n        pop = np.clip(pop, low, high)\n\n        ys = np.empty(lam, dtype=float)\n        valid_count = 0\n        any_finite = False\n\n        for i in range(lam):\n            if evals >= budget:\n                break\n            y = safe_eval(pop[i])\n            ys[valid_count] = y\n            pop[valid_count] = pop[i]\n            valid_count += 1\n            if np.isfinite(y):\n                any_finite = True\n\n        if valid_count == 0 or not any_finite:\n            continue\n\n        ys = ys[:valid_count]\n        pop = pop[:valid_count]\n\n        finite_mask = np.isfinite(ys)\n        if not np.any(finite_mask):\n            continue\n\n        ys_f = ys[finite_mask]\n        pop_f = pop[finite_mask]\n\n        idx_sorted = np.argsort(ys_f)\n        pop_sorted = pop_f[idx_sorted]\n        ys_sorted = ys_f[idx_sorted]\n\n        mu = max(2, lam // 2)\n        mu = min(mu, pop_sorted.shape[0])\n        elites = pop_sorted[:mu]\n\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        new_mean = np.sum(elites * weights[:, None], axis=0)\n\n        best_iter = ys_sorted[0]\n        # Step-size adaptation: shrink if not improving, expand slightly otherwise\n        if np.isfinite(best_iter) and best_iter <= best_y + 1e-12:\n            sigma *= 0.85\n            stagnation_counter += 1\n        else:\n            sigma *= 1.05\n            stagnation_counter = 0\n\n        sigma = np.maximum(sigma, 1e-8 * span_safe)\n        mean = 0.6 * mean + 0.4 * new_mean\n\n        if np.isfinite(best_iter) and best_iter < best_y:\n            best_y = best_iter\n            best_x = pop_sorted[0].copy()\n\n        # If many steps without improvement, gently restart around current best\n        if stagnation_counter >= 3:\n            stagnation_counter = 0\n            sigma = 0.06 * span_safe\n            if best_x is not None:\n                mean = best_x.copy()\n            else:\n                mean = low + span_safe * rng.rand(dim)\n\n    if best_x is None:\n        best_x = np.clip(mean, low, high)\n\n    return np.asarray(best_x, dtype=float)",
    "X": "0.4004138824391519 0.5998764949420968 0.3996223347917866 1.0 0.3998842256983076 0.19966009321282993 0.9999849615123295 0.30037981468459113"
}