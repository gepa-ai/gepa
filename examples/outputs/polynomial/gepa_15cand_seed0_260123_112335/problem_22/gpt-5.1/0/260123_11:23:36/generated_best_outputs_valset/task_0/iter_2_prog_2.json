{
    "score": -0.3575820855789562,
    "Input": "McCourt11",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization with hybrid global/local search.\n\n    Improvements over previous version:\n    - Use numpy RandomState with optional seed for reproducibility if provided.\n    - More adaptive allocation of global vs local evaluations based on budget.\n    - Global search uses simple low-discrepancy-like stratification when budget allows.\n    - Local search uses adaptive step sizes with occasional larger exploratory moves.\n    \"\"\"\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    # Optional reproducibility\n    seed = config.get(\"seed\", None)\n    rng = np.random.RandomState(seed) if seed is not None else np.random\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    def clip(x):\n        return np.minimum(np.maximum(x, low), high)\n\n    best_x = None\n    best_y = np.inf\n    evals = 0\n\n    # Warm start\n    if prev_best_x is not None and budget > 0:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.size == dim:\n                x0 = clip(x0)\n                y0 = objective_function(x0)\n                evals += 1\n                best_x, best_y = x0, y0\n        except Exception:\n            pass\n\n    # Ensure an initial point if none yet\n    if best_x is None and evals < budget:\n        x0 = low + span * rng.rand(dim)\n        y0 = objective_function(x0)\n        evals += 1\n        best_x, best_y = x0, y0\n\n    if evals >= budget:\n        return np.asarray(best_x, dtype=float)\n\n    remaining = budget - evals\n\n    # Adaptive split: for tiny budgets use mostly global; for large, shift to more local\n    if remaining <= 10:\n        global_frac = 0.8\n    elif remaining <= 50:\n        global_frac = 0.6\n    else:\n        global_frac = 0.45\n\n    n_global = max(1, int(remaining * global_frac))\n    n_local = max(0, remaining - n_global)\n\n    # ---------- Global search ----------\n    # When we have enough samples, use a simple stratified scheme per dimension\n    # to reduce clustering of samples.\n    if n_global > 0:\n        # Precompute stratification indices for each dimension if possible\n        # Each candidate uses different permutations across dims.\n        if n_global >= dim:\n            base_linspace = np.linspace(0.0, 1.0, n_global, endpoint=False)\n            # Jitter within each stratum\n            jitter = rng.rand(n_global) / n_global\n            strata = base_linspace + jitter\n            perms = [rng.permutation(n_global) for _ in range(dim)]\n\n            for i in range(n_global):\n                if rng.rand() < 0.25 and best_x is not None:\n                    # sample near best_x with broad perturbation\n                    step = 0.5 * span\n                    cand = best_x + rng.uniform(-step, step)\n                    x = clip(cand)\n                else:\n                    u = np.empty(dim)\n                    for d in range(dim):\n                        u[d] = strata[perms[d][i]]\n                    x = low + span * u\n\n                y = objective_function(x)\n                evals += 1\n                if y < best_y:\n                    best_x, best_y = x, y\n\n                if evals >= budget:\n                    return np.asarray(best_x, dtype=float)\n        else:\n            # Fallback: simple randomized global search with slight bias toward best_x\n            for _ in range(n_global):\n                if rng.rand() < 0.3 and best_x is not None:\n                    step = 0.5 * span\n                    cand = best_x + rng.uniform(-step, step)\n                    x = clip(cand)\n                else:\n                    x = low + span * rng.rand(dim)\n\n                y = objective_function(x)\n                evals += 1\n                if y < best_y:\n                    best_x, best_y = x, y\n\n                if evals >= budget:\n                    return np.asarray(best_x, dtype=float)\n\n    # ---------- Local search ----------\n    if best_x is None or n_local <= 0 or evals >= budget:\n        return np.asarray(best_x, dtype=float)\n\n    base_step = 0.25 * span\n    base_step[base_step == 0] = 1.0\n\n    # Distribute local evaluations\n    passes = min(6, n_local)\n    evals_per_pass = max(1, n_local // passes)\n\n    current_x = best_x.copy()\n    current_y = best_y\n\n    for p in range(passes):\n        # Gradual reduction of step size\n        step_scale = max(0.05, 0.6 ** p)\n        step = base_step * step_scale\n\n        for _ in range(evals_per_pass):\n            # Occasionally take a slightly larger exploratory step\n            if rng.rand() < 0.15:\n                local_step = step * 2.0\n            else:\n                local_step = step\n\n            # Random subset size: between 1 and dim, leaning toward small subsets\n            if dim <= 3:\n                k = 1\n            else:\n                k = rng.randint(1, max(2, dim // 2))\n            idx = rng.choice(dim, size=k, replace=False)\n\n            candidate = current_x.copy()\n            perturb = rng.uniform(-local_step[idx], local_step[idx])\n            candidate[idx] += perturb\n            candidate = clip(candidate)\n\n            y = objective_function(candidate)\n            evals += 1\n\n            if y < best_y:\n                best_x, best_y = candidate, y\n                current_x, current_y = candidate, y\n            elif y < current_y:\n                # accept mild improvements to avoid getting stuck\n                current_x, current_y = candidate, y\n\n            if evals >= budget:\n                return np.asarray(best_x, dtype=float)\n\n    return np.asarray(best_x, dtype=float)",
    "X": "0.408199168061874 0.5958471917275823 0.38894606275584975 1.0 0.39258955733625284 0.18974198995283398 1.0 0.31314237580991605"
}