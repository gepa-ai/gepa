{
    "score": -0.3759994413026826,
    "Input": "McCourt11",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization with hybrid global/local search + simple CMA-ES style refinement.\n\n    Strategy:\n    - Use prev_best_x as warm start if available.\n    - Global search: quasi-stratified sampling + perturbations around best.\n    - Local search: adaptive coordinate-wise search.\n    - CMA-like refinement on final stage when budget allows for exploitation.\n    \"\"\"\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    if dim <= 0 or budget <= 0:\n        # Degenerate cases: return midpoint of bounds\n        low = bounds[:, 0]\n        high = bounds[:, 1]\n        return (low + high) / 2.0\n\n    seed = config.get(\"seed\", None)\n    rng = np.random.RandomState(seed) if seed is not None else np.random\n\n    low = bounds[:, 0].astype(float)\n    high = bounds[:, 1].astype(float)\n    span = high - low\n    span[span == 0.0] = 1.0\n\n    def clip(x):\n        return np.minimum(np.maximum(x, low), high)\n\n    best_x = None\n    best_y = np.inf\n    evals = 0\n\n    # --- Warm start from prev_best_x if valid ---\n    if prev_best_x is not None and budget > 0:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.size == dim and np.all(np.isfinite(x0)):\n                x0 = clip(x0)\n                y0 = objective_function(x0)\n                evals += 1\n                if np.isfinite(y0):\n                    best_x, best_y = x0, y0\n        except Exception:\n            pass\n\n    # Ensure we have at least one valid starting point\n    if (best_x is None or not np.isfinite(best_y)) and evals < budget:\n        x0 = low + span * rng.rand(dim)\n        y0 = objective_function(x0)\n        evals += 1\n        if np.isfinite(y0):\n            best_x, best_y = x0, y0\n        else:\n            best_x, best_y = x0, np.inf\n\n    if evals >= budget:\n        return np.asarray(best_x, dtype=float)\n\n    remaining = budget - evals\n\n    # Adaptive split among global / local / CMA-like refinement\n    # Give some fixed tail portion to CMA-like refinement when budget large\n    if remaining <= 15:\n        global_frac = 0.7\n        cma_evals = 0\n    elif remaining <= 60:\n        global_frac = 0.55\n        cma_evals = max(3, remaining // 8)\n    else:\n        global_frac = 0.45\n        cma_evals = max(5, remaining // 6)\n\n    cma_evals = min(cma_evals, remaining // 2)\n    remaining -= cma_evals\n\n    n_global = max(1, int(remaining * global_frac))\n    n_local = max(0, remaining - n_global)\n\n    # ---------- Global search ----------\n    if n_global > 0:\n        # Stratified sampling when many points, else pure random\n        if n_global >= dim + 3:\n            base_linspace = np.linspace(0.0, 1.0, n_global, endpoint=False)\n            jitter = rng.rand(n_global) / n_global\n            strata = base_linspace + jitter\n            perms = [rng.permutation(n_global) for _ in range(dim)]\n\n            for i in range(n_global):\n                # Mixture: global stratified and wide perturb around best\n                if best_x is not None and rng.rand() < 0.3:\n                    wide_step = 0.5 * span\n                    cand = best_x + rng.uniform(-wide_step, wide_step)\n                    x = clip(cand)\n                else:\n                    u = np.empty(dim)\n                    for d in range(dim):\n                        u[d] = strata[perms[d][i]]\n                    x = low + span * u\n\n                y = objective_function(x)\n                evals += 1\n                if np.isfinite(y) and y < best_y:\n                    best_x, best_y = x, y\n\n                if evals >= budget:\n                    return np.asarray(best_x, dtype=float)\n        else:\n            # Random global search with heavy-tailed perturbations near best\n            for _ in range(n_global):\n                if best_x is not None and rng.rand() < 0.4:\n                    # Cauchy-like heavy-tailed perturbation\n                    step = 0.35 * span\n                    perturb = step * np.tan(np.pi * (rng.rand(dim) - 0.5))\n                    cand = best_x + perturb\n                    x = clip(cand)\n                else:\n                    x = low + span * rng.rand(dim)\n\n                y = objective_function(x)\n                evals += 1\n                if np.isfinite(y) and y < best_y:\n                    best_x, best_y = x, y\n\n                if evals >= budget:\n                    return np.asarray(best_x, dtype=float)\n\n    if best_x is None or evals >= budget:\n        return np.asarray(best_x, dtype=float)\n\n    # ---------- Local coordinate search ----------\n    if n_local > 0:\n        base_step = 0.2 * span\n        base_step[base_step == 0] = 1.0\n\n        passes = min(8, n_local)\n        evals_per_pass = max(1, n_local // passes)\n\n        current_x = best_x.copy()\n        current_y = best_y\n\n        for p in range(passes):\n            # Decay step size\n            step_scale = max(0.03, 0.55 ** p)\n            step = base_step * step_scale\n\n            for _ in range(evals_per_pass):\n                # Randomly pick between 1 and dim/2 coordinates\n                if dim <= 3:\n                    k = 1\n                else:\n                    k = rng.randint(1, max(2, dim // 2))\n\n                idx = rng.choice(dim, size=k, replace=False)\n\n                # Occasionally take larger exploratory move\n                if rng.rand() < 0.2:\n                    local_step = step * 2.0\n                else:\n                    local_step = step\n\n                candidate = current_x.copy()\n                perturb = rng.uniform(-local_step[idx], local_step[idx])\n                candidate[idx] += perturb\n                candidate = clip(candidate)\n\n                y = objective_function(candidate)\n                evals += 1\n\n                if np.isfinite(y) and y < best_y:\n                    best_x, best_y = candidate, y\n                    current_x, current_y = candidate, y\n                elif np.isfinite(y) and y < current_y:\n                    current_x, current_y = candidate, y\n                else:\n                    # small probability to accept uphill move to escape plateaus\n                    if np.isfinite(y) and rng.rand() < 0.02:\n                        current_x, current_y = candidate, y\n\n                if evals >= budget:\n                    return np.asarray(best_x, dtype=float)\n\n    if evals >= budget or cma_evals <= 0 or best_x is None:\n        return np.asarray(best_x, dtype=float)\n\n    # ---------- Simple CMA-ES style refinement ----------\n    # Very lightweight: diagonal covariance, mean at best_x\n    mean = best_x.copy()\n    sigma = 0.15 * span\n    sigma[sigma == 0.0] = 0.5\n\n    lam = max(4, 2 * dim)\n    lam = min(lam, max(4, cma_evals))  # at least 4\n    iterations = max(1, cma_evals // lam)\n\n    for _ in range(iterations):\n        if evals >= budget:\n            break\n\n        # Sample population\n        pop = mean + sigma * rng.randn(lam, dim)\n        pop = np.clip(pop, low, high)\n\n        ys = []\n        for i in range(lam):\n            if evals >= budget:\n                break\n            y = objective_function(pop[i])\n            evals += 1\n            ys.append(y)\n            if np.isfinite(y) and y < best_y:\n                best_x, best_y = pop[i].copy(), y\n\n        if len(ys) == 0:\n            break\n\n        ys = np.asarray(ys)\n        if not np.any(np.isfinite(ys)):\n            continue\n\n        # Sort by fitness\n        idx_sorted = np.argsort(ys)\n        pop_sorted = pop[idx_sorted]\n        ys_sorted = ys[idx_sorted]\n\n        # Weighted recombination using top half\n        mu = max(2, lam // 2)\n        elites = pop_sorted[:mu]\n\n        # Logarithmic weights\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n\n        mean = np.sum(elites * weights[:, None], axis=0)\n\n        # Adapt sigma based on improvement\n        best_iteration = ys_sorted[0]\n        if np.isfinite(best_iteration) and best_iteration < best_y + 1e-12:\n            sigma *= 0.8\n        else:\n            sigma *= 1.05\n\n        sigma = np.maximum(sigma, 1e-8 * span)\n\n    return np.asarray(best_x, dtype=float)",
    "X": "0.40205451795849306 0.597464729246844 0.39549299868332244 1.0 0.3959124494402501 0.19564376888870516 1.0 0.3041263316208678"
}