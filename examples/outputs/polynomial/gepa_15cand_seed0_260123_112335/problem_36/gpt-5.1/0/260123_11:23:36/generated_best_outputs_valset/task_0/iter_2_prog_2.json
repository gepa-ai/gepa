{
    "score": -3.45,
    "Input": "Michalewicz",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization with budget-aware hybrid search:\n    - Global search via Latin Hypercube / low-discrepancy style sampling\n    - Uses warm-start from prev_best_x when available\n    - Local CMA-ES\u2013like evolutionary refinement around best-so-far\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # Handle degenerate budget\n    if budget <= 0:\n        if prev_best_x is not None:\n            return clip(np.asarray(prev_best_x, dtype=float).reshape(dim))\n        return clip((low + high) * 0.5)\n\n    # Warm-start if available (does not consume all budget)\n    if prev_best_x is not None:\n        x0 = clip(np.asarray(prev_best_x, dtype=float).reshape(dim))\n        eval_point(x0)\n\n    # Ensure at least one evaluated point\n    if best_x is None and evals_used < budget:\n        x0 = low + span * rng.rand(dim)\n        eval_point(x0)\n\n    # If tiny remaining budget, just sample randomly\n    if budget - evals_used <= 3:\n        while evals_used < budget:\n            x = low + span * rng.rand(dim)\n            eval_point(x)\n        return best_x\n\n    remaining = budget - evals_used\n\n    # ---------------- Global exploration: Latin Hypercube + random ----------------\n    # Allocate ~50% of remaining to global search\n    global_evals = max(4, int(0.5 * remaining))\n    global_evals = min(global_evals, remaining)\n\n    def latin_hypercube(n_samples, dim):\n        # LHS in [0,1]^dim\n        cut = np.linspace(0, 1, n_samples + 1)\n        u = rng.rand(n_samples, dim)\n        a = cut[:n_samples]\n        b = cut[1:n_samples + 1]\n        rdpoints = a[:, None] + (b - a)[:, None] * u\n        for j in range(dim):\n            rng.shuffle(rdpoints[:, j])\n        return rdpoints\n\n    try:\n        lhs = latin_hypercube(global_evals, dim)\n        global_points = clip(low + span * lhs)\n    except Exception:\n        global_points = clip(low + span * rng.rand(global_evals, dim))\n\n    for i in range(global_evals):\n        if evals_used >= budget:\n            break\n        eval_point(global_points[i])\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # ---------------- Local evolutionary refinement (CMA-ES\u2013like) ----------------\n    # Population size scaled with dimension but kept modest\n    lam = max(4, min(10 + dim, remaining))\n    sigma0 = 0.3  # initial global step size (fraction of span)\n\n    # Diagonal covariance adaptation for robustness\n    cov_diag = (span * sigma0) ** 2 + 1e-12\n    min_sigma = 1e-6\n\n    no_improve_iters = 0\n    best_y_local = best_y\n\n    while remaining > 0:\n        pop_size = min(lam, remaining)\n        # Sample population\n        Z = rng.randn(pop_size, dim)\n        candidates = clip(best_x + Z * np.sqrt(cov_diag))\n\n        ys = []\n        for i in range(pop_size):\n            if evals_used >= budget:\n                break\n            y = eval_point(candidates[i])\n            ys.append(y)\n        remaining = budget - evals_used\n        if not ys:\n            break\n\n        ys = np.array(ys)\n        # Sort by fitness\n        idx = np.argsort(ys)\n        candidates = candidates[idx]\n        ys = ys[idx]\n\n        # Track local improvement\n        if ys[0] < best_y_local - 1e-12:\n            best_y_local = ys[0]\n            no_improve_iters = 0\n        else:\n            no_improve_iters += 1\n\n        # Recompute best_x is already tracked in eval_point, but for covariance center:\n        center = best_x\n\n        # Update diagonal covariance based on top half of population\n        elite_count = max(2, pop_size // 2)\n        elites = candidates[:elite_count]\n        diffs = elites - center\n        # Exponential moving average of squared steps\n        alpha = 0.3\n        cov_diag = (1 - alpha) * cov_diag + alpha * (np.mean(diffs ** 2, axis=0) + 1e-12)\n\n        # Step-size adaptation: shrink when no improvement for several iterations\n        if no_improve_iters >= 3:\n            cov_diag *= 0.5\n            no_improve_iters = 0\n\n        cov_diag = np.maximum(cov_diag, (span * min_sigma) ** 2)\n\n        if remaining <= 0:\n            break\n\n    return best_x",
    "X": "2.1938584037692714 1.6066213983903288 2.2184806043391183 1.0982712353280524"
}