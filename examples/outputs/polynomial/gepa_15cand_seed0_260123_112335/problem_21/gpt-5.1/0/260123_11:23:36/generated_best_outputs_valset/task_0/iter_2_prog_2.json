{
    "score": -2.519395962832244,
    "Input": "McCourt10",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid global-local derivative-free optimization.\n\n    Strategy:\n    - Use quasi-random Sobol-like sampling (via scrambled low-discrepancy sequence)\n      for global exploration, seeded by prev_best_x when available.\n    - Maintain and refine the current best using coordinate pattern search.\n    - Adapt allocation of budget: more global search for large-dimensional or small budgets,\n      more local refinement for moderate/large budgets.\n    \"\"\"\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low, high = bounds[:, 0], bounds[:, 1]\n    span = high - low\n    span[span == 0.0] = 1.0  # avoid zero for scaling\n\n    # Utility: project to bounds\n    def clip_to_bounds(x):\n        return np.minimum(np.maximum(x, low), high)\n\n    evals_used = 0\n\n    # ---- Initial / warm start evaluation ----\n    best_x = None\n    best_y = None\n\n    if prev_best_x is not None:\n        x0 = clip_to_bounds(np.asarray(prev_best_x, dtype=float))\n        best_x = x0\n        best_y = objective_function(best_x)\n        evals_used += 1\n\n    # If no warm start or budget already exhausted, create a random starting point\n    if evals_used >= budget:\n        return best_x\n\n    if best_x is None:\n        x0 = np.random.uniform(low, high)\n        best_x = x0\n        best_y = objective_function(best_x)\n        evals_used += 1\n        if evals_used >= budget:\n            return best_x\n\n    # ---- Global search: low-discrepancy sampling ----\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Heuristic split of budget between global and local search\n    # More global search for higher dimension and small budgets.\n    if budget < 20:\n        global_frac = 0.7\n    elif dim <= 5:\n        global_frac = 0.35\n    elif dim <= 15:\n        global_frac = 0.45\n    else:\n        global_frac = 0.55\n\n    global_evals = max(dim + 3, int(global_frac * budget))\n    global_evals = min(global_evals, remaining)\n\n    # Use a simple scrambled low-discrepancy sequence (Halton-like) for sampling\n    # without importing extra libraries.\n    def _primes(n):\n        # tiny helper to generate first n primes\n        primes = []\n        candidate = 2\n        while len(primes) < n:\n            is_p = True\n            for p in primes:\n                if p * p > candidate:\n                    break\n                if candidate % p == 0:\n                    is_p = False\n                    break\n            if is_p:\n                primes.append(candidate)\n            candidate += 1\n        return primes\n\n    bases = _primes(dim)\n\n    # random offset to \"scramble\" sequence\n    offset = np.random.randint(0, 1000000)\n\n    def halton_point(index):\n        # index >= 1\n        seq = np.empty(dim, dtype=float)\n        i = index + offset\n        for k, b in enumerate(bases):\n            f = 1.0\n            r = 0.0\n            ii = i\n            while ii > 0:\n                f /= b\n                r += f * (ii % b)\n                ii //= b\n            seq[k] = r\n        return seq\n\n    for j in range(1, global_evals + 1):\n        if evals_used >= budget:\n            break\n        u = halton_point(j)\n        x = low + u * (high - low)\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x\n\n    if evals_used >= budget:\n        return best_x\n\n    # ---- Local search: coordinate pattern search starting from best_x ----\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    x = best_x.copy()\n    y = best_y\n\n    # Step size as fraction of span; adapt based on budget and dimension\n    base_step_scale = 0.25\n    if dim > 20:\n        base_step_scale = 0.2\n    if budget < 50:\n        base_step_scale *= 0.7\n\n    step = base_step_scale * span\n    step[step == 0] = base_step_scale  # fallback if any dimension had zero span\n\n    min_step_scale = 1e-6\n\n    # Limiting maximum number of local iterations based on remaining budget\n    # each iteration costs up to 2*dim evaluations\n    max_local_iters = max(1, remaining // (2 * dim if dim > 0 else 1))\n\n    it = 0\n    while evals_used < budget and it < max_local_iters:\n        it += 1\n        improved = False\n\n        for i in range(dim):\n            if evals_used >= budget:\n                break\n\n            # Positive direction\n            xp = x.copy()\n            xp[i] = xp[i] + step[i]\n            xp = clip_to_bounds(xp)\n            yp = objective_function(xp)\n            evals_used += 1\n\n            if yp < y:\n                x, y = xp, yp\n                improved = True\n                if y < best_y:\n                    best_x, best_y = x, y\n                if evals_used >= budget:\n                    break\n                # after improvement, optionally try an extra extrapolated step\n                xpp = x.copy()\n                xpp[i] = xpp[i] + step[i]\n                xpp = clip_to_bounds(xpp)\n                if evals_used < budget:\n                    ypp = objective_function(xpp)\n                    evals_used += 1\n                    if ypp < y:\n                        x, y = xpp, ypp\n                        if y < best_y:\n                            best_x, best_y = x, y\n                continue  # skip negative direction once improved\n\n            if evals_used >= budget:\n                break\n\n            # Negative direction\n            xn = x.copy()\n            xn[i] = xn[i] - step[i]\n            xn = clip_to_bounds(xn)\n            yn = objective_function(xn)\n            evals_used += 1\n            if yn < y:\n                x, y = xn, yn\n                improved = True\n                if y < best_y:\n                    best_x, best_y = x, y\n\n        if not improved:\n            step *= 0.5\n            # if step sizes are tiny relative to domain, stop local search\n            if np.all(step < min_step_scale * np.maximum(span, 1.0)):\n                break\n\n    return best_x",
    "X": "0.5085224859203518 0.5433150447686769 0.22728136272027988 1.0 0.3380430879314512 0.0255126953125 1.0 0.5038059976555963"
}