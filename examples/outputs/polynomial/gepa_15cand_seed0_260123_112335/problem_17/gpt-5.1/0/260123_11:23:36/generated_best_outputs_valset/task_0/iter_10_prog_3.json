{
    "score": 2.8072029759458204,
    "Input": "McCourt06",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Derivative-free optimizer: hybrid global evolutionary search + local refinement.\n\n    Strategy:\n      1. Warm start: if prev_best_x is valid, include it in the initial population.\n      2. Global phase (~70% budget): simple evolutionary algorithm with:\n         - population sampled in normalized [0,1]^d space\n         - tournament selection\n         - blend crossover / directional moves\n         - coordinate-wise and global mutations\n      3. Local phase (~30% budget): adaptive coordinate search around best point.\n      4. Any remaining budget is spent on diversified local/global perturbations.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    # Safety reshape\n    if bounds.shape != (dim, 2):\n        bounds = bounds.reshape(dim, 2)\n    low, high = bounds[:, 0], bounds[:, 1]\n    span = high - low\n    # Avoid zero-span issues\n    zero_span_mask = span == 0.0\n    if np.any(zero_span_mask):\n        span[zero_span_mask] = 1.0\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    # Normalize to [0,1]^dim for evolutionary operations\n    def to_unit(x):\n        return (x - low) / span\n\n    def from_unit(u):\n        return clip_to_bounds(low + u * span)\n\n    evals_used = 0\n\n    # --- Initialization with optional warm start ---\n    best_x = None\n    best_y = np.inf\n\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = clip_to_bounds(x0)\n            y0 = objective_function(x0)\n            evals_used += 1\n            best_x, best_y = x0, y0\n\n    if best_x is None:\n        x0 = rng.uniform(low, high)\n        y0 = objective_function(x0)\n        evals_used += 1\n        best_x, best_y = x0, y0\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # ----------------- Global Evolutionary Phase -----------------\n    # Allocate ~70% of remaining evaluations to EA\n    global_fraction = 0.7\n    global_budget = max(1, int(remaining * global_fraction))\n    global_budget = min(global_budget, remaining)\n\n    # Population size and generations heuristics depending on budget and dim\n    # Aim for reasonable population but ensure multiple generations.\n    min_pop = 4\n    max_pop = 40\n    base_pop = 4 * dim\n    pop_size = int(np.clip(base_pop, min_pop, max_pop))\n    pop_size = min(pop_size, max(2, global_budget // 3))  # ensure enough evals remain\n\n    # Ensure at least one full evaluation of the population\n    if pop_size <= 0:\n        pop_size = 4\n\n    # Initialize population in unit cube\n    pop_u = rng.uniform(0.0, 1.0, size=(pop_size, dim))\n\n    # Inject warm-start point into population\n    pop_u[0] = to_unit(best_x)\n    pop_y = np.empty(pop_size, dtype=float)\n\n    # Evaluate initial population\n    for i in range(pop_size):\n        if evals_used >= budget:\n            break\n        x = from_unit(pop_u[i])\n        y = objective_function(x)\n        evals_used += 1\n        pop_y[i] = y\n        if y < best_y:\n            best_x, best_y = x, y\n\n    if evals_used >= budget:\n        return best_x\n\n    # Evolution parameters\n    tournament_k = 3\n    crossover_prob = 0.7\n    mutation_prob = 0.4\n    coord_mut_prob = 0.6  # probability of coordinate-wise mutation vs global\n    # Mutation step scales adapt with dimension\n    base_sigma = 0.15 / np.sqrt(max(1, dim))\n\n    def tournament_select():\n        idxs = rng.integers(0, pop_size, size=tournament_k)\n        best_idx = idxs[0]\n        best_val = pop_y[best_idx]\n        for j in idxs[1:]:\n            if pop_y[j] < best_val:\n                best_val = pop_y[j]\n                best_idx = j\n        return best_idx\n\n    # How many full generations can we afford?\n    # Each generation roughly costs ~pop_size evaluations.\n    remaining_global = min(global_budget, budget - evals_used)\n    if pop_size > 0:\n        max_gens = max(1, remaining_global // pop_size)\n    else:\n        max_gens = 0\n\n    for _ in range(max_gens):\n        if evals_used >= budget:\n            break\n\n        new_pop_u = np.empty_like(pop_u)\n        new_pop_y = np.empty_like(pop_y)\n\n        # Elitism: carry over the best current individual\n        elite_idx = np.argmin(pop_y)\n        new_pop_u[0] = pop_u[elite_idx]\n        new_pop_y[0] = pop_y[elite_idx]\n        # Also synchronize with global best\n        elite_x = from_unit(new_pop_u[0])\n        if new_pop_y[0] < best_y:\n            best_x, best_y = elite_x, new_pop_y[0]\n        else:\n            new_pop_y[0] = best_y\n            new_pop_u[0] = to_unit(best_x)\n\n        start_i = 1\n\n        for i in range(start_i, pop_size):\n            if evals_used >= budget:\n                # Fill remaining slots with copies to avoid uninitialized data\n                new_pop_u[i] = new_pop_u[0]\n                new_pop_y[i] = new_pop_y[0]\n                continue\n\n            # Parent selection\n            p1 = pop_u[tournament_select()]\n            if rng.random() < crossover_prob:\n                p2 = pop_u[tournament_select()]\n                alpha = rng.uniform(-0.25, 1.25, size=dim)\n                child = np.clip(alpha * p1 + (1 - alpha) * p2, 0.0, 1.0)\n            else:\n                child = p1.copy()\n\n            # Mutation\n            if rng.random() < mutation_prob:\n                if rng.random() < coord_mut_prob:\n                    # Coordinate-wise mutation: pick a few dims to change\n                    num_coords = rng.integers(1, max(2, dim // 3 + 1))\n                    idxs = rng.choice(dim, size=num_coords, replace=False)\n                    child[idxs] += rng.normal(scale=base_sigma, size=num_coords)\n                else:\n                    # Global perturbation\n                    child += rng.normal(scale=base_sigma, size=dim)\n                child = np.clip(child, 0.0, 1.0)\n\n            x_child = from_unit(child)\n            y_child = objective_function(x_child)\n            evals_used += 1\n            new_pop_u[i] = child\n            new_pop_y[i] = y_child\n\n            if y_child < best_y:\n                best_x, best_y = x_child, y_child\n\n        pop_u, pop_y = new_pop_u, new_pop_y\n\n        if evals_used >= budget:\n            break\n\n    if evals_used >= budget:\n        return best_x\n\n    # ----------------- Local Coordinate Search Phase -----------------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Allocate most of remaining to local search\n    local_budget = int(0.9 * remaining)\n    local_budget = max(0, min(local_budget, remaining))\n\n    # Initial step sizes: fraction of span\n    base_step = np.maximum(span * 0.05, span * 1e-3)\n    step = base_step.copy()\n    min_step = span * 1e-5\n\n    no_improve_sweeps = 0\n\n    def local_eval_trial(x_center, coord, step_size):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None, None\n        trial = x_center.copy()\n        trial[coord] += step_size\n        trial = clip_to_bounds(trial)\n        y = objective_function(trial)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = trial, y\n        return trial, y\n\n    # Estimate sweeps based on local_budget and dimension\n    evals_per_coord = 2  # +/- directions\n    evals_per_sweep = max(1, dim * evals_per_coord)\n    max_sweeps = max(1, local_budget // evals_per_sweep)\n\n    for _ in range(max_sweeps):\n        if evals_used >= budget:\n            break\n\n        improved_any = False\n        coords = rng.permutation(dim)\n\n        for d in coords:\n            if evals_used >= budget:\n                break\n\n            curr_best_y = best_y\n            curr_best_x = best_x\n            s = step[d]\n\n            # Try negative and positive directions (randomized order)\n            dirs = [-1.0, 1.0]\n            rng.shuffle(dirs)\n            for direction in dirs:\n                if evals_used >= budget:\n                    break\n                scaled_step = direction * s * rng.uniform(0.5, 1.5)\n                trial, y = local_eval_trial(curr_best_x, d, scaled_step)\n                if trial is None:\n                    break\n                if y < curr_best_y:\n                    curr_best_y = y\n                    curr_best_x = trial\n\n            if curr_best_y < best_y:\n                best_x, best_y = curr_best_x, curr_best_y\n                improved_any = True\n\n        if improved_any:\n            no_improve_sweeps = 0\n        else:\n            no_improve_sweeps += 1\n            step *= 0.5\n            if np.all(step <= min_step) or no_improve_sweeps >= 3:\n                break\n\n        if evals_used >= budget:\n            break\n\n    # ----------------- Final Diversified Search -----------------\n    while evals_used < budget:\n        remaining = budget - evals_used\n        if remaining <= 0:\n            break\n\n        # Mix between global and local perturbations\n        if rng.random() < 0.4:\n            x = rng.uniform(low, high)\n        else:\n            # Local Gaussian perturbation around best_x\n            # Scale shrinks a bit with remaining budget to refine near best\n            scale_factor = 0.1 * max(0.2, remaining / max(1.0, budget))\n            noise = rng.normal(scale=scale_factor, size=dim) * span\n            x = clip_to_bounds(best_x + noise)\n\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = x, y\n\n    return best_x",
    "X": "1.0 1.0 0.7646757156010417 0.5274552851690573 1.0"
}