{
    "score": 2.8072026324474604,
    "Input": "McCourt06",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Derivative-free optimizer: hybrid global evolutionary search + local refinement,\n    with adaptive allocation based on dimension and budget.\n\n    Main changes vs previous version:\n      - Handle very small budgets more gracefully.\n      - Slightly increased exploitation around warm start if provided.\n      - More conservative population and generation sizing to avoid thin exploration.\n      - Simplified / more robust local search when budget is small or dim is large.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    # Safety reshape\n    if bounds.shape != (dim, 2):\n        bounds = bounds.reshape(dim, 2)\n    low, high = bounds[:, 0], bounds[:, 1]\n    span = high - low\n    # Avoid zero-span issues\n    zero_span_mask = span == 0.0\n    if np.any(zero_span_mask):\n        span[zero_span_mask] = 1.0\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    # Normalize to [0,1]^dim for evolutionary operations\n    def to_unit(x):\n        return (x - low) / span\n\n    def from_unit(u):\n        return clip_to_bounds(low + u * span)\n\n    evals_used = 0\n\n    # --- Initialization with optional warm start ---\n    best_x = None\n    best_y = np.inf\n\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = clip_to_bounds(x0)\n            y0 = objective_function(x0)\n            evals_used += 1\n            best_x, best_y = x0, y0\n\n    # If no warm start or invalid, seed with random point\n    if best_x is None:\n        x0 = rng.uniform(low, high)\n        y0 = objective_function(x0)\n        evals_used += 1\n        best_x, best_y = x0, y0\n    else:\n        # Slight local refinement around warm start if there's tiny extra budget\n        if evals_used < budget:\n            step = 0.05 * span\n            # single small perturbation\n            noise = rng.normal(scale=1.0, size=dim) * step\n            x_try = clip_to_bounds(best_x + noise)\n            y_try = objective_function(x_try)\n            evals_used += 1\n            if y_try < best_y:\n                best_x, best_y = x_try, y_try\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # If budget is extremely small, fall back to pure random search around best_x\n    if remaining <= dim + 2:\n        for _ in range(remaining):\n            # mix of global and local samples\n            if rng.random() < 0.5:\n                x = rng.uniform(low, high)\n            else:\n                noise = rng.normal(scale=0.1, size=dim) * span\n                x = clip_to_bounds(best_x + noise)\n            y = objective_function(x)\n            evals_used += 1\n            if y < best_y:\n                best_x, best_y = x, y\n        return best_x\n\n    # ----------------- Global Evolutionary Phase -----------------\n    # Allocate ~55% of remaining evaluations to EA, slightly more for higher dims\n    global_fraction = 0.55 + 0.05 * np.tanh((dim - 5) / 10.0)\n    global_fraction = float(np.clip(global_fraction, 0.4, 0.7))\n    global_budget = int(remaining * global_fraction)\n    global_budget = max(0, min(global_budget, remaining))\n\n    # Population size based on dimension and global_budget:\n    # keep moderate to allow several generations\n    if global_budget > 0:\n        min_pop = 6\n        max_pop = 40\n        # base_pop linear in dim but capped\n        base_pop = 3 * dim\n        pop_size = int(np.clip(base_pop, min_pop, max_pop))\n        # Ensure at least 3 generations if possible\n        if global_budget >= 3 * pop_size:\n            pass\n        else:\n            # Adjust population size so that we have at least ~2-3 generations\n            approx_gens = 3\n            pop_size = max(\n                min_pop,\n                min(global_budget // approx_gens, max_pop, global_budget),\n            )\n        pop_size = max(min_pop, min(pop_size, global_budget))\n    else:\n        pop_size = 0\n\n    if pop_size > 0 and evals_used < budget:\n        # Initialize population in unit cube\n        pop_u = rng.uniform(0.0, 1.0, size=(pop_size, dim))\n        # Inject warm-start/best point into population\n        pop_u[0] = to_unit(best_x)\n        pop_y = np.empty(pop_size, dtype=float)\n\n        # Evaluate initial population, but do not exceed global_budget or total budget\n        init_evals = min(pop_size, global_budget, budget - evals_used)\n        for i in range(init_evals):\n            if evals_used >= budget:\n                break\n            x = from_unit(pop_u[i])\n            y = objective_function(x)\n            evals_used += 1\n            pop_y[i] = y\n            if y < best_y:\n                best_x, best_y = x, y\n\n        # If less than pop_size was evaluated due to budget, truncate arrays\n        if init_evals < pop_size:\n            pop_u = pop_u[:init_evals]\n            pop_y = pop_y[:init_evals]\n            pop_size = init_evals\n\n        # If we could not evaluate any population individuals, skip EA\n        if pop_size > 0 and evals_used < budget:\n            tournament_k = 3 if pop_size >= 3 else 2\n            crossover_prob = 0.7\n            mutation_prob = 0.6\n            coord_mut_prob = 0.7\n            base_sigma = 0.18 / np.sqrt(max(1, dim))\n\n            def tournament_select():\n                idxs = rng.integers(0, pop_size, size=tournament_k)\n                best_idx = idxs[0]\n                best_val = pop_y[best_idx]\n                for j in idxs[1:]:\n                    if pop_y[j] < best_val:\n                        best_val = pop_y[j]\n                        best_idx = j\n                return best_idx\n\n            remaining_global = min(global_budget - init_evals, budget - evals_used)\n            if pop_size > 0:\n                max_gens = max(1, remaining_global // pop_size) if remaining_global > 0 else 0\n            else:\n                max_gens = 0\n\n            for _ in range(max_gens):\n                if evals_used >= budget:\n                    break\n                if budget - evals_used <= 0:\n                    break\n\n                new_pop_u = np.empty_like(pop_u)\n                new_pop_y = np.empty_like(pop_y)\n\n                # Elitism: carry over best individual from population or global best\n                elite_idx = np.argmin(pop_y)\n                elite_u = pop_u[elite_idx]\n                elite_y = pop_y[elite_idx]\n                if best_y < elite_y:\n                    elite_u = to_unit(best_x)\n                    elite_y = best_y\n                new_pop_u[0] = elite_u\n                new_pop_y[0] = elite_y\n\n                # Remaining individuals\n                for i in range(1, pop_size):\n                    if evals_used >= budget:\n                        # replicate elite to keep arrays valid\n                        new_pop_u[i] = new_pop_u[0]\n                        new_pop_y[i] = new_pop_y[0]\n                        continue\n\n                    p1 = pop_u[tournament_select()]\n                    if rng.random() < crossover_prob:\n                        p2 = pop_u[tournament_select()]\n                        alpha = rng.uniform(-0.25, 1.25, size=dim)\n                        child = alpha * p1 + (1.0 - alpha) * p2\n                        child = np.clip(child, 0.0, 1.0)\n                    else:\n                        child = p1.copy()\n\n                    if rng.random() < mutation_prob:\n                        if rng.random() < coord_mut_prob:\n                            num_coords = rng.integers(1, max(2, dim // 3 + 1))\n                            idxs = rng.choice(dim, size=num_coords, replace=False)\n                            child[idxs] += rng.normal(scale=base_sigma, size=num_coords)\n                        else:\n                            child += rng.normal(scale=base_sigma, size=dim)\n                        child = np.clip(child, 0.0, 1.0)\n\n                    x_child = from_unit(child)\n                    y_child = objective_function(x_child)\n                    evals_used += 1\n                    new_pop_u[i] = child\n                    new_pop_y[i] = y_child\n                    if y_child < best_y:\n                        best_x, best_y = x_child, y_child\n\n                    if evals_used >= budget:\n                        # Fill rest with elite to keep arrays valid\n                        for j in range(i + 1, pop_size):\n                            new_pop_u[j] = new_pop_u[0]\n                            new_pop_y[j] = new_pop_y[0]\n                        break\n\n                pop_u, pop_y = new_pop_u, new_pop_y\n\n    if evals_used >= budget:\n        return best_x\n\n    # ----------------- Local Search Phase -----------------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Allocate all remaining to an adaptive local / refined global mix\n    local_budget = remaining\n\n    # Initial step sizes: fraction of span, reduced for high dimensions\n    dim_scale = 1.0 / np.sqrt(max(1, dim))\n    base_step = np.maximum(span * (0.08 * dim_scale), span * 1e-3)\n    step = base_step.copy()\n    min_step = span * 1e-5\n\n    no_improve_sweeps = 0\n\n    def local_eval_trial(x_center, coord, step_size):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None, None\n        trial = x_center.copy()\n        trial[coord] += step_size\n        trial = clip_to_bounds(trial)\n        y = objective_function(trial)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = trial, y\n        return trial, y\n\n    remaining_local = budget - evals_used\n    coord_budget = int(0.55 * remaining_local)\n    coord_budget = max(0, min(coord_budget, remaining_local))\n\n    if coord_budget > 0 and evals_used < budget:\n        evals_per_coord = 2  # +/- directions\n        evals_per_sweep = max(1, dim * evals_per_coord)\n        max_sweeps = max(1, coord_budget // evals_per_sweep)\n\n        for _ in range(max_sweeps):\n            if evals_used >= budget:\n                break\n            improved_any = False\n            coords = rng.permutation(dim)\n\n            for d in coords:\n                if evals_used >= budget:\n                    break\n                curr_best_y = best_y\n                curr_best_x = best_x\n                s = step[d]\n\n                dirs = [-1.0, 1.0]\n                rng.shuffle(dirs)\n                for direction in dirs:\n                    if evals_used >= budget:\n                        break\n                    scaled_step = direction * s * rng.uniform(0.5, 1.5)\n                    trial, y = local_eval_trial(curr_best_x, d, scaled_step)\n                    if trial is None:\n                        break\n                    if y < curr_best_y:\n                        curr_best_y = y\n                        curr_best_x = trial\n\n                if curr_best_y < best_y:\n                    best_x, best_y = curr_best_x, curr_best_y\n                    improved_any = True\n\n            if improved_any:\n                no_improve_sweeps = 0\n            else:\n                no_improve_sweeps += 1\n                step *= 0.5\n                if np.all(step <= min_step) or no_improve_sweeps >= 3:\n                    break\n\n            if evals_used >= budget:\n                break\n\n    if evals_used >= budget:\n        return best_x\n\n    # ----------------- Final Diversified / Anisotropic Refinement -----------------\n    while evals_used < budget:\n        remaining = budget - evals_used\n        if remaining <= 0:\n            break\n\n        # Probability of global vs local perturbation depends on remaining budget\n        frac_remaining = remaining / max(1.0, budget)\n        p_global = 0.25 + 0.45 * frac_remaining  # in [0.25, 0.7]\n\n        if rng.random() < p_global:\n            # Global sampling biased around best_x but still exploring the box\n            if rng.random() < 0.4:\n                x = rng.uniform(low, high)\n            else:\n                # mixture of global and local: wide Gaussian around best\n                scale_factor = 0.25 * max(0.1, frac_remaining)\n                noise = rng.normal(scale=scale_factor, size=dim) * span\n                x = clip_to_bounds(best_x + noise)\n        else:\n            # Local anisotropic perturbation around best_x, decaying with remaining\n            scale_factor = 0.06 * max(0.05, frac_remaining)\n            coord_noise = rng.normal(scale=scale_factor, size=dim) * span\n            x = clip_to_bounds(best_x + coord_noise)\n\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = x, y\n\n    return best_x",
    "X": "1.0 1.0 0.7635454018714856 0.5268558420417567 1.0"
}