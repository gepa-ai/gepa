{
    "score": 2.823663671004785,
    "Input": "McCourt06",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Basic derivative-free optimizer: global random search + local search\n    using coordinate-wise perturbations around the current best point.\n\n    Strategy:\n      1. If prev_best_x is provided and inside bounds \u2013 evaluate and start from it.\n      2. Use ~40% of budget for global random search.\n      3. Use remaining budget for local search (coordinate-wise random steps)\n         around the best point found so far.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    # Safety checks\n    if bounds.shape != (dim, 2):\n        bounds = bounds.reshape(dim, 2)\n    low, high = bounds[:, 0], bounds[:, 1]\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    # Initialize with prev_best_x if valid\n    best_x = None\n    best_y = np.inf\n\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = clip_to_bounds(x0)\n            y0 = objective_function(x0)\n            evals_used += 1\n            best_x, best_y = x0, y0\n\n    # If no valid starting point, do one random evaluation\n    if best_x is None:\n        x0 = rng.uniform(low, high)\n        y0 = objective_function(x0)\n        evals_used += 1\n        best_x, best_y = x0, y0\n\n    # Global random search phase\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    global_fraction = 0.4\n    n_global = max(1, int(remaining * global_fraction))\n    n_global = min(n_global, remaining)\n\n    for _ in range(n_global):\n        x = rng.uniform(low, high)\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = x, y\n\n    # Local search phase around best_x\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Step sizes proportional to search space width\n    span = high - low\n    base_step = span * 0.1  # 10% of range per coordinate\n    min_step = span * 1e-3\n    step = np.maximum(base_step, min_step)\n\n    # Allocate evaluations per local iteration (one per coordinate)\n    # One local \"sweep\" tries dim coordinates, each using 1 eval.\n    evals_per_sweep = max(1, dim)\n    max_sweeps = max(1, remaining // evals_per_sweep)\n\n    for _ in range(max_sweeps):\n        improved = False\n        for d in range(dim):\n            if evals_used >= budget:\n                break\n\n            direction = rng.choice([-1.0, 1.0])\n            trial = best_x.copy()\n            trial[d] += direction * step[d] * rng.uniform(0.5, 1.5)\n            trial = clip_to_bounds(trial)\n\n            y = objective_function(trial)\n            evals_used += 1\n            if y < best_y:\n                best_x, best_y = trial, y\n                improved = True\n\n        # If no improvement in a sweep, shrink step sizes\n        if not improved:\n            step *= 0.5\n            if np.all(step <= min_step * 0.25):\n                # Steps too small; switch to random restarts with remaining budget\n                break\n\n    # If we exited early and still have budget, use pure random search\n    while evals_used < budget:\n        x = rng.uniform(low, high)\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = x, y\n\n    return best_x",
    "X": "1.0 0.935400449665907 0.7691938182938157 0.5257436881322579 1.0"
}