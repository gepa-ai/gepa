{
    "score": 1.8747738582461646,
    "Input": "McCourt13",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid derivative-free optimizer with adaptive global\u2013local balance.\n\n    Strategy:\n    - Robust handling of bounds/dim mismatches and zero-width ranges\n    - Initial global sampling using Halton-like low-discrepancy sequence\n    - Adaptive restart local search (CMA-ES\u2013like but lightweight)\n    - Warm-start via prev_best_x (evaluated early and used as a start point)\n    - Strict budget accounting, always returns best point seen\n    \"\"\"\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    # --- Bounds & dimension handling ---\n    if bounds.ndim == 1:\n        # allow [low, high] for 1D\n        if bounds.size == 2 and dim == 1:\n            bounds = bounds.reshape(1, 2)\n        else:\n            # replicate bounds if only one interval given\n            low_val = bounds[0] if bounds.size > 0 else -1.0\n            high_val = bounds[1] if bounds.size > 1 else 1.0\n            bounds = np.tile([low_val, high_val], (dim, 1))\n    elif bounds.shape[0] != dim:\n        # Fallback: use first row / values\n        if bounds.shape[0] > 0:\n            low_val = bounds[0, 0]\n            high_val = bounds[0, 1] if bounds.shape[1] > 1 else low_val + 1.0\n        else:\n            low_val, high_val = -1.0, 1.0\n        bounds = np.tile([low_val, high_val], (dim, 1))\n\n    low = bounds[:, 0].astype(float)\n    high = bounds[:, 1].astype(float)\n\n    # Swap if any low > high\n    swap_mask = low > high\n    if np.any(swap_mask):\n        tmp = low[swap_mask].copy()\n        low[swap_mask] = high[swap_mask]\n        high[swap_mask] = tmp\n\n    span = high - low\n    # Handle zero-width bounds (fix dimension) by allowing tiny span for search\n    zero_mask = span == 0.0\n    if np.any(zero_mask):\n        span[zero_mask] = 1.0  # artificial, we will clip back to bounds anyway\n\n    remaining = budget\n\n    # If no evaluations allowed, just return a valid point\n    if remaining <= 0:\n        if prev_best_x is not None:\n            x0 = np.clip(np.asarray(prev_best_x, dtype=float), low, high)\n        else:\n            x0 = (low + high) / 2.0\n        x0 = np.resize(x0, (dim,))\n        return x0.astype(float)\n\n    best_x = None\n    best_y = None\n\n    def eval_point(x):\n        nonlocal remaining, best_x, best_y\n        if remaining <= 0:\n            return None\n        # Ensure correct shape and bounds\n        x = np.asarray(x, dtype=float)\n        if x.shape != (dim,):\n            x = np.resize(x, (dim,))\n        x = np.clip(x, low, high)\n        y = objective_function(x)\n        remaining -= 1\n        if best_y is None or y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # --- Helper: Halton-like 1D sequence ---\n    def halton_1d(n, base, seed_shift=0):\n        # numerically stable, simple halton sequence in [0,1)\n        # seed_shift allows mild scrambling across calls\n        result = np.empty(n, dtype=float)\n        for i in range(n):\n            f = 1.0\n            r = 0.0\n            index = i + 1 + seed_shift\n            while index > 0:\n                f /= base\n                r += f * (index % base)\n                index //= base\n            result[i] = r\n        return result\n\n    # --- 1) Initial global search ---\n    # Proportion of budget for global search: higher for higher dimension\n    if dim <= 3:\n        frac_global = 0.3\n    elif dim <= 10:\n        frac_global = 0.4\n    else:\n        frac_global = 0.5\n\n    init_evals = int(frac_global * budget)\n    init_evals = max(4, min(init_evals, max(1, budget - 3)))\n    init_evals = min(init_evals, remaining)\n\n    # Evaluate warm start first, if available\n    if prev_best_x is not None and remaining > 0:\n        eval_point(prev_best_x)\n\n    # Random center baseline if no warm start\n    if best_x is None and remaining > 0:\n        x_c = (low + high) / 2.0\n        eval_point(x_c)\n\n    # Simple prime bases for up to moderate dimensions\n    primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37]\n\n    if init_evals > 0 and remaining > 0:\n        bases = [primes[i % len(primes)] for i in range(dim)]\n        # small random shift to avoid same patterns across calls\n        seed_shift = np.random.randint(1, 10000)\n        halton_cache = [halton_1d(init_evals, b, seed_shift) for b in bases]\n        for k in range(init_evals):\n            if remaining <= 0:\n                break\n            u = np.array([halton_cache[j][k] for j in range(dim)], dtype=float)\n            xr = low + u * span\n            eval_point(xr)\n\n    # Ensure at least one evaluation happened\n    if best_x is None and remaining > 0:\n        xr = low + span * np.random.rand(dim)\n        eval_point(xr)\n\n    if best_x is None:\n        # fall back if objective never called\n        return ((low + high) / 2.0).astype(float)\n\n    # --- Early exit if no budget for local search ---\n    if remaining <= 0:\n        return best_x.astype(float)\n\n    # --- 2) Local search / refinement (CMA-like) ---\n    # Adapt per-dimension effort: more local search for low/moderate dims\n    base_local_per_dim = 12\n    approx_per_run = max(base_local_per_dim * dim, int(0.15 * budget))\n    n_runs = max(1, remaining // max(approx_per_run, 1))\n    n_runs = min(n_runs, 6)  # limit number of tiny runs\n\n    # If budget too small, do a quick stochastic hill-climb\n    if remaining < 4 * dim + 5:\n        current_x = best_x.copy()\n        steps = remaining\n        max_step_frac = 0.25\n        min_step_frac = 0.02\n        for i in range(steps):\n            if remaining <= 0:\n                break\n            t = i / max(1, steps - 1)\n            frac = max_step_frac * (1 - t) + min_step_frac * t\n            step_scale = frac * span\n            cand = current_x + np.random.randn(dim) * step_scale\n            yc = eval_point(cand)\n            if yc is None:\n                break\n            if yc <= best_y:\n                current_x = cand\n        return best_x.astype(float)\n\n    # Prepare starting points for each local run\n    starts = [best_x.copy()]\n    # A mix of perturbations around best and global random starts\n    while len(starts) < n_runs and remaining > 0:\n        if np.random.rand() < 0.6:\n            step = 0.3 * span\n            cand = best_x + np.random.randn(dim) * step\n        else:\n            cand = low + span * np.random.rand(dim)\n        cand = np.clip(cand, low, high)\n        starts.append(cand)\n\n    for run_idx in range(n_runs):\n        if remaining <= 0:\n            break\n\n        runs_left = n_runs - run_idx\n        per_run_budget = max(6 * dim, remaining // runs_left)\n        per_run_budget = min(per_run_budget, remaining)\n        if per_run_budget <= 0:\n            break\n\n        mean = np.clip(starts[run_idx], low, high)\n\n        # Initial sigma scaled with dimension and box size\n        # smaller for high dimensions to avoid too aggressive jumps\n        if dim <= 5:\n            sigma_frac = 0.18\n        elif dim <= 20:\n            sigma_frac = 0.12\n        else:\n            sigma_frac = 0.08\n        sigma = sigma_frac * span\n        sigma = np.where(sigma == 0.0, 1.0, sigma)\n\n        # Population size\n        lam = max(4, 2 * dim)\n        lam = min(lam, max(4, per_run_budget))\n\n        # Learning rates\n        c_mean = 0.4\n        c_sigma_inc = 0.15\n        c_sigma_dec = 0.25\n\n        min_sigma = 1e-6 * span + 1e-12\n\n        # Track local best to add mild restarts\n        local_best = mean.copy()\n        local_best_y = best_y\n\n        it = 0\n        while remaining > 0 and per_run_budget > 0:\n            # adjust population if near end of budget\n            lam_eff = min(lam, per_run_budget)\n            pop = []\n            ys = []\n\n            for _ in range(lam_eff):\n                if remaining <= 0 or per_run_budget <= 0:\n                    break\n                z = np.random.randn(dim)\n                cand = mean + z * sigma\n                cand = np.clip(cand, low, high)\n                y = eval_point(cand)\n                if y is None:\n                    break\n                pop.append(cand)\n                ys.append(y)\n                per_run_budget -= 1\n\n            if len(pop) == 0:\n                break\n\n            pop = np.asarray(pop)\n            ys = np.asarray(ys)\n\n            order = np.argsort(ys)\n            pop = pop[order]\n            ys = ys[order]\n\n            # Local best update\n            if ys[0] < local_best_y:\n                local_best_y = ys[0]\n                local_best = pop[0].copy()\n\n            # Mean move towards best of this generation\n            mean = (1 - c_mean) * mean + c_mean * pop[0]\n\n            # Step-size adaptation\n            improved_global = ys[0] <= best_y\n            if improved_global:\n                sigma *= (1.0 + c_sigma_inc)\n            else:\n                sigma *= (1.0 - c_sigma_dec)\n            sigma = np.maximum(sigma, min_sigma)\n\n            # Light stagnation handling: occasional jump around global best\n            it += 1\n            if it % 12 == 0:\n                if not improved_global:\n                    jitter = 0.15 * span * np.random.randn(dim)\n                    mean = np.clip(best_x + jitter, low, high)\n                    # reset sigma slightly larger to encourage escape\n                    sigma = np.maximum(sigma, 0.12 * span)\n\n            if per_run_budget <= 0 or remaining <= 0:\n                break\n\n        # Post-run local refinement around best_x (short hill-climb)\n        if remaining > 0:\n            steps = min(2 * dim, remaining)\n            cur = best_x.copy()\n            for i in range(steps):\n                if remaining <= 0:\n                    break\n                t = i / max(1, steps - 1)\n                frac = 0.08 * (1 - t) + 0.01 * t\n                step = frac * span\n                cand = cur + np.random.randn(dim) * step\n                yc = eval_point(cand)\n                if yc is None:\n                    break\n                if yc <= best_y:\n                    cur = cand\n\n    return best_x.astype(float)",
    "X": "0.5791864234080256 0.5516852211775164 1.0"
}