{
    "score": 1.8812681218213718,
    "Input": "McCourt13",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Basic derivative-free optimizer combining random search with local perturbation.\n\n    Strategy:\n    - Use a small fraction of budget to build an initial pool (including warm-start if given).\n    - Then perform simple hill-climbing around the current best using decaying step sizes.\n    - Always respect the evaluation budget and bounds.\n    - Return the best point found.\n    \"\"\"\n    bounds = np.asarray(config['bounds'], dtype=float)\n    dim = int(config['dim'])\n    budget = int(config['budget'])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Ensure valid shapes\n    if low.shape[0] != dim:\n        # Fallback in case of inconsistent config\n        low = np.full(dim, low[0] if low.size > 0 else -1.0)\n        high = np.full(dim, high[0] if high.size > 0 else 1.0)\n        span = high - low\n\n    # Handle degenerate span (zero-width bounds)\n    span = np.where(span == 0.0, 1.0, span)\n\n    remaining = budget\n    if remaining <= 0:\n        # No evaluations allowed: return something within bounds\n        if prev_best_x is not None:\n            x0 = np.clip(np.asarray(prev_best_x, dtype=float), low, high)\n        else:\n            x0 = (low + high) / 2.0\n        return x0.astype(float)\n\n    best_x = None\n    best_y = None\n\n    # Helper: evaluate with budget check\n    def eval_point(x):\n        nonlocal remaining, best_x, best_y\n        if remaining <= 0:\n            return None\n        y = objective_function(x)\n        remaining -= 1\n        if best_y is None or y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # 1) Initial exploration population (including warm-start if available)\n    init_evals = max(1, int(0.2 * budget))\n    init_evals = min(init_evals, remaining)\n\n    # Start from prev_best_x if provided\n    if prev_best_x is not None and remaining > 0:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape != (dim,):\n            x0 = np.resize(x0, (dim,))\n        x0 = np.clip(x0, low, high)\n        eval_point(x0)\n\n    # Fill remaining initial evaluations with random samples\n    for _ in range(init_evals):\n        if remaining <= 0:\n            break\n        xr = low + span * np.random.rand(dim)\n        eval_point(xr)\n\n    # If nothing evaluated for some reason, sample once\n    if best_x is None and remaining > 0:\n        x0 = low + span * np.random.rand(dim)\n        eval_point(x0)\n\n    if best_x is None:\n        # Should not happen, but fallback\n        return (low + high) / 2.0\n\n    # 2) Local search: stochastic hill-climbing with decaying step size\n    # Use a schedule that decays from moderate to small perturbations.\n    # Allocate the rest of the budget here.\n    steps = max(1, remaining)\n\n    # Initial step ~ 25% of range, final ~ 1% of range\n    max_step = 0.25\n    min_step = 0.01\n\n    current_x = best_x.copy()\n\n    for i in range(steps):\n        if remaining <= 0:\n            break\n\n        t = i / max(1, steps - 1)\n        frac = max_step * (1 - t) + min_step * t\n        step_scale = frac * span\n\n        # Gaussian perturbation scaled per-dimension\n        perturb = np.random.randn(dim) * step_scale\n        cand = current_x + perturb\n        cand = np.clip(cand, low, high)\n\n        yc = eval_point(cand)\n        if yc is None:\n            break\n\n        # Move current_x towards better solutions to encourage exploitation\n        if yc <= best_y:\n            current_x = cand\n        else:\n            # Occasionally move anyway to maintain some exploration\n            # Simple simulated annealing-like acceptance\n            if np.random.rand() < 0.05:\n                current_x = cand\n\n    return best_x.astype(float)",
    "X": "0.5820551422520038 0.5693367279959074 1.0"
}