{
    "score": 1.4904829635876131,
    "Input": "McCourt13",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid derivative-free optimizer with adaptive global\u2013local balance.\n    This version keeps the previous structure but simplifies and stabilizes\n    local search and budget usage to improve robustness across problems.\n    \"\"\"\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    # --- Bounds & dimension handling ---\n    if bounds.ndim == 1:\n        if bounds.size == 2 and dim == 1:\n            bounds = bounds.reshape(1, 2)\n        else:\n            low_val = bounds[0] if bounds.size > 0 else -1.0\n            high_val = bounds[1] if bounds.size > 1 else 1.0\n            bounds = np.tile([low_val, high_val], (dim, 1))\n    elif bounds.shape[0] != dim:\n        if bounds.shape[0] > 0:\n            low_val = bounds[0, 0]\n            high_val = bounds[0, 1] if bounds.shape[1] > 1 else low_val + 1.0\n        else:\n            low_val, high_val = -1.0, 1.0\n        bounds = np.tile([low_val, high_val], (dim, 1))\n\n    low = bounds[:, 0].astype(float)\n    high = bounds[:, 1].astype(float)\n\n    swap_mask = low > high\n    if np.any(swap_mask):\n        tmp = low[swap_mask].copy()\n        low[swap_mask] = high[swap_mask]\n        high[swap_mask] = tmp\n\n    span = high - low\n    zero_mask = span == 0.0\n    if np.any(zero_mask):\n        span[zero_mask] = 1.0\n\n    remaining = budget\n\n    if remaining <= 0:\n        if prev_best_x is not None:\n            x0 = np.clip(np.asarray(prev_best_x, dtype=float), low, high)\n        else:\n            x0 = (low + high) / 2.0\n        x0 = np.resize(x0, (dim,))\n        return x0.astype(float)\n\n    best_x = None\n    best_y = None\n\n    def eval_point(x):\n        nonlocal remaining, best_x, best_y\n        if remaining <= 0:\n            return None\n        x = np.asarray(x, dtype=float)\n        if x.shape != (dim,):\n            x = np.resize(x, (dim,))\n        x = np.clip(x, low, high)\n        y = objective_function(x)\n        remaining -= 1\n        if best_y is None or y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # --- low-discrepancy-like sampling (Halton 1D) ---\n    def halton_1d(n, base, seed_shift=0):\n        result = np.empty(n, dtype=float)\n        for i in range(n):\n            f = 1.0\n            r = 0.0\n            index = i + 1 + seed_shift\n            while index > 0:\n                f /= base\n                r += f * (index % base)\n                index //= base\n            result[i] = r\n        return result\n\n    # --- 1) Initial global search ---\n    if dim <= 3:\n        frac_global = 0.35\n    elif dim <= 10:\n        frac_global = 0.45\n    else:\n        frac_global = 0.55\n\n    init_evals = int(frac_global * budget)\n    init_evals = max(4, min(init_evals, max(1, budget - 5)))\n    init_evals = min(init_evals, remaining)\n\n    # Warm start first if available\n    if prev_best_x is not None and remaining > 0:\n        eval_point(prev_best_x)\n\n    # Center baseline if nothing yet\n    if best_x is None and remaining > 0:\n        x_c = (low + high) / 2.0\n        eval_point(x_c)\n\n    # Halton global sampling\n    primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37]\n    if init_evals > 0 and remaining > 0:\n        bases = [primes[i % len(primes)] for i in range(dim)]\n        seed_shift = np.random.randint(1, 10000)\n        halton_cache = [halton_1d(init_evals, b, seed_shift) for b in bases]\n        for k in range(init_evals):\n            if remaining <= 0:\n                break\n            u = np.array([halton_cache[j][k] for j in range(dim)], dtype=float)\n            xr = low + u * span\n            eval_point(xr)\n\n    if best_x is None and remaining > 0:\n        xr = low + span * np.random.rand(dim)\n        eval_point(xr)\n\n    if best_x is None:\n        return ((low + high) / 2.0).astype(float)\n\n    if remaining <= 0:\n        return best_x.astype(float)\n\n    # --- 2) Local search / refinement ---\n\n    # For very small remaining budget, do a simple hill-climb\n    if remaining < 6 * dim + 8:\n        current_x = best_x.copy()\n        steps = remaining\n        max_step_frac = 0.25\n        min_step_frac = 0.02\n        for i in range(steps):\n            if remaining <= 0:\n                break\n            t = i / max(1, steps - 1)\n            frac = max_step_frac * (1 - t) + min_step_frac * t\n            step_scale = frac * span\n            cand = current_x + np.random.randn(dim) * step_scale\n            yc = eval_point(cand)\n            if yc is None:\n                break\n            if yc <= best_y:\n                current_x = cand\n        return best_x.astype(float)\n\n    # Number of local runs, modest to avoid oversplitting budget\n    base_local_per_dim = 10\n    approx_per_run = max(base_local_per_dim * dim, int(0.2 * budget))\n    n_runs = max(1, remaining // max(approx_per_run, 1))\n    n_runs = min(n_runs, 4)\n\n    # Prepare starting points\n    starts = [best_x.copy()]\n    while len(starts) < n_runs and remaining > 0:\n        if np.random.rand() < 0.7:\n            step = 0.35 * span\n            cand = best_x + np.random.randn(dim) * step\n        else:\n            cand = low + span * np.random.rand(dim)\n        cand = np.clip(cand, low, high)\n        starts.append(cand)\n\n    for run_idx in range(n_runs):\n        if remaining <= 0:\n            break\n\n        runs_left = n_runs - run_idx\n        # allocate remaining budget fairly among remaining runs\n        per_run_budget = max(8 * dim, remaining // runs_left)\n        per_run_budget = min(per_run_budget, remaining)\n        if per_run_budget <= 0:\n            break\n\n        mean = np.clip(starts[run_idx], low, high)\n\n        # Initial sigma\n        if dim <= 5:\n            sigma_frac = 0.2\n        elif dim <= 20:\n            sigma_frac = 0.12\n        else:\n            sigma_frac = 0.08\n        sigma = sigma_frac * span\n        sigma = np.where(sigma == 0.0, 1.0, sigma)\n        min_sigma = 1e-6 * span + 1e-12\n\n        # Population size\n        lam = max(4, 2 * dim)\n        lam = min(lam, max(4, per_run_budget))\n\n        # Learning rates\n        c_mean = 0.35\n        c_sigma_inc = 0.12\n        c_sigma_dec = 0.20\n\n        local_best_x = mean.copy()\n        local_best_y = best_y\n        no_improve_iters = 0\n        it = 0\n\n        while remaining > 0 and per_run_budget > 0:\n            lam_eff = min(lam, per_run_budget)\n            if lam_eff <= 0:\n                break\n\n            pop = np.empty((lam_eff, dim), dtype=float)\n            ys = np.empty(lam_eff, dtype=float)\n\n            for i in range(lam_eff):\n                if remaining <= 0 or per_run_budget <= 0:\n                    lam_eff = i\n                    break\n                z = np.random.randn(dim)\n                cand = mean + z * sigma\n                cand = np.clip(cand, low, high)\n                y = eval_point(cand)\n                if y is None:\n                    lam_eff = i\n                    break\n                pop[i] = cand\n                ys[i] = y\n                per_run_budget -= 1\n\n            if lam_eff == 0:\n                break\n\n            pop = pop[:lam_eff]\n            ys = ys[:lam_eff]\n            order = np.argsort(ys)\n            pop = pop[order]\n            ys = ys[order]\n\n            if ys[0] < local_best_y:\n                local_best_y = ys[0]\n                local_best_x = pop[0].copy()\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n\n            mean = (1 - c_mean) * mean + c_mean * pop[0]\n\n            improved_global = ys[0] <= best_y\n            if improved_global:\n                sigma *= (1.0 + c_sigma_inc)\n            else:\n                sigma *= (1.0 - c_sigma_dec)\n            sigma = np.maximum(sigma, min_sigma)\n\n            it += 1\n\n            # Stagnation handling: soft restart around global best\n            if no_improve_iters >= 10:\n                jitter = 0.2 * span * np.random.randn(dim)\n                mean = np.clip(best_x + jitter, low, high)\n                sigma = np.maximum(sigma, 0.1 * span)\n                no_improve_iters = 0\n\n            # Occasional contraction around global best to exploit\n            if it % 15 == 0:\n                mean = 0.5 * mean + 0.5 * best_x\n                sigma *= 0.8\n                sigma = np.maximum(sigma, min_sigma)\n\n        # Short hill-climb around the best point after each run\n        if remaining > 0:\n            steps = min(3 * dim, remaining)\n            cur = best_x.copy()\n            for i in range(steps):\n                if remaining <= 0:\n                    break\n                t = i / max(1, steps - 1)\n                frac = 0.06 * (1 - t) + 0.01 * t\n                step = frac * span\n                cand = cur + np.random.randn(dim) * step\n                yc = eval_point(cand)\n                if yc is None:\n                    break\n                if yc <= best_y:\n                    cur = cand\n\n    return best_x.astype(float)",
    "X": "1.0 1.0 1.0"
}