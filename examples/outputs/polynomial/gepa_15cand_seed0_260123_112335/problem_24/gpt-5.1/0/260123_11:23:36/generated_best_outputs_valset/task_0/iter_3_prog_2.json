{
    "score": 1.875138068166695,
    "Input": "McCourt13",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid derivative-free optimizer:\n    - Initial global sampling (Sobol-like low-discrepancy from random seed)\n    - Multiple adaptive local searches (CMA-ES\u2013like covariance adaptation)\n    - Warm-start support via prev_best_x\n    - Strict budget accounting\n    \"\"\"\n    bounds = np.asarray(config['bounds'], dtype=float)\n    dim = int(config['dim'])\n    budget = int(config['budget'])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Fallback if bounds inconsistent with dim\n    if low.shape[0] != dim:\n        low = np.full(dim, low[0] if low.size > 0 else -1.0)\n        high = np.full(dim, high[0] if high.size > 0 else 1.0)\n        span = high - low\n\n    # Handle zero-width bounds\n    span = np.where(span == 0.0, 1.0, span)\n\n    remaining = budget\n\n    # If no evaluations allowed, return a valid point\n    if remaining <= 0:\n        if prev_best_x is not None:\n            x0 = np.clip(np.asarray(prev_best_x, dtype=float), low, high)\n        else:\n            x0 = (low + high) / 2.0\n        if x0.shape != (dim,):\n            x0 = np.resize(x0, (dim,))\n        return x0.astype(float)\n\n    best_x = None\n    best_y = None\n\n    def eval_point(x):\n        nonlocal remaining, best_x, best_y\n        if remaining <= 0:\n            return None\n        y = objective_function(x)\n        remaining -= 1\n        if best_y is None or y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # -------------------------\n    # 1) Initial global search\n    # -------------------------\n    # Use 40% of the budget for global exploration (but at least 5 and at most budget-3)\n    init_evals = int(0.4 * budget)\n    init_evals = max(5, min(init_evals, max(1, budget - 3)))\n    init_evals = min(init_evals, remaining)\n\n    # Include warm start early if provided\n    if prev_best_x is not None and remaining > 0:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape != (dim,):\n            x0 = np.resize(x0, (dim,))\n        x0 = np.clip(x0, low, high)\n        eval_point(x0)\n\n    # Low-discrepancy like sampling using a simple scrambled Halton sequence\n    def halton_indices(n, base):\n        # Returns n Halton numbers in [0,1) for a given base\n        result = np.empty(n, dtype=float)\n        f = 1.0\n        i = 0\n        for idx in range(1, n + 1):\n            f /= base\n            x = 0.0\n            t = idx\n            while t > 0:\n                x += f * (t % base)\n                t //= base\n                f /= base\n            result[i] = x\n            i += 1\n            f = 1.0\n        return result\n\n    # Simple prime list for bases\n    primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n\n    if init_evals > 0:\n        # Generate halton-like samples in [0,1]^dim, then scale to bounds\n        # To avoid cost, build 1D sequences and combine by modular indexing\n        bases = [primes[i % len(primes)] for i in range(dim)]\n        halton_cache = [halton_indices(init_evals, b) for b in bases]\n        for k in range(init_evals):\n            if remaining <= 0:\n                break\n            u = np.array([halton_cache[j][k] for j in range(dim)], dtype=float)\n            xr = low + u * span\n            eval_point(xr)\n\n    # If nothing evaluated for some reason, sample random once\n    if best_x is None and remaining > 0:\n        x0 = low + span * np.random.rand(dim)\n        eval_point(x0)\n\n    if best_x is None:\n        # Fallback: center of bounds\n        return ((low + high) / 2.0).astype(float)\n\n    # -------------------------\n    # 2) Multi-start local search (CMA-like)\n    # -------------------------\n    # Use remaining budget for a few short adaptive local runs.\n    # This tends to work well for a wide variety of smooth / semi-smooth benchmarks.\n\n    # Number of local restarts depends on remaining budget and dimension\n    # Target per-run cost ~ max(10*dim, 0.1*budget)\n    if remaining <= 0:\n        return best_x.astype(float)\n\n    approx_per_run = max(10 * dim, int(0.1 * budget))\n    n_runs = max(1, remaining // max(approx_per_run, 1))\n    n_runs = min(n_runs, 5)  # avoid too many tiny runs\n\n    # If we have very little budget left, just do a simple hill-climb from best_x\n    if remaining < 6 * dim:\n        current_x = best_x.copy()\n        # Step sizes shrink over time; start at 20% of range, end at 1%\n        max_step = 0.2\n        min_step = 0.01\n        steps = remaining\n        for i in range(steps):\n            if remaining <= 0:\n                break\n            t = i / max(1, steps - 1)\n            frac = max_step * (1 - t) + min_step * t\n            step_scale = frac * span\n            perturb = np.random.randn(dim) * step_scale\n            cand = np.clip(current_x + perturb, low, high)\n            yc = eval_point(cand)\n            if yc is None:\n                break\n            if yc <= best_y:\n                current_x = cand\n        return best_x.astype(float)\n\n    # Prepare candidate starting points for local runs:\n    # - Always include current best as first start\n    # - Additional starts: perturbations of best and random points\n    starts = [best_x.copy()]\n    while len(starts) < n_runs and remaining > 0:\n        if np.random.rand() < 0.5:\n            # local perturbation around best\n            step = 0.3 * span\n            cand = np.clip(best_x + np.random.randn(dim) * step, low, high)\n        else:\n            # random in box\n            cand = low + span * np.random.rand(dim)\n        starts.append(cand)\n\n    # Local search parameters (lightweight CMA-ES flavor)\n    for run_idx in range(n_runs):\n        if remaining <= 0:\n            break\n\n        # Allocate per-run budget adaptively\n        runs_left = n_runs - run_idx\n        # Leave at least 1 eval per remaining run\n        per_run_budget = max(5 * dim, remaining // runs_left)\n        per_run_budget = min(per_run_budget, remaining)\n        if per_run_budget <= 0:\n            break\n\n        mean = np.clip(starts[run_idx], low, high)\n        # Initial sigma: 15% of box width\n        sigma = 0.15 * span\n        sigma = np.where(sigma == 0.0, 1.0, sigma)\n\n        # Population size: small but >1\n        lam = max(4, 2 * dim)\n        lam = min(lam, max(4, per_run_budget))\n\n        # Learning rates\n        c_mean = 0.5  # how fast mean moves\n        c_sigma = 0.3  # how fast step-size adapts\n\n        # Minimal sigma to keep some movement\n        min_sigma = 1e-6 * span + 1e-12\n\n        for it in range(per_run_budget // lam + 1):\n            if remaining <= 0:\n                break\n\n            # Sample population\n            pop = []\n            ys = []\n            for _ in range(lam):\n                if remaining <= 0:\n                    break\n                z = np.random.randn(dim)\n                cand = mean + z * sigma\n                cand = np.clip(cand, low, high)\n                y = eval_point(cand)\n                if y is None:\n                    break\n                pop.append(cand)\n                ys.append(y)\n\n            if len(pop) == 0:\n                break\n\n            ys = np.asarray(ys)\n            pop = np.asarray(pop)\n\n            # Sort by fitness\n            idx = np.argsort(ys)\n            pop = pop[idx]\n            ys = ys[idx]\n\n            # Update mean towards best candidates (elitist)\n            best_pop = pop[0]\n            mean = (1 - c_mean) * mean + c_mean * best_pop\n\n            # Step-size adaptation: if best improved global best, slightly increase exploration, else decrease\n            improved = ys[0] <= best_y\n            if improved:\n                sigma = sigma * (1.0 + c_sigma * 0.5)\n            else:\n                sigma = sigma * (1.0 - c_sigma)\n\n            sigma = np.maximum(sigma, min_sigma)\n\n            # Occasional restart-like jitter to escape stagnation\n            if it > 0 and it % 10 == 0:\n                sigma = sigma * (1.0 + 0.2 * np.random.rand(dim))\n\n        # After each run, optionally refine around current global best using a few small perturbations\n        if remaining > 0:\n            local_refine_steps = min(3 * dim, remaining)\n            current_x = best_x.copy()\n            for i in range(local_refine_steps):\n                if remaining <= 0:\n                    break\n                t = i / max(1, local_refine_steps - 1)\n                frac = 0.1 * (1 - t) + 0.01 * t\n                step = frac * span\n                cand = np.clip(current_x + np.random.randn(dim) * step, low, high)\n                yc = eval_point(cand)\n                if yc is None:\n                    break\n                if yc <= best_y:\n                    current_x = cand\n\n    return best_x.astype(float)",
    "X": "0.5764612241385445 0.5572538976860915 1.0"
}