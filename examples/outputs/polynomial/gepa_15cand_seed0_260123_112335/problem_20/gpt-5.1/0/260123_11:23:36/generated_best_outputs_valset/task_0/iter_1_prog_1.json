{
    "score": -10.170773350784453,
    "Input": "McCourt09",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budgeted evaluations.\n\n    Strategy:\n    - Use Latin Hypercube-like diversified sampling for global search.\n    - If prev_best_x is provided and inside bounds, evaluate and keep as incumbent.\n    - Use remaining budget for local search around incumbent with adaptive radius.\n    \"\"\"\n    rng = np.random.RandomState()  # independent RNG\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    if budget <= 0:\n        # No evaluations allowed: return midpoint of bounds\n        return (low + high) / 2.0\n\n    # Helper: clip into bounds\n    def clip_to_bounds(x):\n        return np.minimum(high, np.maximum(low, x))\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    # Warm start from prev_best_x if valid\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim and np.all(x0 >= low) and np.all(x0 <= high):\n            y0 = objective_function(x0)\n            evals_used += 1\n            best_x, best_y = x0, y0\n\n    # If we still have no incumbent, sample one random point\n    if best_x is None and evals_used < budget:\n        x0 = rng.uniform(low, high)\n        y0 = objective_function(x0)\n        evals_used += 1\n        best_x, best_y = x0, y0\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Allocate ~40% budget to global exploration, rest to local search\n    global_budget = max(1, int(0.4 * remaining))\n    local_budget = remaining - global_budget\n\n    # Global search: stratified random sampling (simple LHS-style)\n    if global_budget > 0:\n        # For each dimension, create stratified samples in [0,1]\n        u = (rng.rand(global_budget, dim) + rng.permutation(global_budget)[:, None] / global_budget) % 1.0\n        X_global = low + u * span\n\n        for i in range(global_budget):\n            if evals_used >= budget:\n                break\n            xg = X_global[i]\n            yg = objective_function(xg)\n            evals_used += 1\n            if yg < best_y:\n                best_x, best_y = xg, yg\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Local search around incumbent: adaptive Gaussian perturbations\n    x_center = best_x.copy()\n\n    # Initial radius: 20% of range, clipped to avoid zero-span issues\n    base_radius = 0.2 * np.maximum(span, 1e-8)\n\n    for k in range(remaining):\n        # Decrease radius over iterations (annealing)\n        frac = 1.0 - (k / max(1, remaining - 1))\n        radius = base_radius * (0.1 + 0.9 * frac)  # from 100% to 10% of base_radius\n\n        step = rng.normal(loc=0.0, scale=radius, size=dim)\n        xc = clip_to_bounds(x_center + step)\n\n        yc = objective_function(xc)\n        evals_used += 1\n        if yc < best_y:\n            best_x, best_y = xc, yc\n            x_center = xc  # move center to improvement\n\n        if evals_used >= budget:\n            break\n\n    return best_x",
    "X": "0.5966203951637233 1.0 0.20720881002276545"
}