{
    "score": -12.031249442129091,
    "Input": "Problem03",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization using a hybrid global-local strategy:\n    - Global search with quasi-random (Sobol-like) sampling.\n    - Local refinement via adaptive coordinate-wise search.\n    \"\"\"\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    span[span <= 0] = 1.0  # safeguard for zero-width bounds\n\n    def clip_to_bounds(x):\n        return np.minimum(high, np.maximum(low, x))\n\n    def eval_x(x):\n        return objective_function(x)\n\n    if budget <= 0:\n        if prev_best_x is not None:\n            return clip_to_bounds(np.asarray(prev_best_x, dtype=float).reshape(dim,))\n        return clip_to_bounds(low + 0.5 * span)\n\n    # Initialize best solution\n    best_x = None\n    best_y = None\n    evals_used = 0\n\n    # Warm start from previous best if provided\n    if prev_best_x is not None:\n        x0 = clip_to_bounds(np.asarray(prev_best_x, dtype=float).reshape(dim,))\n        y0 = eval_x(x0)\n        best_x, best_y = x0, y0\n        evals_used += 1\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x.reshape(dim,)\n\n    # --- Global exploration via low-discrepancy-like sampling ---\n    # Allocate about 70% of remaining budget to global exploration\n    n_global = int(0.7 * budget)\n    n_global = max(dim + 2, n_global)\n    n_global = min(n_global, remaining)\n\n    # Sobol-like sequence using direction numbers approximated with random base\n    # If n_global is small, fall back to uniform sampling.\n    if n_global <= 2 * dim:\n        # Small budget: pure uniform random sampling\n        for _ in range(n_global):\n            x = low + span * np.random.rand(dim)\n            y = eval_x(x)\n            evals_used += 1\n            if best_y is None or y < best_y:\n                best_x, best_y = x, y\n    else:\n        # Quasi-random sampling in [0,1]^dim then scaled to bounds\n        # Use a simple scrambled Halton-like sequence.\n        def _van_der_corput(n, base):\n            vdc = 0.0\n            denom = 1.0\n            while n:\n                n, remainder = divmod(n, base)\n                denom *= base\n                vdc += remainder / denom\n            return vdc\n\n        # Choose the first dim primes as bases (fallback if dim large)\n        primes = [\n            2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n            31, 37, 41, 43, 47, 53, 59, 61, 67, 71,\n            73, 79, 83, 89, 97\n        ]\n        bases = []\n        for i in range(dim):\n            if i < len(primes):\n                bases.append(primes[i])\n            else:\n                # Fallback to random large-ish coprime base\n                bases.append(97 + 2 * (i - len(primes) + 1))\n\n        # Random offset for simple scrambling\n        offset = np.random.randint(0, 10_000)\n\n        for i in range(n_global):\n            idx = i + 1 + offset\n            u = np.empty(dim, dtype=float)\n            for d in range(dim):\n                u[d] = _van_der_corput(idx, bases[d])\n            x = low + span * u\n            y = eval_x(x)\n            evals_used += 1\n            if best_y is None or y < best_y:\n                best_x, best_y = x, y\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x.reshape(dim,)\n\n    # --- Local refinement around best_x ---\n    x_center = best_x.copy()\n    base_step = 0.25 * span\n    base_step[base_step == 0] = 1.0\n\n    # Number of sweeps determined by remaining budget\n    max_sweeps = 20\n    sweeps = min(max_sweeps, max(1, remaining // max(1, dim)))\n\n    for s in range(sweeps):\n        if evals_used >= budget:\n            break\n\n        # Geometric decay of step size\n        step_scale = 0.6 ** s\n        step = base_step * step_scale\n\n        improved_any = False\n\n        # Randomized coordinate order for robustness\n        coords = np.arange(dim)\n        np.random.shuffle(coords)\n\n        for d in coords:\n            if evals_used >= budget:\n                break\n\n            # Two-sided trial along dimension d\n            # Try a small normal perturbation scaled by current step\n            perturb = np.zeros(dim)\n            perturb[d] = np.random.randn() * step[d]\n            x_trial = clip_to_bounds(x_center + perturb)\n            y_trial = eval_x(x_trial)\n            evals_used += 1\n\n            if y_trial < best_y:\n                best_x, best_y = x_trial, y_trial\n                x_center = x_trial\n                improved_any = True\n            else:\n                # Try the opposite direction once\n                if evals_used >= budget:\n                    break\n                perturb[d] = -perturb[d]\n                x_trial2 = clip_to_bounds(x_center + perturb)\n                y_trial2 = eval_x(x_trial2)\n                evals_used += 1\n                if y_trial2 < best_y:\n                    best_x, best_y = x_trial2, y_trial2\n                    x_center = x_trial2\n                    improved_any = True\n\n        # If no improvement in this sweep, shrink base_step a bit more\n        if not improved_any:\n            base_step *= 0.5\n\n    return best_x.reshape(dim,)",
    "X": "-6.7745766389539455"
}