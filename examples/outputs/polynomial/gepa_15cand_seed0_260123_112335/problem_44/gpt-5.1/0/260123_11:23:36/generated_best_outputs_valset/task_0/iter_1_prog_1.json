{
    "score": -12.031227270606461,
    "Input": "Problem03",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization using a simple hybrid approach:\n    - Global search with random / quasi-random samples.\n    - Local refinement around the best solution by coordinate-wise perturbations.\n    \"\"\"\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Clamp helper\n    def clip_to_bounds(x):\n        return np.minimum(high, np.maximum(low, x))\n\n    # Evaluate helper\n    def eval_x(x):\n        return objective_function(x)\n\n    if budget <= 0:\n        # If no evaluations allowed, just return something feasible\n        if prev_best_x is not None:\n            return clip_to_bounds(np.asarray(prev_best_x, dtype=float).reshape(dim,))\n        return clip_to_bounds(low + 0.5 * span)\n\n    # Initialize best solution\n    best_x = None\n    best_y = None\n\n    # 1) Use prev_best_x as warm start if provided\n    evals_used = 0\n    if prev_best_x is not None:\n        x0 = clip_to_bounds(np.asarray(prev_best_x, dtype=float).reshape(dim,))\n        y0 = eval_x(x0)\n        best_x, best_y = x0, y0\n        evals_used += 1\n\n    # 2) Global exploration: random sampling (and optionally Sobol-like via random)\n    # Allocate ~60% of budget to global exploration, at least dim+1 points\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    n_global = max(dim + 1, int(0.6 * budget))\n    n_global = min(n_global, remaining)\n\n    # If no warm start yet, use first global sample for initialization\n    for i in range(n_global):\n        # Simple uniform sampling\n        x = low + span * np.random.rand(dim)\n        y = eval_x(x)\n        evals_used += 1\n\n        if best_y is None or y < best_y:\n            best_x, best_y = x, y\n\n    # 3) Local refinement around best_x using coordinate-wise random search\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    x_center = best_x.copy()\n    # Initial step size: fraction of domain span\n    base_step = 0.2 * span\n    base_step[base_step == 0] = 1.0  # avoid zero span dimensions\n\n    # We'll do several sweeps; each sweep uses up to dim proposals\n    # and gradually reduces step size\n    max_sweeps = 10\n    sweeps = min(max_sweeps, max(1, remaining // dim))\n\n    for s in range(sweeps):\n        if evals_used >= budget:\n            break\n\n        # Exponential decay of step size over sweeps\n        step_scale = 0.5 ** s\n        step = base_step * step_scale\n\n        improved = False\n        # Coordinate-wise perturbations\n        for d in range(dim):\n            if evals_used >= budget:\n                break\n\n            # Symmetric random perturbation along dimension d\n            perturb = np.zeros(dim)\n            # Use a scaled normal step, clipped\n            perturb[d] = np.random.randn() * step[d]\n\n            x_new = clip_to_bounds(x_center + perturb)\n            y_new = eval_x(x_new)\n            evals_used += 1\n\n            if y_new < best_y:\n                best_x, best_y = x_new, y_new\n                x_center = x_new\n                improved = True\n\n        # If no improvement in this sweep, consider shrinking step faster\n        if not improved and s < sweeps - 1:\n            # Optional small additional shrink to help convergence\n            base_step *= 0.5\n\n    return best_x.reshape(dim,)",
    "X": "-6.774197854893177"
}