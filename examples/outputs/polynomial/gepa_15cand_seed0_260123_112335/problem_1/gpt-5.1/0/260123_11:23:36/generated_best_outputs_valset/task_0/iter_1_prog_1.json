{
    "score": 0.0,
    "Input": "Ackley",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Basic derivative-free optimizer combining global random search with local hill-climbing.\n\n    Strategy:\n    1. Use up to ~40% of budget for global exploration (random + warm-start perturbations).\n    2. Use remaining budget for local improvement around best found point.\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config.get(\"budget\", 50))\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Ensure positive budget\n    if budget <= 0:\n        # Return mid-point of bounds as a fallback\n        return (low + high) / 2.0\n\n    rng = np.random.default_rng()\n\n    evals_used = 0\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    def evaluate(x):\n        nonlocal evals_used\n        if evals_used >= budget:\n            # Return a large penalty if budget is exhausted\n            return np.inf\n        evals_used += 1\n        return objective_function(x)\n\n    # --- Initialization / warm start ---\n    candidates = []\n    if prev_best_x is not None:\n        prev_best_x = np.asarray(prev_best_x, dtype=float)\n        if prev_best_x.shape == (dim,):\n            # Use clipped previous best as one candidate\n            x0 = clip_to_bounds(prev_best_x)\n            candidates.append(x0)\n\n    # Always include at least one random point\n    while len(candidates) < 2:\n        candidates.append(rng.uniform(low, high))\n\n    # Evaluate initial candidates\n    xs = []\n    ys = []\n    for x in candidates:\n        y = evaluate(x)\n        xs.append(x)\n        ys.append(y)\n\n    # Track global best\n    best_idx = int(np.argmin(ys))\n    best_x = xs[best_idx].copy()\n    best_y = ys[best_idx]\n\n    if evals_used >= budget:\n        return best_x\n\n    # --- Global exploration ---\n    # Use ~40% of budget for exploration, but at least a few points\n    remaining = budget - evals_used\n    global_evals = max(remaining // 2, 3)\n    global_evals = min(global_evals, budget - evals_used)\n\n    # Mix of pure random and local perturbations around best\n    for i in range(global_evals):\n        if prev_best_x is not None and i % 2 == 0:\n            # Perturb best known point (either from prev_best_x or current best)\n            base = best_x\n            # Scale perturbation with problem range\n            step_scale = 0.2\n            step = rng.normal(0.0, 1.0, size=dim) * (step_scale * span)\n            x = clip_to_bounds(base + step)\n        else:\n            # Pure random sample\n            x = rng.uniform(low, high)\n\n        y = evaluate(x)\n        if y < best_y:\n            best_x, best_y = x, y\n\n        if evals_used >= budget:\n            return best_x\n\n    # --- Local search (hill-climbing / stochastic coordinate search) ---\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Start from current best\n    x = best_x.copy()\n    y = best_y\n\n    # Initial step size as fraction of domain\n    step = 0.1 * span\n    min_step = 1e-6 * span + 1e-12\n\n    for _ in range(remaining):\n        # Propose local move (coordinate-wise Gaussian perturbation)\n        noise = rng.normal(0.0, 1.0, size=dim) * step\n        x_new = clip_to_bounds(x + noise)\n        y_new = evaluate(x_new)\n\n        if y_new < y:\n            x, y = x_new, y_new\n            if y_new < best_y:\n                best_x, best_y = x_new, y_new\n        else:\n            # Slightly reduce step size when failing to improve\n            step = np.maximum(step * 0.95, min_step)\n\n        if evals_used >= budget:\n            break\n\n    return best_x",
    "X": "0.17674571205592152 -0.061898521201777815 0.012203310871472112"
}