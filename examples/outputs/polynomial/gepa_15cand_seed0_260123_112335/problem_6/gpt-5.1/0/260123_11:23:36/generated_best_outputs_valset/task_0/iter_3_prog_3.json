{
    "score": -0.8433397246684183,
    "Input": "DeflectedCorrugatedSpring",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budget-aware hybrid search.\n\n    Strategy (budget-scaled):\n      1) Warm start from prev_best_x if provided.\n      2) Global exploration via Latin-hypercube-like sampling.\n      3) Compact, budget-aware CMA-ES style evolutionary search.\n      4) Greedy local Gaussian refinements around best point.\n      5) Always consume (or nearly consume) full evaluation budget.\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    span[span <= 0] = 1.0\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    def eval_point(x, best_x, best_y):\n        nonlocal evals_used\n        if evals_used >= budget:\n            return best_x, best_y, False\n        y = objective_function(x)\n        evals_used += 1\n        if best_y is None or y < best_y:\n            return x.copy(), float(y), True\n        return best_x, best_y, False\n\n    best_x = None\n    best_y = None\n\n    # 1) Warm start\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float)\n            if x0.shape == (dim,):\n                x0 = clip(x0)\n                best_x, best_y, _ = eval_point(x0, best_x, best_y)\n        except Exception:\n            pass\n\n    if evals_used >= budget:\n        if best_x is not None:\n            return best_x\n        return (low + high) / 2.0\n\n    # 2) Global exploration: Latin-hypercube-like sampling\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # Scale global samples with budget and dimension\n    # Use more global exploration for small budgets, moderate for large\n    target_global = max(4 * dim, 16)\n    max_global = int(0.35 * remaining)\n    if max_global < 1:\n        max_global = 1\n    n_global = min(target_global, max_global, 120)\n\n    # Reduce by 1 if we already used prev_best_x as one point\n    if best_x is not None and n_global > 1:\n        global_samples = n_global - 1\n    else:\n        global_samples = n_global\n\n    if global_samples > 0:\n        frac = (np.arange(global_samples) + rng.rand(global_samples)) / global_samples\n        frac = np.clip(frac, 0.0, 1.0)\n        lh_samples = np.empty((global_samples, dim))\n        for d in range(dim):\n            perm = rng.permutation(global_samples)\n            lh_samples[:, d] = frac[perm]\n        Xg = low + lh_samples * span\n        for i in range(global_samples):\n            if evals_used >= budget:\n                break\n            best_x, best_y, _ = eval_point(Xg[i], best_x, best_y)\n\n    if best_x is None:\n        # Fallback random start if everything failed\n        x0 = low + rng.rand(dim) * span\n        best_x, best_y, _ = eval_point(x0, best_x, best_y)\n\n    if evals_used >= budget:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # 3) CMA-ES style evolutionary search (budget-aware)\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Population size: scaled with dim, modest cap\n    lam = max(4, int(6 + 3 * np.log(dim + 1)))\n    lam = min(lam, 24)\n\n    mu = lam // 2\n    idx_mu = np.arange(1, mu + 1)\n    weights = np.log(mu + 0.5) - np.log(idx_mu)\n    weights /= np.sum(weights)\n    mueff = 1.0 / np.sum(weights ** 2)\n\n    # CMA-ES parameters (slightly conservative for robustness)\n    sigma = 0.25  # relative to span\n    cs = (mueff + 2.0) / (dim + mueff + 5.0)\n    ds = 1.0 + cs + 2.0 * max(np.sqrt((mueff - 1.0) / (dim + 1.0)) - 1.0, 0.0)\n    cc = (4.0 + mueff / dim) / (dim + 4.0 + 2.0 * mueff / dim)\n    c1 = 2.0 / ((dim + 1.3) ** 2 + mueff)\n    cmu = min(1.0 - c1, 2.0 * (mueff - 2.0 + 1.0 / mueff) / ((dim + 2.0) ** 2 + mueff))\n    chiN = np.sqrt(dim) * (1.0 - 1.0 / (4.0 * dim) + 1.0 / (21.0 * dim ** 2))\n\n    pc = np.zeros(dim)\n    ps = np.zeros(dim)\n    C = np.eye(dim)\n\n    mean = best_x.copy()\n\n    # Number of CMA-ES generations based on remaining evaluations\n    max_generations = max(1, remaining // lam)\n    max_generations = min(max_generations, 25)\n\n    eigen_eval_freq = max(1, int(0.5 * dim))\n    eigvals = np.ones(dim)\n    B = np.eye(dim)\n    D = np.ones(dim)\n\n    # Precompute span scaling to avoid repeated allocations\n    span_eps = span + 1e-12\n\n    for gen in range(max_generations):\n        if evals_used >= budget:\n            break\n\n        # Recompute eigen-decomposition occasionally\n        if gen % eigen_eval_freq == 0 or B is None or D is None:\n            try:\n                eigvals, B = np.linalg.eigh(C)\n                eigvals = np.maximum(eigvals, 1e-30)\n                D = np.sqrt(eigvals)\n            except np.linalg.LinAlgError:\n                C = np.eye(dim)\n                eigvals = np.ones(dim)\n                B = np.eye(dim)\n                D = np.ones(dim)\n\n        # Sample offspring\n        arz = rng.randn(dim, lam)\n        ary = B @ (D[:, None] * arz)  # (dim, lam)\n        steps = sigma * ary * span_eps[:, None]\n        X = mean[:, None] + steps\n        X = clip(X.T)  # (lam, dim)\n\n        # Evaluate offspring (track their objective values)\n        fvals = np.empty(lam)\n        for k in range(lam):\n            if evals_used >= budget:\n                fvals[k] = np.inf\n                continue\n            y = objective_function(X[k])\n            evals_used += 1\n            fvals[k] = y\n            if best_y is None or y < best_y:\n                best_x = X[k].copy()\n                best_y = float(y)\n\n        if evals_used >= budget:\n            break\n\n        # If all fvals are inf (should only happen if budget exhausted), break\n        if not np.isfinite(fvals).any():\n            break\n\n        # Selection\n        idx_sorted = np.argsort(fvals)\n        elite = X[idx_sorted[:mu]]\n        # Transform elite steps back to standardized coordinates (approximate)\n        elite_z = (elite - mean) / (span_eps * sigma)\n        weights_col = weights.reshape(-1, 1)\n        # New mean in original space\n        mean = np.sum(elite * weights_col, axis=0)\n\n        # Weighted direction in standardized coordinates\n        y_w = np.sum(elite_z * weights_col, axis=0)\n\n        # Update paths\n        Cinv_sqrt_yw = B @ (y_w / (D + 1e-12))\n        ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * Cinv_sqrt_yw\n        norm_ps = np.linalg.norm(ps)\n        hsig = float(\n            (norm_ps / np.sqrt(1 - (1 - cs) ** (2 * (gen + 1)))) /\n            chiN < (1.4 + 2.0 / (dim + 1.0))\n        )\n        pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * (B @ (D * y_w))\n\n        # Covariance update in standardized coordinates\n        z_diff = elite_z - y_w\n        C = (\n            (1 - c1 - cmu) * C\n            + c1 * (np.outer(pc / (span_eps + 1e-12), pc / (span_eps + 1e-12))\n                    + (1 - hsig) * cc * (2 - cc) * C)\n            + cmu * (z_diff.T @ (np.diag(weights) @ z_diff))\n        )\n\n        # Step-size control\n        sigma *= np.exp((cs / ds) * (norm_ps / chiN - 1.0))\n        sigma = max(0.03, min(sigma, 2.5))\n\n        # Early stop CMA if remaining evaluations very small\n        remaining = budget - evals_used\n        if remaining <= lam:\n            break\n\n    if evals_used >= budget:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # 4) Final local Gaussian refinement around best_x\n    remaining = budget - evals_used\n    if remaining <= 0 or best_x is None:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # Local search with decaying step-size\n    sigma_loc = 0.08 * span\n    sigma_min = 0.003 * span\n    sigma_max = 0.25 * span\n    patience = 12\n    no_improve = 0\n\n    for _ in range(remaining):\n        if evals_used >= budget:\n            break\n        step = rng.randn(dim) * sigma_loc\n        cand = clip(best_x + step)\n        prev_y = best_y\n        best_x, best_y, improved = eval_point(cand, best_x, best_y)\n        if improved and prev_y is not None and best_y < prev_y:\n            no_improve = 0\n            sigma_loc = np.minimum(sigma_loc * 1.15, sigma_max)\n        else:\n            no_improve += 1\n            if no_improve >= patience:\n                sigma_loc = np.maximum(sigma_loc * 0.5, sigma_min)\n                no_improve = 0\n\n    if best_x is None:\n        return (low + high) / 2.0\n    return best_x",
    "X": "5.849537204766151 4.9137176154219055 5.179194155710138 4.109404667179916"
}