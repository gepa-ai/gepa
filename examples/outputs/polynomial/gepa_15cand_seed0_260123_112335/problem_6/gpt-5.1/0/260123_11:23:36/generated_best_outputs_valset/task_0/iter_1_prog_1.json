{
    "score": -0.8433299205741728,
    "Input": "DeflectedCorrugatedSpring",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budget-aware hybrid search.\n    Strategy:\n      1) Use small global random / Latin-hypercube-like sampling.\n      2) Start from best (including prev_best_x) and perform local search\n         via Gaussian perturbations with adaptive step size.\n      3) Fully consume the evaluation budget.\n    \"\"\"\n    rng = np.random.RandomState()  # local RNG to avoid side effects\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    span[span <= 0] = 1.0  # safety in case of degenerate bounds\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    # Helper: evaluate point and track best\n    def eval_point(x, best_x, best_y):\n        nonlocal evals_used\n        if evals_used >= budget:\n            return best_x, best_y, False\n        y = objective_function(x)\n        evals_used += 1\n        if best_y is None or y < best_y:\n            return x.copy(), float(y), True\n        return best_x, best_y, False\n\n    best_x = None\n    best_y = None\n\n    # 1) Warm start from prev_best_x if available\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape == (dim,):\n            x0 = clip(x0)\n            best_x, best_y, _ = eval_point(x0, best_x, best_y)\n\n    # 2) Global initialization: Latin-hypercube-like sampling\n    # Reserve ~30% of budget (at least dim+1, at most 50) for global samples\n    remaining = budget - evals_used\n    if remaining <= 0:\n        # No evaluations allowed, fall back to prev_best_x or center\n        if best_x is None:\n            return (low + high) / 2.0\n        return best_x\n\n    n_global = max(min(remaining, max(dim + 1, 10)), 1)\n    n_global = min(n_global, 50)  # cap to avoid overspending on globals\n\n    # If we already evaluated prev_best_x, reduce global samples by 1\n    if best_x is not None and n_global > 1:\n        global_samples = n_global - 1\n    else:\n        global_samples = n_global\n\n    if global_samples > 0:\n        # Latin-hypercube-like: stratify each dim, then shuffle\n        frac = (np.arange(global_samples) + rng.rand(global_samples)) / global_samples\n        frac = np.clip(frac, 0.0, 1.0)\n        lh_samples = np.empty((global_samples, dim))\n        for d in range(dim):\n            perm = rng.permutation(global_samples)\n            lh_samples[:, d] = frac[perm]\n        Xg = low + lh_samples * span\n        for i in range(global_samples):\n            if evals_used >= budget:\n                break\n            best_x, best_y, _ = eval_point(Xg[i], best_x, best_y)\n\n    if evals_used >= budget:\n        if best_x is None:\n            return (low + high) / 2.0\n        return best_x\n\n    # Ensure we have some starting point\n    if best_x is None:\n        x0 = low + rng.rand(dim) * span\n        best_x, best_y, _ = eval_point(x0, best_x, best_y)\n        if evals_used >= budget:\n            return best_x\n\n    # 3) Local search around best_x using adaptive Gaussian perturbations\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Step sizes relative to domain size\n    base_sigma = 0.2 * span  # initial global-ish scale\n    min_sigma = 0.01 * span\n    max_sigma = 0.5 * span\n    sigma = np.clip(base_sigma, min_sigma, max_sigma)\n\n    # Parameters for local search\n    max_iters = remaining  # one candidate per iteration\n    patience = 20  # iterations without improvement before shrinking step\n    improvements = 0\n    no_improve_count = 0\n\n    for _ in range(max_iters):\n        if evals_used >= budget:\n            break\n\n        # Generate candidate with current sigma\n        step = rng.randn(dim) * sigma\n        cand = clip(best_x + step)\n        prev_best_y = best_y\n        best_x, best_y, improved = eval_point(cand, best_x, best_y)\n\n        if improved:\n            improvements += 1\n            no_improve_count = 0\n            # Slightly increase step-size on success (explore a bit more)\n            sigma = np.minimum(sigma * 1.05, max_sigma)\n        else:\n            no_improve_count += 1\n            # If stuck, shrink step-size to refine locally\n            if no_improve_count >= patience:\n                sigma = np.maximum(sigma * 0.5, min_sigma)\n                no_improve_count = 0\n\n        # If progress is extremely small, gradually reduce step size\n        if prev_best_y is not None and not improved and np.all(sigma <= 1.05 * min_sigma):\n            # Already very local; optionally perform a tiny random restart\n            if evals_used + 1 <= budget:\n                restart = low + rng.rand(dim) * span\n                best_x, best_y, _ = eval_point(restart, best_x, best_y)\n\n    return best_x",
    "X": "4.3036692284283244 4.275793464539386 5.023354947869925 5.739243471064348"
}