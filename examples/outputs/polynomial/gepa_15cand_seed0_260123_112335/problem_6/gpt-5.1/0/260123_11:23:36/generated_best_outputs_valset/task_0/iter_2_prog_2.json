{
    "score": -0.8433325346478744,
    "Input": "DeflectedCorrugatedSpring",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budget-aware hybrid search.\n\n    Strategy (budget-scaled):\n      1) Warm start from prev_best_x if provided.\n      2) Global exploration via Latin-hypercube-like sampling.\n      3) CMA-ES style evolutionary search (robust, scale-aware).\n      4) Greedy local Gaussian refinements around best point.\n      5) Always consume (or nearly consume) full evaluation budget.\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    span[span <= 0] = 1.0\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    def eval_point(x, best_x, best_y):\n        nonlocal evals_used\n        if evals_used >= budget:\n            return best_x, best_y, False\n        y = objective_function(x)\n        evals_used += 1\n        if best_y is None or y < best_y:\n            return x.copy(), float(y), True\n        return best_x, best_y, False\n\n    best_x = None\n    best_y = None\n\n    # 1) Warm start\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float)\n            if x0.shape == (dim,):\n                x0 = clip(x0)\n                best_x, best_y, _ = eval_point(x0, best_x, best_y)\n        except Exception:\n            pass  # ignore malformed warm-starts\n\n    if evals_used >= budget:\n        if best_x is not None:\n            return best_x\n        return (low + high) / 2.0\n\n    # 2) Global exploration: Latin-hypercube-like sampling\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # Scale global samples with budget and dimension\n    # At least 2*dim and at most 40% of remaining evaluations (capped)\n    target_global = max(2 * dim, 10)\n    max_global = int(0.4 * remaining)\n    if max_global < 1:\n        max_global = 1\n    n_global = min(target_global, max_global, 80)\n\n    # Reduce by 1 if we already used prev_best_x as one point\n    if best_x is not None and n_global > 1:\n        global_samples = n_global - 1\n    else:\n        global_samples = n_global\n\n    if global_samples > 0:\n        frac = (np.arange(global_samples) + rng.rand(global_samples)) / global_samples\n        frac = np.clip(frac, 0.0, 1.0)\n        lh_samples = np.empty((global_samples, dim))\n        for d in range(dim):\n            perm = rng.permutation(global_samples)\n            lh_samples[:, d] = frac[perm]\n        Xg = low + lh_samples * span\n        for i in range(global_samples):\n            if evals_used >= budget:\n                break\n            best_x, best_y, _ = eval_point(Xg[i], best_x, best_y)\n\n    if best_x is None:\n        # Fallback random start if everything failed\n        x0 = low + rng.rand(dim) * span\n        best_x, best_y, _ = eval_point(x0, best_x, best_y)\n\n    if evals_used >= budget:\n        return best_x\n\n    # 3) CMA-ES style evolutionary search (robust global-local blend)\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Population size: moderate and scaled with dim\n    lam = max(4, int(4 + 3 * np.log(dim + 1)))\n    lam = min(lam, 20)  # keep small to allocate iterations\n\n    # Parent number\n    mu = lam // 2\n    weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n    weights /= np.sum(weights)\n    mueff = 1.0 / np.sum(weights ** 2)\n\n    # Strategy params (simplified CMA-ES)\n    sigma = 0.3  # relative to span (we scale below)\n    cs = (mueff + 2.0) / (dim + mueff + 5.0)\n    ds = 1.0 + cs + 2.0 * max(np.sqrt((mueff - 1.0) / (dim + 1.0)) - 1.0, 0.0)\n    cc = (4.0 + mueff / dim) / (dim + 4.0 + 2.0 * mueff / dim)\n    c1 = 2.0 / ((dim + 1.3) ** 2 + mueff)\n    cmu = min(1.0 - c1, 2.0 * (mueff - 2.0 + 1.0 / mueff) / ((dim + 2.0) ** 2 + mueff))\n    chiN = np.sqrt(dim) * (1.0 - 1.0 / (4.0 * dim) + 1.0 / (21.0 * dim ** 2))\n\n    pc = np.zeros(dim)\n    ps = np.zeros(dim)\n    C = np.eye(dim)\n\n    mean = best_x.copy()\n\n    # Number of CMA-ES generations based on remaining evaluations\n    max_generations = max(1, (remaining // lam))\n    max_generations = min(max_generations, 30)  # don't overspend on CMA-ES\n\n    eigen_eval_freq = max(1, int(0.5 * dim))\n    eigvals = None\n    B = None\n    D = None\n\n    for gen in range(max_generations):\n        if evals_used >= budget:\n            break\n\n        # Recompute eigen-decomposition occasionally\n        if gen % eigen_eval_freq == 0 or B is None or D is None:\n            try:\n                eigvals, B = np.linalg.eigh(C)\n                eigvals = np.maximum(eigvals, 1e-30)\n                D = np.sqrt(eigvals)\n            except np.linalg.LinAlgError:\n                C = np.eye(dim)\n                eigvals = np.ones(dim)\n                B = np.eye(dim)\n                D = np.ones(dim)\n\n        # Sample offspring\n        arz = rng.randn(dim, lam)\n        ary = (B @ (D[:, None] * arz))  # shape (dim, lam)\n        # Scale steps relative to domain span\n        steps = sigma * ary * span[:, None]\n        X = mean[:, None] + steps\n        X = clip(X.T)  # shape (lam, dim)\n\n        # Evaluate offspring\n        ys = []\n        for k in range(lam):\n            if evals_used >= budget:\n                break\n            bx_before, by_before = best_x, best_y\n            best_x, best_y, _ = eval_point(X[k], best_x, best_y)\n            ys.append(float(objective_function(X[k])) if False else 0.0)  # placeholder\n\n        # If we ran out of budget in the middle, stop CMA-ES\n        if evals_used >= budget:\n            break\n\n        # To preserve objective evaluations count, we re-evaluate only once\n        # Replace ys with a single pass to minimize overhead:\n        # We re-run selection based on a separate evaluation array:\n        # For black-box, we need the values; track them explicitly.\n        # Re-evaluate population properly (costs lam evaluations but within loop):\n        # NOTE: We replace the dummy above: re-sample and evaluate succinctly.\n\n        # Resample + evaluate properly (ensure lam evaluations per generation if allowed)\n        # Guard against tight budgets (if remaining small, break)\n        remaining_here = budget - evals_used\n        if remaining_here < lam:\n            break\n\n        X = mean[:, None] + sigma * (B @ (D[:, None] * rng.randn(dim, lam))) * span[:, None]\n        X = clip(X.T)\n        fvals = np.empty(lam)\n        for k in range(lam):\n            if evals_used >= budget:\n                fvals[k] = np.inf\n                continue\n            y = objective_function(X[k])\n            evals_used += 1\n            fvals[k] = y\n            if best_y is None or y < best_y:\n                best_x = X[k].copy()\n                best_y = float(y)\n\n        # Selection\n        idx = np.argsort(fvals)\n        elite = X[idx[:mu]]\n        elite_z = (elite - mean) / (span * sigma + 1e-12)  # scale back to y-space approx\n        weights_col = weights.reshape(-1, 1)\n        mean = np.sum(elite * weights_col, axis=0)\n\n        # Update paths\n        y_w = np.sum((elite_z * weights_col), axis=0)\n        Cinv_sqrt_yw = (B @ (y_w / (D + 1e-12)))\n        ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * Cinv_sqrt_yw\n        hsig = float(\n            (np.linalg.norm(ps) / np.sqrt(1 - (1 - cs) ** (2 * (gen + 1)))) /\n            chiN < (1.4 + 2.0 / (dim + 1.0))\n        )\n        pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * y_w\n\n        # Covariance update\n        artmp = elite_z - y_w\n        C = (\n            (1 - c1 - cmu) * C\n            + c1 * (np.outer(pc, pc) + (1 - hsig) * cc * (2 - cc) * C)\n            + cmu * (artmp.T @ (np.diag(weights) @ artmp))\n        )\n\n        # Step-size control\n        sigma *= np.exp((cs / ds) * (np.linalg.norm(ps) / chiN - 1.0))\n        sigma = max(0.05, min(sigma, 2.0))  # keep within reasonable bounds\n\n    if evals_used >= budget:\n        return best_x\n\n    # 4) Final local Gaussian refinement around best_x\n    remaining = budget - evals_used\n    if remaining <= 0 or best_x is None:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # Local search with decaying step-size\n    sigma_loc = 0.1 * span\n    sigma_min = 0.005 * span\n    sigma_max = 0.3 * span\n    patience = 15\n    no_improve = 0\n\n    for _ in range(remaining):\n        if evals_used >= budget:\n            break\n        step = rng.randn(dim) * sigma_loc\n        cand = clip(best_x + step)\n        prev_y = best_y\n        best_x, best_y, improved = eval_point(cand, best_x, best_y)\n        if improved and prev_y is not None and best_y < prev_y:\n            no_improve = 0\n            sigma_loc = np.minimum(sigma_loc * 1.1, sigma_max)\n        else:\n            no_improve += 1\n            if no_improve >= patience:\n                sigma_loc = np.maximum(sigma_loc * 0.5, sigma_min)\n                no_improve = 0\n\n    if best_x is None:\n        return (low + high) / 2.0\n    return best_x",
    "X": "5.782643186727187 4.503785184129841 5.325003016754308 4.230793926407028"
}