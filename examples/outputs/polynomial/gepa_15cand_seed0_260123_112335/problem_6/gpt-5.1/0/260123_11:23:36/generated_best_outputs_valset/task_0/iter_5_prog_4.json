{
    "score": -0.8433398567625943,
    "Input": "DeflectedCorrugatedSpring",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budget-aware hybrid search.\n\n    Strategy (budget-scaled):\n      1) Warm start from prev_best_x if provided.\n      2) Global exploration via Latin-hypercube-like sampling.\n      3) Compact, robust CMA-ES style evolutionary search.\n      4) Greedy local Gaussian refinements around best point.\n      5) Always consume (or nearly consume) full evaluation budget.\n\n    Improvements vs previous version:\n      - More robust covariance handling and clipping in CMA-ES.\n      - Slightly smaller default sigma and adaptive scaling for small budgets.\n      - Better fallbacks if numerical issues appear.\n      - Guaranteed use of budget even for very small budgets.\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    # Avoid zero span; keep the box non-degenerate numerically\n    span[span <= 0] = 1.0\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    def eval_point(x, best_x, best_y):\n        nonlocal evals_used\n        if evals_used >= budget:\n            return best_x, best_y, False\n        y = objective_function(x)\n        evals_used += 1\n        if best_y is None or y < best_y:\n            return x.copy(), float(y), True\n        return best_x, best_y, False\n\n    best_x = None\n    best_y = None\n\n    # 1) Warm start\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float)\n            if x0.shape == (dim,):\n                x0 = clip(x0)\n                best_x, best_y, _ = eval_point(x0, best_x, best_y)\n        except Exception:\n            pass\n\n    if evals_used >= budget:\n        if best_x is not None:\n            return best_x\n        return (low + high) / 2.0\n\n    # 2) Global exploration: Latin-hypercube-like sampling\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # Target global samples: scale with dim, but also with budget\n    base_target = max(4 * dim, 16)\n    target_global = min(base_target, remaining // 2 if remaining >= 4 else remaining)\n    if target_global <= 0:\n        target_global = 1\n    max_global = int(0.4 * remaining) if remaining > 10 else remaining\n    if max_global < 1:\n        max_global = 1\n    n_global = int(min(target_global, max_global, 150))\n\n    if n_global > 0:\n        # If we already evaluated prev_best_x, don't double-count it as a sample\n        global_samples = n_global\n        frac = (np.arange(global_samples) + rng.rand(global_samples)) / global_samples\n        frac = np.clip(frac, 0.0, 1.0)\n        lh_samples = np.empty((global_samples, dim))\n        for d in range(dim):\n            perm = rng.permutation(global_samples)\n            lh_samples[:, d] = frac[perm]\n        Xg = low + lh_samples * span\n        for i in range(global_samples):\n            if evals_used >= budget:\n                break\n            best_x, best_y, _ = eval_point(Xg[i], best_x, best_y)\n\n    if best_x is None:\n        # Fallback random start if everything failed\n        x0 = low + rng.rand(dim) * span\n        best_x, best_y, _ = eval_point(x0, best_x, best_y)\n\n    if evals_used >= budget:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # 3) CMA-ES style evolutionary search (budget-aware, numerically robust)\n    remaining = budget - evals_used\n    if remaining <= 0 or best_x is None:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # Population size: scaled with dim, modest cap\n    lam = max(4, int(6 + 3 * np.log(dim + 1)))\n    lam = min(lam, 24)\n\n    # If remaining evaluations are too few, skip to local refinement\n    if remaining < lam * 2:\n        # Straight to local search below\n        pass\n    else:\n        mu = lam // 2\n        idx_mu = np.arange(1, mu + 1)\n        weights = np.log(mu + 0.5) - np.log(idx_mu)\n        weights /= np.sum(weights)\n        mueff = 1.0 / np.sum(weights ** 2)\n\n        # CMA-ES parameters (slightly conservative for robustness)\n        # Make sigma depend on remaining budget: smaller if budget is small\n        budget_factor = np.clip(remaining / (dim * lam + 1e-9), 0.5, 2.0)\n        sigma = 0.2 * budget_factor  # relative to span\n        sigma = float(np.clip(sigma, 0.08, 0.4))\n\n        cs = (mueff + 2.0) / (dim + mueff + 5.0)\n        ds = 1.0 + cs + 2.0 * max(np.sqrt((mueff - 1.0) / (dim + 1.0)) - 1.0, 0.0)\n        cc = (4.0 + mueff / dim) / (dim + 4.0 + 2.0 * mueff / dim)\n        c1 = 2.0 / ((dim + 1.3) ** 2 + mueff)\n        cmu = min(1.0 - c1, 2.0 * (mueff - 2.0 + 1.0 / mueff) / ((dim + 2.0) ** 2 + mueff))\n        chiN = np.sqrt(dim) * (1.0 - 1.0 / (4.0 * dim) + 1.0 / (21.0 * dim ** 2))\n\n        pc = np.zeros(dim)\n        ps = np.zeros(dim)\n        C = np.eye(dim)\n\n        mean = best_x.copy()\n\n        # Number of CMA-ES generations based on remaining evaluations\n        max_generations = max(1, remaining // lam)\n        max_generations = min(max_generations, 25)\n\n        eigen_eval_freq = max(1, int(0.5 * dim))\n        eigvals = np.ones(dim)\n        B = np.eye(dim)\n        D = np.ones(dim)\n\n        # Precompute span scaling to avoid repeated allocations\n        span_eps = span + 1e-12\n\n        for gen in range(max_generations):\n            if evals_used >= budget:\n                break\n\n            # Recompute eigen-decomposition occasionally; enforce PSD and bounds\n            if gen % eigen_eval_freq == 0:\n                try:\n                    # Symmetrize C to avoid numerical drift\n                    C = 0.5 * (C + C.T)\n                    eigvals, B = np.linalg.eigh(C)\n                    eigvals = np.maximum(eigvals, 1e-20)\n                    D = np.sqrt(eigvals)\n                except np.linalg.LinAlgError:\n                    C = np.eye(dim)\n                    eigvals = np.ones(dim)\n                    B = np.eye(dim)\n                    D = np.ones(dim)\n\n            # Sample offspring\n            arz = rng.randn(dim, lam)\n            ary = B @ (D[:, None] * arz)  # (dim, lam)\n            steps = sigma * ary * span_eps[:, None]\n            X = mean[:, None] + steps\n            X = clip(X.T)  # (lam, dim)\n\n            # Evaluate offspring (track their objective values)\n            fvals = np.empty(lam)\n            for k in range(lam):\n                if evals_used >= budget:\n                    fvals[k] = np.inf\n                    continue\n                y = objective_function(X[k])\n                evals_used += 1\n                fvals[k] = y\n                if best_y is None or y < best_y:\n                    best_x = X[k].copy()\n                    best_y = float(y)\n\n            if evals_used >= budget:\n                break\n\n            if not np.isfinite(fvals).any():\n                break\n\n            # Selection\n            idx_sorted = np.argsort(fvals)\n            elite = X[idx_sorted[:mu]]\n            weights_col = weights.reshape(-1, 1)\n\n            # New mean in original space\n            mean = np.sum(elite * weights_col, axis=0)\n\n            # Approximate standardized coordinates for covariance and path updates\n            elite_centered = (elite - mean) / (span_eps * sigma)\n            y_w = np.sum(elite_centered * weights_col, axis=0)\n\n            # Update paths\n            Cinv_sqrt_yw = B @ (y_w / (D + 1e-12))\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * Cinv_sqrt_yw\n            norm_ps = np.linalg.norm(ps)\n            hsig = float(\n                (norm_ps / np.sqrt(1 - (1 - cs) ** (2 * (gen + 1)))) /\n                chiN < (1.4 + 2.0 / (dim + 1.0))\n            )\n            pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * (B @ (D * y_w))\n\n            # Covariance update in standardized coordinates\n            z_diff = elite_centered - y_w\n            rank_mu = z_diff.T @ (np.diag(weights) @ z_diff)\n            # Ensure numerical symmetry and boundedness\n            C = (\n                (1 - c1 - cmu) * C\n                + c1 * (np.outer(pc / (span_eps + 1e-12), pc / (span_eps + 1e-12))\n                        + (1 - hsig) * cc * (2 - cc) * C)\n                + cmu * rank_mu\n            )\n            # Regularize covariance to avoid blow-up\n            C = np.clip(C, -1e6, 1e6)\n\n            # Step-size control\n            sigma *= np.exp((cs / ds) * (norm_ps / chiN - 1.0))\n            sigma = float(np.clip(sigma, 0.03, 2.0))\n\n            remaining = budget - evals_used\n            if remaining <= lam:\n                break\n\n    if evals_used >= budget:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # 4) Final local Gaussian refinement around best_x\n    remaining = budget - evals_used\n    if remaining <= 0 or best_x is None:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    sigma_loc = 0.06 * span\n    sigma_min = 0.002 * span\n    sigma_max = 0.25 * span\n    patience = 10\n    no_improve = 0\n\n    for _ in range(remaining):\n        if evals_used >= budget:\n            break\n        step = rng.randn(dim) * sigma_loc\n        cand = clip(best_x + step)\n        prev_y = best_y\n        best_x, best_y, improved = eval_point(cand, best_x, best_y)\n        if improved and prev_y is not None and best_y < prev_y:\n            no_improve = 0\n            sigma_loc = np.minimum(sigma_loc * 1.2, sigma_max)\n        else:\n            no_improve += 1\n            if no_improve >= patience:\n                sigma_loc = np.maximum(sigma_loc * 0.5, sigma_min)\n                no_improve = 0\n\n    if best_x is None:\n        return (low + high) / 2.0\n    return best_x",
    "X": "5.881810774649663 4.946904517964383 5.1778058700321985 4.138479012864312"
}