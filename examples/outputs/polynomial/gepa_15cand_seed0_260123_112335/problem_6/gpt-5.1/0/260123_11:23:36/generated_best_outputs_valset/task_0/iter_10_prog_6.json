{
    "score": -0.843339870488413,
    "Input": "DeflectedCorrugatedSpring",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budget-aware hybrid search.\n\n    Strategy (budget-scaled):\n      1) Warm start from prev_best_x if provided.\n      2) Global exploration via Latin-hypercube-like sampling + some pure random.\n      3) Compact, robust CMA-ES style evolutionary search (with simplified, safer updates).\n      4) Greedy local Gaussian refinements around best point.\n      5) Always consume (or nearly consume) full evaluation budget.\n    \"\"\"\n    bounds_arr = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    # Deterministic but varied seed based on bounds, dim and budget\n    seed = (dim * 73856093 + budget * 19349663 + bounds_arr.size * 83492791) & 0xFFFFFFFF\n    rng = np.random.RandomState(seed)\n\n    bounds = bounds_arr\n    low = bounds[:, 0].astype(float)\n    high = bounds[:, 1].astype(float)\n    span = high - low\n    # Avoid zero span; keep the box non-degenerate numerically\n    span[span <= 0] = 1.0\n    span_eps = span + 1e-12\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    def eval_point(x, best_x, best_y):\n        nonlocal evals_used\n        if evals_used >= budget:\n            return best_x, best_y, False\n        try:\n            y = float(objective_function(np.asarray(x, dtype=float)))\n        except Exception:\n            # If objective fails, treat as very bad but count evaluation\n            y = float(\"inf\")\n        evals_used += 1\n        if best_y is None or y < best_y:\n            return np.asarray(x, dtype=float).copy(), y, True\n        return best_x, best_y, False\n\n    best_x = None\n    best_y = None\n\n    # 1) Warm start\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float)\n            if x0.shape == (dim,):\n                x0 = clip(x0)\n                best_x, best_y, _ = eval_point(x0, best_x, best_y)\n        except Exception:\n            pass\n\n    if evals_used >= budget:\n        if best_x is not None:\n            return best_x\n        return (low + high) / 2.0\n\n    # 2) Global exploration: Latin-hypercube-like sampling + random\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    base_target = max(8 * dim, 24)\n    target_global = min(base_target, max(1, (remaining * 2) // 3))\n    max_global = remaining\n    n_global = int(min(target_global, max_global))\n\n    if n_global > 0:\n        global_samples = n_global\n        frac = (np.arange(global_samples) + rng.rand(global_samples)) / global_samples\n        frac = np.clip(frac, 0.0, 1.0)\n        lh_samples = np.empty((global_samples, dim))\n        for d in range(dim):\n            perm = rng.permutation(global_samples)\n            lh_samples[:, d] = frac[perm]\n        Xg = low + lh_samples * span\n\n        # Add a few pure random samples to increase robustness\n        n_rand = min(global_samples // 4 + 1, max_global - global_samples)\n        if n_rand > 0:\n            Xr = low + rng.rand(n_rand, dim) * span\n            Xg = np.vstack([Xg, Xr])\n            global_samples = Xg.shape[0]\n\n        for i in range(global_samples):\n            if evals_used >= budget:\n                break\n            best_x, best_y, _ = eval_point(Xg[i], best_x, best_y)\n\n    if best_x is None:\n        # Fallback random start if everything failed\n        x0 = low + rng.rand(dim) * span\n        best_x, best_y, _ = eval_point(x0, best_x, best_y)\n\n    if evals_used >= budget:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # 3) CMA-ES style evolutionary search (simplified, robust)\n    remaining = budget - evals_used\n    if remaining <= 0 or best_x is None:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    lam = max(4, int(6 + 3 * np.log(dim + 1)))\n    lam = min(lam, 24)\n\n    if remaining >= 2 * lam:\n        mu = lam // 2\n        idx_mu = np.arange(1, mu + 1)\n        weights = np.log(mu + 0.5) - np.log(idx_mu)\n        weights /= np.sum(weights)\n        mueff = 1.0 / np.sum(weights ** 2)\n\n        # Initial step-size based on box scale instead of relative span only\n        box_diag = np.linalg.norm(span_eps)\n        rel_sigma = 0.2 / (1.0 + 0.3 * np.log(dim + 1))\n        sigma = rel_sigma * box_diag / max(np.sqrt(dim), 1.0)\n        sigma = float(np.clip(sigma, 1e-3 * box_diag, 0.5 * box_diag))\n\n        # Step-size adaptation parameters\n        cs = (mueff + 2.0) / (dim + mueff + 5.0)\n        ds = 1.0 + cs + 2.0 * max(np.sqrt((mueff - 1.0) / (dim + 1.0)) - 1.0, 0.0)\n\n        # Covariance parameters (slightly conservative)\n        cc = (4.0 + mueff / dim) / (dim + 4.0 + 2.0 * mueff / dim)\n        c1 = 2.0 / ((dim + 1.3) ** 2 + mueff)\n        cmu = min(\n            1.0 - c1,\n            2.0 * (mueff - 2.0 + 1.0 / mueff) / ((dim + 2.0) ** 2 + mueff),\n        )\n        c1 *= 0.6\n        cmu *= 0.6\n\n        chiN = np.sqrt(dim) * (1.0 - 1.0 / (4.0 * dim) + 1.0 / (21.0 * dim ** 2))\n\n        pc = np.zeros(dim)\n        ps = np.zeros(dim)\n        C = np.eye(dim)\n\n        mean = best_x.copy()\n\n        max_generations = max(1, remaining // lam)\n        max_generations = min(max_generations, 30)\n\n        eigen_eval_freq = max(1, int(0.5 * dim))\n        eigvals = np.ones(dim)\n        B = np.eye(dim)\n        D = np.ones(dim)\n\n        max_cond = 1e7\n        min_eig = 1e-20\n        max_eig = 1e5\n\n        for gen in range(max_generations):\n            if evals_used >= budget:\n                break\n\n            # Eigen-decomposition and regularization\n            if gen % eigen_eval_freq == 0:\n                try:\n                    C = 0.5 * (C + C.T)\n                    eigvals, B = np.linalg.eigh(C)\n                    eigvals = np.clip(eigvals, min_eig, max_eig)\n                    cond = eigvals.max() / eigvals.min()\n                    if cond > max_cond:\n                        scale = np.sqrt(cond / max_cond)\n                        eigvals = eigvals / scale\n                    D = np.sqrt(eigvals)\n                    # Rebuild C from regularized eigensystem for consistency\n                    C = (B * eigvals) @ B.T\n                except np.linalg.LinAlgError:\n                    C = np.eye(dim)\n                    eigvals = np.ones(dim)\n                    B = np.eye(dim)\n                    D = np.ones(dim)\n\n            # Sample offspring in standardized space and map to box\n            arz = rng.randn(dim, lam)\n            ary = B @ (D[:, None] * arz)\n            # Scale by global sigma and absolute box scale\n            steps = sigma * ary / max(np.sqrt(dim), 1.0)\n            X = mean[:, None] + steps\n            X = clip(X.T)\n\n            fvals = np.empty(lam)\n            for k in range(lam):\n                if evals_used >= budget:\n                    fvals[k] = np.inf\n                    continue\n                y = float(objective_function(X[k]))\n                evals_used += 1\n                fvals[k] = y\n                if best_y is None or y < best_y:\n                    best_x = X[k].copy()\n                    best_y = y\n\n            if not np.isfinite(fvals).any() or evals_used >= budget:\n                break\n\n            idx_sorted = np.argsort(fvals)\n            elite = X[idx_sorted[:mu]]\n            weights_col = weights.reshape(-1, 1)\n\n            new_mean = np.sum(elite * weights_col, axis=0)\n            diff = new_mean - mean\n\n            # Update evolutionary paths\n            # Standardized step for ps (in eigensystem coordinates)\n            z_step = (B.T @ (diff * max(np.sqrt(dim), 1.0) / (sigma + 1e-12)))\n            z_step = z_step / (D + 1e-12)\n\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * z_step\n            norm_ps = np.linalg.norm(ps)\n\n            hsig = float(\n                (norm_ps / np.sqrt(1 - (1 - cs) ** (2 * (gen + 1)))) /\n                chiN < (1.4 + 2.0 / (dim + 1.0))\n            )\n            pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * diff\n\n            # Covariance update using standardized elite steps\n            zs = []\n            for k in range(mu):\n                step_k = elite[k] - mean\n                z_k = B.T @ (step_k * max(np.sqrt(dim), 1.0) / (sigma + 1e-12))\n                z_k = z_k / (D + 1e-12)\n                zs.append(z_k)\n            zs = np.asarray(zs)\n            z_mean = np.sum(zs * weights_col, axis=0)\n            z_diff = zs - z_mean\n            rank_mu = z_diff.T @ (np.diag(weights) @ z_diff)\n\n            # Transform rank_mu back to original coordinates\n            rank_mu_C = (B @ (rank_mu @ B.T))\n\n            C = (\n                (1 - c1 - cmu) * C\n                + c1 * (np.outer(pc, pc) + (1 - hsig) * cc * (2 - cc) * C)\n                + cmu * rank_mu_C\n            )\n            C = np.clip(C, -1e4, 1e4)\n\n            # Step-size control (damped)\n            sigma *= np.exp(0.3 * (cs / ds) * (norm_ps / chiN - 1.0))\n            sigma = float(\n                np.clip(\n                    sigma,\n                    1e-5 * box_diag,\n                    2.0 * box_diag,\n                )\n            )\n\n            mean = new_mean\n\n            remaining = budget - evals_used\n            if remaining <= lam:\n                break\n\n    if evals_used >= budget:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # 4) Final local Gaussian refinement around best_x\n    remaining = budget - evals_used\n    if remaining <= 0 or best_x is None:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    sigma_loc = 0.08 * span\n    sigma_min = 0.001 * span\n    sigma_max = 0.3 * span\n    patience = 10\n    no_improve = 0\n\n    for _ in range(remaining):\n        if evals_used >= budget:\n            break\n        step = rng.randn(dim) * sigma_loc\n        cand = clip(best_x + step)\n        prev_y = best_y\n        best_x, best_y, improved = eval_point(cand, best_x, best_y)\n        if improved and prev_y is not None and best_y < prev_y:\n            no_improve = 0\n            sigma_loc = np.minimum(sigma_loc * 1.3, sigma_max)\n        else:\n            no_improve += 1\n            if no_improve >= patience:\n                sigma_loc = np.maximum(sigma_loc * 0.4, sigma_min)\n                no_improve = 0\n\n    if best_x is None:\n        return (low + high) / 2.0\n    return best_x",
    "X": "5.877328147200203 4.947625280261136 5.181498647908909 4.13468687048984"
}