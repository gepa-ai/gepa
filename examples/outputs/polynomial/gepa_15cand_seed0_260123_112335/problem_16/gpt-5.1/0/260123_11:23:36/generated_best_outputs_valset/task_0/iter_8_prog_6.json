{
    "score": -3.023796317210424,
    "Input": "McCourt03",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid optimizer: quasi-random global exploration + warm-started local search\n    with adaptive step sizes and dynamic restart allocation. Designed to be\n    robust across dimensions and budgets while fully using the evaluation budget.\n    \"\"\"\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    lower, upper = bounds[:, 0], bounds[:, 1]\n    width = upper - lower\n\n    # Degenerate budget: just return a feasible point, no evaluation\n    if budget <= 0:\n        if prev_best_x is not None:\n            x0 = np.clip(np.asarray(prev_best_x, dtype=float), lower, upper)\n        else:\n            x0 = np.random.uniform(lower, upper, size=dim)\n        return x0.astype(float)\n\n    # Low-discrepancy sequence in [0,1]^dim (simple Van-der-Corput based)\n    # To avoid identical points across dimensions, we offset the index by dim*i.\n    def quasi_random(n_points):\n        seq = np.empty((n_points, dim), dtype=float)\n        for i in range(n_points):\n            for d in range(dim):\n                x = i + 1 + d * n_points\n                vdc = 0.0\n                denom = 1.0\n                while x:\n                    denom *= 2.0\n                    x, r = divmod(x, 2)\n                    vdc += r / denom\n                seq[i, d] = vdc\n        return seq\n\n    def local_search(x0, remaining_budget, y0=None):\n        \"\"\"\n        Adaptive Gaussian random search around x0.\n        Returns: (x_best, y_best, evals_used)\n        \"\"\"\n        if remaining_budget <= 0:\n            x_feas = np.clip(np.asarray(x0, dtype=float), lower, upper)\n            if y0 is None:\n                return x_feas.astype(float), float(\"inf\"), 0\n            return x_feas.astype(float), float(y0), 0\n\n        x_best = np.clip(np.asarray(x0, dtype=float), lower, upper)\n        evals_used = 0\n\n        if y0 is None:\n            y_best = objective_function(x_best)\n            evals_used += 1\n        else:\n            y_best = float(y0)\n\n        if remaining_budget <= evals_used:\n            return x_best.astype(float), float(y_best), evals_used\n\n        # Local search hyperparameters: dimension-aware exploration\n        base_step = 0.4\n        min_step = 1e-4\n        # More candidates in low dimension, capped to keep stability\n        n_candidates = max(6, min(24, 3 * dim))\n\n        remaining_after_init = max(0, remaining_budget - evals_used)\n        if remaining_after_init <= 0:\n            return x_best.astype(float), float(y_best), evals_used\n\n        max_iters = max(1, remaining_after_init // n_candidates)\n        step_scales = np.geomspace(\n            base_step, max(min_step, base_step * 1e-3), num=max_iters\n        )\n\n        it = 0\n        no_improve_streak = 0\n        max_no_improve = 10\n\n        cov_diag = (0.5 * width) ** 2\n        cov_diag[cov_diag == 0.0] = 1.0\n        cov_decay = 0.9\n        cov_gain = 0.15\n\n        while evals_used < remaining_budget:\n            step_scale = step_scales[min(it, max_iters - 1)]\n            it += 1\n\n            base_std = step_scale * np.sqrt(cov_diag)\n            base_std = np.where(base_std > 0, base_std, (upper - lower + 1e-9) * step_scale)\n\n            improved_in_iter = False\n\n            this_iter_candidates = min(n_candidates, remaining_budget - evals_used)\n\n            for _ in range(this_iter_candidates):\n                if evals_used >= remaining_budget:\n                    break\n\n                r = np.random.rand()\n                if r < 0.1:\n                    # Larger global jump\n                    exp_scale = min(1.0, step_scale * 8.0)\n                    std = exp_scale * width\n                    std = np.where(std > 0, std, (upper - lower + 1e-9) * exp_scale)\n                elif r < 0.35:\n                    # Coordinate-wise move\n                    std = np.zeros(dim, dtype=float)\n                    idx = np.random.randint(dim)\n                    std[idx] = base_std[idx] * 1.7\n                else:\n                    std = base_std\n\n                noise = np.random.normal(0.0, std, size=dim)\n                x_trial = x_best + noise\n                x_trial = np.clip(x_trial, lower, upper)\n                y_trial = objective_function(x_trial)\n                evals_used += 1\n\n                if y_trial < y_best:\n                    cov_diag = cov_decay * cov_diag + cov_gain * (noise ** 2)\n                    x_best, y_best = x_trial, y_trial\n                    improved_in_iter = True\n\n                if evals_used >= remaining_budget:\n                    break\n\n            if improved_in_iter:\n                no_improve_streak = 0\n            else:\n                no_improve_streak += 1\n                if no_improve_streak >= max_no_improve:\n                    no_improve_streak = 0\n                    # accelerate step size reduction and exploration reset\n                    it = min(it + 4, max_iters - 1)\n                    cov_diag = (0.5 * width) ** 2\n                    cov_diag[cov_diag == 0.0] = 1.0\n\n            if it >= max_iters and evals_used >= remaining_budget:\n                break\n\n        return x_best.astype(float), float(y_best), evals_used\n\n    evals_total = 0\n    x_global_best = None\n    y_global_best = None\n\n    # Warm-start phase from previous best\n    if prev_best_x is not None and budget > 0:\n        x0 = np.clip(np.asarray(prev_best_x, dtype=float), lower, upper)\n        y0 = objective_function(x0)\n        evals_total += 1\n        x_global_best = x0\n        y_global_best = float(y0)\n\n    if evals_total >= budget:\n        if x_global_best is None:\n            x_global_best = np.random.uniform(lower, upper, size=dim)\n        return x_global_best.astype(float)\n\n    remaining = budget - evals_total\n\n    # Initial global scouting if we don't have an evaluated incumbent yet\n    if y_global_best is None:\n        if remaining == 1:\n            x0 = np.random.uniform(lower, upper, size=dim)\n            y0 = objective_function(x0)\n            evals_total += 1\n            remaining = budget - evals_total\n            x_global_best = x0\n            y_global_best = float(y0)\n        else:\n            # Allocate ~30% of budget (at least 2) for global scouting\n            scout_budget = max(2, int(0.3 * remaining))\n            scout_budget = min(scout_budget, remaining)\n            n_scout = scout_budget\n\n            frac = quasi_random(n_scout)\n            candidates = lower + frac * width\n            ys = np.empty(n_scout, dtype=float)\n            for i in range(n_scout):\n                ys[i] = objective_function(candidates[i])\n            evals_total += n_scout\n            remaining = budget - evals_total\n            idx = int(np.argmin(ys))\n            x_global_best = candidates[idx]\n            y_global_best = float(ys[idx])\n\n    if remaining <= 0:\n        if x_global_best is None:\n            x_global_best = np.random.uniform(lower, upper, size=dim)\n        return x_global_best.astype(float)\n\n    # Decide number of local-search restarts dynamically\n    dim_factor = 1 + (dim > 10) + (dim > 30)\n    # Base maximum restarts\n    base_max_starts = 3 * dim_factor\n    # Scale with budget: more budget -> more restarts\n    max_starts_by_budget = max(1, remaining // max(6, 2 * dim))\n    max_starts = min(base_max_starts, max_starts_by_budget)\n\n    # Reserve a modest fraction of leftover for final intensification\n    reserve = max(1, remaining // 12)\n    budget_for_starts = max(1, remaining - reserve)\n\n    n_starts = max(1, max_starts)\n\n    # Per-start budget; at least 4 evaluations\n    per_start_budget = max(4, budget_for_starts // n_starts)\n\n    # First local search from current global best\n    starts_done = 0\n    if remaining > 0 and x_global_best is not None:\n        run_budget = min(per_start_budget, remaining)\n        x_ls, y_ls, used = local_search(x_global_best, run_budget, y_global_best)\n        evals_total += used\n        remaining -= used\n        x_global_best, y_global_best = x_ls, y_ls\n        starts_done += 1\n\n    # Additional restarts\n    while starts_done < n_starts and remaining > 0:\n        # Alternate between uniform and quasi-random seeds\n        if starts_done % 2 == 0:\n            frac = np.random.rand(dim)\n        else:\n            frac = quasi_random(1)[0]\n        x0 = lower + frac * width\n\n        run_budget = min(per_start_budget, remaining)\n        x_ls, y_ls, used = local_search(x0, run_budget)\n        evals_total += used\n        remaining -= used\n        starts_done += 1\n\n        if y_global_best is None or y_ls < y_global_best:\n            x_global_best, y_global_best = x_ls, y_ls\n\n    # Final intensification with any remaining budget\n    if remaining > 0 and x_global_best is not None:\n        x_ls, y_ls, used = local_search(x_global_best, remaining, y_global_best)\n        evals_total += used\n        if y_ls < y_global_best:\n            x_global_best, y_global_best = x_ls, y_ls\n\n    if x_global_best is None:\n        x_global_best = np.random.uniform(lower, upper, size=dim)\n\n    return x_global_best.astype(float)",
    "X": "0.9316260733273415 0.18908715657723404 0.2503026895781232 0.36446639763044353 0.16038893394095316 0.9828199212960645 0.03916569284172789 0.32643586358422216 0.6522396993694326"
}