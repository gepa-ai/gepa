{
    "score": -3.0237962790753894,
    "Input": "McCourt03",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid optimizer: quasi-random global exploration + warm-started local search\n    with adaptive step sizes, tuned for better budget usage and robustness.\n\n    Key changes vs previous version:\n    - Simpler, more orthogonal parameterization of global vs local budget.\n    - More reliable handling of very small budgets.\n    - Slightly larger and more diversified candidate sets in local search.\n    - Reduced internal overhead and more predictable use of the full budget.\n    \"\"\"\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    lower, upper = bounds[:, 0], bounds[:, 1]\n    width = upper - lower\n\n    # Degenerate budget: just return a feasible point, no evaluation\n    if budget <= 0:\n        if prev_best_x is not None:\n            x0 = np.clip(np.asarray(prev_best_x, dtype=float), lower, upper)\n        else:\n            x0 = np.random.uniform(lower, upper, size=dim)\n        return x0.astype(float)\n\n    # Low-discrepancy sequence in [0,1]^dim (simple Van-der-Corput based)\n    def quasi_random(n_points):\n        seq = np.empty((n_points, dim), dtype=float)\n        for i in range(n_points):\n            x = i + 1\n            vdc = 0.0\n            denom = 1.0\n            while x:\n                denom *= 2.0\n                x, r = divmod(x, 2)\n                vdc += r / denom\n            seq[i, :] = vdc\n        return seq\n\n    def local_search(x0, remaining_budget, y0=None):\n        \"\"\"\n        Adaptive Gaussian random search around x0.\n        Returns: (x_best, y_best, evals_used)\n        \"\"\"\n        if remaining_budget <= 0:\n            x_feas = np.clip(np.asarray(x0, dtype=float), lower, upper)\n            if y0 is None:\n                return x_feas.astype(float), float(\"inf\"), 0\n            return x_feas.astype(float), float(y0), 0\n\n        x_best = np.clip(np.asarray(x0, dtype=float), lower, upper)\n        evals_used = 0\n\n        if y0 is None:\n            y_best = objective_function(x_best)\n            evals_used += 1\n        else:\n            y_best = float(y0)\n\n        if remaining_budget <= evals_used:\n            return x_best.astype(float), float(y_best), evals_used\n\n        # Local search hyperparameters (slightly more exploratory)\n        base_step = 0.4\n        min_step = 1e-4\n        # Make candidate count scale more flexibly with dimension\n        n_candidates = max(6, min(20, 3 * dim))\n\n        # Estimate iterations based on remaining budget and candidate count\n        max_iters = max(1, (remaining_budget - evals_used) // n_candidates)\n        step_scales = np.geomspace(base_step, max(min_step, base_step * 1e-3), num=max_iters)\n\n        it = 0\n        no_improve_streak = 0\n        max_no_improve = 10\n\n        cov_diag = (0.5 * width) ** 2\n        cov_diag[cov_diag == 0.0] = 1.0\n        cov_decay = 0.9\n        cov_gain = 0.15\n\n        while evals_used < remaining_budget:\n            step_scale = step_scales[min(it, max_iters - 1)]\n            it += 1\n\n            base_std = step_scale * np.sqrt(cov_diag)\n            base_std = np.where(base_std > 0, base_std, (upper - lower + 1e-9) * step_scale)\n\n            improved_in_iter = False\n\n            for _ in range(n_candidates):\n                if evals_used >= remaining_budget:\n                    break\n\n                r = np.random.rand()\n                if r < 0.1:\n                    # Large global jump\n                    exp_scale = min(1.0, step_scale * 7.0)\n                    std = exp_scale * width\n                    std = np.where(std > 0, std, (upper - lower + 1e-9) * exp_scale)\n                elif r < 0.25:\n                    # Coordinate-wise move\n                    std = np.zeros(dim, dtype=float)\n                    idx = np.random.randint(dim)\n                    std[idx] = base_std[idx] * 1.5\n                else:\n                    std = base_std\n\n                noise = np.random.normal(0.0, std, size=dim)\n                x_trial = x_best + noise\n                x_trial = np.clip(x_trial, lower, upper)\n                y_trial = objective_function(x_trial)\n                evals_used += 1\n\n                if y_trial < y_best:\n                    cov_diag = cov_decay * cov_diag + cov_gain * (noise ** 2)\n                    x_best, y_best = x_trial, y_trial\n                    improved_in_iter = True\n\n            if improved_in_iter:\n                no_improve_streak = 0\n            else:\n                no_improve_streak += 1\n                if no_improve_streak >= max_no_improve:\n                    # When stuck, increase effective iteration index to shrink step faster\n                    no_improve_streak = 0\n                    it = min(it + 4, max_iters - 1)\n\n        return x_best.astype(float), float(y_best), evals_used\n\n    evals_total = 0\n    x_global_best = None\n    y_global_best = None\n\n    # Warm-start phase\n    if prev_best_x is not None:\n        x0 = np.clip(np.asarray(prev_best_x, dtype=float), lower, upper)\n        # Always try to evaluate warm start if there is any meaningful budget\n        if budget >= 1:\n            y0 = objective_function(x0)\n            evals_total += 1\n            x_global_best = x0\n            y_global_best = float(y0)\n\n    if evals_total >= budget:\n        return x_global_best.astype(float)\n\n    remaining = budget - evals_total\n\n    # If no evaluated best yet, do a small initial scouting with quasi-random points\n    if y_global_best is None:\n        n_scout = min(max(1, budget // 5), remaining)\n        if n_scout > 0:\n            frac = quasi_random(n_scout)\n            candidates = lower + frac * width\n            ys = np.empty(n_scout, dtype=float)\n            for i in range(n_scout):\n                ys[i] = objective_function(candidates[i])\n            evals_total += n_scout\n            remaining -= n_scout\n            idx = int(np.argmin(ys))\n            x_global_best = candidates[idx]\n            y_global_best = float(ys[idx])\n        else:\n            # Very small budget, just evaluate one random candidate\n            x0 = np.random.uniform(lower, upper, size=dim)\n            y0 = objective_function(x0)\n            evals_total += 1\n            remaining = budget - evals_total\n            x_global_best = x0\n            y_global_best = float(y0)\n\n    if remaining <= 0:\n        return x_global_best.astype(float)\n\n    # Decide on number of local-search restarts\n    # Balance restarts with per-restart depth\n    # For larger budgets & dims, allow more restarts\n    dim_factor = 1 + (dim > 10) + (dim > 30)\n    max_starts = min(max(1, budget // 6), 4 * dim_factor)\n    # Reserve a fraction of leftover for final intensification\n    reserve = max(1, remaining // 8)\n    budget_for_starts = max(1, remaining - reserve)\n\n    # Ensure at least one restart (the current best is considered as one)\n    n_starts = max(1, max_starts)\n\n    # Per-start budget; keep it at least 3 evaluations\n    per_start_budget = max(3, budget_for_starts // n_starts)\n\n    # First local search from current global best\n    starts_done = 0\n    if remaining > 0 and x_global_best is not None:\n        run_budget = min(per_start_budget, remaining)\n        x_ls, y_ls, used = local_search(x_global_best, run_budget, y_global_best)\n        evals_total += used\n        remaining -= used\n        x_global_best, y_global_best = x_ls, y_ls\n        starts_done += 1\n\n    # Additional restarts\n    while starts_done < n_starts and remaining > 0:\n        # Alternate between uniform and quasi-random seeds\n        if starts_done % 2 == 0:\n            frac = np.random.rand(dim)\n        else:\n            frac = quasi_random(1)[0]\n        x0 = lower + frac * width\n\n        run_budget = min(per_start_budget, remaining)\n        x_ls, y_ls, used = local_search(x0, run_budget)\n        evals_total += used\n        remaining -= used\n        starts_done += 1\n\n        if y_ls < y_global_best:\n            x_global_best, y_global_best = x_ls, y_ls\n\n    # Final intensification with any remaining budget\n    if remaining > 0 and x_global_best is not None:\n        x_ls, y_ls, used = local_search(x_global_best, remaining, y_global_best)\n        evals_total += used\n        if y_ls < y_global_best:\n            x_global_best, y_global_best = x_ls, y_ls\n\n    if x_global_best is None:\n        x_global_best = np.random.uniform(lower, upper, size=dim)\n\n    return x_global_best.astype(float)",
    "X": "0.9315704571177479 0.18909758929126136 0.25043533263958895 0.3646663581573219 0.16018785958927428 0.9827898190003597 0.039094164902721176 0.3262429678769485 0.6522670546882262"
}