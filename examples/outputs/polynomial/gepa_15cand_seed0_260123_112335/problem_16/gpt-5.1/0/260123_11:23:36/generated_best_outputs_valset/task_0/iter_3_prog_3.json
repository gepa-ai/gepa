{
    "score": -3.0237962763758723,
    "Input": "McCourt03",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid optimizer: combines warm-started local search with opportunistic\n    global restarts under a strict evaluation budget.\n\n    Main ideas:\n    - Start from prev_best_x if available, otherwise several random / quasi-random points.\n    - Maintain a global best across all starts.\n    - Local search: adaptive Gaussian random search with shrinking step size.\n    - Simple covariance adaptation based on successful moves.\n    - Occasional larger exploratory moves even late in the search.\n    - Full budget use and strict bound compliance.\n    \"\"\"\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    lower, upper = bounds[:, 0], bounds[:, 1]\n    width = upper - lower\n\n    # Handle degenerate cases quickly\n    if budget <= 0:\n        if prev_best_x is not None:\n            x0 = np.clip(np.asarray(prev_best_x, dtype=float), lower, upper)\n        else:\n            x0 = np.random.uniform(lower, upper, size=dim)\n        return x0.astype(float)\n\n    # Low-discrepancy uniform samples in [0,1]^dim (Sobol-like using Van der Corput in base 2)\n    def quasi_random(n_points):\n        # Simple Halton sequence with base 2 per dimension (shifted)\n        seq = np.empty((n_points, dim), dtype=float)\n        for i in range(n_points):\n            x = (i + 1)  # avoid 0\n            vdc = 0.0\n            denom = 1.0\n            while x:\n                denom *= 2.0\n                x, remainder = divmod(x, 2)\n                vdc += remainder / denom\n            seq[i, :] = vdc\n        return seq\n\n    # Helper: single local search run starting from x0\n    def local_search(x0, remaining_budget, y0=None):\n        if remaining_budget <= 0:\n            x_feas = np.clip(np.asarray(x0, dtype=float), lower, upper)\n            if y0 is None:\n                # We are not allowed to evaluate here, just return x0\n                return x_feas.astype(float), float(\"inf\"), 0\n            return x_feas.astype(float), float(y0), 0\n\n        # Ensure feasible start\n        x_best = np.clip(np.asarray(x0, dtype=float), lower, upper)\n        evals_used = 0\n\n        # Evaluate if not already evaluated\n        if y0 is None:\n            y_best = objective_function(x_best)\n            evals_used += 1\n        else:\n            y_best = float(y0)\n\n        if remaining_budget <= evals_used:\n            return x_best.astype(float), float(y_best), evals_used\n\n        # Local search hyperparameters\n        base_step = 0.3\n        min_step = 1e-4\n        n_candidates = max(4, min(12, dim * 2))\n\n        # Conservative estimate of iterations for remaining budget\n        max_iters = max(1, (remaining_budget - evals_used) // n_candidates)\n        step_scales = np.geomspace(base_step, max(min_step, base_step * 1e-3), num=max_iters)\n\n        it = 0\n        no_improve_streak = 0\n        max_no_improve = 8\n\n        # Simple diagonal covariance adaptation\n        cov_diag = (0.5 * width) ** 2\n        cov_diag[cov_diag == 0.0] = 1.0\n        cov_decay = 0.9\n        cov_gain = 0.1\n\n        while evals_used < remaining_budget:\n            if it >= max_iters:\n                step_scale = step_scales[-1]\n            else:\n                step_scale = step_scales[it]\n            it += 1\n\n            base_std = step_scale * np.sqrt(cov_diag)\n            base_std = np.where(base_std > 0, base_std, (upper - lower + 1e-9) * step_scale)\n\n            improved_in_iter = False\n\n            for _ in range(n_candidates):\n                if evals_used >= remaining_budget:\n                    break\n\n                # With small probability, do a larger exploratory move\n                if np.random.rand() < 0.12:\n                    exp_scale = min(1.0, step_scale * 5.0)\n                    std = exp_scale * width\n                    std = np.where(std > 0, std, (upper - lower + 1e-9) * exp_scale)\n                else:\n                    std = base_std\n\n                noise = np.random.normal(0.0, std, size=dim)\n                x_trial = x_best + noise\n                x_trial = np.clip(x_trial, lower, upper)\n                y_trial = objective_function(x_trial)\n                evals_used += 1\n\n                if y_trial < y_best:\n                    # Update diagonal covariance towards successful direction\n                    cov_diag = cov_decay * cov_diag + cov_gain * (noise ** 2)\n                    x_best, y_best = x_trial, y_trial\n                    improved_in_iter = True\n\n            if improved_in_iter:\n                no_improve_streak = 0\n            else:\n                no_improve_streak += 1\n                if no_improve_streak >= max_no_improve:\n                    no_improve_streak = 0\n                    it = min(it + 3, max_iters - 1)\n\n        return x_best.astype(float), float(y_best), evals_used\n\n    # Global bookkeeping\n    evals_total = 0\n    x_global_best = None\n    y_global_best = None\n\n    # 1) Optional evaluation / refinement of warm start if provided\n    if prev_best_x is not None:\n        x0 = np.clip(np.asarray(prev_best_x, dtype=float), lower, upper)\n        # Only evaluate warm start if budget is reasonably large; otherwise save for search\n        if budget >= 3:\n            y0 = objective_function(x0)\n            evals_total += 1\n            x_global_best = x0\n            y_global_best = float(y0)\n        else:\n            x_global_best = x0\n            y_global_best = None\n\n    if evals_total >= budget:\n        return x_global_best.astype(float)\n\n    remaining = budget - evals_total\n\n    # 2) Decide number of additional starting points\n    # Use more restarts for larger budgets and higher dimensions\n    if prev_best_x is None:\n        base_starts = 4\n    else:\n        base_starts = 3\n\n    # Slightly scale with dimension and remaining budget\n    dim_factor = 1 + (dim > 10) + (dim > 30)\n    max_starts = base_starts * dim_factor\n    max_starts = min(max_starts, max(1, remaining // 4))\n\n    n_starts = max(1, max_starts)\n\n    # Allocate budget: keep a small reserve for a final intensification\n    reserve = max(0, remaining // 10)\n    budget_for_starts = max(1, remaining - reserve)\n\n    # Distribute approximately evenly across starts\n    per_start_budget = max(3, budget_for_starts // n_starts)\n\n    # 3) First run: from warm start if available, otherwise quasi-random best-of-k\n    starts_done = 0\n    if x_global_best is not None and remaining > 0:\n        run_budget = min(per_start_budget, remaining)\n        x_ls, y_ls, used = local_search(x_global_best, run_budget, y_global_best)\n        evals_total += used\n        remaining -= used\n        x_global_best, y_global_best = x_ls, y_ls\n        starts_done += 1\n\n    # If no warm start, pick a good initial point using quasi-random sampling\n    if x_global_best is None:\n        # Use up to min(10, remaining) points for scouting\n        n_scout = min(10, remaining)\n        if n_scout <= 0:\n            x_global_best = np.random.uniform(lower, upper, size=dim)\n        else:\n            frac = quasi_random(n_scout)\n            candidates = lower + frac * width\n            ys = []\n            for i in range(n_scout):\n                ys.append(objective_function(candidates[i]))\n            ys = np.asarray(ys, dtype=float)\n            evals_total += n_scout\n            remaining -= n_scout\n            best_idx = int(np.argmin(ys))\n            x_global_best = candidates[best_idx]\n            y_global_best = float(ys[best_idx])\n            starts_done += 1\n\n    # 4) Additional restarts from random / quasi-random points\n    while starts_done < n_starts and remaining > 0:\n        # mix quasi-random and uniform\n        if starts_done % 2 == 0:\n            frac = np.random.rand(dim)\n        else:\n            frac = quasi_random(1)[0]\n        x0 = lower + frac * width\n\n        run_budget = min(per_start_budget, remaining)\n        x_ls, y_ls, used = local_search(x0, run_budget)\n        evals_total += used\n        remaining -= used\n        starts_done += 1\n\n        if (y_global_best is None) or (y_ls < y_global_best):\n            x_global_best, y_global_best = x_ls, y_ls\n\n    # 5) Final intensification around global best with any leftover budget\n    if remaining > 0 and x_global_best is not None:\n        x_ls, y_ls, used = local_search(x_global_best, remaining, y_global_best)\n        evals_total += used\n        if y_ls < y_global_best:\n            x_global_best, y_global_best = x_ls, y_ls\n\n    # Fallback if, for some reason, we never evaluated anything\n    if x_global_best is None:\n        x_global_best = np.random.uniform(lower, upper, size=dim)\n\n    return x_global_best.astype(float)",
    "X": "0.9315704571177479 0.18909758929126136 0.25043533263958895 0.36467743011373477 0.16018785958927428 0.9827898190003597 0.039094164902721176 0.3262429678769485 0.6522670546882262"
}