{
    "score": -3.02379632499136,
    "Input": "McCourt03",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid optimizer: quasi-random global exploration + warm-started local search\n    with adaptive step sizes and dynamic restart allocation.\n\n    This version simplifies and stabilizes local search, improves budget usage\n    in low-budget regimes, and keeps behavior robust across dimensions.\n    \"\"\"\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    lower, upper = bounds[:, 0], bounds[:, 1]\n    width = upper - lower\n\n    # Degenerate budget: just return a feasible point, no evaluation\n    if budget <= 0:\n        if prev_best_x is not None:\n            x0 = np.clip(np.asarray(prev_best_x, dtype=float), lower, upper)\n        else:\n            x0 = np.random.uniform(lower, upper, size=dim)\n        return x0.astype(float)\n\n    # Simple low-discrepancy-like sequence in [0,1]^dim (Van-der-Corput based)\n    def quasi_random(n_points, offset=0):\n        seq = np.empty((n_points, dim), dtype=float)\n        for i in range(n_points):\n            for d in range(dim):\n                x = i + 1 + d * (n_points + offset)\n                vdc = 0.0\n                denom = 1.0\n                while x:\n                    denom *= 2.0\n                    x, r = divmod(x, 2)\n                    vdc += r / denom\n                seq[i, d] = vdc\n        return seq\n\n    def local_search(x0, remaining_budget, y0=None):\n        \"\"\"\n        Simpler, robust Gaussian random search around x0 with adaptive radius.\n        Returns: (x_best, y_best, evals_used)\n        \"\"\"\n        if remaining_budget <= 0:\n            x_feas = np.clip(np.asarray(x0, dtype=float), lower, upper)\n            return x_feas.astype(float), (float(\"inf\") if y0 is None else float(y0)), 0\n\n        x_best = np.clip(np.asarray(x0, dtype=float), lower, upper)\n        evals_used = 0\n\n        if y0 is None:\n            y_best = objective_function(x_best)\n            evals_used += 1\n        else:\n            y_best = float(y0)\n\n        if remaining_budget <= evals_used:\n            return x_best.astype(float), float(y_best), evals_used\n\n        # Dimension-aware initial step (fraction of domain width)\n        dom_scale = np.where(width > 0, width, 1.0)\n        # Smaller step in high dimension to avoid too aggressive moves\n        base_frac = 0.3 / max(1.0, np.sqrt(dim))\n        step = base_frac * dom_scale\n        min_step = 1e-4 * dom_scale\n        max_step = 0.5 * dom_scale\n\n        # Per-iteration candidates (balance exploration vs stability)\n        n_candidates = max(4, min(16, 2 * dim))\n        max_iters = max(1, (remaining_budget - evals_used) // n_candidates)\n\n        no_improve = 0\n        max_no_improve = 8\n\n        for it in range(max_iters):\n            if evals_used >= remaining_budget:\n                break\n\n            improved_in_iter = False\n            this_iter_candidates = min(\n                n_candidates, remaining_budget - evals_used\n            )\n\n            for _ in range(this_iter_candidates):\n                if evals_used >= remaining_budget:\n                    break\n\n                r = np.random.rand()\n                if r < 0.15:\n                    # larger global-ish jump\n                    std = np.minimum(max_step, 2.0 * step)\n                elif r < 0.35 and dim > 1:\n                    # coordinate-wise move\n                    std = np.zeros(dim, dtype=float)\n                    idx = np.random.randint(dim)\n                    std[idx] = step[idx]\n                else:\n                    std = 0.7 * step\n\n                noise = np.random.normal(0.0, std, size=dim)\n                x_trial = np.clip(x_best + noise, lower, upper)\n                y_trial = objective_function(x_trial)\n                evals_used += 1\n\n                if y_trial < y_best:\n                    x_best, y_best = x_trial, y_trial\n                    improved_in_iter = True\n\n                if evals_used >= remaining_budget:\n                    break\n\n            if improved_in_iter:\n                no_improve = 0\n                # slight step increase to keep exploring\n                step = np.minimum(max_step, step * 1.1)\n            else:\n                no_improve += 1\n                # contract step when stagnating\n                step = np.maximum(min_step, step * 0.5)\n\n            if no_improve >= max_no_improve:\n                # soft restart around current best with moderate radius\n                step = np.maximum(min_step * 10, 0.2 * dom_scale)\n                no_improve = 0\n\n        return x_best.astype(float), float(y_best), evals_used\n\n    evals_total = 0\n    x_global_best = None\n    y_global_best = None\n\n    # Warm-start from previous best if provided\n    if prev_best_x is not None and budget > 0:\n        x0 = np.clip(np.asarray(prev_best_x, dtype=float), lower, upper)\n        y0 = objective_function(x0)\n        evals_total += 1\n        x_global_best = x0\n        y_global_best = float(y0)\n\n    if evals_total >= budget:\n        if x_global_best is None:\n            x_global_best = np.random.uniform(lower, upper, size=dim)\n        return x_global_best.astype(float)\n\n    remaining = budget - evals_total\n\n    # Initial global scouting if we don't have an incumbent yet\n    if y_global_best is None:\n        if remaining == 1:\n            # Only one eval left: just sample one point\n            x0 = np.random.uniform(lower, upper, size=dim)\n            y0 = objective_function(x0)\n            evals_total += 1\n            x_global_best = x0\n            y_global_best = float(y0)\n            remaining = budget - evals_total\n        else:\n            # Allocate ~40% of remaining (at least 3, at most half) for scouting\n            scout_budget = max(3, int(0.4 * remaining))\n            scout_budget = min(scout_budget, remaining // 2 if remaining > 4 else remaining)\n            n_scout = max(1, scout_budget)\n\n            frac = quasi_random(n_scout)\n            candidates = lower + frac * width\n            ys = np.empty(n_scout, dtype=float)\n            for i in range(n_scout):\n                ys[i] = objective_function(candidates[i])\n            evals_total += n_scout\n            remaining = budget - evals_total\n            idx = int(np.argmin(ys))\n            x_global_best = candidates[idx]\n            y_global_best = float(ys[idx])\n\n    if remaining <= 0:\n        if x_global_best is None:\n            x_global_best = np.random.uniform(lower, upper, size=dim)\n        return x_global_best.astype(float)\n\n    # Decide number of local-search restarts\n    dim_factor = 1 + (dim > 10) + (dim > 30)\n    base_max_starts = 3 * dim_factor\n\n    # Each start should get a meaningful number of evals\n    min_per_start = max(6, 2 * dim)\n    max_starts_by_budget = max(1, remaining // min_per_start)\n    max_starts = min(base_max_starts, max_starts_by_budget)\n\n    # Reserve some budget for final intensification\n    reserve = max(1, remaining // 10)\n    budget_for_starts = max(1, remaining - reserve)\n\n    n_starts = max(1, max_starts)\n    per_start_budget = max(4, budget_for_starts // n_starts)\n\n    # First local search from current global best\n    starts_done = 0\n    if remaining > 0 and x_global_best is not None:\n        run_budget = min(per_start_budget, remaining)\n        x_ls, y_ls, used = local_search(x_global_best, run_budget, y_global_best)\n        evals_total += used\n        remaining -= used\n        x_global_best, y_global_best = x_ls, y_ls\n        starts_done += 1\n\n    # Additional restarts\n    qr_offset = 0\n    while starts_done < n_starts and remaining > 0:\n        if starts_done % 2 == 0:\n            frac = np.random.rand(dim)\n        else:\n            frac = quasi_random(1, offset=qr_offset)[0]\n            qr_offset += 1\n        x0 = lower + frac * width\n\n        run_budget = min(per_start_budget, remaining)\n        x_ls, y_ls, used = local_search(x0, run_budget)\n        evals_total += used\n        remaining -= used\n        starts_done += 1\n\n        if y_global_best is None or y_ls < y_global_best:\n            x_global_best, y_global_best = x_ls, y_ls\n\n    # Final intensification with any remaining budget\n    if remaining > 0 and x_global_best is not None:\n        x_ls, y_ls, used = local_search(x_global_best, remaining, y_global_best)\n        evals_total += used\n        if y_ls < y_global_best:\n            x_global_best, y_global_best = x_ls, y_ls\n\n    if x_global_best is None:\n        x_global_best = np.random.uniform(lower, upper, size=dim)\n\n    return x_global_best.astype(float)",
    "X": "0.9316260733273415 0.18908715657723404 0.2503026895781232 0.36446639763044353 0.16038893394095316 0.9828199212960645 0.03916569284172789 0.32623694327209546 0.6522396993694326"
}