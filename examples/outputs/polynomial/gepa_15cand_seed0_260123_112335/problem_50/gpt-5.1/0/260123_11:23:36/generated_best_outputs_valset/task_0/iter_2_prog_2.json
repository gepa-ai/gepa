{
    "score": -3455.9999999999995,
    "Input": "Schwefel36",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budget-aware black-box minimization with:\n      - optional warm start from prev_best_x\n      - quasi-random global search (Sobol-like via Halton sequence)\n      - followed by adaptive local search around the best point\n\n    Strategy:\n    1. Use prev_best_x if available (counts as one eval).\n    2. Use ~60% of remaining budget for global exploration with low-discrepancy\n       sampling inside bounds.\n    3. Use remaining budget for adaptive local search around current best,\n       with step-size adaptation based on success rate.\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    # Degenerate cases: fabricate a feasible point (midpoint of bounds)\n    if dim <= 0 or budget <= 0:\n        return np.mean(bounds, axis=1)\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # ---------- helper: evaluation with budget tracking ----------\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # ---------- helper: simple Halton sequence for better coverage ----------\n    def halton_sequence(size, dim, scramble=False, seed=None):\n        \"\"\"Generate 'size' points of a Halton sequence in [0,1]^dim.\"\"\"\n        # First dim prime numbers (good enough for modest dim)\n        # Taken from a standard prime list\n        primes = [\n            2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n            31, 37, 41, 43, 47, 53, 59, 61, 67, 71,\n            73, 79, 83, 89, 97, 101, 103, 107, 109, 113\n        ]\n        base_list = primes[:dim]\n\n        rng = np.random.RandomState(seed)\n\n        def van_der_corput(n, base, scramble_bit=False):\n            vdc, denom = 0.0, 1.0\n            while n:\n                n, remainder = divmod(n, base)\n                digit = remainder\n                if scramble_bit:\n                    # simple digit-scramble: random permutation of digits\n                    # (fixed per call to keep determinism for given seed)\n                    digit = scramble_map[base][remainder]\n                denom *= base\n                vdc += digit / denom\n            return vdc\n\n        # Per-base scramble maps if requested\n        scramble_map = {}\n        if scramble:\n            for b in base_list:\n                perm = np.arange(b)\n                rng.shuffle(perm)\n                scramble_map[b] = perm\n\n        seq = np.empty((size, dim), dtype=float)\n        for d, base in enumerate(base_list):\n            for i in range(size):\n                seq[i, d] = van_der_corput(i + 1, base, scramble_bit=scramble)\n        return seq\n\n    # ---------- 1) Warm-start from prev_best_x ----------\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = np.clip(x0, low, high)\n            eval_point(x0)\n\n    # If warm start exhausted budget (very small budget case)\n    if evals_used >= budget:\n        if best_x is None:\n            best_x = np.clip(np.mean(bounds, axis=1), low, high)\n        return best_x\n\n    # ---------- 2) Global exploration (Halton-based) ----------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        if best_x is None:\n            best_x = np.random.uniform(low, high)\n        return best_x\n\n    # Allocate about 60% of remaining evaluations to global search\n    global_budget = max(1, int(0.6 * remaining))\n\n    # Use a scrambled Halton sequence for quasi-random coverage\n    halton_pts = halton_sequence(global_budget, dim, scramble=True)\n\n    for i in range(global_budget):\n        if evals_used >= budget:\n            break\n        u = halton_pts[i]\n        x = low + u * span\n        # If any dimension is degenerate (span == 0), just use low\n        x = np.where(span > 0, x, low)\n        eval_point(x)\n\n    # ---------- 3) Local adaptive search around best_x ----------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    if best_x is None:\n        # No valid evaluation yet for some reason, sample once\n        x_init = np.random.uniform(low, high)\n        eval_point(x_init)\n        remaining = budget - evals_used\n        if remaining <= 0:\n            return best_x\n\n    # Initial step proportional to span, with fallback\n    base_step_scale = 0.2\n    step = np.where(span > 0, base_step_scale * span, 0.1)\n\n    # Adaptive step size: track successes over a small window\n    success_count = 0\n    window = max(5, dim)  # adapt for dimension\n\n    for i in range(remaining):\n        if evals_used >= budget:\n            break\n\n        # Candidate from Gaussian around current best\n        candidate = best_x + np.random.normal(loc=0.0, scale=step)\n        candidate = np.clip(candidate, low, high)\n        old_best_y = best_y\n        eval_point(candidate)\n\n        # Success tracking\n        if best_y < old_best_y:\n            success_count += 1\n\n        # Periodically adjust step sizes:\n        if (i + 1) % window == 0:\n            success_rate = success_count / float(window)\n            # Target success rate ~0.3: adjust multiplicatively\n            if success_rate > 0.4:\n                step *= 1.5\n            elif success_rate < 0.2:\n                step *= 0.5\n            # Keep step size within reasonable limits\n            step = np.clip(step, 1e-8, np.where(span > 0, span, 1.0))\n            success_count = 0\n\n    return best_x",
    "X": "12.000000177248214 11.999999865972057"
}