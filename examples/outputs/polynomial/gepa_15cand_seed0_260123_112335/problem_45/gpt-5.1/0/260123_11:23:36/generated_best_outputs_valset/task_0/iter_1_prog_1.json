{
    "score": 1.7661751202930274,
    "Input": "RosenbrockLog",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Simple derivative-free optimizer: global sampling + local search (coordinate descent).\n    Uses full evaluation budget and supports warm start via prev_best_x.\n    \"\"\"\n    rng = np.random.RandomState()  # local RNG\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n\n    # Ensure dimensions are consistent\n    if bounds.shape[0] != dim:\n        dim = bounds.shape[0]\n\n    def clip_to_bounds(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n\n    # Evaluate a point, keeping track of eval budget\n    def eval_point(x):\n        nonlocal evals_used\n        if evals_used >= budget:\n            # Should not happen if we manage budget correctly, but safety check\n            return np.inf\n        fx = objective_function(x)\n        evals_used += 1\n        return fx\n\n    # Initialization: use prev_best_x if valid, otherwise random uniform\n    best_x = None\n    best_y = np.inf\n\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.size == dim:\n                x0 = clip_to_bounds(x0)\n                y0 = eval_point(x0)\n                if y0 < best_y:\n                    best_x, best_y = x0, y0\n        except Exception:\n            pass\n\n    # If we still don't have a valid best, sample once randomly\n    if best_x is None and evals_used < budget:\n        x = rng.uniform(lower, upper)\n        y = eval_point(x)\n        best_x, best_y = x, y\n\n    # If budget is exhausted, return current best\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n\n    # Global random search phase: use a fraction of remaining evaluations\n    # to explore space broadly.\n    global_frac = 0.6\n    global_evals = max(1, int(remaining * global_frac))\n    local_evals = remaining - global_evals\n\n    # Global sampling around entire box, mildly biased toward current best\n    for _ in range(global_evals):\n        # With some probability, sample near best_x; otherwise pure uniform\n        if rng.rand() < 0.5 and best_x is not None:\n            # Local perturbation around best_x within a shrinking radius\n            radius = span * 0.2\n            cand = best_x + rng.normal(scale=radius)\n            cand = clip_to_bounds(cand)\n        else:\n            cand = rng.uniform(lower, upper)\n        fy = eval_point(cand)\n        if fy < best_y:\n            best_x, best_y = cand, fy\n        if evals_used >= budget:\n            return best_x\n\n    # Local coordinate search around best solution\n    if local_evals <= 0 or best_x is None:\n        return best_x\n\n    # Adaptive step sizes per dimension\n    step = span * 0.1\n    min_step = span * 1e-4 + 1e-12\n\n    # Simple greedy coordinate descent with step-halving\n    while evals_used < budget and np.any(step > min_step):\n        improved = False\n        for i in range(dim):\n            if evals_used >= budget:\n                break\n\n            for direction in (+1.0, -1.0):\n                if evals_used >= budget:\n                    break\n                cand = best_x.copy()\n                cand[i] = cand[i] + direction * step[i]\n                cand = clip_to_bounds(cand)\n                fy = eval_point(cand)\n                if fy < best_y:\n                    best_x, best_y = cand, fy\n                    improved = True\n\n        if not improved:\n            # Reduce step sizes if no improvement in a full sweep\n            step *= 0.5\n        # If improved, keep step size, try again until no further gains or budget runs out\n\n    return best_x",
    "X": "0.9776518725313361 0.9450000000000001 0.95 0.9717642356469363 0.9200000000000002 0.8662323320187595 0.8466941298497528 0.8167375717066095 0.7649999999999999 0.6746828827616286 0.45643881296732436"
}