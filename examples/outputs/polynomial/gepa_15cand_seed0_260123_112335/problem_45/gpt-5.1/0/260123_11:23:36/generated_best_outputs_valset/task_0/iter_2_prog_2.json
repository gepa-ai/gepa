{
    "score": 0.4737257901824784,
    "Input": "RosenbrockLog",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid derivative-free optimizer:\n    - Uses warm start if available\n    - Global search via low-discrepancy (Sobol-like) sampling + local perturbations\n    - Local search via adaptive coordinate descent with anisotropic steps\n    - Uses full evaluation budget and is robust to dim/bounds mismatches\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim_cfg = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    # Ensure dimensional consistency with bounds taking precedence\n    if bounds.ndim != 2 or bounds.shape[1] != 2:\n        raise ValueError(\"bounds must be of shape (dim, 2)\")\n    dim_bounds = bounds.shape[0]\n    if dim_cfg != dim_bounds:\n        dim = dim_bounds\n    else:\n        dim = dim_cfg\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n    # Avoid zero-span dimensions (degenerate bounds)\n    span[span == 0.0] = 1.0\n\n    def clip_to_bounds(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n\n    def eval_point(x):\n        nonlocal evals_used\n        if evals_used >= budget:\n            return np.inf\n        fx = objective_function(x)\n        evals_used += 1\n        return fx\n\n    # --- Initialization with warm start ---\n    best_x = None\n    best_y = np.inf\n\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.size == dim:\n                x0 = clip_to_bounds(x0)\n                y0 = eval_point(x0)\n                if y0 < best_y:\n                    best_x, best_y = x0, y0\n        except Exception:\n            pass\n\n    if evals_used >= budget:\n        # Budget exhausted on warm start evaluation\n        if best_x is None:\n            # In pathological case, fall back to mid-point in bounds\n            return (lower + upper) / 2.0\n        return best_x\n\n    # If we still don't have a best point, start from a random sample\n    if best_x is None:\n        x_init = rng.uniform(lower, upper)\n        y_init = eval_point(x_init)\n        best_x, best_y = x_init, y_init\n\n    if evals_used >= budget:\n        return best_x\n\n    # --- Budget split between global and local search ---\n    remaining = budget - evals_used\n    # Slightly more budget for global exploration; aggressive exploit later\n    global_frac = 0.65\n    global_evals = max(1, int(remaining * global_frac))\n    local_evals = remaining - global_evals\n\n    # --- Global search: quasi-random box sampling + local perturbations ---\n\n    # Pre-generate Sobol-like low-discrepancy sequence using scrambled Halton\n    def halton_sequence(size, dim, base_primes=None):\n        # Simple Halton implementation; good enough for moderate dim\n        if base_primes is None:\n            # First few primes; extend if needed\n            default_primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n                              31, 37, 41, 43, 47, 53, 59, 61, 67, 71]\n            if dim > len(default_primes):\n                # Fallback: approximate with random bases > 1\n                bases = default_primes + [97 + i for i in range(dim - len(default_primes))]\n            else:\n                bases = default_primes[:dim]\n        else:\n            bases = base_primes\n\n        def van_der_corput(n, base):\n            vdc = 0.0\n            denom = 1.0\n            while n:\n                n, rem = divmod(n, base)\n                denom *= base\n                vdc += rem / denom\n            return vdc\n\n        seq = np.empty((size, dim), dtype=float)\n        # Random offset to scramble sequence a bit\n        offset = rng.randint(0, 1000000)\n        for j in range(dim):\n            base = bases[j]\n            for i in range(size):\n                seq[i, j] = van_der_corput(i + 1 + offset, base)\n        return seq\n\n    # Ensure we don't exceed remaining budget during global phase\n    max_global_steps = min(global_evals, max(1, budget - evals_used))\n\n    # Generate a quasi-random batch once, then iterate\n    # To keep code simple, generate max_global_steps points at once\n    halton_pts = halton_sequence(max_global_steps, dim)\n    halton_scaled = lower + halton_pts * (upper - lower)\n\n    # Mixing probabilities\n    p_local_around_best = 0.4  # some chance to sample around current best\n    p_halton = 0.4             # some chance to use Halton box samples\n    # remaining 0.2 -> pure random uniform\n\n    # Local perturbation radius starts large and shrinks over time\n    base_radius = 0.25 * span\n    min_radius = 0.02 * span\n\n    for k in range(max_global_steps):\n        if evals_used >= budget:\n            return best_x\n\n        r_frac = 1.0 - (k / max(1, max_global_steps - 1))  # from 1 -> 0\n        radius = min_radius + r_frac * (base_radius - min_radius)\n\n        u = rng.rand()\n        if u < p_local_around_best and best_x is not None:\n            # Gaussian perturbation around best_x, clipped to bounds\n            cand = best_x + rng.normal(scale=radius)\n            cand = clip_to_bounds(cand)\n        elif u < p_local_around_best + p_halton:\n            cand = halton_scaled[k]\n        else:\n            cand = rng.uniform(lower, upper)\n\n        fy = eval_point(cand)\n        if fy < best_y:\n            best_x, best_y = cand, fy\n\n    if evals_used >= budget or local_evals <= 0 or best_x is None:\n        return best_x\n\n    # --- Local search: adaptive coordinate descent ---\n\n    # Start with anisotropic step sizes informed by global sampling\n    step = 0.15 * span\n    # Per-dimension minimum step; add absolute epsilon for small spans\n    min_step = np.maximum(span * 1e-4, 1e-8)\n\n    # Track failed sweeps to allow early exit if clearly stuck\n    no_improve_sweeps = 0\n    max_no_improve_sweeps = 6\n\n    # Limit number of local iterations by remaining budget and a heuristic cap\n    # Each sweep costs up to 2*dim evaluations\n    max_sweeps_by_budget = max(1, (budget - evals_used) // (2 * dim))\n    max_sweeps = max_sweeps_by_budget\n\n    sweeps_done = 0\n    while (evals_used < budget and\n           np.any(step > min_step) and\n           sweeps_done < max_sweeps and\n           no_improve_sweeps < max_no_improve_sweeps):\n\n        improved = False\n        sweeps_done += 1\n\n        for i in range(dim):\n            if evals_used >= budget:\n                break\n\n            # Try positive and negative directions with greedy acceptance\n            for direction in (+1.0, -1.0):\n                if evals_used >= budget:\n                    break\n                if step[i] <= min_step[i]:\n                    continue\n\n                cand = best_x.copy()\n                cand[i] = cand[i] + direction * step[i]\n                cand = clip_to_bounds(cand)\n                fy = eval_point(cand)\n                if fy < best_y:\n                    best_x, best_y = cand, fy\n                    improved = True\n                    # Greedy: stay with same step in this dimension, but allow\n                    # another try in same direction in a future sweep\n                    break  # break direction loop; move to next dimension\n\n        if not improved:\n            # No improvement in full sweep: shrink steps globally\n            step *= 0.5\n            no_improve_sweeps += 1\n        else:\n            # If improved, reset stagnation counter and slightly increase moves\n            no_improve_sweeps = 0\n            # Mild step adaptation: grow where span is still large\n            step = np.minimum(step * 1.2, 0.3 * span)\n\n    return best_x",
    "X": "0.9992518725313362 0.9896400000000001 0.9862249999999999 0.9717642356469363 0.9630200000000001 0.9602073320187595 0.9216941298497527 0.8767375717066095 0.811605 0.6746828827616286 0.45643881296732436"
}