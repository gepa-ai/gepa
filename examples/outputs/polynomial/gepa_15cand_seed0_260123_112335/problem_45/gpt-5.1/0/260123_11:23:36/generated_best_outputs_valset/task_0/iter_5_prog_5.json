{
    "score": 0.16467593996499377,
    "Input": "RosenbrockLog",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid derivative-free optimizer combining:\n    - Diagonal CMA-ES style global search (robust, exploration-oriented)\n    - Adaptive coordinate / pattern local search (exploitation / refinement)\n    - Optional Nelder\u2013Mead-style simplex refinement for very low dimensions\n\n    Design goals vs previous version:\n    - More robust behavior across budgets and dimensions\n    - Better use of warm-start information (prev_best_x) including safety checks\n    - Slightly stronger global exploration, especially early in the run\n    - Tighter integration between global and local phases\n    - More careful step-size / sigma handling for ill-scaled problems\n    \"\"\"\n\n    # --- RNG / configuration ---\n    seed = int(config.get(\"seed\", 0)) if \"seed\" in config else None\n    rng = np.random.RandomState(seed)\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim_cfg = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    if budget <= 0:\n        # Return mid-point if no budget available\n        return np.mean(bounds, axis=1)\n\n    # --- Dimension / bounds handling ---\n    if bounds.ndim != 2 or bounds.shape[1] != 2:\n        raise ValueError(\"bounds must be of shape (dim, 2)\")\n    dim_bounds = bounds.shape[0]\n    dim = dim_bounds if dim_cfg != dim_bounds else dim_cfg\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n    # avoid degenerate span, but keep notion of scale\n    span_safe = span.copy()\n    span_safe[span_safe == 0.0] = 1.0\n\n    def clip_to_bounds(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n\n    def eval_point(x):\n        nonlocal evals_used\n        if evals_used >= budget:\n            return np.inf\n        x = np.asarray(x, dtype=float).reshape(-1)\n        if x.size != dim:\n            # Defensive: wrong dimension shouldn't crash, just penalize\n            return np.inf\n        fx = objective_function(x)\n        evals_used += 1\n        return fx\n\n    # --- Initialization & warm start handling ---\n    best_x = None\n    best_y = np.inf\n\n    # Safe evaluation of warm start and a small neighborhood cloud\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.size == dim:\n                x0 = clip_to_bounds(x0)\n                y0 = eval_point(x0)\n                if y0 < best_y:\n                    best_x, best_y = x0, y0\n\n                # Explore neighborhood around warm start only if budget allows\n                remaining = budget - evals_used\n                if remaining > 2 * dim:\n                    n_perturb = min(6, remaining // dim)\n                    # Local sigma scaled with span and problem dimension\n                    local_sigma = 0.03 * span_safe / max(1.0, np.sqrt(dim))\n                    for _ in range(n_perturb):\n                        cand = x0 + rng.randn(dim) * local_sigma\n                        cand = clip_to_bounds(cand)\n                        fy = eval_point(cand)\n                        if fy < best_y:\n                            best_x, best_y = cand, fy\n        except Exception:\n            # Ignore any warm-start issues and fall back to default init\n            pass\n\n    if evals_used >= budget:\n        if best_x is None:\n            return (lower + upper) / 2.0\n        return best_x\n\n    # Always ensure at least one evaluated point\n    if best_x is None:\n        # Use mid-point and one random point for more robust start\n        x_mid = (lower + upper) / 2.0\n        y_mid = eval_point(x_mid)\n        best_x, best_y = x_mid, y_mid\n\n        if evals_used < budget:\n            x_rand = rng.uniform(lower, upper)\n            y_rand = eval_point(x_rand)\n            if y_rand < best_y:\n                best_x, best_y = x_rand, y_rand\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n\n    # --- Budget split: global vs local search ---\n    # For small budgets or high dimension: more global search\n    if budget < 20 * dim:\n        global_frac = 0.8\n    else:\n        global_frac = 0.65\n    global_evals = max(1, int(remaining * global_frac))\n    local_evals = max(0, remaining - global_evals)\n\n    # ----------------- GLOBAL SEARCH: diagonal CMA-ES variant -----------------\n    lam = int(4 + np.floor(3 * np.log(dim + 1)))\n    lam = max(4, lam)\n    mu = lam // 2\n\n    # Weights for CMA-ES recombination\n    weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n    weights /= np.sum(weights)\n    mueff = (np.sum(weights) ** 2) / np.sum(weights ** 2)\n\n    # Step-size control parameters (simplified)\n    cs = (mueff + 2) / (dim + mueff + 3)\n    ds = 1 + cs + 2 * max(0.0, np.sqrt((mueff - 1) / (dim + 1)) - 1)\n\n    # Initial mean and sigma\n    x_mean = best_x.copy()\n    sigma = 0.3 * np.mean(span_safe)\n\n    # Evolution path for sigma\n    ps = np.zeros(dim)\n\n    # Diagonal covariance initialization\n    diag_C = (0.4 * span_safe) ** 2\n    min_diag_C = (1e-10 * span_safe) ** 2\n\n    evals_before_global = evals_used\n    max_generations = max(1, global_evals // lam)\n\n    # Early-global random injection probability for exploration\n    rand_inject_prob = 0.15\n\n    for gen in range(max_generations):\n        if evals_used >= budget:\n            break\n        remaining_evals = budget - evals_used\n        if remaining_evals <= 0:\n            break\n\n        cur_lambda = min(lam, remaining_evals)\n        if cur_lambda <= 0:\n            break\n\n        # Sample population from current distribution\n        arz = rng.randn(cur_lambda, dim)\n        ary = np.sqrt(diag_C)[None, :] * arz\n        arx = x_mean[None, :] + sigma * ary\n\n        # Occasional random restart style injection for exploration\n        inject_mask = rng.rand(cur_lambda) < rand_inject_prob\n        if np.any(inject_mask):\n            arx[inject_mask] = rng.uniform(lower, upper, size=(inject_mask.sum(), dim))\n\n        # Repair out-of-bounds by clipping\n        arx = clip_to_bounds(arx)\n\n        fitness = np.empty(cur_lambda, dtype=float)\n        for k in range(cur_lambda):\n            fitness[k] = eval_point(arx[k])\n            if fitness[k] < best_y:\n                best_x, best_y = arx[k].copy(), fitness[k]\n\n        if evals_used >= budget:\n            break\n\n        # Sort by fitness\n        idx = np.argsort(fitness)\n        x_selected = arx[idx[:mu]]\n        old_mean = x_mean.copy()\n        x_mean = np.sum(weights[:, None] * x_selected, axis=0)\n\n        # Update evolution path for sigma (diagonal C)\n        y_mean = (x_mean - old_mean) / max(1e-12, sigma)\n        ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * (\n            y_mean / np.maximum(np.sqrt(diag_C), 1e-12)\n        )\n\n        # Adapt global step size sigma via path length\n        norm_ps = np.linalg.norm(ps)\n        expected_norm = np.sqrt(dim) * (1 - 1.0 / (4 * dim) + 1.0 / (21 * dim ** 2))\n        sigma *= np.exp((cs / ds) * (norm_ps / (expected_norm + 1e-12) - 1.0))\n\n        # Mild adaptation of diagonal covariance from selected steps\n        dy = x_selected - old_mean\n        if dy.shape[0] > 0:\n            cov_update = np.sum(weights[:, None] * (dy ** 2), axis=0)\n            diag_C = 0.85 * diag_C + 0.15 * cov_update\n            diag_C = np.maximum(diag_C, min_diag_C)\n\n        # Guard against stagnation: if step size becomes tiny, break\n        if sigma < 1e-14 * np.mean(span_safe):\n            break\n\n    global_used = evals_used - evals_before_global\n\n    # Ensure local search has a non-trivial budget\n    if global_used < global_evals // 3:\n        remaining = budget - evals_used\n        if remaining <= 0:\n            return best_x\n        local_evals = max(local_evals, remaining // 2)\n\n    if evals_used >= budget or best_x is None:\n        return best_x\n\n    # ----------------- LOCAL SEARCH: coordinate / pattern search -----------------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Recompute local_evals with remaining, ensuring at least a few sweeps\n    min_local_evals = min(4 * dim, remaining)\n    local_evals = max(local_evals, min_local_evals)\n\n    x_curr = best_x.copy()\n    y_curr = best_y\n\n    # Step sizes: refine CMA-ES result; scale with resulting sigma span\n    base_step = 0.08 * span_safe\n    # If CMA-ES ended with large sigma, reflect this in initial local step\n    if np.isfinite(sigma) and sigma > 0:\n        base_step = np.minimum(base_step, 0.5 * sigma * np.ones(dim))\n    step = base_step\n    min_step = np.maximum(span_safe * 1e-4, 1e-8)\n\n    no_improve_sweeps = 0\n    max_no_improve_sweeps = 12\n\n    # Guard on number of sweeps from budget; use more refined computation\n    avg_evals_per_coord = 3  # pos, neg, occasional perturb\n    approx_evals_per_sweep = max(dim * avg_evals_per_coord, 1)\n    max_sweeps_by_budget = max(1, (budget - evals_used) // approx_evals_per_sweep)\n    max_sweeps = max_sweeps_by_budget\n\n    sweeps_done = 0\n    while (\n        evals_used < budget\n        and np.any(step > min_step)\n        and sweeps_done < max_sweeps\n        and no_improve_sweeps < max_no_improve_sweeps\n    ):\n        sweeps_done += 1\n        improved = False\n\n        # Slight random shuffle of coordinate order to avoid systematic bias\n        coord_order = np.arange(dim)\n        rng.shuffle(coord_order)\n\n        for idx_i in coord_order:\n            if evals_used >= budget:\n                break\n            i = int(idx_i)\n            if step[i] <= min_step[i]:\n                continue\n\n            base_val = x_curr[i]\n\n            # Positive direction\n            cand = x_curr.copy()\n            cand[i] = base_val + step[i]\n            cand = clip_to_bounds(cand)\n            fy = eval_point(cand)\n            if fy < y_curr:\n                x_curr, y_curr = cand, fy\n                if y_curr < best_y:\n                    best_x, best_y = x_curr.copy(), y_curr\n                improved = True\n                continue  # move to next coordinate\n\n            if evals_used >= budget:\n                break\n\n            # Negative direction\n            cand = x_curr.copy()\n            cand[i] = base_val - step[i]\n            cand = clip_to_bounds(cand)\n            fy = eval_point(cand)\n            if fy < y_curr:\n                x_curr, y_curr = cand, fy\n                if y_curr < best_y:\n                    best_x, best_y = x_curr.copy(), y_curr\n                improved = True\n\n        # Occasional small random perturbation around current best to escape shallow traps\n        if not improved and evals_used < budget:\n            perturb_scale = 0.015 * span_safe\n            # In very small budgets, skip perturbation to preserve budget\n            if budget - evals_used > dim:\n                cand = x_curr + rng.randn(dim) * perturb_scale\n                cand = clip_to_bounds(cand)\n                fy = eval_point(cand)\n                if fy < y_curr:\n                    x_curr, y_curr = cand, fy\n                    if y_curr < best_y:\n                        best_x, best_y = x_curr.copy(), y_curr\n                    improved = True\n\n        if not improved:\n            step *= 0.5\n            no_improve_sweeps += 1\n        else:\n            no_improve_sweeps = 0\n            # allow mild expansion but keep steps modest\n            step = np.minimum(step * 1.3, 0.25 * span_safe)\n\n    # ----------------- OPTIONAL SIMPLEX REFINEMENT FOR LOW DIMENSION -----------------\n    # For 1D/2D with remaining budget, a tiny Nelder\u2013Mead-like refinement:\n    remaining = budget - evals_used\n    if remaining > dim + 1 and dim <= 2:\n        # Initialize a small simplex around best_x\n        simplex = [best_x.copy()]\n        for j in range(dim):\n            v = best_x.copy()\n            v[j] = clip_to_bounds(v[j] + 0.05 * span_safe[j])\n            simplex.append(v)\n        simplex = np.array(simplex[: dim + 1])\n        fvals = np.array([eval_point(p) for p in simplex])\n        for _ in range(20):\n            if evals_used >= budget:\n                break\n            idx = np.argsort(fvals)\n            simplex = simplex[idx]\n            fvals = fvals[idx]\n            if fvals[0] < best_y:\n                best_x, best_y = simplex[0].copy(), fvals[0]\n            # Centroid of best dim points\n            centroid = np.mean(simplex[:-1], axis=0)\n            worst = simplex[-1]\n            # Reflection\n            xr = centroid + (centroid - worst)\n            xr = clip_to_bounds(xr)\n            fr = eval_point(xr)\n            if fr < fvals[0]:\n                # Expansion\n                xe = centroid + 2 * (xr - centroid)\n                xe = clip_to_bounds(xe)\n                fe = eval_point(xe)\n                if fe < fr:\n                    simplex[-1] = xe\n                    fvals[-1] = fe\n                else:\n                    simplex[-1] = xr\n                    fvals[-1] = fr\n            elif fr < fvals[-2]:\n                simplex[-1] = xr\n                fvals[-1] = fr\n            else:\n                # Contraction\n                xc = centroid + 0.5 * (worst - centroid)\n                xc = clip_to_bounds(xc)\n                fc = eval_point(xc)\n                if fc < fvals[-1]:\n                    simplex[-1] = xc\n                    fvals[-1] = fc\n                else:\n                    # Shrink\n                    for i in range(1, dim + 1):\n                        simplex[i] = simplex[0] + 0.5 * (simplex[i] - simplex[0])\n                        simplex[i] = clip_to_bounds(simplex[i])\n                        fvals[i] = eval_point(simplex[i])\n            if np.min(fvals) < best_y:\n                best_x = simplex[np.argmin(fvals)].copy()\n                best_y = np.min(fvals)\n\n    return best_x",
    "X": "0.9992518725313362 0.9937645924614781 0.9937249999999999 0.9917642356469363 0.98177 0.9719918819086967 0.9534786797396899 0.9045221215965467 0.818105 0.6746828827616286 0.45643881296732436"
}