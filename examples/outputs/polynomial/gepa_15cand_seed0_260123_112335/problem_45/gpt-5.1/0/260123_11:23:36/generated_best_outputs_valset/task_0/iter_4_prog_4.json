{
    "score": 0.24520704814672714,
    "Input": "RosenbrockLog",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid derivative-free optimizer with CMA-ES style global search\n    plus adaptive coordinate/local pattern search refinement.\n\n    Improvements over previous version:\n    - Use RandomState with seed from config if available for reproducibility\n    - More robust dimension handling and warm start usage\n    - Better budget allocation based on dimension and remaining evaluations\n    - Slightly more exploratory global phase and more focused local phase\n    - Small stochastic perturbation around best during local search\n    \"\"\"\n    # --- RNG / configuration ---\n    seed = int(config.get(\"seed\", 0)) if \"seed\" in config else None\n    rng = np.random.RandomState(seed)\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim_cfg = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    # --- Dimension / bounds handling ---\n    if bounds.ndim != 2 or bounds.shape[1] != 2:\n        raise ValueError(\"bounds must be of shape (dim, 2)\")\n    dim_bounds = bounds.shape[0]\n    dim = dim_bounds if dim_cfg != dim_bounds else dim_cfg\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n    # avoid degenerate span\n    span[span == 0.0] = 1.0\n\n    def clip_to_bounds(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n\n    def eval_point(x):\n        nonlocal evals_used\n        if evals_used >= budget:\n            return np.inf\n        fx = objective_function(x)\n        evals_used += 1\n        return fx\n\n    # --- Initialization & warm start ---\n    best_x = None\n    best_y = np.inf\n\n    # If warm start is provided, evaluate it and a small cloud around it\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.size == dim:\n                x0 = clip_to_bounds(x0)\n                y0 = eval_point(x0)\n                if y0 < best_y:\n                    best_x, best_y = x0, y0\n                # explore a few perturbations around warm start (if budget allows)\n                if budget - evals_used > 3 * dim:\n                    n_perturb = min(4, (budget - evals_used) // dim)\n                    local_sigma = 0.05 * span\n                    for _ in range(n_perturb):\n                        cand = x0 + rng.randn(dim) * local_sigma\n                        cand = clip_to_bounds(cand)\n                        fy = eval_point(cand)\n                        if fy < best_y:\n                            best_x, best_y = cand, fy\n        except Exception:\n            pass\n\n    if evals_used >= budget:\n        if best_x is None:\n            return (lower + upper) / 2.0\n        return best_x\n\n    # Always ensure at least one evaluated point\n    if best_x is None:\n        # Use mid-point and one random point for a bit of diversity\n        x_mid = (lower + upper) / 2.0\n        y_mid = eval_point(x_mid)\n        best_x, best_y = x_mid, y_mid\n\n        if evals_used < budget:\n            x_rand = rng.uniform(lower, upper)\n            y_rand = eval_point(x_rand)\n            if y_rand < best_y:\n                best_x, best_y = x_rand, y_rand\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n\n    # --- Budget split: CMA-ES style global search + local search ---\n    # For low budget or high dimensions, favor global search more\n    if budget < 30 * dim:\n        global_frac = 0.8\n    else:\n        global_frac = 0.65\n    global_evals = max(1, int(remaining * global_frac))\n    local_evals = max(0, remaining - global_evals)\n\n    # ----------------- GLOBAL SEARCH: light CMA-ES variant -----------------\n    lam = int(4 + np.floor(3 * np.log(dim + 1)))\n    lam = max(4, lam)\n    mu = lam // 2\n\n    # Weights for CMA-ES recombination\n    weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n    weights /= np.sum(weights)\n    mueff = (np.sum(weights) ** 2) / np.sum(weights ** 2)\n\n    # Step-size control parameters (simplified)\n    cs = (mueff + 2) / (dim + mueff + 3)\n    ds = 1 + cs + 2 * max(0.0, np.sqrt((mueff - 1) / (dim + 1)) - 1)\n\n    # Initial mean and sigma\n    x_mean = best_x.copy()\n    sigma = 0.35 * np.mean(span)\n\n    # Evolution path for sigma\n    ps = np.zeros(dim)\n\n    diag_C = (0.5 * span) ** 2  # variance ~ (0.5*range)^2\n    min_diag_C = (1e-8 * span) ** 2\n\n    evals_before_global = evals_used\n\n    max_generations = max(1, global_evals // lam)\n    for gen in range(max_generations):\n        if evals_used >= budget:\n            break\n        remaining_evals = budget - evals_used\n        if remaining_evals <= 0:\n            break\n\n        cur_lambda = min(lam, remaining_evals)\n        if cur_lambda <= 0:\n            break\n\n        # Sample population\n        arz = rng.randn(cur_lambda, dim)\n        ary = np.sqrt(diag_C)[None, :] * arz\n        arx = x_mean[None, :] + sigma * ary\n\n        # Repair out-of-bounds by clipping\n        arx = clip_to_bounds(arx)\n\n        fitness = np.empty(cur_lambda, dtype=float)\n        for k in range(cur_lambda):\n            fitness[k] = eval_point(arx[k])\n            if fitness[k] < best_y:\n                best_x, best_y = arx[k].copy(), fitness[k]\n\n        if evals_used >= budget:\n            break\n\n        # Sort by fitness\n        idx = np.argsort(fitness)\n        x_selected = arx[idx[:mu]]\n        old_mean = x_mean.copy()\n        x_mean = np.sum(weights[:, None] * x_selected, axis=0)\n\n        # Update evolution path for sigma (CMA-ES style but diagonal C)\n        y_mean = (x_mean - old_mean) / max(1e-12, sigma)\n        ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * (\n            y_mean / np.maximum(np.sqrt(diag_C), 1e-12)\n        )\n\n        # Adapt global step size sigma via path length\n        norm_ps = np.linalg.norm(ps)\n        expected_norm = np.sqrt(dim) * (1 - 1.0 / (4 * dim) + 1.0 / (21 * dim ** 2))\n        sigma *= np.exp((cs / ds) * (norm_ps / (expected_norm + 1e-12) - 1.0))\n\n        # Mild adaptation of diagonal covariance from selected steps\n        dy = x_selected - old_mean\n        if dy.shape[0] > 0:\n            cov_update = np.sum(weights[:, None] * (dy ** 2), axis=0)\n            diag_C = 0.9 * diag_C + 0.1 * cov_update\n            diag_C = np.maximum(diag_C, min_diag_C)\n\n        # Avoid wasting evaluations once sigma is extremely small\n        if sigma < 1e-13 * np.mean(span):\n            break\n\n    global_used = evals_used - evals_before_global\n\n    # Ensure local search has a non-trivial budget\n    if global_used < global_evals // 3:\n        remaining = budget - evals_used\n        if remaining <= 0:\n            return best_x\n        local_evals = max(local_evals, remaining // 2)\n\n    if evals_used >= budget or best_x is None:\n        return best_x\n\n    # ----------------- LOCAL SEARCH: adaptive coordinate / pattern search -----------------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Recompute local_evals with remaining, ensuring at least dim * 2 attempts if possible\n    min_local_evals = min(4 * dim, remaining)\n    local_evals = max(local_evals, min_local_evals)\n\n    x_curr = best_x.copy()\n    y_curr = best_y\n\n    # Step sizes: start modest to refine CMA-ES result\n    step = 0.08 * span\n    min_step = np.maximum(span * 1e-4, 1e-8)\n\n    no_improve_sweeps = 0\n    max_no_improve_sweeps = 10\n\n    max_sweeps_by_budget = max(1, min(local_evals, budget - evals_used) // (3 * dim))\n    max_sweeps = max_sweeps_by_budget\n\n    sweeps_done = 0\n    while (\n        evals_used < budget\n        and np.any(step > min_step)\n        and sweeps_done < max_sweeps\n        and no_improve_sweeps < max_no_improve_sweeps\n    ):\n        sweeps_done += 1\n        improved = False\n\n        # Slight random shuffle of coordinate order to avoid systematic bias\n        coord_order = np.arange(dim)\n        rng.shuffle(coord_order)\n\n        for idx_i in coord_order:\n            if evals_used >= budget:\n                break\n            i = int(idx_i)\n            if step[i] <= min_step[i]:\n                continue\n\n            base_val = x_curr[i]\n\n            # Positive direction\n            cand = x_curr.copy()\n            cand[i] = base_val + step[i]\n            cand = clip_to_bounds(cand)\n            fy = eval_point(cand)\n            if fy < y_curr:\n                x_curr, y_curr = cand, fy\n                if y_curr < best_y:\n                    best_x, best_y = x_curr.copy(), y_curr\n                improved = True\n                continue  # move to next coordinate\n\n            if evals_used >= budget:\n                break\n\n            # Negative direction\n            cand = x_curr.copy()\n            cand[i] = base_val - step[i]\n            cand = clip_to_bounds(cand)\n            fy = eval_point(cand)\n            if fy < y_curr:\n                x_curr, y_curr = cand, fy\n                if y_curr < best_y:\n                    best_x, best_y = x_curr.copy(), y_curr\n                improved = True\n\n        # Occasional small random perturbation around current best to escape shallow traps\n        if not improved and evals_used < budget:\n            perturb_scale = 0.02 * span\n            cand = x_curr + rng.randn(dim) * perturb_scale\n            cand = clip_to_bounds(cand)\n            fy = eval_point(cand)\n            if fy < y_curr:\n                x_curr, y_curr = cand, fy\n                if y_curr < best_y:\n                    best_x, best_y = x_curr.copy(), y_curr\n                improved = True\n\n        if not improved:\n            step *= 0.5\n            no_improve_sweeps += 1\n        else:\n            no_improve_sweeps = 0\n            # allow mild expansion but keep steps modest\n            step = np.minimum(step * 1.25, 0.2 * span)\n\n    return best_x",
    "X": "0.9992518725313362 0.9896400000000001 0.9937249999999999 0.9917642356469363 0.98177 0.9602073320187595 0.9416941298497528 0.8927375717066095 0.818105 0.6746828827616286 0.45643881296732436"
}