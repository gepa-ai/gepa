{
    "score": 0.4521949291800401,
    "Input": "RosenbrockLog",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid derivative-free optimizer with CMA-ES style global search\n    plus adaptive coordinate descent local refinement.\n\n    - Uses warm start if available\n    - Respects bounds robustly\n    - Uses full evaluation budget whenever possible\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim_cfg = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    # --- Dimension / bounds handling ---\n    if bounds.ndim != 2 or bounds.shape[1] != 2:\n        raise ValueError(\"bounds must be of shape (dim, 2)\")\n    dim_bounds = bounds.shape[0]\n    dim = dim_bounds if dim_cfg != dim_bounds else dim_cfg\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n    # avoid degenerate span\n    span[span == 0.0] = 1.0\n\n    def clip_to_bounds(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n\n    def eval_point(x):\n        nonlocal evals_used\n        if evals_used >= budget:\n            return np.inf\n        fx = objective_function(x)\n        evals_used += 1\n        return fx\n\n    # --- Initialization & warm start ---\n    best_x = None\n    best_y = np.inf\n\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.size == dim:\n                x0 = clip_to_bounds(x0)\n                y0 = eval_point(x0)\n                if y0 < best_y:\n                    best_x, best_y = x0, y0\n        except Exception:\n            pass\n\n    if evals_used >= budget:\n        if best_x is None:\n            return (lower + upper) / 2.0\n        return best_x\n\n    if best_x is None:\n        x_init = rng.uniform(lower, upper)\n        y_init = eval_point(x_init)\n        best_x, best_y = x_init, y_init\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n\n    # --- Budget split: CMA-ES style global search + local search ---\n    global_frac = 0.7\n    global_evals = max(1, int(remaining * global_frac))\n    local_evals = remaining - global_evals\n\n    # ----------------- GLOBAL SEARCH: light CMA-ES variant -----------------\n    # Adapted for bounded domains via resampling/repair\n    # Heuristic population size\n    lam = int(4 + np.floor(3 * np.log(dim + 1)))\n    lam = max(4, lam)\n    mu = lam // 2\n\n    # Weights for CMA-ES recombination\n    weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n    weights /= np.sum(weights)\n    mueff = (np.sum(weights) ** 2) / np.sum(weights ** 2)\n\n    # Step-size control parameters (simplified)\n    cs = (mueff + 2) / (dim + mueff + 3)\n    ds = 1 + cs + 2 * max(0, np.sqrt((mueff - 1) / (dim + 1)) - 1)\n\n    # Initial mean: use current best or mid-point if best is at boundary\n    x_mean = best_x.copy()\n    # Initial global step (sigma) proportional to domain span\n    sigma = 0.3 * np.mean(span)\n\n    # Evolution path for sigma\n    ps = np.zeros(dim)\n\n    # We'll limit number of CMA-ES generations by global_evals\n    # One generation ~ lam evaluations\n    max_generations = max(1, global_evals // lam)\n\n    # Simple diagonal covariance (per-dimension sigma scaling)\n    diag_C = (0.5 * span) ** 2  # variance ~ (0.5*range)^2\n    min_diag_C = (1e-6 * span) ** 2\n\n    evals_before_global = evals_used\n\n    for gen in range(max_generations):\n        if evals_used >= budget:\n            break\n        # Adjust effective lambda if budget is nearly exhausted\n        remaining_evals = budget - evals_used\n        if remaining_evals <= 0:\n            break\n        cur_lambda = min(lam, remaining_evals)\n\n        # Sample population\n        arz = rng.randn(cur_lambda, dim)\n        ary = np.sqrt(diag_C)[None, :] * arz\n        arx = x_mean[None, :] + sigma * ary\n\n        # Repair out-of-bounds by clipping and small Gaussian nudge\n        arx = clip_to_bounds(arx)\n        # Evaluate\n        fitness = np.empty(cur_lambda, dtype=float)\n        for k in range(cur_lambda):\n            fitness[k] = eval_point(arx[k])\n            if fitness[k] < best_y:\n                best_x, best_y = arx[k].copy(), fitness[k]\n                x_mean = best_x.copy()  # re-center on global best\n\n        # If budget gone, exit\n        if evals_used >= budget:\n            break\n\n        # Sort by fitness, compute new mean\n        idx = np.argsort(fitness)\n        x_selected = arx[idx[:mu]]\n        # Recombination\n        old_mean = x_mean.copy()\n        x_mean = np.sum(weights[:, None] * x_selected, axis=0)\n\n        # Update evolution path for sigma (CMA-ES style but diagonal C)\n        y_mean = (x_mean - old_mean) / max(1e-12, sigma)\n        ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * (\n            y_mean / np.maximum(np.sqrt(diag_C), 1e-12)\n        )\n\n        # Adapt global step size sigma via path length (no covariance matrix)\n        norm_ps = np.linalg.norm(ps)\n        expected_norm = np.sqrt(dim) * (1 - 1 / (4 * dim) + 1 / (21 * dim ** 2))\n        sigma *= np.exp((cs / ds) * (norm_ps / expected_norm - 1))\n\n        # Mild adaptation of diagonal covariance from selected steps\n        dy = x_selected - old_mean\n        if dy.shape[0] > 0:\n            # Variance estimate along coordinates\n            cov_update = np.sum(weights[:, None] * (dy ** 2), axis=0)\n            diag_C = 0.9 * diag_C + 0.1 * cov_update\n            diag_C = np.maximum(diag_C, min_diag_C)\n\n        # Early break if sigma becomes extremely small -> switch to local\n        if sigma < 1e-12 * np.mean(span):\n            break\n\n    # If global stage used almost no budget, do not starve local search\n    global_used = evals_used - evals_before_global\n    if global_used < global_evals // 3:\n        # recompute remaining and assign at least some for local\n        remaining = budget - evals_used\n        if remaining <= 0:\n            return best_x\n        # use half of remaining for local to still exploit\n        local_evals = max(local_evals, remaining // 2)\n\n    if evals_used >= budget or best_x is None:\n        return best_x\n\n    # ----------------- LOCAL SEARCH: adaptive coordinate descent -----------------\n    remaining = budget - evals_used\n    if remaining <= 0 or local_evals <= 0:\n        return best_x\n\n    # Start near best solution\n    x_curr = best_x.copy()\n    y_curr = best_y\n\n    # Step sizes: start relatively small to refine CMA-ES result\n    step = 0.1 * span\n    min_step = np.maximum(span * 1e-4, 1e-8)\n\n    no_improve_sweeps = 0\n    max_no_improve_sweeps = 8\n\n    max_sweeps_by_budget = max(1, min(local_evals, budget - evals_used) // (2 * dim))\n    max_sweeps = max_sweeps_by_budget\n\n    sweeps_done = 0\n    while (\n        evals_used < budget\n        and np.any(step > min_step)\n        and sweeps_done < max_sweeps\n        and no_improve_sweeps < max_no_improve_sweeps\n    ):\n        sweeps_done += 1\n        improved = False\n\n        for i in range(dim):\n            if evals_used >= budget:\n                break\n            if step[i] <= min_step[i]:\n                continue\n\n            base_val = x_curr[i]\n\n            # positive direction\n            cand = x_curr.copy()\n            cand[i] = base_val + step[i]\n            cand = clip_to_bounds(cand)\n            fy = eval_point(cand)\n            if fy < y_curr:\n                x_curr, y_curr = cand, fy\n                if y_curr < best_y:\n                    best_x, best_y = x_curr.copy(), y_curr\n                improved = True\n                continue  # move to next coordinate\n\n            if evals_used >= budget:\n                break\n\n            # negative direction\n            cand = x_curr.copy()\n            cand[i] = base_val - step[i]\n            cand = clip_to_bounds(cand)\n            fy = eval_point(cand)\n            if fy < y_curr:\n                x_curr, y_curr = cand, fy\n                if y_curr < best_y:\n                    best_x, best_y = x_curr.copy(), y_curr\n                improved = True\n\n        if not improved:\n            step *= 0.5\n            no_improve_sweeps += 1\n        else:\n            no_improve_sweeps = 0\n            # allow mild expansion but keep steps modest\n            step = np.minimum(step * 1.3, 0.25 * span)\n\n    return best_x",
    "X": "0.9992518725313362 0.9896400000000001 0.9862249999999999 0.9717642356469363 0.9682700000000001 0.9602073320187595 0.9216941298497527 0.8767375717066095 0.811605 0.6746828827616286 0.45643881296732436"
}