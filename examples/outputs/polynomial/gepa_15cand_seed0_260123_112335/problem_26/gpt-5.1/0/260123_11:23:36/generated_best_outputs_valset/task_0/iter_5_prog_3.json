{
    "score": -0.8422170096529369,
    "Input": "McCourt16",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Derivative-free optimizer combining:\n      - Sobol-like global sampling (via scrambled Halton)\n      - Adaptive CMA-ES style local search around the best point\n      - Robust fallback local search for low-/medium-dimensional problems\n    Uses the full evaluation budget and warm-starts from prev_best_x when provided.\n    \"\"\"\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", len(bounds)))\n    budget = int(config.get(\"budget\", 100))\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Helper: clip to bounds\n    def clip(x):\n        return np.clip(x, low, high)\n\n    # Early exit when no evaluations are allowed\n    if budget <= 0:\n        if prev_best_x is not None:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.shape[0] == dim:\n                return clip(x0)\n        return (low + high) / 2.0\n\n    rng = np.random.default_rng()\n\n    # -------------------------\n    # Quasi-random sequence (scrambled Halton-like)\n    # -------------------------\n    # Precompute first `dim` primes for bases\n    def _first_primes(n):\n        if n <= 0:\n            return []\n        primes = [2]\n        candidate = 3\n        while len(primes) < n:\n            is_p = True\n            limit = int(candidate**0.5) + 1\n            for p in primes:\n                if p > limit:\n                    break\n                if candidate % p == 0:\n                    is_p = False\n                    break\n            if is_p:\n                primes.append(candidate)\n            candidate += 2\n        return primes\n\n    bases = _first_primes(dim)\n\n    def halton_point(index, scramble=True):\n        # index >= 1\n        if dim == 0:\n            return np.array([], dtype=float)\n        x = np.empty(dim, dtype=float)\n        for j, base in enumerate(bases):\n            f = 1.0\n            r = 0.0\n            i = index\n            # Bound loop length defensively\n            while i > 0 and f > 1e-16:\n                f /= base\n                digit = i % base\n                if scramble:\n                    # Simple digit scramble with rng\n                    digit = (digit + int(rng.integers(0, base))) % base\n                r += f * digit\n                i //= base\n            x[j] = r\n        return x\n\n    def sample_halton_scaled(idx):\n        # idx starts from 1\n        if span.size == 0:\n            return np.array([], dtype=float)\n        u = halton_point(idx)\n        return low + u * span\n\n    # -------------------------\n    # Evaluation helpers\n    # -------------------------\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        # Ensure shape and bounds\n        x = np.asarray(x, dtype=float).reshape(-1)\n        if x.shape[0] != dim:\n            # Fallback to center if something goes wrong\n            x = (low + high) / 2.0\n        x = clip(x)\n        y = objective_function(x)\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_x, best_y = x, y\n        return y\n\n    # -------------------------\n    # Initialization / warm start\n    # -------------------------\n    # Start from prev_best_x if valid\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.shape[0] == dim:\n            x0 = clip(x0)\n            if evals_used < budget:\n                eval_point(x0)\n\n    # If we still have no evaluated best_x, use center then one random point\n    if best_x is None and evals_used < budget:\n        center = (low + high) / 2.0\n        eval_point(center)\n\n    if best_x is None and evals_used < budget:\n        x_rand = rng.uniform(low, high)\n        eval_point(x_rand)\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # -------------------------\n    # Budget split: more global for low budgets, more local for high\n    # -------------------------\n    if budget < 30:\n        global_frac = 0.7\n    elif budget < 80:\n        global_frac = 0.55\n    else:\n        global_frac = 0.4\n\n    n_global = max(1, int(global_frac * remaining))\n    n_local = max(0, remaining - n_global)\n\n    # -------------------------\n    # Global search via scrambled Halton sequence + some Gaussian around best\n    # -------------------------\n    halton_index = 1\n    for i in range(n_global):\n        if evals_used >= budget:\n            return best_x\n\n        if best_x is not None and rng.random() < 0.35:\n            # Biased Gaussian around best with mild decay\n            rel = (i + 1) / max(1, n_global)\n            scale = 0.3 * (0.4 + 0.6 * (1.0 - rel))  # ~0.3 -> ~0.12\n            step = rng.normal(0.0, 1.0, size=dim) * (scale * (span if span.size else 1.0))\n            cand = clip(best_x + step)\n        else:\n            cand = clip(sample_halton_scaled(halton_index))\n            halton_index += 1\n\n        eval_point(cand)\n\n    if evals_used >= budget or n_local <= 0:\n        return best_x\n\n    # -------------------------\n    # Local search: simplified CMA-ES style adaptation\n    # -------------------------\n    if dim == 0:\n        # Degenerate case: nothing to optimize\n        return best_x\n\n    # Strategy parameters\n    mu = min(4, max(2, max(1, dim // 2)))     # parent size\n    lam = min(24, max(6, 4 * dim))            # offspring per generation\n    # Scale lambda to available budget\n    lam = max(4, min(lam, n_local))\n\n    # Initial mean and step-size\n    mean = best_x.copy()\n    avg_span = float(np.mean(span + 1e-8))\n    sigma = 0.3 * avg_span\n\n    # Diagonal covariance, robust initialization\n    diag_C = (0.2 * (span + 1e-8)) ** 2\n    diag_C[diag_C <= 0.0] = (0.1 * avg_span) ** 2\n\n    # Learning rates (moderate adaptation)\n    c_c = 0.4\n    c_sigma = 0.3\n    d_sigma = 1.0 + 0.3 * max(0, dim - 2) / 10.0\n    c_cov = 0.2\n    alpha_cov = 1.5\n\n    p_c = np.zeros(dim)\n    p_sigma = np.zeros(dim)\n\n    remaining_local = budget - evals_used\n    if remaining_local <= 0:\n        return best_x\n\n    # Precompute weights\n    weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n    weights = weights / np.sum(weights)\n    mu_eff = 1.0 / np.sum(weights ** 2)\n\n    # Track stagnation\n    no_improve_generations = 0\n    last_best_y = best_y\n\n    # Simple coordinate local search fallback (for small dim)\n    def coordinate_local_search(x_start, eval_budget):\n        x_best = x_start.copy()\n        y_best = best_y\n        # Step sizes per dimension\n        step = 0.1 * (span + 1e-8)\n        remaining_eval = eval_budget\n        while remaining_eval > 0:\n            improved = False\n            for d in range(dim):\n                if remaining_eval <= 0:\n                    break\n                # Try plus step\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] + step[d], low[d], high[d])\n                if evals_used >= budget:\n                    return\n                y_try = eval_point(x_try)\n                remaining_eval = budget - evals_used\n                if y_try < y_best:\n                    x_best, y_best = x_try, y_try\n                    improved = True\n                    continue\n\n                if remaining_eval <= 0:\n                    break\n                # Try minus step\n                x_try = x_best.copy()\n                x_try[d] = np.clip(x_try[d] - step[d], low[d], high[d])\n                if evals_used >= budget:\n                    return\n                y_try = eval_point(x_try)\n                remaining_eval = budget - evals_used\n                if y_try < y_best:\n                    x_best, y_best = x_try, y_try\n                    improved = True\n\n            if not improved:\n                # Reduce step sizes\n                step *= 0.5\n                if np.all(step < 1e-6 * (span + 1e-8)):\n                    break\n\n    # Main CMA-ES-style loop\n    while remaining_local > 0:\n        lam_gen = min(lam, remaining_local)\n        if lam_gen <= 0:\n            break\n\n        # Sample offspring\n        z_list = rng.normal(0.0, 1.0, size=(lam_gen, dim))\n        steps = z_list * np.sqrt(diag_C)\n        offspring = mean + sigma * steps\n        offspring = clip(offspring)\n\n        ys = []\n        for k in range(lam_gen):\n            if evals_used >= budget:\n                return best_x\n            yk = eval_point(offspring[k])\n            ys.append(yk)\n\n        remaining_local = budget - evals_used\n        if remaining_local <= 0:\n            break\n\n        ys = np.asarray(ys)\n        # Select best mu\n        order = np.argsort(ys)\n        idx = order[:mu]\n        selected = offspring[idx]\n        selected_z = z_list[idx]\n\n        new_mean = np.sum(selected * weights[:, None], axis=0)\n        y_best_gen = ys[order[0]]\n\n        # Track stagnation\n        if best_y is not None and (last_best_y is None or best_y < last_best_y - 1e-12):\n            no_improve_generations = 0\n            last_best_y = best_y\n        else:\n            no_improve_generations += 1\n\n        # Update evolution paths\n        y_step = (new_mean - mean) / max(1e-12, sigma)\n        C_inv_sqrt = 1.0 / np.sqrt(diag_C)\n        p_sigma = (1 - c_sigma) * p_sigma + np.sqrt(c_sigma * (2 - c_sigma) * mu_eff) * (y_step * C_inv_sqrt)\n        p_c = (1 - c_c) * p_c + np.sqrt(c_c * (2 - c_c) * mu_eff) * y_step\n\n        # Adapt covariance (diagonal)\n        delta_C = np.zeros(dim)\n        for w, z in zip(weights, selected_z):\n            delta_C += w * (z ** 2 - 1.0)\n        diag_C = (1 - c_cov) * diag_C + c_cov * diag_C * (1 + alpha_cov * delta_C)\n\n        # Adapt sigma\n        norm_p_sigma = np.linalg.norm(p_sigma)\n        sigma *= np.exp((c_sigma / d_sigma) * (norm_p_sigma / np.sqrt(dim) - 1.0))\n\n        # Clamp sigma and covariance to avoid degeneracy\n        span_safe = span + 1e-8\n        avg_span = float(np.mean(span_safe))\n        sigma = float(np.clip(sigma, 1e-8 * avg_span, 2.0 * avg_span))\n        diag_C = np.clip(diag_C, (1e-10 * span_safe) ** 2, (2.0 * span_safe) ** 2)\n\n        mean = new_mean\n\n        # Recentre to global best if local mean drifts worse\n        if best_y is not None and y_best_gen > best_y and rng.random() < 0.2:\n            mean = best_x.copy()\n\n        # If CMA stagnates, switch remaining budget to simple coordinate local search\n        if no_improve_generations >= 4 and dim <= 20 and remaining_local > 0:\n            coordinate_local_search(best_x.copy(), remaining_local)\n            break\n\n        remaining_local = budget - evals_used\n        if remaining_local <= 0:\n            break\n\n    return best_x",
    "X": "0.18580355343410368 0.6858033507494214 0.1858032095363949 0.4858040729592558"
}