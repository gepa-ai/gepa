{
    "score": -0.8421360127990489,
    "Input": "McCourt16",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Simple derivative-free optimizer that combines global random search\n    with local hill-climbing around the best solutions found so far.\n    \"\"\"\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", len(bounds)))\n    budget = int(config.get(\"budget\", 100))\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Helper: clip to bounds\n    def clip(x):\n        return np.clip(x, low, high)\n\n    # Helper: evaluate a point safely\n    def eval_point(x):\n        return objective_function(x)\n\n    # Determine how many evaluations to allocate\n    if budget <= 0:\n        # No evaluations allowed: just return prev_best_x or center of bounds\n        if prev_best_x is not None:\n            return clip(np.asarray(prev_best_x, dtype=float))\n        return (low + high) / 2.0\n\n    rng = np.random.default_rng()\n\n    evals_used = 0\n\n    # Initialize from prev_best_x if available and within bounds\n    best_x = None\n    best_y = None\n\n    if prev_best_x is not None:\n        x0 = clip(np.asarray(prev_best_x, dtype=float).reshape(-1))\n        if x0.shape[0] == dim:\n            y0 = eval_point(x0)\n            evals_used += 1\n            best_x, best_y = x0, y0\n\n    # If we still don't have an initial point, sample one at random\n    if best_x is None:\n        x0 = rng.uniform(low, high)\n        y0 = eval_point(x0)\n        evals_used += 1\n        best_x, best_y = x0, y0\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Allocate evaluations between global and local search\n    # Use more global search for small budgets\n    global_frac = 0.6 if budget < 40 else 0.4\n    n_global = max(1, int(global_frac * remaining))\n    n_local = max(0, remaining - n_global)\n\n    # --- Global random search ---\n    # Sample candidates with a mild bias around current best\n    # Mix: 70% pure random, 30% around best_x (if available)\n    for i in range(n_global):\n        if best_x is not None and rng.random() < 0.3:\n            # Sample around best_x with decaying radius\n            scale = 0.25 * (0.5 ** (i / max(1, n_global - 1)))\n            step = rng.normal(0.0, 1.0, size=dim) * (scale * span)\n            cand = clip(best_x + step)\n        else:\n            cand = rng.uniform(low, high)\n\n        y = eval_point(cand)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = cand, y\n\n        if evals_used >= budget:\n            return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Local search around best_x ---\n    # Simple stochastic hill-climbing with exponentially shrinking steps\n    # Use coordinate-wise perturbations for robustness in higher dimensions.\n    base_step = 0.2  # fraction of span\n    for k in range(remaining):\n        # Decay step size over iterations\n        frac = (k + 1) / remaining\n        step_scale = base_step * (1.0 - 0.8 * frac)  # from 0.2 to ~0.04\n\n        # Randomly choose perturbation type\n        if rng.random() < 0.5:\n            # Coordinate perturbation\n            cand = best_x.copy()\n            idx = rng.integers(0, dim)\n            step = rng.normal(0.0, 1.0) * (step_scale * span[idx])\n            cand[idx] = cand[idx] + step\n            cand = clip(cand)\n        else:\n            # Full-dimensional perturbation\n            step = rng.normal(0.0, 1.0, size=dim) * (step_scale * span)\n            cand = clip(best_x + step)\n\n        y = eval_point(cand)\n        evals_used += 1\n        if y < best_y:\n            best_x, best_y = cand, y\n\n        if evals_used >= budget:\n            break\n\n    return best_x",
    "X": "0.18833345031568458 0.6834203026838148 0.1850702555669866 0.4883889638801042"
}