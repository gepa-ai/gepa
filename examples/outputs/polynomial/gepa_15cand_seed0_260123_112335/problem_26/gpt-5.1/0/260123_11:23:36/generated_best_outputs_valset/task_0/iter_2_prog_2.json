{
    "score": -0.8422169105554347,
    "Input": "McCourt16",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Derivative-free optimizer combining:\n      - Sobol-like global sampling (via scrambled Halton)\n      - Adaptive CMA-ES style local search around the best point\n    Uses the full evaluation budget and warm-starts from prev_best_x when provided.\n    \"\"\"\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", len(bounds)))\n    budget = int(config.get(\"budget\", 100))\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Helper: clip to bounds\n    def clip(x):\n        return np.clip(x, low, high)\n\n    # Early exit when no evaluations are allowed\n    if budget <= 0:\n        if prev_best_x is not None:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if x0.shape[0] == dim:\n                return clip(x0)\n        return (low + high) / 2.0\n\n    rng = np.random.default_rng()\n\n    # -------------------------\n    # Quasi-random sequence (scrambled Halton-like)\n    # -------------------------\n    # Precompute first `dim` primes for bases\n    def _first_primes(n):\n        primes = []\n        candidate = 2\n        while len(primes) < n:\n            is_p = True\n            for p in primes:\n                if p * p > candidate:\n                    break\n                if candidate % p == 0:\n                    is_p = False\n                    break\n            if is_p:\n                primes.append(candidate)\n            candidate += 1\n        return primes\n\n    bases = _first_primes(dim)\n\n    def halton_point(index, scramble=True):\n        # index >= 1\n        x = np.empty(dim, dtype=float)\n        for j, base in enumerate(bases):\n            f = 1.0\n            r = 0.0\n            i = index\n            while i > 0:\n                f /= base\n                digit = i % base\n                if scramble:\n                    # Simple digit scramble with rng\n                    digit = (digit + rng.integers(0, base)) % base\n                r += f * digit\n                i //= base\n            x[j] = r\n        return x\n\n    def sample_halton_scaled(idx):\n        # idx starts from 1\n        u = halton_point(idx)\n        return low + u * span\n\n    # -------------------------\n    # Evaluation helpers\n    # -------------------------\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        y = objective_function(x)\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_x, best_y = x, y\n        return y\n\n    # -------------------------\n    # Initialization / warm start\n    # -------------------------\n    # Start from prev_best_x if valid\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.shape[0] == dim:\n            x0 = clip(x0)\n            if evals_used < budget:\n                eval_point(x0)\n\n    # If we still have no evaluated best_x, use center then one random point\n    if best_x is None and evals_used < budget:\n        center = (low + high) / 2.0\n        eval_point(center)\n\n    if best_x is None and evals_used < budget:\n        x_rand = rng.uniform(low, high)\n        eval_point(x_rand)\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # -------------------------\n    # Budget split: more global for low budgets, more local for high\n    # -------------------------\n    if budget < 30:\n        global_frac = 0.7\n    elif budget < 80:\n        global_frac = 0.5\n    else:\n        global_frac = 0.35\n\n    n_global = max(1, int(global_frac * remaining))\n    n_local = max(0, remaining - n_global)\n\n    # -------------------------\n    # Global search via scrambled Halton sequence + some Gaussian around best\n    # -------------------------\n    halton_index = 1\n    for i in range(n_global):\n        if best_x is not None and rng.random() < 0.3:\n            # Biased Gaussian around best with mild decay\n            rel = (i + 1) / max(1, n_global)\n            scale = 0.35 * (0.4 + 0.6 * (1.0 - rel))  # ~0.35 -> ~0.14\n            step = rng.normal(0.0, 1.0, size=dim) * (scale * span)\n            cand = clip(best_x + step)\n        else:\n            cand = clip(sample_halton_scaled(halton_index))\n            halton_index += 1\n\n        if evals_used >= budget:\n            return best_x\n        eval_point(cand)\n\n    if evals_used >= budget or n_local <= 0:\n        return best_x\n\n    # -------------------------\n    # Local search: simplified CMA-ES style adaptation\n    # -------------------------\n    # Strategy parameters\n    mu = min(4, max(2, dim // 2))          # parent size\n    lam = min(20, max(4, 4 * dim))         # offspring per generation\n    # Scale lambda to available budget\n    est_generations = max(1, n_local // lam)\n    lam = max(4, min(lam, n_local))        # may shrink if budget small\n\n    # Initial mean and step-size\n    mean = best_x.copy()\n    sigma = 0.25 * np.mean(span) if dim > 0 else 0.25\n\n    # Diagonal covariance\n    diag_C = (0.25 * span) ** 2\n    diag_C[diag_C == 0.0] = 1.0\n\n    # Learning rates (simple pre-set)\n    c_c = 0.5\n    c_sigma = 0.3\n    d_sigma = 1.0 + 0.5 * max(0, dim - 2) / 10.0\n    c_cov = 0.2\n\n    p_c = np.zeros(dim)\n    p_sigma = np.zeros(dim)\n\n    remaining_local = budget - evals_used\n    if remaining_local <= 0:\n        return best_x\n\n    # Precompute weights\n    weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n    weights = weights / np.sum(weights)\n    mu_eff = 1.0 / np.sum(weights ** 2)\n\n    while remaining_local > 0:\n        # Number of offspring this generation (respect budget)\n        lam_gen = min(lam, remaining_local)\n\n        # Sample offspring\n        z_list = rng.normal(0.0, 1.0, size=(lam_gen, dim))\n        steps = z_list * np.sqrt(diag_C)\n        offspring = clip(mean + sigma * steps)\n\n        ys = []\n        for k in range(lam_gen):\n            if evals_used >= budget:\n                return best_x\n            yk = eval_point(offspring[k])\n            ys.append(yk)\n\n        remaining_local = budget - evals_used\n        if remaining_local <= 0:\n            break\n\n        # Select best mu\n        idx = np.argsort(ys)[:mu]\n        selected = offspring[idx]\n        selected_z = z_list[idx]\n\n        new_mean = np.sum(selected * weights[:, None], axis=0)\n        y_best_gen = ys[idx[0]]\n\n        # Update evolution paths\n        y_step = (new_mean - mean) / max(1e-12, sigma)\n        C_inv_sqrt = 1.0 / np.sqrt(diag_C)\n        p_sigma = (1 - c_sigma) * p_sigma + np.sqrt(c_sigma * (2 - c_sigma) * mu_eff) * (y_step * C_inv_sqrt)\n        p_c = (1 - c_c) * p_c + np.sqrt(c_c * (2 - c_c) * mu_eff) * y_step\n\n        # Adapt covariance (diagonal)\n        delta_C = np.zeros(dim)\n        for w, z in zip(weights, selected_z):\n            delta_C += w * (z ** 2 - 1.0)\n        diag_C = (1 - c_cov) * diag_C + c_cov * diag_C * (1 + delta_C)\n\n        # Adapt sigma\n        norm_p_sigma = np.linalg.norm(p_sigma)\n        sigma *= np.exp((c_sigma / d_sigma) * (norm_p_sigma / np.sqrt(dim) - 1.0))\n\n        # Clamp sigma and covariance to avoid degeneracy\n        sigma = float(np.clip(sigma, 1e-8 * np.mean(span + 1e-8), 2.0 * np.mean(span + 1e-8)))\n        diag_C = np.clip(diag_C, 1e-12, (2.0 * span + 1e-8) ** 2)\n\n        mean = new_mean\n\n        # Occasional recentring to global best if local mean drifts worse\n        if best_y is not None and y_best_gen > best_y and rng.random() < 0.1:\n            mean = best_x.copy()\n\n        if remaining_local <= 0:\n            break\n\n    return best_x",
    "X": "0.18589358029047273 0.685789617839128 0.18574522613737757 0.48590935860483997"
}