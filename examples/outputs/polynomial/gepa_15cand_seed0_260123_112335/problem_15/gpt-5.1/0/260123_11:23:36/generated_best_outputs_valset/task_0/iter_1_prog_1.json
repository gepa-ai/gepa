{
    "score": -0.1,
    "Input": "McCourt01",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budget-aware hybrid global/local search.\n\n    Strategy:\n    1. Use a batch of global random samples (uniform in bounds) to explore.\n    2. Seed search with prev_best_x if provided and inside bounds.\n    3. Perform iterative local search (stochastic hill-climbing) around\n       the current best, with step sizes adapted to bounds.\n    4. Always respect the evaluation budget and use it fully when possible.\n    \"\"\"\n    rng = np.random.default_rng()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n\n    if budget <= 0:\n        # Fallback: return center of bounds\n        return (lower + upper) / 2.0\n\n    def clip_to_bounds(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n\n    # --- Initialization: global exploration + optional warm start ---\n    # Use up to 20% of budget (at least 5, at most 50) for global random sampling.\n    n_global = min(max(int(0.2 * budget), 5), 50, budget)\n\n    # Collect candidate points for initial evaluation\n    candidates = []\n\n    # Include prev_best_x if valid\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).flatten()\n        if x0.shape[0] == dim:\n            # Clip into bounds to guarantee validity\n            x0 = clip_to_bounds(x0)\n            candidates.append(x0)\n\n    # Fill the rest with random points\n    n_needed = max(0, n_global - len(candidates))\n    if n_needed > 0:\n        rand_points = rng.uniform(lower, upper, size=(n_needed, dim))\n        candidates.extend(rand_points)\n\n    # Evaluate initial candidates\n    best_x = None\n    best_y = None\n\n    for x in candidates:\n        if evals_used >= budget:\n            break\n        y = objective_function(x)\n        evals_used += 1\n\n        if best_y is None or y < best_y:\n            best_y = y\n            best_x = x\n\n    # If no evaluations yet (e.g., budget == 0 after checks), return something valid\n    if best_x is None:\n        # Single evaluation at random if possible\n        x = rng.uniform(lower, upper, size=dim)\n        y = objective_function(x)\n        return x\n\n    # --- Local search: stochastic hill-climbing with decaying step size ---\n    remaining_budget = budget - evals_used\n    if remaining_budget <= 0:\n        return best_x\n\n    # Initial step size: fraction of search span\n    # Smaller in high dimensions to avoid too large jumps.\n    base_step_frac = 0.15 / np.sqrt(max(dim, 1))\n    base_step = span * base_step_frac\n\n    # At least some minimal step to enable movement when span is tiny\n    min_step = 1e-8 * np.maximum(np.abs(span), 1.0)\n\n    # Number of local iterations equals remaining evaluations (one eval per neighbor)\n    current_x = best_x.copy()\n    current_y = best_y\n\n    for i in range(remaining_budget):\n        # Linearly decay step size over iterations\n        t = i / max(remaining_budget - 1, 1)\n        step_scale = (1.0 - 0.9 * t)  # from 1.0 down to 0.1\n        step = np.maximum(base_step * step_scale, min_step)\n\n        # Gaussian perturbation scaled per dimension\n        noise = rng.normal(0.0, 1.0, size=dim)\n        candidate_x = current_x + noise * step\n        candidate_x = clip_to_bounds(candidate_x)\n\n        y = objective_function(candidate_x)\n        evals_used += 1\n\n        if y < current_y:\n            current_x = candidate_x\n            current_y = y\n            if y < best_y:\n                best_y = y\n                best_x = candidate_x\n\n        if evals_used >= budget:\n            break\n\n    return best_x",
    "X": "0.5093827109630792 0.719256758947126 0.9434775070683457 0.28586612145100465 0.6973307262836393 0.8645880088996575 0.6051664696754903"
}