{
    "score": -24.156592357843714,
    "Input": "CarromTable",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budgeted blackbox minimization.\n\n    Hybrid strategy with clearer phase allocation:\n    - Robust initialization: warm start, center point, diversified global sampling.\n    - Global exploration: low\u2011discrepancy / random search over the whole domain.\n    - CMA\u2011like diagonal evolutionary refinement using elites.\n    - Local adaptive search around best and elites with directional memory.\n    \"\"\"\n\n    rng = np.random.default_rng()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    span = np.where(span <= 0, 1.0, span)\n\n    def project(x):\n        return np.clip(x, low, high)\n\n    # ---------- Handle zero/negative budget ----------\n    if budget <= 0:\n        if prev_best_x is not None:\n            x0 = np.asarray(prev_best_x, dtype=float).ravel()\n            if x0.size == dim:\n                return project(x0)\n        return project((low + high) / 2.0)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # ---------- Elite management ----------\n    elite_size = max(10, min(40, 6 * dim))\n    elite_X = []\n    elite_Y = []\n\n    def update_elite(x, y):\n        nonlocal elite_X, elite_Y\n        if len(elite_X) < elite_size:\n            elite_X.append(x)\n            elite_Y.append(y)\n            return\n        worst_idx = int(np.argmax(elite_Y))\n        if y < elite_Y[worst_idx]:\n            elite_X[worst_idx] = x\n            elite_Y[worst_idx] = y\n\n    def eval_candidate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = project(x)\n        y = objective_function(x)\n        evals_used += 1\n        if (best_x is None) or (y < best_y):\n            best_y = y\n            best_x = x\n        update_elite(x, y)\n        return y\n\n    # ---------- Initialization / warm start ----------\n    # Try warm start if shape matches\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).ravel()\n        if x0.size == dim:\n            eval_candidate(x0)\n\n    # Always evaluate center point as a robust baseline\n    if evals_used < budget:\n        center = (low + high) / 2.0\n        if (best_x is None) or np.any(np.abs(center - best_x) > 1e-12):\n            eval_candidate(center)\n\n    if evals_used >= budget:\n        return best_x\n\n    # ---------- Initial quick local/global probes ----------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Slightly increase early diversification\n    quick_probes = min(max(3 * dim, 12), remaining // 6 if remaining > 60 else remaining)\n    if quick_probes > 0:\n        for i in range(quick_probes):\n            if evals_used >= budget:\n                break\n            if i % 3 == 0:\n                # Local Gaussian around current best (or center if none)\n                base = best_x if best_x is not None else (low + high) / 2.0\n                scale = span / 4.0\n                cand = base + rng.normal(0.0, 1.0, size=dim) * scale\n            else:\n                # Global uniform\n                cand = rng.uniform(low, high, size=dim)\n            eval_candidate(cand)\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # ---------- Global exploration ----------\n    # Use a fixed fraction of total budget for global exploration (including quick_probes)\n    # Target around 55% of total budget for global search.\n    global_target_total = int(0.55 * budget)\n    global_evals_left = max(0, global_target_total - evals_used)\n    global_evals_left = min(global_evals_left, budget - evals_used)\n\n    def halton_samples(n, d):\n        def _bases(k):\n            bases = [2, 3, 5, 7, 11, 13, 17, 19]\n            while len(bases) < k:\n                bases.append(bases[-1] + 2)\n            return bases[:k]\n\n        def vdc(num, base):\n            vdc_val = 0.0\n            denom = 1.0\n            while num:\n                num, rem = divmod(num, base)\n                denom *= base\n                vdc_val += rem / denom\n            return vdc_val\n\n        bases = _bases(d)\n        start = int(rng.integers(1, 1_000_000))\n        seq = np.empty((n, d), dtype=float)\n        for i in range(n):\n            idx = i + start\n            for j in range(d):\n                seq[i, j] = vdc(idx, bases[j])\n        return seq\n\n    if global_evals_left > 0:\n        if (dim <= 40) and (global_evals_left >= dim + 4):\n            samples_unit = halton_samples(global_evals_left, dim)\n            samples = low + samples_unit * span\n        else:\n            samples = rng.uniform(low, high, size=(global_evals_left, dim))\n\n        rng.shuffle(samples, axis=0)\n        for i in range(global_evals_left):\n            if evals_used >= budget:\n                break\n            eval_candidate(samples[i])\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # ---------- Evolutionary / CMA-like refinement ----------\n    # Use around 25% of total budget for evolutionary refinement\n    evo_target_total = int(0.25 * budget)\n    evo_evals = max(0, evo_target_total - evals_used)\n    evo_evals = min(evo_evals, budget - evals_used)\n\n    if evo_evals > 0 and len(elite_X) >= 2:\n        elite_arr = np.array(elite_X)\n        elite_var = np.var(elite_arr, axis=0) + 1e-18\n        diag_cov = np.maximum(elite_var, (span / 10.0) ** 2)\n        cov_diag = np.array(diag_cov, copy=True)\n\n        mean = np.array(best_x, copy=True)\n\n        # Slightly more aggressive adaptation\n        c_cov = 0.25\n        c_mean = 0.35\n\n        min_var = (span / 8000.0) ** 2 + 1e-18\n\n        # Population size adaptive to dimension and budget\n        lamb = max(4, min(30, 4 * dim))\n        lamb = min(lamb, max(4, evo_evals // 2))\n        n_generations = max(1, evo_evals // max(1, lamb))\n\n        mean += rng.normal(0.0, 1.0, size=dim) * (span / 60.0)\n\n        for _ in range(n_generations):\n            if evals_used >= budget:\n                break\n\n            cov_diag = np.maximum(cov_diag, min_var)\n            sqrt_cov = np.sqrt(cov_diag)\n            sqrt_cov = np.where(np.isfinite(sqrt_cov), sqrt_cov, np.sqrt(min_var))\n\n            offspring = []\n            scores = []\n            for _j in range(lamb):\n                if evals_used >= budget:\n                    break\n                z = rng.normal(size=dim)\n                y = mean + sqrt_cov * z\n                f = eval_candidate(y)\n                if f is None:\n                    break\n                offspring.append(y)\n                scores.append(f)\n\n            if not offspring:\n                break\n\n            offspring = np.array(offspring)\n            scores = np.array(scores)\n\n            idx = np.argsort(scores)\n            offspring = offspring[idx]\n            scores = scores[idx]\n\n            mu = max(2, min(len(offspring), dim + 4))\n            top = offspring[:mu]\n            w = np.ones(mu) / mu\n\n            new_mean = np.sum(top * w[:, None], axis=0)\n            mean = (1.0 - c_mean) * mean + c_mean * new_mean\n\n            diff = top - mean\n            cov_update = np.sum((diff ** 2) * w[:, None], axis=0)\n            cov_diag = (1.0 - c_cov) * cov_diag + c_cov * cov_update\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # ---------- Local refinement ----------\n    # Use all remaining evaluations for local search\n    base_step = span / 20.0\n    min_step = span / 10000.0 + 1e-12\n\n    dir_memory = []\n    max_dir_memory = 16\n\n    for k in range(remaining):\n        if evals_used >= budget:\n            break\n\n        frac = (k + 1) / max(1, remaining)\n        step = np.maximum(base_step * (0.3 ** frac), min_step)\n\n        # Choose center: best or random elite\n        if elite_X and rng.random() < 0.4:\n            center_idx = int(rng.integers(0, len(elite_X)))\n            center_point = elite_X[center_idx]\n        else:\n            center_point = best_x\n\n        move_type = rng.random()\n        if move_type < 0.45:\n            # Isotropic Gaussian exploration around center\n            noise = rng.normal(0.0, 1.0, size=dim)\n            candidate = center_point + noise * step\n        elif move_type < 0.8 and dir_memory:\n            # Directional move along successful directions\n            d = dir_memory[int(rng.integers(0, len(dir_memory)))]\n            length = rng.uniform(0.4, 1.5)\n            candidate = center_point + length * step * d\n        else:\n            # Coordinate-wise perturbation\n            candidate = np.array(center_point, copy=True)\n            if dim <= 4:\n                n_coords = 1\n            else:\n                n_coords = max(1, min(dim, dim // 3 + 1))\n            idxs = rng.choice(dim, size=n_coords, replace=False)\n            noise = rng.normal(0.0, 1.0, size=n_coords)\n            candidate[idxs] += noise * step[idxs]\n\n        old_best_y = best_y\n        y = eval_candidate(candidate)\n\n        # Update directional memory only on strict improvements\n        if y is not None and y < old_best_y:\n            d = project(candidate) - best_x\n            norm = np.linalg.norm(d)\n            if norm > 1e-12:\n                d /= norm\n                dir_memory.append(d)\n                if len(dir_memory) > max_dir_memory:\n                    dir_memory.pop(0)\n\n    return best_x",
    "X": "9.646657233965056 9.643219943553657"
}