{
    "score": -23.62769941722653,
    "Input": "CarromTable",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization with budgeted evaluations.\n\n    Strategy:\n    - Use a mix of global random search and local refinement around the best points.\n    - Warm-start from prev_best_x if provided (and within bounds).\n    - Always respect evaluation budget and return best point found.\n    \"\"\"\n    rng = np.random.default_rng()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Helper: project into bounds\n    def project(x):\n        return np.clip(x, low, high)\n\n    # If no budget, return something valid\n    if budget <= 0:\n        if prev_best_x is not None:\n            x0 = np.asarray(prev_best_x, dtype=float).ravel()\n            if x0.size == dim:\n                return project(x0)\n        return project((low + high) / 2.0)\n\n    evals_used = 0\n\n    best_x = None\n    best_y = np.inf\n\n    # Evaluate a candidate (with bound projection) and track best\n    def eval_candidate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = project(x)\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x\n        return y\n\n    # --- Initialization phase ---\n    # Use warm-start if available and feasible\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).ravel()\n        if x0.size == dim:\n            eval_candidate(x0)\n\n    # If we still have not evaluated anything (no prev_best_x), start from center\n    if best_x is None and evals_used < budget:\n        center = (low + high) / 2.0\n        eval_candidate(center)\n\n    # Global random exploration: use ~60% of remaining budget\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    global_evals = max(1, int(0.6 * remaining))\n\n    # Use quasi-uniform sampling (latin-hypercube style approximation)\n    # if we have enough evals; else pure random.\n    if global_evals >= dim:\n        # For each dimension, create stratified samples, then shuffle per-dim\n        base = (np.arange(global_evals) + rng.random(global_evals)) / global_evals\n        samples = np.empty((global_evals, dim))\n        for d in range(dim):\n            perm = rng.permutation(global_evals)\n            samples[:, d] = low[d] + base[perm] * span[d]\n    else:\n        samples = rng.uniform(low, high, size=(global_evals, dim))\n\n    for i in range(global_evals):\n        if evals_used >= budget:\n            break\n        eval_candidate(samples[i])\n\n    if evals_used >= budget:\n        return best_x\n\n    # --- Local refinement phase ---\n    # Use remaining budget for local perturbations around best solution\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Start with a moderate step size proportional to bounds span\n    # and gradually shrink it.\n    base_step = span / 5.0\n    min_step = span / 1000.0 + 1e-12\n\n    for k in range(remaining):\n        if evals_used >= budget:\n            break\n\n        frac = (k + 1) / max(1, remaining)\n        # Exponential decay of step size\n        step = np.maximum(base_step * (0.2 ** frac), min_step)\n\n        # Sample Gaussian perturbation scaled by step\n        noise = rng.normal(0.0, 1.0, size=dim)\n        candidate = best_x + noise * step\n        eval_candidate(candidate)\n\n    return best_x",
    "X": "9.79016861752729 9.650802170809168"
}