{
    "score": -24.155546908765402,
    "Input": "CarromTable",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization with budgeted evaluations.\n\n    Strategy (derivative\u2011free, budget\u2011aware):\n    - Robust initialization: warm start, center point, and diversified global sampling.\n    - Three-phase search with adaptive budget split:\n        * Global: quasi-random sampling (Halton-like) for coverage.\n        * Evolutionary: recombination/mutation on an elite set.\n        * Local: adaptive Gaussian + coordinate search around best and elites.\n    - Always respects bounds and uses the full evaluation budget.\n    \"\"\"\n\n    rng = np.random.default_rng()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Guard against degenerate spans\n    span = np.where(span <= 0, 1.0, span)\n\n    def project(x):\n        return np.clip(x, low, high)\n\n    # Handle zero/negative budget gracefully\n    if budget <= 0:\n        if prev_best_x is not None:\n            x0 = np.asarray(prev_best_x, dtype=float).ravel()\n            if x0.size == dim:\n                return project(x0)\n        return project((low + high) / 2.0)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # Elite set for exploitation and local search\n    elite_size = max(8, min(25, dim * 4))\n    elite_X = []\n    elite_Y = []\n\n    def update_elite(x, y):\n        nonlocal elite_X, elite_Y\n        if len(elite_X) < elite_size:\n            elite_X.append(x)\n            elite_Y.append(y)\n            return\n\n        worst_idx = int(np.argmax(elite_Y))\n        if y < elite_Y[worst_idx]:\n            elite_X[worst_idx] = x\n            elite_Y[worst_idx] = y\n\n    def eval_candidate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = project(x)\n        y = objective_function(x)\n        evals_used += 1\n        if (best_x is None) or (y < best_y):\n            best_y = y\n            best_x = x\n        update_elite(x, y)\n        return y\n\n    # ---------- Initialization ----------\n    # Warm start from previous run if compatible\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).ravel()\n        if x0.size == dim:\n            eval_candidate(x0)\n\n    # Also add center point (robust baseline)\n    if evals_used < budget:\n        center = (low + high) / 2.0\n        if (best_x is None) or np.any(np.abs(center - best_x) > 1e-12):\n            eval_candidate(center)\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # ---------- Global exploration ----------\n    # Allocate about half of remaining budget (but at least dim+5, at most 70%)\n    global_evals = max(dim + 5, int(0.5 * remaining))\n    global_evals = min(global_evals, int(0.7 * remaining))\n    global_evals = min(global_evals, remaining)\n\n    if global_evals > 0:\n        if global_evals >= dim:\n            # Halton-like sampling with Van der Corput\n            def van_der_corput(n, base):\n                vdc = 0.0\n                denom = 1.0\n                while n:\n                    n, rem = divmod(n, base)\n                    denom *= base\n                    vdc += rem / denom\n                return vdc\n\n            # Simple prime list extended with odd numbers if needed\n            primes = [2, 3, 5, 7, 11, 13, 17, 19]\n            while len(primes) < dim:\n                primes.append(primes[-1] + 2)\n            primes = primes[:dim]\n\n            samples = np.empty((global_evals, dim), dtype=float)\n            offset = rng.integers(1, 1_000_000)\n            for i in range(global_evals):\n                idx = i + offset\n                for d in range(dim):\n                    u = van_der_corput(idx, primes[d])\n                    samples[i, d] = low[d] + u * span[d]\n        else:\n            samples = rng.uniform(low, high, size=(global_evals, dim))\n\n        rng.shuffle(samples, axis=0)\n        for i in range(global_evals):\n            if evals_used >= budget:\n                break\n            eval_candidate(samples[i])\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # ---------- Evolutionary refinement ----------\n    # Use ~30% of remaining for evolutionary search\n    evo_evals = max(1, int(0.3 * remaining))\n    evo_evals = min(evo_evals, remaining)\n\n    if evo_evals > 0 and len(elite_X) >= 2:\n        # Step sizes scale with domain; smaller than global but larger than local\n        base_step = span / 7.0\n        min_step = span / 3000.0 + 1e-12\n\n        for k in range(evo_evals):\n            if evals_used >= budget:\n                break\n\n            # Tournament selection for two parents\n            m = min(5, len(elite_X))\n            idxs1 = rng.choice(len(elite_X), size=m, replace=False)\n            ys1 = [elite_Y[i] for i in idxs1]\n            p1 = elite_X[idxs1[int(np.argmin(ys1))]]\n\n            idxs2 = rng.choice(len(elite_X), size=m, replace=False)\n            ys2 = [elite_Y[i] for i in idxs2]\n            p2 = elite_X[idxs2[int(np.argmin(ys2))]]\n\n            alpha = rng.random()\n            child = alpha * p1 + (1.0 - alpha) * p2\n\n            frac = (k + 1) / max(1, evo_evals)\n            step = np.maximum(base_step * (0.4 ** frac), min_step)\n\n            # Mixed mutation: global Gaussian + sparse coordinate noise\n            noise = rng.normal(0.0, 1.0, size=dim)\n            coord_mask = rng.random(dim) < min(0.3, 3.0 / max(1, dim))\n            coord_noise = rng.normal(0.0, 0.5, size=dim) * coord_mask\n            candidate = child + (noise + coord_noise) * step\n\n            eval_candidate(candidate)\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # ---------- Local refinement ----------\n    # Start near the best, but sometimes around random elites for diversification\n    base_step = span / 12.0\n    min_step = span / 5000.0 + 1e-12\n\n    dir_memory = []\n    max_dir_memory = 6\n\n    for k in range(remaining):\n        if evals_used >= budget:\n            break\n\n        # Exponential decay of step size within local phase\n        frac = (k + 1) / max(1, remaining)\n        step = np.maximum(base_step * (0.25 ** frac), min_step)\n\n        # Choose center: mostly best, occasionally an elite\n        if elite_X and rng.random() < 0.3:\n            center_idx = int(rng.integers(0, len(elite_X)))\n            center_point = elite_X[center_idx]\n        else:\n            center_point = best_x\n\n        move_type = rng.random()\n        if move_type < 0.45:\n            # Isotropic Gaussian exploration\n            noise = rng.normal(0.0, 1.0, size=dim)\n            candidate = center_point + noise * step\n        elif move_type < 0.75 and dir_memory:\n            # Directional move along successful directions\n            d = dir_memory[int(rng.integers(0, len(dir_memory)))]\n            length = rng.uniform(0.5, 1.5)\n            candidate = center_point + length * step * d\n        else:\n            # Coordinate-wise perturbation; adapt number of perturbed dims to dim\n            candidate = np.array(center_point, copy=True)\n            n_coords = max(1, dim // 6)\n            idxs = rng.choice(dim, size=n_coords, replace=False)\n            noise = rng.normal(0.0, 1.0, size=n_coords)\n            candidate[idxs] += noise * step[idxs]\n\n        old_best_y = best_y\n        y = eval_candidate(candidate)\n\n        # Update direction memory when best improves\n        if y is not None and y < old_best_y:\n            d = project(candidate) - best_x\n            norm = np.linalg.norm(d)\n            if norm > 1e-12:\n                d /= norm\n                dir_memory.append(d)\n                if len(dir_memory) > max_dir_memory:\n                    dir_memory.pop(0)\n\n    return best_x",
    "X": "9.643903950048783 9.639449883067407"
}