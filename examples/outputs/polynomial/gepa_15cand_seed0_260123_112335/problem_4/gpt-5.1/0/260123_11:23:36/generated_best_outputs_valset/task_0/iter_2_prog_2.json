{
    "score": -24.12212049995379,
    "Input": "CarromTable",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization with budgeted evaluations.\n\n    Strategy:\n    - Robust initialization using warm start, center, and diversified global sampling.\n    - Two-phase optimization:\n        * Global: low-discrepancy / quasi-LHS sampling plus a small evolutionary step.\n        * Local: adaptive Gaussian search around an evolving elite set.\n    - Fully consumes available budget and always respects bounds.\n    \"\"\"\n    rng = np.random.default_rng()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Helper: project into bounds\n    def project(x):\n        return np.clip(x, low, high)\n\n    # If no budget, return something valid\n    if budget <= 0:\n        if prev_best_x is not None:\n            x0 = np.asarray(prev_best_x, dtype=float).ravel()\n            if x0.size == dim:\n                return project(x0)\n        return project((low + high) / 2.0)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # Maintain a small elite set for local search (diversity + robustness)\n    elite_size = max(3, min(10, dim * 2))\n    elite_X = []\n    elite_Y = []\n\n    def update_elite(x, y):\n        nonlocal elite_X, elite_Y\n        if len(elite_X) < elite_size:\n            elite_X.append(x)\n            elite_Y.append(y)\n        else:\n            idx = int(np.argmax(elite_Y))\n            if y < elite_Y[idx]:\n                elite_Y[idx] = y\n                elite_X[idx] = x\n\n    # Evaluate a candidate (with bound projection) and track best + elite\n    def eval_candidate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = project(x)\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y or best_x is None:\n            best_y = y\n            best_x = x\n        update_elite(x, y)\n        return y\n\n    # --- Initialization phase ---\n    # Warm-start if available and feasible\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).ravel()\n        if x0.size == dim:\n            eval_candidate(x0)\n\n    # Also evaluate center point for robustness\n    if evals_used < budget:\n        center = (low + high) / 2.0\n        eval_candidate(center)\n\n    if evals_used >= budget:\n        return best_x\n\n    # --- Global exploration phase ---\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Allocate ~55% of remaining to global sampling (but at least dim+3)\n    global_evals = max(dim + 3, int(0.55 * remaining))\n    global_evals = min(global_evals, remaining)\n\n    if global_evals <= 0:\n        return best_x\n\n    # Quasi-random (Sobol-like) sequence using a simple low-discrepancy construction\n    # Fallback to random if very small budget.\n    if global_evals >= dim:\n        # Use a simple Halton-like base scheme for [0,1]^dim then affine to bounds\n        def van_der_corput(n, base):\n            vdc = 0.0\n            denom = 1.0\n            while n:\n                n, remainder = divmod(n, base)\n                denom *= base\n                vdc += remainder / denom\n            return vdc\n\n        primes = [2, 3, 5, 7, 11, 13, 17, 19]\n        while len(primes) < dim:\n            # simple extension with odd numbers (not strictly primes but OK for diversity)\n            candidate = primes[-1] + 2\n            primes.append(candidate)\n        primes = primes[:dim]\n\n        samples = np.empty((global_evals, dim))\n        offset = rng.integers(0, 1000000)\n        for i in range(global_evals):\n            for d in range(dim):\n                u = van_der_corput(i + 1 + offset, primes[d])\n                samples[i, d] = low[d] + u * span[d]\n    else:\n        samples = rng.uniform(low, high, size=(global_evals, dim))\n\n    for i in range(global_evals):\n        if evals_used >= budget:\n            break\n        eval_candidate(samples[i])\n\n    if evals_used >= budget:\n        return best_x\n\n    # --- Simple evolutionary refinement on global pool (if budget allows) ---\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    evo_iters = max(1, int(0.15 * remaining))\n    evo_iters = min(evo_iters, remaining)\n    if evo_iters > 0 and len(elite_X) >= 2:\n        base_step = span / 6.0\n        min_step = span / 1000.0 + 1e-12\n        for k in range(evo_iters):\n            if evals_used >= budget:\n                break\n            # Pick two parents from elite\n            idxs = rng.choice(len(elite_X), size=2, replace=False)\n            p1, p2 = elite_X[idxs[0]], elite_X[idxs[1]]\n            alpha = rng.random()\n            child = alpha * p1 + (1.0 - alpha) * p2\n            frac = (k + 1) / max(1, evo_iters)\n            step = np.maximum(base_step * (0.3 ** frac), min_step)\n            noise = rng.normal(0.0, 1.0, size=dim)\n            candidate = child + noise * step\n            eval_candidate(candidate)\n\n    if evals_used >= budget:\n        return best_x\n\n    # --- Local refinement phase ---\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Adaptive scale: start larger, shrink geometrically\n    base_step = span / 8.0\n    min_step = span / 2000.0 + 1e-12\n\n    for k in range(remaining):\n        if evals_used >= budget:\n            break\n\n        frac = (k + 1) / max(1, remaining)\n        step = np.maximum(base_step * (0.15 ** frac), min_step)\n\n        # Sample around a randomly chosen elite point to keep some diversity\n        if elite_X:\n            center_idx = int(rng.integers(0, len(elite_X)))\n            center_point = elite_X[center_idx]\n        else:\n            center_point = best_x\n\n        noise = rng.normal(0.0, 1.0, size=dim)\n        candidate = center_point + noise * step\n        eval_candidate(candidate)\n\n    return best_x",
    "X": "9.681540847533814 9.656950662346137"
}