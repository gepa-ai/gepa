{
    "score": 0.41116110319184496,
    "Input": "Schwefel20",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budgeted evaluations.\n\n    Strategy:\n    - Use a mix of global random search and local search around best-so-far.\n    - Warm start from prev_best_x when available and inside bounds.\n    - Always consume (almost) full evaluation budget.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    # Initialize best solution\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape != (dim,):\n            x0 = x0.ravel()[:dim]\n        x0 = clip_to_bounds(x0)\n    else:\n        x0 = rng.uniform(low, high)\n\n    y_best = objective_function(x0)\n    x_best = x0.copy()\n    evals_used += 1\n\n    if budget <= evals_used:\n        return x_best\n\n    # Use remaining budget in two phases:\n    # 1) Global random search\n    # 2) Local search around current best\n    # Split budget adaptively: more global for higher dimensions\n    if dim <= 5:\n        frac_global = 0.4\n    elif dim <= 20:\n        frac_global = 0.6\n    else:\n        frac_global = 0.75\n\n    remaining = budget - evals_used\n    n_global = max(1, int(remaining * frac_global))\n    n_local = max(0, remaining - n_global)\n\n    # --- Global random search ---\n    # Use low-discrepancy-like sampling via random but across whole space\n    for _ in range(n_global):\n        x = rng.uniform(low, high)\n        y = objective_function(x)\n        evals_used += 1\n        if y < y_best:\n            y_best = y\n            x_best = x\n        if evals_used >= budget:\n            return x_best\n\n    # --- Local search (stochastic hill-climbing with adaptive step) ---\n    if n_local <= 0:\n        return x_best\n\n    # Initial step size proportional to search space size\n    base_step = span / 6.0\n    step = np.where(span > 0, base_step, 1.0)\n\n    # Exponential decay schedule for step size\n    for i in range(n_local):\n        # Decay step size over time\n        t = i / max(1, n_local - 1)\n        curr_step = step * (0.7 ** (3 * t))  # from ~step to ~step * 0.343\n\n        # Propose local move (Gaussian around best)\n        perturb = rng.normal(loc=0.0, scale=curr_step)\n        x_candidate = clip_to_bounds(x_best + perturb)\n\n        # Occasionally inject a small random restart to escape local minima\n        if (i % max(5, dim)) == 0:\n            if rng.random() < 0.15:\n                alpha = rng.random()\n                x_rand = rng.uniform(low, high)\n                x_candidate = clip_to_bounds(alpha * x_rand + (1 - alpha) * x_best)\n\n        y = objective_function(x_candidate)\n        evals_used += 1\n        if y < y_best:\n            y_best = y\n            x_best = x_candidate\n\n        if evals_used >= budget:\n            break\n\n    return x_best",
    "X": "0.016237761678679963 0.394923341513165"
}