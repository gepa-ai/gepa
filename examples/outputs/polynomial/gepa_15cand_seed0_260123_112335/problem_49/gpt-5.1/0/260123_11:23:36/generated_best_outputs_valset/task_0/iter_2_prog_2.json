{
    "score": 0.000749470855915294,
    "Input": "Schwefel20",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budgeted evaluations.\n\n    Strategy (hybrid global + local search):\n    - Low-discrepancy-style global search using scrambled Sobol-like sampling via RNG shuffle\n      over a stratified grid when budget and dimension allow, else uniform sampling.\n    - CMA-ES\u2013inspired covariance adaptation around best-so-far for local search when budget\n      is sufficient; otherwise, simpler Gaussian hill-climbing.\n    - Warm start from prev_best_x when available and inside bounds.\n    - Always consume (almost) the full evaluation budget.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    # --- Initialization (warm start if available) ---\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.size < dim:\n            # pad with midpoints if too short\n            mid = (low + high) * 0.5\n            x_padded = np.full(dim, 0.0, dtype=float)\n            x_padded[: x0.size] = x0.ravel()\n            x_padded[x0.size :] = mid[x0.size :]\n            x0 = x_padded\n        else:\n            x0 = x0.ravel()[:dim]\n        x0 = clip_to_bounds(x0)\n    else:\n        x0 = rng.uniform(low, high)\n\n    y_best = objective_function(x0)\n    x_best = x0.copy()\n    evals_used += 1\n\n    if evals_used >= budget:\n        return x_best\n\n    remaining = budget - evals_used\n\n    # --- Choose global/local budget split adaptively ---\n    # More global search for higher dimensions and low budgets\n    if dim <= 5:\n        frac_global = 0.35\n    elif dim <= 15:\n        frac_global = 0.55\n    else:\n        frac_global = 0.7\n\n    # If budget is very small, reduce local fraction (global tends to help more)\n    if budget < 20:\n        frac_global = 0.8\n\n    n_global = max(1, int(remaining * frac_global))\n    n_local = max(0, remaining - n_global)\n\n    # --- Global search: stratified sampling when possible, else uniform ---\n    # Use up to a max number of strata along each dimension to avoid explosion\n    max_strata = 16\n\n    if n_global >= dim and dim <= 20:\n        # Stratified: build per-dimension strata indices and shuffle\n        # Each dimension has k bins; k chosen so that k^dim <= n_global approximately.\n        # For larger dimensions this quickly becomes huge, so we only go here for dim <= 20.\n        k = int(np.floor(n_global ** (1.0 / dim)))\n        k = max(2, min(k, max_strata))\n        # Total possible grid points\n        total_grid = k**dim\n        # We only need n_global points; sample a subset of grid indices\n        if total_grid <= 2 * n_global:\n            # Enumerate all grid points then shuffle\n            # Construct grid using base-k representation\n            grid_indices = np.arange(total_grid)\n            rng.shuffle(grid_indices)\n            chosen = grid_indices[:n_global]\n        else:\n            # Uniformly sample distinct grid indices when grid is much larger\n            chosen = rng.choice(total_grid, size=n_global, replace=False)\n\n        # Convert linear index to k-ary vector per point\n        def index_to_coords(idx):\n            coords = np.empty(dim, dtype=int)\n            for d in range(dim - 1, -1, -1):\n                coords[d] = idx % k\n                idx //= k\n            return coords\n\n        for idx in chosen:\n            coords = index_to_coords(int(idx))\n            # Map coords in {0,...,k-1} to [0,1] with jitter\n            u = (coords + rng.random(dim)) / k\n            x = low + u * span\n            y = objective_function(x)\n            evals_used += 1\n            if y < y_best:\n                y_best = y\n                x_best = x\n            if evals_used >= budget:\n                return x_best\n    else:\n        # Plain uniform global search\n        for _ in range(n_global):\n            x = rng.uniform(low, high)\n            y = objective_function(x)\n            evals_used += 1\n            if y < y_best:\n                y_best = y\n                x_best = x\n            if evals_used >= budget:\n                return x_best\n\n    if n_local <= 0 or evals_used >= budget:\n        return x_best\n\n    # --- Local search ---\n    remaining = budget - evals_used\n\n    # Decide between simple hill-climbing and covariance-adaptive search\n    # CMA-like search is only worthwhile if we have enough evaluations and moderate dim\n    use_covariance = (remaining >= 20) and (dim <= 40)\n\n    if not use_covariance:\n        # Simple stochastic hill-climbing with adaptive step\n        base_step = span / 6.0\n        step = np.where(span > 0, base_step, 1.0)\n        for i in range(remaining):\n            t = i / max(1, remaining - 1)\n            curr_step = step * (0.7 ** (3 * t))\n\n            perturb = rng.normal(loc=0.0, scale=curr_step)\n            x_candidate = clip_to_bounds(x_best + perturb)\n\n            # Occasional biased restart towards random point\n            if (i % max(5, dim)) == 0:\n                if rng.random() < 0.2:\n                    alpha = rng.random()\n                    x_rand = rng.uniform(low, high)\n                    x_candidate = clip_to_bounds(alpha * x_rand + (1 - alpha) * x_best)\n\n            y = objective_function(x_candidate)\n            evals_used += 1\n            if y < y_best:\n                y_best = y\n                x_best = x_candidate\n\n            if evals_used >= budget:\n                break\n\n        return x_best\n\n    # --- Covariance-adaptive local search (CMA-ES\u2013inspired, simplified) ---\n    # Initialize mean at best solution so far\n    mean = x_best.copy()\n    # Initial step size relative to domain\n    sigma = float(np.mean(span) / (3.0 * np.sqrt(dim))) if dim > 0 else 1.0\n    if not np.isfinite(sigma) or sigma <= 0.0:\n        sigma = 1.0\n\n    # Isotropic covariance (we'll adapt it slightly based on successes)\n    cov = np.eye(dim)\n\n    # Population size and number of elites\n    lam = min(max(4, 4 + int(3 * np.log(dim + 1))), remaining)\n    mu = max(1, lam // 2)\n\n    # Learning rates\n    c_sigma = 0.2\n    c_cov = 2.0 / (dim + 2.0)\n\n    # Precompute number of generations from remaining evaluations\n    max_generations = max(1, remaining // lam)\n\n    for g in range(max_generations):\n        # Draw population\n        try:\n            z = rng.normal(size=(lam, dim))\n            # Simple covariance handling via Cholesky when possible\n            try:\n                A = np.linalg.cholesky(cov)\n                steps = z @ A.T\n            except np.linalg.LinAlgError:\n                # Fall back to diagonal if covariance becomes non-PSD\n                diag = np.clip(np.diag(cov), 1e-8, None)\n                steps = z * np.sqrt(diag)\n        except Exception:\n            # Fully fallback to isotropic if something goes wrong numerically\n            steps = rng.normal(size=(lam, dim))\n\n        X = mean + sigma * steps\n        X = clip_to_bounds(X)\n\n        ys = []\n        for i in range(lam):\n            y = objective_function(X[i])\n            ys.append(y)\n            evals_used += 1\n            if y < y_best:\n                y_best = y\n                x_best = X[i].copy()\n            if evals_used >= budget:\n                break\n\n        if evals_used >= budget:\n            break\n\n        ys = np.asarray(ys)\n        order = np.argsort(ys)\n        elites = X[order[:mu]]\n\n        # Update mean towards weighted average of elites\n        weights = np.linspace(mu, 1, mu)\n        weights = weights / np.sum(weights)\n        new_mean = np.sum(elites * weights[:, None], axis=0)\n        step_mean = (new_mean - mean) / max(1e-12, sigma)\n        mean = new_mean\n\n        # Update step size sigma based on success of mean move\n        success = ys[order[0]] < y_best\n        # Even if elite best isn't better than global best, still use step length\n        norm_step = np.linalg.norm(step_mean) / max(1.0, np.sqrt(dim))\n        target = 1.0\n        sigma *= np.exp(c_sigma * (norm_step - target))\n\n        # Adapt covariance slightly using elite deviations\n        centered = (elites - mean) / max(1e-12, sigma)\n        cov_update = (centered.T @ centered) / mu\n        cov = (1.0 - c_cov) * cov + c_cov * cov_update\n\n        # Keep covariance numerically stable\n        diag = np.clip(np.diag(cov), 1e-12, None)\n        cov = (cov + cov.T) * 0.5\n        np.fill_diagonal(cov, diag)\n\n    return x_best",
    "X": "0.0006636625136180259 -8.580834229726802e-05"
}