{
    "score": 8.490775411442637e-07,
    "Input": "Schwefel20",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budgeted evaluations.\n\n    Strategy (hybrid global + local search):\n    - Low-discrepancy-style global search using stratified jittered sampling when feasible,\n      else uniform sampling.\n    - CMA-ES\u2013inspired covariance adaptation around best-so-far for local search when budget\n      is sufficient; otherwise, simpler Gaussian hill-climbing.\n    - Warm start from prev_best_x when available and inside bounds.\n    - Always consume (almost) the full evaluation budget.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    # Handle degenerate case\n    if budget <= 0 or dim <= 0:\n        return clip_to_bounds(np.zeros(dim, dtype=float))\n\n    evals_used = 0\n\n    # --- Initialization (warm start if available) ---\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        # Adjust dimensionality\n        if x0.size < dim:\n            mid = (low + high) * 0.5\n            x_padded = np.empty(dim, dtype=float)\n            x_padded[: x0.size] = x0.ravel()\n            x_padded[x0.size:] = mid[x0.size:]\n            x0 = x_padded\n        else:\n            x0 = x0.ravel()[:dim]\n        x0 = clip_to_bounds(x0)\n    else:\n        # Use center point as a robust initial guess, especially helpful on symmetric problems\n        x0 = (low + high) * 0.5\n\n    y_best = objective_function(x0)\n    x_best = x0.copy()\n    evals_used += 1\n\n    if evals_used >= budget:\n        return x_best\n\n    remaining = budget - evals_used\n\n    # --- Choose global/local budget split adaptively ---\n    # Slightly more local search than before to exploit warm starts and structured problems.\n    if dim <= 5:\n        frac_global = 0.3\n    elif dim <= 15:\n        frac_global = 0.45\n    else:\n        frac_global = 0.6\n\n    # If budget is very small, prioritize global search more.\n    if budget < 20:\n        frac_global = 0.8\n\n    n_global = max(1, int(remaining * frac_global))\n    n_local = max(0, remaining - n_global)\n\n    # --- Global search: stratified sampling when possible, else uniform ---\n    max_strata = 16\n\n    if n_global >= dim and dim <= 20:\n        # Choose k so that k^dim ~ n_global but not exploding\n        k = int(np.floor(n_global ** (1.0 / dim)))\n        k = max(2, min(k, max_strata))\n        total_grid = k**dim\n\n        if total_grid <= 2 * n_global:\n            grid_indices = np.arange(total_grid)\n            rng.shuffle(grid_indices)\n            chosen = grid_indices[:n_global]\n        else:\n            chosen = rng.choice(total_grid, size=n_global, replace=False)\n\n        def index_to_coords(idx):\n            coords = np.empty(dim, dtype=int)\n            for d in range(dim - 1, -1, -1):\n                coords[d] = idx % k\n                idx //= k\n            return coords\n\n        for idx in chosen:\n            coords = index_to_coords(int(idx))\n            u = (coords + rng.random(dim)) / k\n            x = low + u * span\n            y = objective_function(x)\n            evals_used += 1\n            if y < y_best:\n                y_best = y\n                x_best = x\n            if evals_used >= budget:\n                return x_best\n    else:\n        # Latin-hypercube-like sampling via stratification per dimension\n        # when we can afford at least one sample per dim; else uniform.\n        if n_global >= dim:\n            # Create n_global stratified samples per dimension and shuffle each dim\n            u = (np.arange(n_global) + rng.random(n_global)) / n_global\n            X = np.empty((n_global, dim), dtype=float)\n            for d in range(dim):\n                rng.shuffle(u)\n                X[:, d] = u\n            X = low + X * span\n            for i in range(n_global):\n                y = objective_function(X[i])\n                evals_used += 1\n                if y < y_best:\n                    y_best = y\n                    x_best = X[i].copy()\n                if evals_used >= budget:\n                    return x_best\n        else:\n            # Plain uniform global search\n            for _ in range(n_global):\n                x = rng.uniform(low, high)\n                y = objective_function(x)\n                evals_used += 1\n                if y < y_best:\n                    y_best = y\n                    x_best = x\n                if evals_used >= budget:\n                    return x_best\n\n    if n_local <= 0 or evals_used >= budget:\n        return x_best\n\n    # --- Local search ---\n    remaining = budget - evals_used\n\n    # Decide between simple hill-climbing and covariance-adaptive search\n    use_covariance = (remaining >= 20) and (dim <= 60)\n\n    if not use_covariance:\n        # Simple stochastic hill-climbing with adaptive step\n        # Initial step scale slightly reduced for better fine-tuning.\n        base_step = span / 8.0\n        step = np.where(span > 0, base_step, 1.0)\n        for i in range(remaining):\n            # Exponential decay of step size over iterations\n            t = i / max(1, remaining - 1)\n            curr_step = step * (0.7 ** (4 * t))\n\n            perturb = rng.normal(loc=0.0, scale=curr_step)\n            x_candidate = clip_to_bounds(x_best + perturb)\n\n            # Occasional biased restart towards random point\n            if (i % max(5, dim)) == 0 and rng.random() < 0.25:\n                alpha = rng.random()\n                x_rand = rng.uniform(low, high)\n                x_candidate = clip_to_bounds(alpha * x_rand + (1 - alpha) * x_best)\n\n            y = objective_function(x_candidate)\n            evals_used += 1\n            if y < y_best:\n                y_best = y\n                x_best = x_candidate\n\n            if evals_used >= budget:\n                break\n\n        return x_best\n\n    # --- Covariance-adaptive local search (CMA-ES\u2013inspired, simplified) ---\n    mean = x_best.copy()\n\n    # Initial step size relative to domain (slightly smaller to improve stability)\n    if np.all(span > 0):\n        sigma = float(np.mean(span) / (4.0 * np.sqrt(dim)))\n    else:\n        sigma = 1.0\n    if not np.isfinite(sigma) or sigma <= 0.0:\n        sigma = 1.0\n\n    cov = np.eye(dim)\n\n    # Population size and number of elites\n    lam = min(max(4, 4 + int(3 * np.log(dim + 1))), remaining)\n    mu = max(1, lam // 2)\n\n    # Learning rates tuned for more conservative adaptation\n    c_sigma = 0.15\n    c_cov = 1.5 / (dim + 2.0)\n\n    max_generations = max(1, remaining // lam)\n\n    for _ in range(max_generations):\n        # Draw population with current covariance\n        try:\n            z = rng.normal(size=(lam, dim))\n            try:\n                A = np.linalg.cholesky(cov)\n                steps = z @ A.T\n            except np.linalg.LinAlgError:\n                # Fall back to diagonal covariance if non-PSD\n                diag = np.clip(np.diag(cov), 1e-10, None)\n                steps = z * np.sqrt(diag)\n        except Exception:\n            # Fully fallback to isotropic if something goes wrong numerically\n            steps = rng.normal(size=(lam, dim))\n\n        X = mean + sigma * steps\n        X = clip_to_bounds(X)\n\n        ys = []\n        for i in range(lam):\n            y = objective_function(X[i])\n            ys.append(y)\n            evals_used += 1\n            if y < y_best:\n                y_best = y\n                x_best = X[i].copy()\n            if evals_used >= budget:\n                break\n\n        if evals_used >= budget:\n            break\n\n        ys = np.asarray(ys)\n        order = np.argsort(ys)\n        elites = X[order[:mu]]\n\n        # Weighted recombination of elites\n        weights = np.linspace(mu, 1, mu)\n        weights = weights / np.sum(weights)\n        new_mean = np.sum(elites * weights[:, None], axis=0)\n        step_mean = (new_mean - mean) / max(1e-12, sigma)\n        mean = new_mean\n\n        # Step-size control: target normalized step length around 1\n        norm_step = np.linalg.norm(step_mean) / max(1.0, np.sqrt(dim))\n        target = 1.0\n        sigma *= np.exp(c_sigma * (norm_step - target))\n\n        # Clamp sigma to avoid degeneracy or explosion\n        sigma = float(np.clip(sigma, 1e-12, np.max(span) if np.all(span > 0) else 1.0))\n\n        # Adapt covariance based on elite deviations\n        centered = (elites - mean) / max(1e-12, sigma)\n        cov_update = (centered.T @ centered) / mu\n        cov = (1.0 - c_cov) * cov + c_cov * cov_update\n\n        # Symmetrize and stabilize covariance\n        cov = (cov + cov.T) * 0.5\n        diag = np.clip(np.diag(cov), 1e-12, None)\n        np.fill_diagonal(cov, diag)\n\n    return x_best",
    "X": "1.1177659874792708e-07 -7.373009423963366e-07"
}