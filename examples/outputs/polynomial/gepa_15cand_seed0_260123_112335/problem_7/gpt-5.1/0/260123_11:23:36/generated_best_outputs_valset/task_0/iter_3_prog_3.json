{
    "score": -0.8433397905038464,
    "Input": "DeflectedCorrugatedSpring",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef _project_bounds(x, bounds):\n    \"\"\"Clamp x to the provided box-constraints.\"\"\"\n    return np.clip(x, bounds[:, 0], bounds[:, 1])\n\n\ndef _ensure_array(x, dim, bounds, rng):\n    \"\"\"Convert x to a valid numpy array inside bounds.\"\"\"\n    try:\n        x = np.asarray(x, dtype=float).reshape(-1)\n        if x.shape[0] != dim or not np.all(np.isfinite(x)):\n            raise ValueError\n    except Exception:\n        x = rng.uniform(bounds[:, 0], bounds[:, 1])\n    return _project_bounds(x, bounds)\n\n\ndef _latin_hypercube(rng, n_samples, dim):\n    \"\"\"Generate LHS samples in [0,1]^dim.\"\"\"\n    cut = np.linspace(0, 1, n_samples + 1)\n    u = rng.uniform(low=cut[:-1], high=cut[1:], size=(dim, n_samples))\n    for j in range(dim):\n        rng.shuffle(u[j])\n    return u.T  # shape (n_samples, dim)\n\n\ndef _shrink_towards_center(x, bounds, factor=0.2):\n    \"\"\"\n    Heuristic: for DeflectedCorrugatedSpring-like problems where good\n    points are often near a central region, move x towards the box center.\n    factor in (0,1): 0 -> no move, 1 -> move fully to center.\n    \"\"\"\n    center = 0.5 * (bounds[:, 0] + bounds[:, 1])\n    return _project_bounds(center + (1.0 - factor) * (x - center), bounds)\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid search:\n    1) Use prev_best_x if available and valid (and refine around it early).\n    2) Global exploration with latin-hypercube-like sampling mix including\n       a few points biased towards the box center.\n    3) Local search around current best using adaptive Gaussian perturbations\n       with simple 1+\u03bb evolution-strategy style adaptation.\n    \"\"\"\n    bounds = np.array(config['bounds'], dtype=float)\n    dim = int(config['dim'])\n    budget = int(config['budget'])\n\n    if budget <= 0:\n        # Degenerate case: no evaluations allowed, just return a random point.\n        rng_fallback = np.random.default_rng()\n        return rng_fallback.uniform(bounds[:, 0], bounds[:, 1], size=dim)\n\n    rng = np.random.default_rng()\n    evaluations_used = 0\n\n    # ---- Initialization / warm start ----\n    if prev_best_x is not None:\n        x_best = _ensure_array(prev_best_x, dim, bounds, rng)\n    else:\n        # Bias one starting point slightly towards box center as a heuristic.\n        x_rand = rng.uniform(bounds[:, 0], bounds[:, 1])\n        x_best = _shrink_towards_center(x_rand, bounds, factor=0.3)\n\n    y_best = objective_function(x_best)\n    evaluations_used += 1\n    if evaluations_used >= budget:\n        return x_best\n\n    box_sizes = bounds[:, 1] - bounds[:, 0]\n    box_sizes[box_sizes == 0.0] = 1.0\n\n    # ---- Budget allocation ----\n    remaining = budget - evaluations_used\n\n    # Warm local refinement if a previous solution is provided.\n    if prev_best_x is not None and remaining > 3 * dim:\n        warm_local_evals = min(max(3 * dim, remaining // 8), remaining)\n    else:\n        warm_local_evals = 0\n\n    remaining -= warm_local_evals\n\n    # Reserve a bit more for local search which often pays off.\n    global_evals = int(0.4 * remaining)\n    global_evals = max(1, min(global_evals, remaining))\n    remaining_after_global = remaining - global_evals\n    local_evals = max(0, remaining_after_global)\n\n    # ---- Warm-start local refinement (small radius) ----\n    if warm_local_evals > 0:\n        sigma_warm = 0.03 * box_sizes\n        for _ in range(warm_local_evals):\n            if evaluations_used >= budget:\n                break\n            noise = rng.normal(0.0, 1.0, size=dim)\n            step = noise * sigma_warm\n            x_cand = _project_bounds(x_best + step, bounds)\n            y_cand = objective_function(x_cand)\n            evaluations_used += 1\n            if y_cand < y_best:\n                x_best, y_best = x_cand, y_cand\n        if evaluations_used >= budget:\n            return x_best\n\n    # ---- Global exploration: LHS in box with a few center-biased points ----\n    remaining = budget - evaluations_used\n    global_evals = max(1, min(global_evals, remaining))\n    if global_evals > 0:\n        # LHS samples\n        u = _latin_hypercube(rng, global_evals, dim)\n        xs = bounds[:, 0] + u * (bounds[:, 1] - bounds[:, 0])\n\n        # Replace a small subset with points shrunk towards center\n        n_center = max(1, global_evals // 10)\n        idxs = rng.choice(global_evals, size=n_center, replace=False)\n        for idx in idxs:\n            xs[idx] = _shrink_towards_center(xs[idx], bounds, factor=0.4)\n\n        for i in range(global_evals):\n            if evaluations_used >= budget:\n                break\n            x = xs[i]\n            y = objective_function(x)\n            evaluations_used += 1\n            if y < y_best:\n                x_best, y_best = x, y\n        if evaluations_used >= budget:\n            return x_best\n\n    # ---- Local search: 1+\u03bb ES style around best ----\n    remaining = budget - evaluations_used\n    if remaining <= 0:\n        return x_best\n\n    sigma = 0.15 * box_sizes\n    lam = min(max(4, 2 * dim), 30)\n    generations = max(1, remaining // lam)\n\n    # Ensure we do not exceed the available evaluations\n    for gen in range(generations):\n        if evaluations_used >= budget:\n            break\n\n        # Mild decay to focus search over time\n        decay = 0.97 ** gen\n        cur_sigma = sigma * decay\n\n        best_offspring_y = None\n        best_offspring_x = None\n        n_improved = 0\n\n        for _ in range(lam):\n            if evaluations_used >= budget:\n                break\n\n            noise = rng.normal(0.0, 1.0, size=dim)\n            step = noise * cur_sigma\n            x_cand = _project_bounds(x_best + step, bounds)\n            y_cand = objective_function(x_cand)\n            evaluations_used += 1\n\n            if best_offspring_y is None or y_cand < best_offspring_y:\n                best_offspring_y = y_cand\n                best_offspring_x = x_cand\n            if y_cand < y_best:\n                n_improved += 1\n\n        if best_offspring_y is not None and best_offspring_y < y_best:\n            x_best, y_best = best_offspring_x, best_offspring_y\n\n        # Step-size control\n        if n_improved == 0:\n            sigma *= 0.65\n        elif n_improved > lam // 3:\n            sigma *= 1.25\n\n        sigma = np.clip(sigma, 0.01 * box_sizes, 0.5 * box_sizes)\n\n    return x_best",
    "X": "5.23726714849548 4.789247940690321 4.804738187846369 6.0878628185519 5.045133621949516 5.359612805178537 4.682642373884012"
}