{
    "score": -0.3733623960544724,
    "Input": "DeflectedCorrugatedSpring",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef _project_bounds(x, bounds):\n    \"\"\"Clamp x to the provided box-constraints.\"\"\"\n    return np.clip(x, bounds[:, 0], bounds[:, 1])\n\n\ndef _ensure_array(x, dim, bounds, rng):\n    \"\"\"Convert x to a valid numpy array inside bounds.\"\"\"\n    try:\n        x = np.asarray(x, dtype=float).reshape(-1)\n        if x.shape[0] != dim or not np.all(np.isfinite(x)):\n            raise ValueError\n    except Exception:\n        x = rng.uniform(bounds[:, 0], bounds[:, 1])\n    return _project_bounds(x, bounds)\n\n\ndef _latin_hypercube(rng, n_samples, dim):\n    \"\"\"Generate LHS samples in [0,1]^dim.\"\"\"\n    # Classic LHS: divide [0,1] into n_samples strata per dim and permute\n    cut = np.linspace(0, 1, n_samples + 1)\n    u = rng.uniform(low=cut[:-1], high=cut[1:], size=(dim, n_samples))\n    # permute within each dimension\n    for j in range(dim):\n        rng.shuffle(u[j])\n    return u.T  # shape (n_samples, dim)\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid search:\n    1) Use prev_best_x if available and valid (and refine around it early).\n    2) Global exploration with latin-hypercube-like sampling.\n    3) Local search around current best using adaptive Gaussian perturbations\n       with simple 1+\u03bb evolution-strategy style adaptation.\n    \"\"\"\n    bounds = np.array(config['bounds'], dtype=float)\n    dim = int(config['dim'])\n    budget = int(config['budget'])\n\n    # Handle degenerate budget\n    if budget <= 0:\n        return np.random.uniform(bounds[:, 0], bounds[:, 1], size=dim)\n\n    rng = np.random.default_rng()\n    evaluations_used = 0\n\n    # ----- Initialization / warm start -----\n    # Prefer prev_best_x if sensible; otherwise random.\n    if prev_best_x is not None:\n        x_best = _ensure_array(prev_best_x, dim, bounds, rng)\n    else:\n        x_best = rng.uniform(bounds[:, 0], bounds[:, 1])\n\n    y_best = objective_function(x_best)\n    evaluations_used += 1\n    if evaluations_used >= budget:\n        return x_best\n\n    box_sizes = bounds[:, 1] - bounds[:, 0]\n    box_sizes[box_sizes == 0.0] = 1.0\n\n    # ----- Allocate budget among phases -----\n    # Early local refinement near warm start (if warm start is given)\n    remaining = budget - evaluations_used\n    if prev_best_x is not None and remaining > 3 * dim:\n        warm_local_evals = min(max(3 * dim, remaining // 10), remaining)\n    else:\n        warm_local_evals = 0\n\n    remaining -= warm_local_evals\n    global_evals = int(0.5 * remaining)\n    global_evals = max(1, min(global_evals, remaining))\n    remaining_after_global = remaining - global_evals\n    local_evals = max(0, remaining_after_global)\n\n    # ----- Warm-start local refinement (small radius) -----\n    if warm_local_evals > 0:\n        # Small radius around warm start to quickly exploit it\n        sigma_warm = 0.05 * box_sizes\n        for _ in range(warm_local_evals):\n            if evaluations_used >= budget:\n                break\n            noise = rng.normal(0.0, 1.0, size=dim)\n            step = noise * sigma_warm\n            x_cand = _project_bounds(x_best + step, bounds)\n            y_cand = objective_function(x_cand)\n            evaluations_used += 1\n            if y_cand < y_best:\n                x_best, y_best = x_cand, y_cand\n        if evaluations_used >= budget:\n            return x_best\n\n    # ----- Global exploration: LHS in box -----\n    remaining = budget - evaluations_used\n    global_evals = max(1, min(global_evals, remaining))\n    if global_evals > 0:\n        # Generate LHS samples; if more dims than samples, it's still fine.\n        u = _latin_hypercube(rng, global_evals, dim)  # in [0,1]^dim\n        xs = bounds[:, 0] + u * (bounds[:, 1] - bounds[:, 0])\n        for i in range(global_evals):\n            if evaluations_used >= budget:\n                break\n            x = xs[i]\n            y = objective_function(x)\n            evaluations_used += 1\n            if y < y_best:\n                x_best, y_best = x, y\n        if evaluations_used >= budget:\n            return x_best\n\n    # ----- Local search: 1+\u03bb ES style around best -----\n    remaining = budget - evaluations_used\n    if remaining <= 0:\n        return x_best\n\n    # Start with moderate step size\n    sigma = 0.2 * box_sizes\n    # \u03bb depends on dimension (small for low dim, larger for high dim)\n    lam = min(max(4, 2 * dim), 20)\n    # Number of generations\n    generations = max(1, remaining // lam)\n\n    for gen in range(generations):\n        if evaluations_used >= budget:\n            break\n        # Slight decay over generations to focus search\n        decay = 0.95 ** gen\n        cur_sigma = sigma * decay\n\n        best_offspring_y = None\n        best_offspring_x = None\n        n_improved = 0\n\n        for _ in range(lam):\n            if evaluations_used >= budget:\n                break\n            noise = rng.normal(0.0, 1.0, size=dim)\n            step = noise * cur_sigma\n            x_cand = _project_bounds(x_best + step, bounds)\n            y_cand = objective_function(x_cand)\n            evaluations_used += 1\n\n            if best_offspring_y is None or y_cand < best_offspring_y:\n                best_offspring_y = y_cand\n                best_offspring_x = x_cand\n            if y_cand < y_best:\n                n_improved += 1\n\n        # Selection: move to best offspring if it's better\n        if best_offspring_y is not None and best_offspring_y < y_best:\n            x_best, y_best = best_offspring_x, best_offspring_y\n\n        # Very simple step-size control: if too many improvements, increase;\n        # if none, decrease slightly.\n        if n_improved == 0:\n            sigma *= 0.7\n        elif n_improved > lam // 3:\n            sigma *= 1.3\n\n        # Keep sigma within reasonable bounds\n        sigma = np.clip(sigma, 0.01 * box_sizes, 0.5 * box_sizes)\n\n    return x_best",
    "X": "4.579780634544527 4.325856109187662 6.536312619687266 3.7911924502763417 4.387921118723333 5.670946424727443 5.968797664572575"
}