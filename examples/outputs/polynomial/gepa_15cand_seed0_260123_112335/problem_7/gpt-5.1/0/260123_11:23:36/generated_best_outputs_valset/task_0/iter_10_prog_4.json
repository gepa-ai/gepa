{
    "score": -0.8433398180818339,
    "Input": "DeflectedCorrugatedSpring",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef _project_bounds(x, bounds):\n    \"\"\"Clamp x to the provided box-constraints.\"\"\"\n    return np.clip(x, bounds[:, 0], bounds[:, 1])\n\n\ndef _ensure_array(x, dim, bounds, rng):\n    \"\"\"Convert x to a valid numpy array inside bounds.\"\"\"\n    try:\n        x = np.asarray(x, dtype=float).reshape(-1)\n        if x.shape[0] != dim or not np.all(np.isfinite(x)):\n            raise ValueError\n    except Exception:\n        x = rng.uniform(bounds[:, 0], bounds[:, 1])\n    return _project_bounds(x, bounds)\n\n\ndef _latin_hypercube(rng, n_samples, dim):\n    \"\"\"Generate LHS samples in [0,1]^dim.\"\"\"\n    if n_samples <= 0:\n        return np.empty((0, dim))\n    cut = np.linspace(0, 1, n_samples + 1)\n    u = rng.uniform(low=cut[:-1], high=cut[1:], size=(dim, n_samples))\n    for j in range(dim):\n        rng.shuffle(u[j])\n    return u.T  # shape (n_samples, dim)\n\n\ndef _shrink_towards_center(x, bounds, factor=0.2):\n    \"\"\"\n    Move x towards the box center by 'factor' (0->no move, 1->center).\n    \"\"\"\n    center = 0.5 * (bounds[:, 0] + bounds[:, 1])\n    return _project_bounds(center + (1.0 - factor) * (x - center), bounds)\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid global-local search with warm-start support.\n    Strategy:\n      - Use prev_best_x if provided, otherwise a slightly center-biased start.\n      - Early local refinement if warm-start is available.\n      - Global exploration via LHS with some center-biased points.\n      - Local search via 1+\u03bb ES with adaptive step-size.\n    \"\"\"\n    bounds = np.array(config['bounds'], dtype=float)\n    dim = int(config['dim'])\n    budget = int(config['budget'])\n\n    if budget <= 0:\n        rng_fallback = np.random.default_rng()\n        return rng_fallback.uniform(bounds[:, 0], bounds[:, 1], size=dim)\n\n    rng = np.random.default_rng()\n    evaluations_used = 0\n\n    # ---- Initialization / warm start ----\n    if prev_best_x is not None:\n        x_best = _ensure_array(prev_best_x, dim, bounds, rng)\n    else:\n        x_rand = rng.uniform(bounds[:, 0], bounds[:, 1])\n        x_best = _shrink_towards_center(x_rand, bounds, factor=0.3)\n\n    y_best = objective_function(x_best)\n    evaluations_used += 1\n    if evaluations_used >= budget:\n        return x_best\n\n    box_sizes = bounds[:, 1] - bounds[:, 0]\n    # Avoid zero sigma if dimension has zero width\n    box_sizes[box_sizes == 0.0] = 1.0\n\n    # ---- Budget allocation ----\n    remaining = budget - evaluations_used\n\n    # Warm local refinement around good warm-start if enough budget\n    if prev_best_x is not None and remaining > 3 * dim:\n        warm_local_evals = min(max(3 * dim, remaining // 6), remaining)\n    else:\n        warm_local_evals = 0\n\n    remaining -= warm_local_evals\n\n    # Slightly increase global share for rugged multi-modal problems\n    global_evals = int(0.5 * remaining)\n    global_evals = max(1, min(global_evals, remaining))\n    remaining_after_global = remaining - global_evals\n    local_evals = max(0, remaining_after_global)\n\n    # ---- Warm-start local refinement (small radius) ----\n    if warm_local_evals > 0:\n        sigma_warm = 0.02 * box_sizes\n        for _ in range(warm_local_evals):\n            if evaluations_used >= budget:\n                break\n            noise = rng.normal(0.0, 1.0, size=dim)\n            step = noise * sigma_warm\n            x_cand = _project_bounds(x_best + step, bounds)\n            y_cand = objective_function(x_cand)\n            evaluations_used += 1\n            if y_cand < y_best:\n                x_best, y_best = x_cand, y_cand\n        if evaluations_used >= budget:\n            return x_best\n\n    # ---- Global exploration: LHS in box with a few center-biased points ----\n    remaining = budget - evaluations_used\n    global_evals = max(1, min(global_evals, remaining))\n    if global_evals > 0 and remaining > 0:\n        u = _latin_hypercube(rng, global_evals, dim)\n        xs = bounds[:, 0] + u * (bounds[:, 1] - bounds[:, 0])\n\n        # Replace a subset with points shrunk towards center\n        n_center = max(1, global_evals // 8)\n        n_center = min(global_evals, n_center)\n        idxs = rng.choice(global_evals, size=n_center, replace=False)\n        for idx in idxs:\n            xs[idx] = _shrink_towards_center(xs[idx], bounds, factor=0.4)\n\n        for i in range(global_evals):\n            if evaluations_used >= budget:\n                break\n            x = xs[i]\n            y = objective_function(x)\n            evaluations_used += 1\n            if y < y_best:\n                x_best, y_best = x, y\n        if evaluations_used >= budget:\n            return x_best\n\n    # ---- Local search: 1+\u03bb ES style around best ----\n    remaining = budget - evaluations_used\n    if remaining <= 0:\n        return x_best\n\n    # Initial step-size: slightly smaller to better handle rugged landscapes\n    sigma = 0.1 * box_sizes\n    lam = min(max(4, 2 * dim), 40)\n    generations = max(1, remaining // lam)\n\n    for gen in range(generations):\n        if evaluations_used >= budget:\n            break\n\n        # Decay factor for step-size across generations\n        decay = 0.98 ** gen\n        cur_sigma = sigma * decay\n\n        best_offspring_y = None\n        best_offspring_x = None\n        n_improved = 0\n\n        for _ in range(lam):\n            if evaluations_used >= budget:\n                break\n\n            noise = rng.normal(0.0, 1.0, size=dim)\n            step = noise * cur_sigma\n            x_cand = _project_bounds(x_best + step, bounds)\n            y_cand = objective_function(x_cand)\n            evaluations_used += 1\n\n            if best_offspring_y is None or y_cand < best_offspring_y:\n                best_offspring_y = y_cand\n                best_offspring_x = x_cand\n            if y_cand < y_best:\n                n_improved += 1\n\n        if best_offspring_y is not None and best_offspring_y < y_best:\n            x_best, y_best = best_offspring_x, best_offspring_y\n\n        # Step-size adaptation (1/5 success-ish rule variant)\n        if n_improved == 0:\n            sigma *= 0.6\n        elif n_improved > lam // 3:\n            sigma *= 1.3\n\n        sigma = np.clip(sigma, 0.01 * box_sizes, 0.5 * box_sizes)\n\n    return x_best",
    "X": "5.14727269071623 4.796261699175907 4.794725215037782 6.119004950787433 5.0647254475202566 5.326123427316597 4.706287567788864"
}