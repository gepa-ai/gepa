{
    "score": -5.100226010501987,
    "Input": "Shekel05",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    bounds = np.array(config['bounds'], dtype=float)\n    dim = int(config.get('dim', len(bounds)))\n    budget = int(config.get('budget', 1))\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n\n    def clip_to_bounds(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # Robust evaluation helper with bound handling\n    def evaluate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = clip_to_bounds(np.asarray(x, dtype=float).reshape(-1))\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # If no budget, return center of bounds\n    if budget <= 0:\n        return (lower + upper) / 2.0\n\n    # === 1) Warm start handling ===\n    if prev_best_x is not None:\n        prev_best_x = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if prev_best_x.size == dim:\n            evaluate(prev_best_x)\n\n    # Ensure at least one evaluation\n    if evals_used == 0:\n        x0 = np.random.uniform(lower, upper)\n        evaluate(x0)\n        if evals_used >= budget:\n            return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # === 2) Adaptive budget split: global vs local ===\n    # Slightly more exploitation for moderate/large budgets\n    if budget < 15:\n        global_frac = 0.6\n    elif budget < 60:\n        global_frac = 0.5\n    else:\n        global_frac = 0.4\n\n    n_global = max(1, int(remaining * global_frac))\n    n_local = max(0, remaining - n_global)\n\n    # === 3) Global sampling: low-discrepancy + warm-start reuse ===\n    # Use a Sobol-like sequence via incremental halton-style stratification.\n    def global_sample(i):\n        # base-2 and base-3 radical inverse for first two dims, then fallback to uniform\n        idx = i + 1\n        u = np.random.rand(dim)\n        if dim >= 1:\n            # base 2\n            f, r = 0.5, 0.0\n            n = idx\n            while n > 0:\n                r += f * (n & 1)\n                n >>= 1\n                f *= 0.5\n            u[0] = r\n        if dim >= 2:\n            # base 3\n            f, r = 1.0 / 3.0, 0.0\n            n = idx\n            while n > 0:\n                n, a = divmod(n, 3)\n                r += f * a\n                f /= 3.0\n            u[1] = r\n        return lower + u * span\n\n    for i in range(n_global):\n        if evals_used >= budget:\n            break\n        x = global_sample(i)\n        evaluate(x)\n\n    if evals_used >= budget or n_local <= 0:\n        return best_x\n\n    # === 4) Local search: adaptive covariance random search ===\n    # Work in normalized [0,1]^dim space for better scaling\n    base_step = 0.25\n    # Track a diagonal covariance in normalized space\n    cov_diag = np.ones(dim) * (base_step ** 2)\n\n    best_norm = (best_x - lower) / np.maximum(span, 1e-12)\n\n    for k in range(n_local):\n        if evals_used >= budget:\n            break\n\n        frac = (k + 1) / (n_local + 1.0)\n        # Step size decays but not too aggressively\n        step_scale = base_step * (0.7 ** frac)\n\n        # Propose candidate in normalized space\n        noise = np.random.randn(dim) * np.sqrt(cov_diag) * step_scale\n        cand_norm = best_norm + noise\n        cand_norm = np.clip(cand_norm, 0.0, 1.0)\n        cand = lower + cand_norm * span\n\n        old_best_y = best_y\n        y = evaluate(cand)\n\n        if y is not None and y < old_best_y:\n            # Successful step: move center and mildly increase variance along successful directions\n            best_norm = (best_x - lower) / np.maximum(span, 1e-12)\n            diff = cand_norm - best_norm\n            cov_diag = 0.9 * cov_diag + 0.1 * (diff ** 2 + 1e-6)\n        else:\n            # Unsuccessful step: slightly contract covariance\n            cov_diag *= 0.98\n\n    if best_x is None:\n        best_x = np.random.uniform(lower, upper)\n\n    return best_x",
    "X": "7.996956439374388 8.00122532876346 8.002085635082247 7.997833332678695"
}