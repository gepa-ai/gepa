{
    "score": -10.107626326685331,
    "Input": "Shekel05",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    bounds = np.array(config['bounds'], dtype=float)\n    dim = int(config.get('dim', len(bounds)))\n    budget = int(config.get('budget', 1))\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n    span_safe = np.where(span > 0, span, 1.0)\n\n    def clip_to_bounds(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # Robust evaluation helper with bound handling\n    def evaluate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = clip_to_bounds(np.asarray(x, dtype=float).reshape(-1))\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # If no budget, return center of bounds\n    if budget <= 0:\n        return (lower + upper) / 2.0\n\n    # === 1) Warm start handling ===\n    if prev_best_x is not None:\n        prev_best_x = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if prev_best_x.size == dim:\n            evaluate(prev_best_x)\n\n    # Ensure at least one evaluation\n    if evals_used == 0:\n        x0 = np.random.uniform(lower, upper)\n        evaluate(x0)\n        if evals_used >= budget:\n            return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # === 2) Adaptive budget split: global vs local ===\n    # Increase global exploration for multimodal landscapes like Shekel.\n    if budget < 15:\n        global_frac = 0.75\n    elif budget < 60:\n        global_frac = 0.65\n    else:\n        global_frac = 0.55\n\n    n_global = max(1, int(remaining * global_frac))\n    n_local = max(0, remaining - n_global)\n\n    # === 3) Global sampling: stratified random + mild quasi-randomness ===\n    # Use stratified sampling per dimension to cover space better.\n    def global_sample(i, total):\n        # i in [0, total-1]\n        # Create a stratified point index for each dimension with jitter\n        t = (i + np.random.rand(dim)) / max(total, 1)\n        t = np.clip(t, 0.0, 1.0)\n        return lower + t * span_safe\n\n    for i in range(n_global):\n        if evals_used >= budget:\n            break\n        x = global_sample(i, n_global)\n        evaluate(x)\n\n    if evals_used >= budget or n_local <= 0:\n        return best_x\n\n    # === 4) Local search: adaptive random search around best ===\n    # Work in normalized [0,1]^dim space for better scaling\n    best_norm = (best_x - lower) / span_safe\n    best_norm = np.clip(best_norm, 0.0, 1.0)\n\n    # Initial step size tuned for typical benchmark scales\n    base_step = 0.3\n    step = np.full(dim, base_step, dtype=float)\n\n    # Track simple success history to adapt step sizes (1/5 success rule style)\n    success_count = 0\n    total_count = 0\n\n    for k in range(n_local):\n        if evals_used >= budget:\n            break\n\n        # Smooth annealing of step magnitude over local iterations\n        frac = (k + 1) / (n_local + 1.0)\n        global_decay = 0.4 + 0.6 * (1.0 - frac)\n\n        # Propose candidate in normalized space\n        noise = np.random.randn(dim) * step * global_decay\n        cand_norm = best_norm + noise\n        cand_norm = np.clip(cand_norm, 0.0, 1.0)\n        cand = lower + cand_norm * span_safe\n\n        old_best_y = best_y\n        y = evaluate(cand)\n        total_count += 1\n\n        if y is not None and y < old_best_y:\n            # Successful move: update center\n            best_norm = (best_x - lower) / span_safe\n            best_norm = np.clip(best_norm, 0.0, 1.0)\n            success_count += 1\n        # Periodically adapt step sizes using a simplified 1/5 success rule\n        if total_count >= 5:\n            success_rate = success_count / max(total_count, 1)\n            if success_rate > 0.3:\n                step *= 1.15\n            elif success_rate < 0.15:\n                step *= 0.85\n            step = np.clip(step, 0.02, 0.5)\n            success_count = 0\n            total_count = 0\n\n    if best_x is None:\n        best_x = np.random.uniform(lower, upper)\n\n    return best_x",
    "X": "4.003526543961017 4.005528270411525 4.010382442244545 4.017590895301246"
}