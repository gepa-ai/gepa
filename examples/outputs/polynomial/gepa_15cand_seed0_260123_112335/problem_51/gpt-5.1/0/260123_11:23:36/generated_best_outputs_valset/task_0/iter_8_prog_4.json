{
    "score": -10.11026874641603,
    "Input": "Shekel05",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    bounds = np.array(config['bounds'], dtype=float)\n    dim = int(config.get('dim', len(bounds)))\n    budget = int(config.get('budget', 1))\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n    span_safe = np.where(span > 0, span, 1.0)\n\n    def clip_to_bounds(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # Robust evaluation helper with bound handling\n    def evaluate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = clip_to_bounds(np.asarray(x, dtype=float).reshape(-1))\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # If no budget, return center of bounds\n    if budget <= 0:\n        return (lower + upper) / 2.0\n\n    # === 1) Warm start handling ===\n    if prev_best_x is not None:\n        prev_best_x = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if prev_best_x.size == dim:\n            evaluate(prev_best_x)\n\n    # Ensure at least one evaluation\n    if evals_used == 0:\n        # Use quasi-random-like initial point: center + small jitter\n        center = (lower + upper) / 2.0\n        jitter = (np.random.rand(dim) - 0.5) * 0.1 * span_safe\n        x0 = clip_to_bounds(center + jitter)\n        evaluate(x0)\n        if evals_used >= budget:\n            return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # === 2) Adaptive budget split: global vs local ===\n    # Favor more global exploration for multimodal landscapes like Shekel\n    if budget < 20:\n        global_frac = 0.8\n    elif budget < 80:\n        global_frac = 0.7\n    else:\n        global_frac = 0.6\n\n    n_global = max(1, int(remaining * global_frac))\n    n_local = max(0, remaining - n_global)\n\n    # === 3) Global sampling: low-discrepancy style using scrambled offsets ===\n    # Use per-dimension random shift to reduce clustering.\n    scramble = np.random.rand(dim)\n\n    def global_sample(i, total):\n        # i in [0, total-1]\n        # Create a stratified point index for each dimension with jitter\n        base = (i + 0.5) / max(total, 1)\n        # Add per-dim scramble and small noise\n        t = base + scramble / (total + 1) + (np.random.rand(dim) - 0.5) / (total + 1)\n        t = np.mod(t, 1.0)\n        return lower + t * span_safe\n\n    # Use a small portion of global budget for a quick \"mini-local\" refinement\n    # around warm start or current best when budget is very small.\n    for i in range(n_global):\n        if evals_used >= budget:\n            break\n        x = global_sample(i, n_global)\n        evaluate(x)\n\n    if evals_used >= budget or n_local <= 0:\n        return best_x\n\n    # === 4) Local search: covariance-adaptive random search (simplified CMA flavor) ===\n    # Work in normalized [0,1]^dim space\n    best_norm = (best_x - lower) / span_safe\n    best_norm = np.clip(best_norm, 0.0, 1.0)\n\n    # Initial global step size in normalized space\n    # Smaller for higher dimensions to avoid huge jumps\n    base_sigma = 0.25 if dim <= 5 else 0.18\n    sigma = base_sigma\n\n    # Population size per iteration (lambda)\n    if dim <= 3:\n        lam = 6\n    elif dim <= 10:\n        lam = 8\n    else:\n        lam = 10\n    # Ensure we do not exceed remaining budget\n    lam = max(2, min(lam, n_local))\n\n    # Number of generations\n    max_gens = max(1, n_local // lam)\n\n    # Diagonal covariance in normalized space\n    cov = np.ones(dim, dtype=float)\n\n    successes = 0\n    evals_local = 0\n\n    for g in range(max_gens):\n        if evals_used >= budget:\n            break\n\n        # Anneal step size over generations to encourage convergence\n        frac = (g + 1) / (max_gens + 1.0)\n        gen_sigma = sigma * (0.6 + 0.4 * (1.0 - frac))\n\n        # Sample population around current best\n        # Use ranking to keep top candidates\n        cand_norms = []\n        cand_scores = []\n\n        # Early escape if no remaining budget for this generation\n        remaining_gen = budget - evals_used\n        if remaining_gen <= 0:\n            break\n        lam_eff = min(lam, remaining_gen)\n\n        for _ in range(lam_eff):\n            if evals_used >= budget:\n                break\n            # Gaussian noise with diagonal covariance, clipped to [0,1]\n            noise = np.random.randn(dim) * np.sqrt(cov) * gen_sigma\n            cand_norm = best_norm + noise\n            cand_norm = np.clip(cand_norm, 0.0, 1.0)\n            cand = lower + cand_norm * span_safe\n            y = evaluate(cand)\n            evals_local += 1\n            if y is not None:\n                cand_norms.append(cand_norm)\n                cand_scores.append(y)\n\n        if len(cand_scores) == 0:\n            break\n\n        # Rank candidates\n        idx = np.argsort(cand_scores)\n        cand_norms = np.array(cand_norms)[idx]\n        cand_scores = np.array(cand_scores)[idx]\n\n        # Move center towards the best candidates (comma-selection)\n        elite_count = max(1, len(cand_norms) // 3)\n        elites = cand_norms[:elite_count]\n        old_best_y = best_y\n        best_norm = np.mean(elites, axis=0)\n        best_norm = np.clip(best_norm, 0.0, 1.0)\n        best_x_candidate = lower + best_norm * span_safe\n        y_center = evaluate(best_x_candidate)\n\n        if y_center is not None and y_center < old_best_y:\n            successes += 1\n\n        # Update diagonal covariance based on elite spread\n        if elite_count > 1:\n            spread = np.var(elites, axis=0)\n            # Blend with previous covariance\n            cov = 0.7 * cov + 0.3 * (spread + 1e-6)\n            cov = np.clip(cov, 1e-4, 1.0)\n\n        # Adapt step size using simplified success rule every few generations\n        if (g + 1) % 3 == 0:\n            success_rate = successes / 3.0\n            if success_rate > 0.25:\n                sigma *= 1.2\n            elif success_rate < 0.15:\n                sigma *= 0.8\n            sigma = np.clip(sigma, 0.02, 0.5)\n            successes = 0\n\n    if best_x is None:\n        best_x = np.random.uniform(lower, upper)\n\n    return best_x",
    "X": "4.007514387975698 4.007238397754716 4.017280651496111 3.995371688502824"
}