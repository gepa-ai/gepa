{
    "score": -10.151838343031686,
    "Input": "Shekel05",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    bounds = np.array(config['bounds'], dtype=float)\n    dim = int(config.get('dim', len(bounds)))\n    budget = int(config.get('budget', 1))\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n    span_safe = np.where(span > 0, span, 1.0)\n\n    def clip_to_bounds(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    def evaluate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = clip_to_bounds(np.asarray(x, dtype=float).reshape(-1))\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    if budget <= 0:\n        return (lower + upper) / 2.0\n\n    # Warm start from previous run if provided\n    if prev_best_x is not None:\n        prev_best_x = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if prev_best_x.size == dim:\n            evaluate(prev_best_x)\n\n    # Ensure at least one evaluation\n    if evals_used == 0:\n        center = (lower + upper) / 2.0\n        jitter = (np.random.rand(dim) - 0.5) * 0.05 * span_safe\n        x0 = clip_to_bounds(center + jitter)\n        evaluate(x0)\n        if evals_used >= budget:\n            return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Adaptive split between global and local evaluations\n    if budget < 20:\n        global_frac = 0.8\n    elif budget < 80:\n        global_frac = 0.65\n    else:\n        global_frac = 0.55\n\n    n_global = max(1, int(remaining * global_frac))\n    n_local = max(0, remaining - n_global)\n\n    # --- Global exploration: hybrid low-discrepancy + best-centered sampling ---\n\n    scramble = np.random.rand(dim)\n\n    def global_sample(i, total):\n        base = (i + 0.5) / max(total, 1)\n        t = base + scramble / (total + 1) + (np.random.rand(dim) - 0.5) / (total + 1)\n        t = np.mod(t, 1.0)\n        return lower + t * span_safe\n\n    # Use half of global samples as space-filling, half biased around current best\n    n_global_space = n_global // 2\n    n_global_best = n_global - n_global_space\n\n    # Space-filling samples\n    for i in range(n_global_space):\n        if evals_used >= budget:\n            break\n        x = global_sample(i, max(1, n_global_space))\n        evaluate(x)\n\n    # Update remaining budget\n    if evals_used >= budget or n_global_best <= 0:\n        return best_x\n\n    # Biased global search around current best (if available)\n    if best_x is not None:\n        best_norm = (best_x - lower) / span_safe\n        best_norm = np.clip(best_norm, 0.0, 1.0)\n        # Radius shrinks mildly with remaining evals to mix exploration/exploitation\n        base_radius = 0.35 if dim <= 5 else 0.25\n        for _ in range(n_global_best):\n            if evals_used >= budget:\n                break\n            radius = base_radius * (0.6 + 0.4 * np.random.rand())\n            direction = np.random.randn(dim)\n            direction /= (np.linalg.norm(direction) + 1e-12)\n            step = direction * radius * np.random.rand()\n            cand_norm = np.clip(best_norm + step, 0.0, 1.0)\n            cand = lower + cand_norm * span_safe\n            evaluate(cand)\n\n    remaining = budget - evals_used\n    if remaining <= 0 or n_local <= 0:\n        return best_x\n\n    # --- Local refinement: simplified diagonal CMA-ES style search ---\n\n    # Work in normalized coordinates\n    best_norm = (best_x - lower) / span_safe\n    best_norm = np.clip(best_norm, 0.0, 1.0)\n\n    # Step size tuned more conservatively for multimodal Shekel-like functions\n    if dim <= 3:\n        base_sigma = 0.22\n    elif dim <= 10:\n        base_sigma = 0.18\n    else:\n        base_sigma = 0.15\n    sigma = base_sigma\n\n    # Population size\n    if dim <= 3:\n        lam = 8\n    elif dim <= 10:\n        lam = 10\n    else:\n        lam = 12\n    lam = max(3, min(lam, n_local))\n\n    max_gens = max(1, n_local // lam)\n\n    cov = np.ones(dim, dtype=float)\n    successes = 0\n\n    for g in range(max_gens):\n        if evals_used >= budget:\n            break\n\n        remaining_gen = budget - evals_used\n        if remaining_gen <= 0:\n            break\n        lam_eff = min(lam, remaining_gen)\n\n        # Generation-dependent sigma annealing\n        frac = (g + 1) / (max_gens + 1.0)\n        gen_sigma = sigma * (0.7 + 0.3 * (1.0 - frac))\n\n        cand_norms = []\n        cand_scores = []\n\n        # Always include current center once to avoid drifting away too much\n        center_eval_done = False\n\n        for i in range(lam_eff):\n            if evals_used >= budget:\n                break\n            if not center_eval_done and i == 0:\n                cand_norm = best_norm.copy()\n                center_eval_done = True\n            else:\n                noise = np.random.randn(dim) * np.sqrt(cov) * gen_sigma\n                cand_norm = np.clip(best_norm + noise, 0.0, 1.0)\n\n            cand = lower + cand_norm * span_safe\n            y = evaluate(cand)\n            if y is not None:\n                cand_norms.append(cand_norm)\n                cand_scores.append(y)\n\n        if len(cand_scores) == 0:\n            break\n\n        cand_norms = np.array(cand_norms)\n        cand_scores = np.array(cand_scores)\n        idx = np.argsort(cand_scores)\n        cand_norms = cand_norms[idx]\n        cand_scores = cand_scores[idx]\n\n        elite_count = max(1, len(cand_norms) // 3)\n        elites = cand_norms[:elite_count]\n        old_best_y = best_y\n\n        # New center: weighted mean of elites (slightly more weight to best)\n        weights = np.linspace(1.0, 0.5, elite_count)\n        weights /= weights.sum()\n        best_norm = np.clip(np.average(elites, axis=0, weights=weights), 0.0, 1.0)\n\n        # Evaluate updated center explicitly\n        center_x = lower + best_norm * span_safe\n        y_center = evaluate(center_x)\n\n        if y_center is not None and y_center < old_best_y:\n            successes += 1\n\n        # Update diagonal covariance using elite spread\n        if elite_count > 1:\n            spread = np.var(elites, axis=0)\n            cov = 0.6 * cov + 0.4 * (spread + 1e-6)\n            cov = np.clip(cov, 1e-4, 1.0)\n\n        # Step-size adaptation every few generations\n        if (g + 1) % 3 == 0:\n            success_rate = successes / 3.0\n            if success_rate > 0.3:\n                sigma *= 1.25\n            elif success_rate < 0.15:\n                sigma *= 0.8\n            sigma = np.clip(sigma, 0.03, 0.45)\n            successes = 0\n\n    if best_x is None:\n        best_x = np.random.uniform(lower, upper)\n\n    return best_x",
    "X": "4.0027365165123845 4.000910836876227 4.000021068866822 4.001113532980246"
}