{
    "score": 0.17504902432558822,
    "Input": "Schwefel20",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    bounds = np.array(config['bounds'], dtype=float)\n    dim = int(config.get('dim', bounds.shape[0]))\n    budget = int(config.get('budget', 1))\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # Evaluate a point, tracking budget\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # 1) Warm start if available and within bounds\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape == (dim,):\n            x0 = clip(x0)\n            eval_point(x0)\n            if evals_used >= budget:\n                return best_x\n\n    # 2) Global random / quasi-Latin sampling phase\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x if best_x is not None else clip(np.random.uniform(low, high))\n\n    # Use a reasonable number of global samples; keep some budget for local search\n    n_global = max(1, int(0.4 * remaining))\n    # Simple stratified sampling per dimension (cheap LHS-like)\n    rng = np.random.default_rng()\n    samples = rng.random((n_global, dim))\n    # Shuffle per-dim to decorrelate\n    for d in range(dim):\n        rng.shuffle(samples[:, d])\n    X_global = low + samples * span\n\n    for i in range(n_global):\n        if evals_used >= budget:\n            break\n        eval_point(X_global[i])\n\n    if evals_used >= budget:\n        return best_x\n\n    if best_x is None:\n        # Should not happen, but be robust\n        best_x = clip(np.random.uniform(low, high))\n        best_y = eval_point(best_x)\n        if evals_used >= budget:\n            return best_x\n\n    # 3) Local search around best solution found so far: adaptive coordinate search\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    x = best_x.copy()\n    # Initial step size proportional to bounds\n    step = 0.2 * span\n    step[step == 0] = 1.0  # fallback if zero span\n\n    # Number of local iterations bounded by remaining budget\n    # Each iteration can cost up to 2*dim evaluations (forward/backward per dim)\n    max_iters = max(1, remaining // (2 * dim))\n\n    for _ in range(max_iters):\n        improved = False\n        for d in range(dim):\n            if evals_used >= budget:\n                break\n\n            # Try positive step\n            x_pos = x.copy()\n            x_pos[d] = x[d] + step[d]\n            x_pos = clip(x_pos)\n            y_pos = eval_point(x_pos)\n            if evals_used >= budget:\n                break\n\n            if y_pos is not None and y_pos < best_y:\n                x = x_pos\n                improved = True\n                continue  # try same direction next dim\n\n            # Try negative step\n            if evals_used >= budget:\n                break\n            x_neg = x.copy()\n            x_neg[d] = x[d] - step[d]\n            x_neg = clip(x_neg)\n            y_neg = eval_point(x_neg)\n\n            if y_neg is not None and y_neg < best_y:\n                x = x_neg\n                improved = True\n\n        if evals_used >= budget:\n            break\n\n        # If no improvement this iteration, reduce step size\n        if not improved:\n            step *= 0.5\n            # If steps are very small, stop local search\n            if np.all(step < 1e-6 * np.maximum(span, 1.0)):\n                break\n\n    # Ensure we return something valid\n    if best_x is None:\n        best_x = clip(np.random.uniform(low, high))\n    return best_x",
    "X": "0.09523600031884882 -0.0798130240067394"
}