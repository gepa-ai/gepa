{
    "score": 1.7446844270114284e-05,
    "Input": "Schwefel20",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    bounds = np.array(config['bounds'], dtype=float)\n    dim = int(config.get('dim', bounds.shape[0]))\n    budget = int(config.get('budget', 1))\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    # Fallback span for degenerate bounds, but still respect clip\n    span_safe = np.where(span > 0, span, 1.0)\n\n    rng = np.random.default_rng()\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = np.asarray(x, dtype=float).reshape(dim,)\n        x = clip(x)\n        try:\n            y = objective_function(x)\n        except Exception:\n            # Robustness: if a point fails, treat as very bad and continue\n            y = np.inf\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # Early exit if no budget\n    if budget <= 0:\n        if prev_best_x is not None:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim,)\n            return clip(x0)\n        return clip(rng.uniform(low, high))\n\n    # 1) Warm start if available\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(dim,)\n        eval_point(x0)\n        if evals_used >= budget:\n            return best_x if best_x is not None else clip(x0)\n\n    if evals_used >= budget:\n        if best_x is None:\n            best_x = clip(rng.uniform(low, high))\n        return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        if best_x is None:\n            best_x = clip(rng.uniform(low, high))\n        return best_x\n\n    # 2) GLOBAL EXPLORATION with quasi-random / LHS-like coverage\n    if dim <= 5:\n        global_frac = 0.35\n    elif dim <= 20:\n        global_frac = 0.5  # slightly more global in moderate dim\n    else:\n        global_frac = 0.65  # high-dim needs more exploration\n\n    n_global = max(1, int(global_frac * remaining))\n    n_global = min(n_global, remaining)\n\n    if n_global > 0:\n        # Latin-hypercube-like randomization for better coverage\n        samples = rng.random((n_global, dim))\n        for d in range(dim):\n            rng.shuffle(samples[:, d])\n        X_global = low + samples * span_safe\n\n        for i in range(n_global):\n            if evals_used >= budget:\n                break\n            eval_point(X_global[i])\n\n    if evals_used >= budget:\n        return best_x if best_x is not None else clip(rng.uniform(low, high))\n\n    # Ensure we have an incumbent\n    if best_x is None:\n        x_init = clip(rng.uniform(low, high))\n        eval_point(x_init)\n        if evals_used >= budget:\n            return best_x if best_x is not None else x_init\n\n    # 3) EVOLUTION STRATEGY (NES-style) AROUND BEST\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Population size tuned to use budget smoothly\n    if dim <= 5:\n        base_pop = 20\n    elif dim <= 20:\n        base_pop = 32\n    else:\n        base_pop = 48\n\n    max_generations_target = 16\n    base_pop = min(base_pop, max(4, remaining // max(1, max_generations_target)))\n    pop_size = max(4, base_pop)\n\n    elite_frac = 0.25\n    n_elite = max(1, int(elite_frac * pop_size))\n\n    # ES uses a significant chunk but leaves evaluations for local search\n    es_budget = int(0.75 * remaining)\n    es_budget = max(pop_size * 3, es_budget)\n    es_budget = min(es_budget, remaining)\n\n    if es_budget > 0 and pop_size > 1:\n        mean = best_x.copy()\n\n        # Use scale relative to bounds; smaller for large dim to avoid too aggressive jumps\n        dim_scale = np.sqrt(dim) if dim > 0 else 1.0\n        initial_sigma = 0.5 * span_safe / dim_scale\n        initial_sigma = np.maximum(initial_sigma, 1e-3 * span_safe)\n        sigma = np.minimum(initial_sigma, span_safe)\n        min_sigma = 1e-7 * span_safe\n        max_sigma = span_safe\n\n        no_improve_iters = 0\n        last_best_y = best_y\n        es_iters = 0\n\n        while evals_used < budget and es_budget > 0:\n            cur_pop = min(pop_size, es_budget, budget - evals_used)\n            if cur_pop <= 0:\n                break\n\n            # Antithetic sampling for variance reduction\n            half = (cur_pop + 1) // 2\n            z = rng.standard_normal((half, dim))\n            z = np.vstack([z, -z])[:cur_pop]\n            offspring = mean + sigma * z\n            offspring = clip(offspring)\n\n            ys = []\n            for i in range(cur_pop):\n                if evals_used >= budget or es_budget <= 0:\n                    break\n                y = eval_point(offspring[i])\n                es_budget -= 1\n                if y is not None:\n                    ys.append((y, offspring[i].copy()))\n\n            if evals_used >= budget or len(ys) == 0:\n                break\n\n            ys.sort(key=lambda t: t[0])\n            elites = np.array([t[1] for t in ys[:n_elite]])\n\n            new_mean = elites.mean(axis=0)\n            elite_spread = np.std(elites - new_mean, axis=0)\n\n            # Conservative sigma adaptation: mix with elite spread\n            sigma = 0.6 * sigma + 0.4 * np.maximum(elite_spread, min_sigma)\n            sigma = np.clip(sigma, min_sigma, max_sigma)\n            mean = new_mean\n            es_iters += 1\n\n            # Improvement tracking\n            if best_y < last_best_y - 1e-12:\n                last_best_y = best_y\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n\n            # Mild restart / diversification on stagnation\n            if no_improve_iters >= 5 and es_budget > 0:\n                # a) random global probes\n                n_global_jumps = min(4, es_budget)\n                if n_global_jumps > 0:\n                    rand_points = low + rng.random((n_global_jumps, dim)) * span_safe\n                    for j in range(n_global_jumps):\n                        if evals_used >= budget or es_budget <= 0:\n                            break\n                        _ = eval_point(rand_points[j])\n                        es_budget -= 1\n\n                # b) local-best-centered jumps\n                jump_radius = 0.4 * span_safe\n                n_jumps = min(4, es_budget)\n                for _ in range(n_jumps):\n                    if evals_used >= budget or es_budget <= 0:\n                        break\n                    cand = best_x + rng.uniform(-jump_radius, jump_radius, size=dim)\n                    cand = clip(cand)\n                    _ = eval_point(cand)\n                    es_budget -= 1\n\n                # Re-center on best and moderately re-expand sigma\n                mean = best_x.copy()\n                sigma = np.minimum(1.8 * sigma + 0.1 * span_safe / dim_scale, max_sigma)\n                no_improve_iters = 0\n\n            # Safety re-expansion if ES collapsed too early\n            if np.all(sigma < 1e-7 * span_safe) and es_iters < 6 and es_budget > pop_size:\n                sigma = np.maximum(0.2 * span_safe / dim_scale, min_sigma)\n\n    if evals_used >= budget:\n        return best_x\n\n    # 4) LOCAL SEARCH: adaptive coordinate pattern search\n    remaining = budget - evals_used\n    if remaining <= 0 or best_x is None:\n        if best_x is None:\n            best_x = clip(rng.uniform(low, high))\n        return best_x\n\n    x = best_x.copy()\n\n    # Step size relative to span; slightly smaller initial step for rugged functions\n    step = 0.06 * span_safe\n    step = np.minimum(step, 0.25 * span_safe)\n    step[span == 0] = 0.0\n\n    if dim > 0:\n        max_iters = max(1, remaining // (2 * dim))\n    else:\n        max_iters = 1\n\n    no_improve_iters = 0\n\n    for _ in range(max_iters):\n        if evals_used >= budget:\n            break\n        improved = False\n\n        coords = np.arange(dim)\n        rng.shuffle(coords)\n\n        for d in coords:\n            if evals_used >= budget:\n                break\n            if step[d] == 0.0:\n                continue\n\n            # Positive direction\n            x_pos = x.copy()\n            x_pos[d] = x[d] + step[d]\n            x_pos = clip(x_pos)\n            y_pos = eval_point(x_pos)\n\n            if evals_used >= budget:\n                break\n\n            if y_pos is not None and y_pos < best_y:\n                x = x_pos\n                improved = True\n                continue\n\n            # Negative direction\n            x_neg = x.copy()\n            x_neg[d] = x[d] - step[d]\n            x_neg = clip(x_neg)\n            y_neg = eval_point(x_neg)\n\n            if y_neg is not None and y_neg < best_y:\n                x = x_neg\n                improved = True\n\n            if evals_used >= budget:\n                break\n\n        if evals_used >= budget:\n            break\n\n        if not improved:\n            step *= 0.5\n            no_improve_iters += 1\n            if np.all(step < 1e-7 * span_safe):\n                break\n\n            # Occasional restart or broadened search to escape local minima\n            if no_improve_iters >= 3 and evals_used < budget:\n                if rng.random() < 0.5:\n                    # global restart\n                    x_restart = low + rng.random(dim) * span_safe\n                else:\n                    # broader neighborhood around best_x\n                    radius = np.maximum(0.2 * span_safe, step)\n                    noise = rng.uniform(-radius, radius, size=dim)\n                    x_restart = clip(best_x + noise)\n                y_restart = eval_point(x_restart)\n                if y_restart is not None and y_restart < best_y:\n                    x = x_restart\n                no_improve_iters = 0\n        else:\n            no_improve_iters = 0\n            # Mild step increase when improving to accelerate convergence\n            step *= 1.15\n            step = np.minimum(step, 0.3 * span_safe)\n\n    if best_x is None:\n        best_x = clip(rng.uniform(low, high))\n    return best_x",
    "X": "-9.624399975342609e-06 7.822444294771676e-06"
}