{
    "score": -3.6561381980880316,
    "Input": "Michalewicz",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid black-box minimization with adaptive global/local search.\n\n    Strategy:\n    - Warm-start from prev_best_x when available.\n    - Initial evaluation at starting point.\n    - Global low-discrepancy (Halton) sampling.\n    - Local adaptive random search around best.\n    - Final mixed global/local phase with adaptive step sizes.\n\n    Designed to:\n    - Use (almost) full evaluation budget.\n    - Be robust across dimensions and landscapes.\n    \"\"\"\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config.get(\"budget\", 1))\n\n    low, high = bounds[:, 0], bounds[:, 1]\n    width = high - low\n\n    # Fallback if budget non-positive\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    rng = np.random.default_rng()\n\n    # -------- Initialization / warm start --------\n    x0 = None\n    if prev_best_x is not None:\n        try:\n            cand = np.asarray(prev_best_x, dtype=float).reshape(-1)\n            if cand.shape[0] == dim:\n                x0 = clip_to_bounds(cand)\n        except Exception:\n            x0 = None\n\n    if x0 is None:\n        x0 = rng.uniform(low, high)\n\n    evals_used = 0\n    best_x = x0\n    best_y = objective_function(best_x)\n    evals_used += 1\n\n    # -------- Budget partitioning --------\n    # Emphasize global for deceptive problems, but keep enough local.\n    global_frac = 0.4\n    local_frac = 0.3 if prev_best_x is not None else 0.25\n    final_frac = 1.0 - global_frac - local_frac\n    if final_frac < 0.1:\n        scale = (1.0 - 0.1) / (global_frac + local_frac + 1e-12)\n        global_frac *= scale\n        local_frac *= scale\n        final_frac = 0.1\n\n    global_budget = int(budget * global_frac)\n    local_budget = int(budget * local_frac)\n\n    # Ensure we do not exceed budget (subtract 1 already used)\n    max_alloc = max(budget - 1, 0)\n    if global_budget + local_budget > max_alloc:\n        excess = global_budget + local_budget - max_alloc\n        reduce_local = min(excess, local_budget)\n        local_budget -= reduce_local\n        excess -= reduce_local\n        if excess > 0:\n            global_budget = max(0, global_budget - excess)\n\n    # -------- Halton sequence utilities --------\n    def _van_der_corput_single(index, base):\n        vdc = 0.0\n        denom = 1.0\n        while index > 0:\n            index, remainder = divmod(index, base)\n            denom *= base\n            vdc += remainder / denom\n        return vdc\n\n    def halton_points(n_points, dim_, scramble=True):\n        if n_points <= 0:\n            return np.empty((0, dim_), dtype=float)\n        # First primes as bases\n        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n                  31, 37, 41, 43, 47, 53, 59, 61, 67, 71,\n                  73, 79, 83, 89, 97, 101]\n        # If dimension exceeds available primes, wrap around\n        bases = [primes[i % len(primes)] for i in range(max(dim_, 1))]\n\n        samples = np.empty((n_points, dim_), dtype=float)\n        idxs = np.arange(1, n_points + 1, dtype=int)\n\n        for d in range(dim_):\n            base = bases[d]\n            v = np.empty(n_points, dtype=float)\n            for i, idx in enumerate(idxs):\n                v[i] = _van_der_corput_single(idx, base)\n            if scramble:\n                shift = rng.random()\n                v = (v + shift) % 1.0\n            samples[:, d] = v\n        return samples\n\n    # -------- Phase 1: Global exploration --------\n    n = min(global_budget, max(0, budget - evals_used))\n    if n > 0:\n        if n >= 2 * dim:\n            u = halton_points(n, dim)\n            xs = low + u * width\n            # Mild jitter to avoid lattice artifacts\n            jitter_scale = 0.01\n            xs += rng.normal(scale=jitter_scale * width, size=(n, dim))\n            xs = clip_to_bounds(xs)\n        else:\n            xs = rng.uniform(low, high, size=(n, dim))\n\n        for i in range(n):\n            if evals_used >= budget:\n                break\n            x = xs[i]\n            y = objective_function(x)\n            evals_used += 1\n            if y < best_y:\n                best_y = y\n                best_x = x.copy()\n\n    # -------- Phase 2: Local refinement (adaptive random search) --------\n    n = min(local_budget, max(0, budget - evals_used))\n    if n > 0:\n        base_radius = 0.25 * width\n        min_radius = np.maximum(0.005 * width, 1e-12)\n        max_radius = 0.5 * width\n        radius = base_radius.copy()\n        success_count = 0\n        fail_count = 0\n\n        for _ in range(n):\n            if evals_used >= budget:\n                break\n            step = rng.normal(scale=radius, size=dim)\n            candidate = clip_to_bounds(best_x + step)\n            y = objective_function(candidate)\n            evals_used += 1\n\n            if y < best_y:\n                best_y = y\n                best_x = candidate\n                success_count += 1\n                fail_count = 0\n                if success_count >= 2:\n                    radius = np.minimum(max_radius, radius * 1.1)\n                    success_count = 0\n            else:\n                fail_count += 1\n                if fail_count >= 3:\n                    radius = np.maximum(min_radius, radius * 0.5)\n                    fail_count = 0\n\n    # -------- Phase 3: Mixed adaptive search --------\n    n = max(0, budget - evals_used)\n    if n > 0:\n        local_radius = 0.1 * width\n        global_radius = 0.5 * width\n        min_local_radius = np.maximum(0.01 * width, 1e-12)\n        min_global_radius = np.maximum(0.2 * width, 1e-12)\n        max_local_radius = 0.3 * width\n        max_global_radius = width\n\n        no_improve = 0\n        for k in range(n):\n            if evals_used >= budget:\n                break\n            t = k / max(1, n - 1)\n\n            # Time-varying global probability; increased if stagnating\n            global_prob = 0.8 - 0.6 * t  # from 0.8 down to 0.2\n            global_prob = np.clip(global_prob, 0.2, 0.8)\n            if no_improve > 10:\n                global_prob = max(global_prob, 0.6)\n\n            if rng.random() < global_prob:\n                # Global-biased proposal\n                center = (1.0 - t) * best_x + t * (low + high) * 0.5\n                step = rng.normal(scale=global_radius, size=dim)\n                candidate = clip_to_bounds(center + step)\n            else:\n                # Local exploitation\n                step = rng.normal(scale=local_radius, size=dim)\n                candidate = clip_to_bounds(best_x + step)\n\n            y = objective_function(candidate)\n            evals_used += 1\n\n            if y < best_y:\n                best_y = y\n                best_x = candidate\n                no_improve = 0\n                # Exploit success: reduce radii mildly\n                local_radius = np.maximum(min_local_radius, local_radius * 0.9)\n                global_radius = np.maximum(min_global_radius, global_radius * 0.95)\n            else:\n                no_improve += 1\n                # On periodic stagnation, expand radii (escape local minima)\n                if no_improve % 15 == 0:\n                    local_radius = np.minimum(max_local_radius, local_radius * 1.3)\n                    global_radius = np.minimum(max_global_radius, global_radius * 1.3)\n\n    return np.asarray(best_x, dtype=float).reshape(dim,)",
    "X": "2.2050713130813575 1.5682988769208746 1.2821408549990394 1.11232589196723"
}