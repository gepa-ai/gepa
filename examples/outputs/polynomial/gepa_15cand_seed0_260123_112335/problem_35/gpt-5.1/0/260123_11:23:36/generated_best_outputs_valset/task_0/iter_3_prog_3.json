{
    "score": -3.6553243214001925,
    "Input": "Michalewicz",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Improved hybrid black-box minimization.\n\n    Key changes vs previous version:\n    - Corrected Halton implementation (true Van der Corput sequence).\n    - Budget handling is stricter and simpler; all remaining evaluations are used.\n    - Local search is converted into a simple covariance-adapting random search\n      around the best point to better exploit structure.\n    - Warm-start is integrated into both initialization and local search.\n    \"\"\"\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config.get(\"budget\", 1))\n\n    low, high = bounds[:, 0], bounds[:, 1]\n    width = high - low\n\n    # Ensure at least one evaluation\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    rng = np.random.default_rng()\n\n    # -------- Initialization: pick starting point --------\n    x0 = None\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.shape[0] != dim:\n            x0 = None\n        else:\n            x0 = clip_to_bounds(x0)\n\n    if x0 is None:\n        x0 = rng.uniform(low, high)\n\n    evals_used = 0\n    best_x = x0\n    best_y = objective_function(best_x)\n    evals_used += 1\n\n    # -------- Budget splitting --------\n    # Slightly more global search for robustness on deceptive landscapes.\n    base_global_frac = 0.4\n    base_local_frac = 0.3 if prev_best_x is not None else 0.25\n    base_final_frac = 1.0 - base_global_frac - base_local_frac\n    if base_final_frac < 0.1:\n        scale = (1.0 - 0.1) / (base_global_frac + base_local_frac + 1e-12)\n        base_global_frac *= scale\n        base_local_frac *= scale\n        base_final_frac = 0.1\n\n    global_budget = int(budget * base_global_frac)\n    local_budget = int(budget * base_local_frac)\n    # Ensure at least 1 eval already consumed, adjust if necessary\n    if global_budget + local_budget > budget - 1:\n        excess = global_budget + local_budget - (budget - 1)\n        reduce_local = min(excess, local_budget)\n        local_budget -= reduce_local\n        excess -= reduce_local\n        if excess > 0:\n            global_budget = max(0, global_budget - excess)\n\n    # Final phase gets all remaining evaluations\n    # (we don't pre-allocate to avoid rounding problems)\n    # -------- Helper: proper Halton sequence sampler --------\n    def _van_der_corput_single(index, base):\n        vdc = 0.0\n        denom = 1.0\n        while index > 0:\n            index, remainder = divmod(index, base)\n            denom *= base\n            vdc += remainder / denom\n        return vdc\n\n    def halton_points(n_points, dim_, scramble=True):\n        # first few primes as bases\n        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n                  31, 37, 41, 43, 47, 53, 59, 61, 67, 71]\n        bases = primes[:max(dim_, 1)]\n        samples = np.empty((n_points, dim_), dtype=float)\n        # generate indices 1..n_points to avoid the all-zero start\n        idxs = np.arange(1, n_points + 1)\n        for d in range(dim_):\n            base = bases[d if d < len(bases) else -1]\n            v = np.empty(n_points, dtype=float)\n            for i, idx in enumerate(idxs):\n                v[i] = _van_der_corput_single(idx, base)\n            if scramble:\n                shift = rng.random()\n                v = (v + shift) % 1.0\n            samples[:, d] = v\n        return samples\n\n    # -------- Phase 1: global space-filling exploration --------\n    n = min(global_budget, budget - evals_used)\n    if n > 0:\n        if n >= 2 * dim:\n            u = halton_points(n, dim)\n            xs = low + u * width\n            xs += rng.normal(scale=0.01 * width, size=(n, dim))\n            xs = clip_to_bounds(xs)\n        else:\n            xs = rng.uniform(low, high, size=(n, dim))\n        for i in range(n):\n            if evals_used >= budget:\n                break\n            y = objective_function(xs[i])\n            evals_used += 1\n            if y < best_y:\n                best_y = y\n                best_x = xs[i].copy()\n\n    # -------- Phase 2: local refinement around best (adaptive random search) --------\n    n = min(local_budget, budget - evals_used)\n    if n > 0:\n        # Start radius relative to domain, adapt based on success\n        base_radius = 0.25 * width\n        min_radius = 0.01 * width\n        max_radius = 0.5 * width\n        radius = np.copy(base_radius)\n        success_count = 0\n        fail_count = 0\n        for k in range(n):\n            if evals_used >= budget:\n                break\n            # Propose candidate around current best using Gaussian steps\n            step = rng.normal(scale=radius, size=dim)\n            candidate = clip_to_bounds(best_x + step)\n            y = objective_function(candidate)\n            evals_used += 1\n            if y < best_y:\n                best_y = y\n                best_x = candidate\n                success_count += 1\n                fail_count = 0\n                # Slightly increase radius on success to explore more\n                radius = np.minimum(max_radius, radius * 1.1)\n            else:\n                fail_count += 1\n                # Shrink radius after several failures\n                if fail_count >= 3:\n                    radius = np.maximum(min_radius, radius * 0.5)\n                    fail_count = 0\n\n    # -------- Phase 3: adaptive exploitation / exploration --------\n    # Use all remaining budget here.\n    n = max(0, budget - evals_used)\n    if n > 0:\n        # Mix local and global proposals; bias over time depending on improvements.\n        local_radius = 0.1 * width\n        global_radius = 0.5 * width\n        no_improve = 0\n        for k in range(n):\n            if evals_used >= budget:\n                break\n            t = k / max(1, n - 1)\n            # Early iterations: more global. Later: more local unless stagnating.\n            global_prob = max(0.2, 0.8 - 0.6 * t)\n            # If no improvement for a while, temporarily increase global moves\n            if no_improve > 10:\n                global_prob = 0.7\n            if rng.random() < global_prob:\n                # Global-biased step\n                center = (1 - t) * best_x + t * (low + high) * 0.5\n                step = rng.normal(scale=global_radius, size=dim)\n                candidate = clip_to_bounds(center + step)\n            else:\n                # Local exploitation around best\n                step = rng.normal(scale=local_radius, size=dim)\n                candidate = clip_to_bounds(best_x + step)\n\n            y = objective_function(candidate)\n            evals_used += 1\n            if y < best_y:\n                best_y = y\n                best_x = candidate\n                no_improve = 0\n                # Slightly adapt radii\n                local_radius = np.maximum(0.02 * width, local_radius * 0.9)\n                global_radius = np.maximum(0.2 * width, global_radius * 0.95)\n            else:\n                no_improve += 1\n                if no_improve % 15 == 0:\n                    # Expand search to escape local minima\n                    local_radius = np.minimum(0.2 * width, local_radius * 1.3)\n                    global_radius = np.minimum(width, global_radius * 1.3)\n\n    return best_x.reshape(dim,)",
    "X": "2.1978535845061216 1.5737975740341752 1.2826882067520544 1.1168354441937753"
}