{
    "score": -3.632884408474349,
    "Input": "Michalewicz",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid black-box minimization using:\n      - Sobol-like low-discrepancy initialization (via scrambled Halton fallback)\n      - Adaptive global random search\n      - Local refinement around the current best (and warm start if provided)\n\n    Strategy:\n    - Use ~30% of budget for global space-filling exploration.\n    - Allocate ~30% for focused local search (stronger if prev_best_x is given).\n    - Use remaining budget for adaptive mixed global/local exploitation.\n    \"\"\"\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config.get(\"budget\", 1))\n\n    low, high = bounds[:, 0], bounds[:, 1]\n    width = high - low\n\n    # Ensure at least one evaluation\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    rng = np.random.default_rng()\n\n    # -------- Initialization: pick starting point --------\n    x0 = None\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.shape[0] != dim:\n            x0 = None\n        else:\n            x0 = clip_to_bounds(x0)\n\n    if x0 is None:\n        x0 = rng.uniform(low, high)\n\n    evals_used = 0\n    best_x = x0\n    best_y = objective_function(best_x)\n    evals_used += 1\n\n    # -------- Budget splitting --------\n    base_global_frac = 0.3\n    base_local_frac = 0.3 if prev_best_x is not None else 0.2\n    base_final_frac = 1.0 - base_global_frac - base_local_frac\n    if base_final_frac < 0.1:\n        # ensure some budget for final phase\n        scale = (1.0 - 0.1) / (base_global_frac + base_local_frac + 1e-12)\n        base_global_frac *= scale\n        base_local_frac *= scale\n        base_final_frac = 0.1\n\n    global_budget = int(budget * base_global_frac)\n    local_budget = int(budget * base_local_frac)\n    if global_budget + local_budget > budget - 1:\n        excess = global_budget + local_budget - (budget - 1)\n        reduce_local = min(excess, local_budget)\n        local_budget -= reduce_local\n        excess -= reduce_local\n        if excess > 0:\n            global_budget = max(0, global_budget - excess)\n\n    final_budget = max(0, budget - evals_used - global_budget - local_budget)\n\n    # -------- Helper: simple Halton sequence sampler --------\n    # This is a lightweight low-discrepancy sampler to replace naive uniform-only.\n    def _van_der_corput(n, base):\n        seq = np.zeros(n)\n        denom = 1.0\n        while n:\n            n, remainder = divmod(n, base)\n            denom *= base\n            seq += remainder / denom\n        return seq\n\n    def halton_points(n_points, dim_, scramble=True):\n        # first few primes as bases\n        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n                  31, 37, 41, 43, 47, 53, 59, 61, 67, 71]\n        bases = primes[:dim_]\n        samples = np.empty((n_points, dim_), dtype=float)\n        for d in range(dim_):\n            base = bases[d if d < len(bases) else -1]\n            # indices 1..n_points for better dispersion\n            idxs = np.arange(1, n_points + 1)\n            vdc = np.zeros(n_points)\n            # compute Van der Corput in base\n            denom = 1.0\n            while np.any(idxs):\n                idxs, remainder = divmod(idxs, base)\n                denom *= base\n                vdc += remainder / denom\n            if scramble:\n                shift = rng.random()\n                vdc = (vdc + shift) % 1.0\n            samples[:, d] = vdc\n        return samples\n\n    # -------- Phase 1: global space-filling exploration --------\n    n = min(global_budget, budget - evals_used)\n    if n > 0:\n        if n >= 2 * dim:\n            # low-discrepancy + small jitter\n            u = halton_points(n, dim)\n            xs = low + u * width\n            xs += rng.normal(scale=0.01 * width, size=(n, dim))\n            xs = clip_to_bounds(xs)\n        else:\n            xs = rng.uniform(low, high, size=(n, dim))\n        for i in range(n):\n            if evals_used >= budget:\n                break\n            y = objective_function(xs[i])\n            evals_used += 1\n            if y < best_y:\n                best_y = y\n                best_x = xs[i].copy()\n\n    # -------- Phase 2: local refinement around best (and warm start) --------\n    n = min(local_budget, budget - evals_used)\n    if n > 0:\n        # initial radius scaled with box width, shrinks over evaluations\n        base_radius = 0.25 * width\n        min_radius = 0.02 * width\n        failures = 0\n        for k in range(n):\n            if evals_used >= budget:\n                break\n            t = k / max(1, n - 1)\n            radius = (1 - t) * base_radius + t * min_radius\n            # adapt radius a bit based on recent failures\n            adapt_scale = 1.0 / (1.0 + 0.1 * failures)\n            step_radius = radius * adapt_scale\n            # sample normal direction\n            direction = rng.normal(size=dim)\n            norm = np.linalg.norm(direction)\n            if norm == 0:\n                direction = rng.normal(size=dim)\n                norm = np.linalg.norm(direction) + 1e-12\n            direction /= norm\n            candidate = best_x + direction * step_radius\n            candidate = clip_to_bounds(candidate)\n            y = objective_function(candidate)\n            evals_used += 1\n            if y < best_y:\n                best_y = y\n                best_x = candidate\n                failures = 0\n            else:\n                failures += 1\n\n    # -------- Phase 3: adaptive exploitation / exploration around best --------\n    n = max(0, budget - evals_used)\n    if n > 0:\n        # mix local and global moves; gradually bias toward more global\n        local_scale_start = 0.05\n        local_scale_end = 0.2\n        global_scale_start = 0.3\n        global_scale_end = 0.7\n        for k in range(n):\n            if evals_used >= budget:\n                break\n            t = k / max(1, n - 1)\n            # interpolate scales\n            local_scale = (1 - t) * local_scale_start + t * local_scale_end\n            global_scale = (1 - t) * global_scale_start + t * global_scale_end\n            if rng.random() < 0.6:\n                # local-ish step around best\n                step_radius = local_scale * width\n                direction = rng.normal(size=dim)\n                norm = np.linalg.norm(direction)\n                if norm == 0:\n                    direction = rng.normal(size=dim)\n                    norm = np.linalg.norm(direction) + 1e-12\n                direction /= norm\n                candidate = best_x + direction * step_radius\n            else:\n                # global-biased step\n                center = (1 - t) * best_x + t * (low + high) * 0.5\n                step_radius = global_scale * width\n                direction = rng.normal(size=dim)\n                norm = np.linalg.norm(direction)\n                if norm == 0:\n                    direction = rng.normal(size=dim)\n                    norm = np.linalg.norm(direction) + 1e-12\n                direction /= norm\n                candidate = center + direction * step_radius\n            candidate = clip_to_bounds(candidate)\n            y = objective_function(candidate)\n            evals_used += 1\n            if y < best_y:\n                best_y = y\n                best_x = candidate\n\n    return best_x.reshape(dim,)",
    "X": "2.2024560213627264 1.5903410806270581 1.2731352577637205 1.1109988163741051"
}