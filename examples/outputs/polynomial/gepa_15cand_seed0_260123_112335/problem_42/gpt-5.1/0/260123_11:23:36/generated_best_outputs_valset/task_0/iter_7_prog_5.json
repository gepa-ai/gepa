{
    "score": 2.60810370981508e-14,
    "Input": "Pinter",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization using an adaptive hybrid global-local strategy.\n\n    - Quasi-random global sampling (Halton-like / scrambled VdC) with randomness\n    - Elite-based stochastic refinement around promising regions\n    - Local coordinate-wise hill climbing from the best point\n    - Budget-aware and robust to edge cases\n    - Warm start from prev_best_x if provided\n\n    Assumes:\n        config['bounds']: array-like of shape (dim, 2)\n        config['dim']: int\n        config['budget']: int\n    Returns:\n        best_x: numpy array of shape (dim,)\n    \"\"\"\n    rng = np.random.RandomState()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    span = np.where(span <= 0, 1.0, span)\n\n    def clip(x):\n        return np.minimum(high, np.maximum(low, x))\n\n    def eval_point(x):\n        return objective_function(x)\n\n    # Trivial case: no evaluations allowed\n    if budget <= 0:\n        return ((low + high) / 2.0).astype(float)\n\n    evals = 0\n    best_x = None\n    best_y = None\n\n    # ---------------- Initialization (warm start + jitter, or random) ----------------\n    if prev_best_x is not None:\n        base = clip(np.asarray(prev_best_x, dtype=float).reshape(dim))\n        jitter_scale = 0.02 * span / np.sqrt(max(dim, 1))\n        jitter = rng.normal(scale=jitter_scale)\n        x0 = clip(base + jitter)\n    else:\n        x0 = rng.uniform(low, high)\n\n    y0 = eval_point(x0)\n    evals += 1\n    best_x, best_y = x0, y0\n\n    if evals >= budget:\n        return best_x.astype(float)\n\n    remaining = budget - evals\n\n    # ---------------- Phase budget allocation ----------------\n    if dim <= 4:\n        frac_global = 0.35\n        frac_refine = 0.35\n    elif dim <= 15:\n        frac_global = 0.5\n        frac_refine = 0.3\n    else:\n        frac_global = 0.6\n        frac_refine = 0.25\n\n    target_global_total = int(frac_global * budget)\n    target_refine_total = int(frac_refine * budget)\n\n    # Ensure at least a minimal amount of global exploration\n    target_global_total = max(dim + 5, target_global_total)\n\n    global_budget = max(0, min(target_global_total - 1, remaining))\n\n    # ---------------- Global sampling (Halton-like + random noise) ----------------\n    def halton_sequence(n, d, base_primes):\n        \"\"\"\n        Generate a Halton-like sequence in [0,1]^d.\n\n        This implementation is simplified and vectorized over n where possible\n        to avoid heavy Python loops for large n.\n        \"\"\"\n        if n <= 0 or d <= 0:\n            return np.empty((0, d), dtype=float)\n\n        seq = np.empty((n, d), dtype=float)\n        # Pre-generate indices with random offset per dimension to decorrelate\n        indices = np.arange(1, n + 1, dtype=np.int64)\n\n        for j in range(d):\n            base = base_primes[j % len(base_primes)]\n            offset = rng.randint(1, 10_000)\n            idx = indices + offset\n\n            # Van der Corput in base 'base' (vectorized)\n            vdc = np.zeros(n, dtype=float)\n            denom = 1.0\n            # Cap iterations to avoid long loops for large idx\n            # Because idx shrinks by division, log_base(max_idx) iterations suffice\n            max_idx = idx.max()\n            max_iters = int(np.ceil(np.log(max_idx + 1) / np.log(base))) + 2\n            tmp = idx.copy()\n            for _ in range(max_iters):\n                denom *= base\n                tmp, rem = divmod(tmp, base)\n                vdc += rem / denom\n                if not tmp.any():\n                    break\n            seq[:, j] = vdc\n\n        return seq\n\n    if global_budget > 0:\n        n_global = global_budget\n\n        if dim <= 64:\n            primes = [\n                2, 3, 5, 7, 11, 13, 17, 19,\n                23, 29, 31, 37, 41, 43, 47, 53,\n                59, 61, 67, 71, 73\n            ]\n            strata = halton_sequence(n_global, dim, primes)\n            # Add small random noise to break exact lattice patterns\n            noise = (rng.rand(n_global, dim) - 0.5) / max(4, n_global)\n            strata = np.clip(strata + noise, 0.0, 1.0)\n        else:\n            # Crude LHS-like design via random permutation per dimension\n            base_grid = (np.arange(n_global) + 0.5) / n_global\n            strata = np.empty((n_global, dim), dtype=float)\n            for d_idx in range(dim):\n                perm = rng.permutation(n_global)\n                strata[:, d_idx] = base_grid[perm]\n\n        samples = low + strata * span\n        ys = np.empty(samples.shape[0], dtype=float)\n\n        for i in range(samples.shape[0]):\n            if evals >= budget:\n                ys = ys[:i]\n                samples = samples[:i]\n                break\n            x = samples[i]\n            y = eval_point(x)\n            ys[i] = y\n            evals += 1\n            if best_y is None or y < best_y:\n                best_x, best_y = x, y\n\n        remaining = budget - evals\n    else:\n        samples = np.empty((0, dim), dtype=float)\n        ys = np.empty(0, dtype=float)\n\n    if remaining <= 0:\n        return best_x.astype(float)\n\n    # ---------------- Elite refinement around top candidates ----------------\n    if samples.shape[0] > 0:\n        all_points = np.vstack([best_x[None, :], samples])\n        all_scores = np.concatenate([[best_y], ys])\n    else:\n        all_points = best_x[None, :]\n        all_scores = np.array([best_y], dtype=float)\n\n    k_elite = min(12, all_points.shape[0])\n    elite_idx = np.argpartition(all_scores, k_elite - 1)[:k_elite]\n    elites = all_points[elite_idx]\n    elite_scores = all_scores[elite_idx]\n\n    refine_budget = max(0, min(target_refine_total, remaining))\n\n    if refine_budget > 0 and k_elite > 0:\n        ranks = np.argsort(elite_scores)\n        elites = elites[ranks]\n        elite_scores = elite_scores[ranks]\n\n        # Geometric allocation; slightly flatter than before for robustness\n        weights = 0.65 ** np.arange(k_elite)\n        weights /= weights.sum()\n        alloc = (refine_budget * weights).astype(int)\n        alloc = np.maximum(1, alloc)\n        diff = refine_budget - int(alloc.sum())\n        if diff > 0:\n            alloc[0] += diff\n        elif diff < 0:\n            for i in range(k_elite - 1, -1, -1):\n                take = min(alloc[i] - 1, -diff)\n                if take > 0:\n                    alloc[i] -= take\n                    diff += take\n                if diff == 0:\n                    break\n\n        if k_elite > 1:\n            elite_spread = np.std(elites, axis=0)\n            sigma_base = 0.35 * np.maximum(elite_spread, 0.03 * span)\n        else:\n            sigma_base = 0.2 * span\n\n        sigma_base = np.where(sigma_base <= 0, 0.05 * span, sigma_base)\n\n        for e in range(k_elite):\n            if evals >= budget:\n                break\n            center = elites[e].copy()\n            sigma = sigma_base.copy()\n\n            n_evals_e = min(int(alloc[e]), budget - evals)\n            failures = 0\n            for _ in range(n_evals_e):\n                if evals >= budget:\n                    break\n                z = rng.normal(size=dim)\n                cand = clip(center + z * sigma)\n                if np.allclose(cand, center, atol=1e-15):\n                    sigma *= 0.7\n                    continue\n                y = eval_point(cand)\n                evals += 1\n\n                if y < best_y:\n                    best_x, best_y = cand, y\n\n                if y < elite_scores[e]:\n                    center = cand\n                    elite_scores[e] = y\n                    sigma *= 0.93\n                    failures = 0\n                else:\n                    failures += 1\n                    if failures >= 5:\n                        sigma *= 0.7\n                        failures = 0\n                        if np.all(sigma < 1e-8 * np.maximum(1.0, span)):\n                            break\n\n    remaining = budget - evals\n    if remaining <= 0:\n        return best_x.astype(float)\n\n    # ---------------- Local coordinate search from best_x ----------------\n    current_x = clip(np.asarray(best_x, dtype=float).reshape(dim))\n    current_y = float(best_y)\n\n    base_step = 0.06 * span / np.sqrt(max(dim, 1))\n    base_step = np.where(base_step <= 0, 0.06, base_step)\n    step = base_step.copy()\n\n    # Each iteration uses up to 2 * dim evals; allow more iters but with cap\n    max_iters = max(1, remaining // max(1, 2 * dim))\n    max_iters = min(max_iters, 80)\n\n    for _ in range(max_iters):\n        if evals >= budget:\n            break\n        improved_any = False\n\n        coord_order = np.arange(dim)\n        rng.shuffle(coord_order)\n\n        for d_idx in coord_order:\n            if evals >= budget:\n                break\n\n            # Try positive direction\n            cand_pos = current_x.copy()\n            cand_pos[d_idx] = cand_pos[d_idx] + step[d_idx]\n            cand_pos = clip(cand_pos)\n            if not np.allclose(cand_pos, current_x, atol=1e-15):\n                y_pos = eval_point(cand_pos)\n                evals += 1\n                if y_pos < current_y:\n                    current_x, current_y = cand_pos, y_pos\n                    if current_y < best_y:\n                        best_x, best_y = current_x, current_y\n                    improved_any = True\n                    continue\n\n            if evals >= budget:\n                break\n\n            # Try negative direction\n            cand_neg = current_x.copy()\n            cand_neg[d_idx] = cand_neg[d_idx] - step[d_idx]\n            cand_neg = clip(cand_neg)\n            if not np.allclose(cand_neg, current_x, atol=1e-15):\n                y_neg = eval_point(cand_neg)\n                evals += 1\n                if y_neg < current_y:\n                    current_x, current_y = cand_neg, y_neg\n                    if current_y < best_y:\n                        best_x, best_y = current_x, current_y\n                    improved_any = True\n\n            if evals >= budget:\n                break\n\n        if evals >= budget:\n            break\n\n        if not improved_any:\n            step *= 0.5\n            if np.all(step < 1e-8 * np.maximum(1.0, span)):\n                break\n\n    return np.asarray(best_x, dtype=float)",
    "X": "1.9799806109034465e-08 2.048974127972946e-08"
}