{
    "score": 1.9375404963408776e-11,
    "Input": "Pinter",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization using a hybrid global-local strategy:\n\n    - Low-discrepancy global sampling (Sobol-like via Owen-scrambled Halton fallback)\n    - Adaptive elite refinement around promising points\n    - Local coordinate-wise hill climbing from the best point\n    - Warm start from prev_best_x if provided\n\n    Assumes:\n        config['bounds']: array-like of shape (dim, 2)\n        config['dim']: int\n        config['budget']: int\n    Returns:\n        best_x: numpy array of shape (dim,)\n    \"\"\"\n    rng = np.random.RandomState()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    # Handle degenerate spans robustly\n    span = np.where(span <= 0, 1.0, span)\n\n    def clip(x):\n        return np.minimum(high, np.maximum(low, x))\n\n    def eval_point(x):\n        return objective_function(x)\n\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    evals = 0\n    best_x = None\n    best_y = None\n\n    # --- Initialization: warm start + small random jitter, or random in bounds ---\n    if prev_best_x is not None:\n        base = clip(np.asarray(prev_best_x, dtype=float).reshape(dim))\n        jitter_scale = 0.02 * span\n        jitter = rng.normal(scale=jitter_scale)\n        x0 = clip(base + jitter)\n    else:\n        x0 = rng.uniform(low, high)\n\n    y0 = eval_point(x0)\n    evals += 1\n    best_x, best_y = x0, y0\n\n    if evals >= budget:\n        return best_x\n\n    remaining = budget - evals\n\n    # --- Phase budget allocation: global, refine, local ---\n    # Bias slightly more to global for larger dims, more to local for small dims\n    if dim <= 4:\n        frac_global = 0.5\n        frac_refine = 0.25\n    elif dim <= 15:\n        frac_global = 0.6\n        frac_refine = 0.2\n    else:\n        frac_global = 0.7\n        frac_refine = 0.15\n\n    target_global_total = int(frac_global * budget)\n    target_refine_total = int(frac_refine * budget)\n\n    target_global_total = max(dim + 5, target_global_total)\n\n    global_budget = max(0, min(target_global_total - 1, remaining))\n\n    # --- Global sampling: low-discrepancy (Halton-like) + random fallback ---\n    def halton_sequence(n, d, base_primes):\n        \"\"\"Generate a simple Halton sequence in [0,1]^d.\"\"\"\n        def van_der_corput(num, base):\n            vdc, denom = 0.0, 1.0\n            while num > 0:\n                num, rem = divmod(num, base)\n                denom *= base\n                vdc += rem / denom\n            return vdc\n\n        seq = np.empty((n, d), dtype=float)\n        for j in range(d):\n            base = base_primes[j % len(base_primes)]\n            # Offset by a random integer to decorrelate between runs\n            offset = rng.randint(0, 10_000)\n            for i in range(n):\n                seq[i, j] = van_der_corput(i + 1 + offset, base)\n        return seq\n\n    if global_budget > 0:\n        n_global = global_budget\n\n        # Use Halton-based sampling if dim is moderate, else random LHS-like\n        if dim <= 64:\n            primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n                      31, 37, 41, 43, 47, 53, 59, 61]\n            strata = halton_sequence(n_global, dim, primes)\n            noise = (rng.rand(n_global, dim) - 0.5) / n_global\n            strata = np.clip(strata + noise, 0.0, 1.0)\n        else:\n            strata = (rng.rand(n_global, dim) + np.arange(n_global)[:, None]) / n_global\n            for d in range(dim):\n                rng.shuffle(strata[:, d])\n\n        samples = low + strata * span\n\n        ys = np.empty(n_global)\n        for i in range(n_global):\n            if evals >= budget:\n                ys = ys[:i]\n                samples = samples[:i]\n                break\n            x = samples[i]\n            y = eval_point(x)\n            ys[i] = y\n            evals += 1\n            if y < best_y:\n                best_x, best_y = x, y\n\n        remaining = budget - evals\n    else:\n        ys = np.empty(0)\n        samples = np.empty((0, dim))\n\n    if remaining <= 0:\n        return best_x\n\n    # --- Elite refinement: adaptive sampling around top-k points + best_x ---\n    if samples.shape[0] > 0:\n        all_points = np.vstack([best_x[None, :], samples])\n        all_scores = np.concatenate([[best_y], ys])\n    else:\n        all_points = best_x[None, :]\n        all_scores = np.array([best_y])\n\n    k_elite = min(8, all_points.shape[0])\n    elite_idx = np.argpartition(all_scores, k_elite - 1)[:k_elite]\n    elites = all_points[elite_idx]\n    elite_scores = all_scores[elite_idx]\n\n    refine_budget = max(0, min(target_refine_total, remaining))\n\n    if refine_budget > 0:\n        # Weight elites by quality (lower is better)\n        ranks = np.argsort(elite_scores)\n        elites = elites[ranks]\n        elite_scores = elite_scores[ranks]\n\n        # Allocate more budget to better elites\n        weights = np.linspace(1.0, 0.3, num=k_elite)\n        weights /= weights.sum()\n        alloc = np.maximum(1, (refine_budget * weights).astype(int))\n        alloc[-1] += max(0, refine_budget - int(alloc.sum()))\n\n        # Initial radius scaled to variability of elites if possible\n        if k_elite > 1:\n            elite_spread = np.std(elites, axis=0)\n            sigma_base = 0.5 * np.maximum(elite_spread, 0.05 * span)\n        else:\n            sigma_base = 0.15 * span\n\n        sigma_base = np.where(sigma_base <= 0, 0.1 * span, sigma_base)\n\n        for e in range(k_elite):\n            if evals >= budget:\n                break\n            center = elites[e].copy()\n            sigma = sigma_base.copy()\n\n            n_evals = min(int(alloc[e]), budget - evals)\n            failures = 0\n            for _ in range(n_evals):\n                if evals >= budget:\n                    break\n                z = rng.normal(size=dim)\n                cand = clip(center + z * sigma)\n                y = eval_point(cand)\n                evals += 1\n                if y < best_y:\n                    best_x, best_y = cand, y\n                if y < elite_scores[e]:\n                    center = cand\n                    elite_scores[e] = y\n                    sigma *= 0.95\n                    failures = 0\n                else:\n                    failures += 1\n                    if failures >= 5:\n                        sigma *= 0.7\n                        failures = 0\n                        if np.all(sigma < 1e-6 * np.maximum(1.0, span)):\n                            break\n\n    remaining = budget - evals\n    if remaining <= 0:\n        return best_x\n\n    # --- Local search: robust coordinate-wise hill climbing from best_x ---\n    current_x = clip(np.array(best_x, dtype=float))\n    current_y = best_y\n\n    base_step = 0.1 * span\n    base_step = np.where(base_step <= 0, 0.1, base_step)\n\n    step = base_step.copy()\n\n    # Use remaining budget; each iter up to 2*dim evaluations\n    max_iters = max(1, remaining // max(1, 2 * dim))\n    max_iters = min(max_iters, 40)\n\n    for _ in range(max_iters):\n        if evals >= budget:\n            break\n        improved_any = False\n\n        for d in range(dim):\n            if evals >= budget:\n                break\n\n            # Positive step\n            cand_pos = current_x.copy()\n            cand_pos[d] = cand_pos[d] + step[d]\n            cand_pos = clip(cand_pos)\n            if not np.array_equal(cand_pos, current_x):\n                y_pos = eval_point(cand_pos)\n                evals += 1\n                if y_pos < current_y:\n                    current_x, current_y = cand_pos, y_pos\n                    if current_y < best_y:\n                        best_x, best_y = current_x, current_y\n                    improved_any = True\n                    continue\n\n            if evals >= budget:\n                break\n\n            # Negative step\n            cand_neg = current_x.copy()\n            cand_neg[d] = cand_neg[d] - step[d]\n            cand_neg = clip(cand_neg)\n            if not np.array_equal(cand_neg, current_x):\n                y_neg = eval_point(cand_neg)\n                evals += 1\n                if y_neg < current_y:\n                    current_x, current_y = cand_neg, y_neg\n                    if current_y < best_y:\n                        best_x, best_y = current_x, current_y\n                    improved_any = True\n\n            if evals >= budget:\n                break\n\n        if evals >= budget:\n            break\n\n        if not improved_any:\n            step *= 0.5\n            if np.all(step < 1e-7 * np.maximum(1.0, span)):\n                break\n\n    return best_x",
    "X": "-4.161621167711e-07 3.910693280833121e-07"
}