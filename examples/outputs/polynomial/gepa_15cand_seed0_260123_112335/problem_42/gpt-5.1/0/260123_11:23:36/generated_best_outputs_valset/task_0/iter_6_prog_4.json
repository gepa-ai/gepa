{
    "score": 9.083352325161846e-14,
    "Input": "Pinter",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization using an adaptive hybrid global-local strategy.\n\n    - Low-discrepancy style global sampling (Halton-like) with randomness\n    - Elite-based stochastic refinement around promising regions\n    - Local coordinate-wise hill climbing from the best point\n    - Careful budget management and robust handling of edge cases\n    - Warm start from prev_best_x if provided\n\n    Assumes:\n        config['bounds']: array-like of shape (dim, 2)\n        config['dim']: int\n        config['budget']: int\n    Returns:\n        best_x: numpy array of shape (dim,)\n    \"\"\"\n    rng = np.random.RandomState()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    span = np.where(span <= 0, 1.0, span)\n\n    def clip(x):\n        return np.minimum(high, np.maximum(low, x))\n\n    def eval_point(x):\n        return objective_function(x)\n\n    # Trivial case: no evaluations allowed\n    if budget <= 0:\n        return ((low + high) / 2.0).astype(float)\n\n    evals = 0\n    best_x = None\n    best_y = None\n\n    # ---------------- Initialization (warm start + jitter, or random) ----------------\n    if prev_best_x is not None:\n        base = clip(np.asarray(prev_best_x, dtype=float).reshape(dim))\n        # Smaller jitter for large dimensions to avoid huge jumps\n        jitter_scale = 0.02 * span / np.sqrt(dim)\n        jitter = rng.normal(scale=jitter_scale)\n        x0 = clip(base + jitter)\n    else:\n        x0 = rng.uniform(low, high)\n\n    y0 = eval_point(x0)\n    evals += 1\n    best_x, best_y = x0, y0\n\n    if evals >= budget:\n        return best_x.astype(float)\n\n    remaining = budget - evals\n\n    # ---------------- Phase budget allocation ----------------\n    # Allocate fractions of total budget to (global, refine, local) phases.\n    # More global for higher dimensions, more local for low dimensions.\n    if dim <= 4:\n        frac_global = 0.4\n        frac_refine = 0.3\n    elif dim <= 15:\n        frac_global = 0.55\n        frac_refine = 0.25\n    else:\n        frac_global = 0.65\n        frac_refine = 0.2\n\n    target_global_total = int(frac_global * budget)\n    target_refine_total = int(frac_refine * budget)\n\n    # Ensure at least a minimal amount of global exploration\n    target_global_total = max(dim + 5, target_global_total)\n\n    global_budget = max(0, min(target_global_total - 1, remaining))\n\n    # ---------------- Global sampling (Halton-like + random noise) ----------------\n    def halton_sequence(n, d, base_primes):\n        \"\"\"Generate a Halton-like sequence in [0,1]^d (O(n * d * log(n))).\"\"\"\n        # Pre-allocate\n        seq = np.empty((n, d), dtype=float)\n\n        # For numerical stability and speed, bound the index range\n        max_index = n + 10_000\n\n        for j in range(d):\n            base = base_primes[j % len(base_primes)]\n            offset = rng.randint(0, 10_000)\n\n            # Simple Van der Corput computation\n            for i in range(n):\n                num = i + 1 + offset\n                if num > max_index:\n                    num = num % max_index + 1\n                vdc = 0.0\n                denom = 1.0\n                while num:\n                    num, rem = divmod(num, base)\n                    denom *= base\n                    vdc += rem / denom\n                seq[i, j] = vdc\n        return seq\n\n    if global_budget > 0:\n        n_global = global_budget\n\n        if dim <= 64:\n            primes = [\n                2, 3, 5, 7, 11, 13, 17, 19,\n                23, 29, 31, 37, 41, 43, 47, 53,\n                59, 61\n            ]\n            strata = halton_sequence(n_global, dim, primes)\n            # Add small random noise to break exact lattice patterns\n            noise = (rng.rand(n_global, dim) - 0.5) / max(4, n_global)\n            strata = np.clip(strata + noise, 0.0, 1.0)\n        else:\n            # Crude LHS-like design via random permutation per dimension\n            strata = (rng.rand(n_global, dim) + np.arange(n_global)[:, None]) / n_global\n            for d_idx in range(dim):\n                rng.shuffle(strata[:, d_idx])\n\n        samples = low + strata * span\n        ys = np.empty(n_global, dtype=float)\n\n        for i in range(n_global):\n            if evals >= budget:\n                ys = ys[:i]\n                samples = samples[:i]\n                break\n            x = samples[i]\n            y = eval_point(x)\n            ys[i] = y\n            evals += 1\n            if best_y is None or y < best_y:\n                best_x, best_y = x, y\n\n        remaining = budget - evals\n    else:\n        samples = np.empty((0, dim), dtype=float)\n        ys = np.empty(0, dtype=float)\n\n    if remaining <= 0:\n        return best_x.astype(float)\n\n    # ---------------- Elite refinement around top candidates ----------------\n    if samples.shape[0] > 0:\n        all_points = np.vstack([best_x[None, :], samples])\n        all_scores = np.concatenate([[best_y], ys])\n    else:\n        all_points = best_x[None, :]\n        all_scores = np.array([best_y], dtype=float)\n\n    # Keep a small elite set for refinement\n    k_elite = min(10, all_points.shape[0])\n    elite_idx = np.argpartition(all_scores, k_elite - 1)[:k_elite]\n    elites = all_points[elite_idx]\n    elite_scores = all_scores[elite_idx]\n\n    refine_budget = max(0, min(target_refine_total, remaining))\n\n    if refine_budget > 0 and k_elite > 0:\n        # Sort elites by quality\n        ranks = np.argsort(elite_scores)\n        elites = elites[ranks]\n        elite_scores = elite_scores[ranks]\n\n        # Allocate more samples to better elites (geometric decay)\n        # Avoid extremely small allocations for worse elites.\n        weights = 0.6 ** np.arange(k_elite)\n        weights /= weights.sum()\n        alloc = (refine_budget * weights).astype(int)\n        alloc = np.maximum(1, alloc)\n        # Adjust total to match refine_budget\n        diff = refine_budget - int(alloc.sum())\n        if diff > 0:\n            alloc[0] += diff\n        elif diff < 0:\n            # remove from worst elites\n            for i in range(k_elite - 1, -1, -1):\n                take = min(alloc[i] - 1, -diff)\n                if take > 0:\n                    alloc[i] -= take\n                    diff += take\n                if diff == 0:\n                    break\n\n        # Estimate scale from elite spread, but keep it moderate\n        if k_elite > 1:\n            elite_spread = np.std(elites, axis=0)\n            sigma_base = 0.4 * np.maximum(elite_spread, 0.05 * span)\n        else:\n            sigma_base = 0.2 * span\n\n        sigma_base = np.where(sigma_base <= 0, 0.1 * span, sigma_base)\n\n        for e in range(k_elite):\n            if evals >= budget:\n                break\n            center = elites[e].copy()\n            sigma = sigma_base.copy()\n\n            n_evals_e = min(int(alloc[e]), budget - evals)\n            failures = 0\n            for _ in range(n_evals_e):\n                if evals >= budget:\n                    break\n                z = rng.normal(size=dim)\n                cand = clip(center + z * sigma)\n                # Skip duplicates (degenerate sigma or clipped to center)\n                if np.allclose(cand, center, atol=1e-15):\n                    sigma *= 0.7\n                    continue\n                y = eval_point(cand)\n                evals += 1\n\n                if y < best_y:\n                    best_x, best_y = cand, y\n\n                if y < elite_scores[e]:\n                    center = cand\n                    elite_scores[e] = y\n                    sigma *= 0.93\n                    failures = 0\n                else:\n                    failures += 1\n                    if failures >= 5:\n                        sigma *= 0.7\n                        failures = 0\n                        # Stop if search radius is negligibly small\n                        if np.all(sigma < 1e-8 * np.maximum(1.0, span)):\n                            break\n\n    remaining = budget - evals\n    if remaining <= 0:\n        return best_x.astype(float)\n\n    # ---------------- Local coordinate search from best_x ----------------\n    current_x = clip(np.asarray(best_x, dtype=float).reshape(dim))\n    current_y = float(best_y)\n\n    # Step size relative to span, scaled by dimension\n    base_step = 0.08 * span / np.sqrt(dim)\n    base_step = np.where(base_step <= 0, 0.08, base_step)\n    step = base_step.copy()\n\n    # Use remaining budget, each iteration uses up to 2 * dim evaluations\n    max_iters = max(1, remaining // max(1, 2 * dim))\n    # Allow more refinement iterations but cap to avoid over-commitment\n    max_iters = min(max_iters, 60)\n\n    for _ in range(max_iters):\n        if evals >= budget:\n            break\n        improved_any = False\n\n        # Shuffle coordinate order for robustness\n        coord_order = np.arange(dim)\n        rng.shuffle(coord_order)\n\n        for d_idx in coord_order:\n            if evals >= budget:\n                break\n\n            # Try positive direction\n            cand_pos = current_x.copy()\n            cand_pos[d_idx] = cand_pos[d_idx] + step[d_idx]\n            cand_pos = clip(cand_pos)\n            if not np.allclose(cand_pos, current_x, atol=1e-15):\n                y_pos = eval_point(cand_pos)\n                evals += 1\n                if y_pos < current_y:\n                    current_x, current_y = cand_pos, y_pos\n                    if current_y < best_y:\n                        best_x, best_y = current_x, current_y\n                    improved_any = True\n                    continue\n\n            if evals >= budget:\n                break\n\n            # Try negative direction\n            cand_neg = current_x.copy()\n            cand_neg[d_idx] = cand_neg[d_idx] - step[d_idx]\n            cand_neg = clip(cand_neg)\n            if not np.allclose(cand_neg, current_x, atol=1e-15):\n                y_neg = eval_point(cand_neg)\n                evals += 1\n                if y_neg < current_y:\n                    current_x, current_y = cand_neg, y_neg\n                    if current_y < best_y:\n                        best_x, best_y = current_x, current_y\n                    improved_any = True\n\n            if evals >= budget:\n                break\n\n        if evals >= budget:\n            break\n\n        if not improved_any:\n            step *= 0.5\n            if np.all(step < 1e-8 * np.maximum(1.0, span)):\n                break\n\n    return np.asarray(best_x, dtype=float)",
    "X": "3.6501661573723175e-08 -1.013987214434869e-08"
}