{
    "score": -0.4166666666666667,
    "Input": "McCourt07",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid derivative-free optimizer:\n    - Quasi-random (Sobol-like) global search (~60% of remaining budget)\n    - CMA-ES style local search (~40% of remaining budget)\n    - Supports warm start from prev_best_x\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    # Handle bounds shape: tile if needed\n    if bounds.shape[0] != dim:\n        bounds = np.tile(bounds.reshape(-1, 2), (dim // bounds.shape[0] + 1, 1))[:dim]\n\n    low = bounds[:, 0].astype(float)\n    high = bounds[:, 1].astype(float)\n    span = high - low\n    span[span <= 0] = 1.0  # avoid degenerate intervals\n\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    def project(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    # Initialize best point\n    if prev_best_x is not None:\n        x_best = project(np.asarray(prev_best_x, dtype=float).reshape(-1)[:dim])\n    else:\n        x_best = low + rng.rand(dim) * span\n\n    y_best = objective_function(x_best)\n    evals_used += 1\n\n    if budget == 1:\n        return x_best\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # Split budget: global then local\n    global_budget = max(1, int(0.6 * remaining))\n    local_budget = max(0, remaining - global_budget)\n\n    # --- Global search with low-discrepancy-like sampling ---\n    # Simple Halton sequence implementation (base primes) for better coverage than pure random\n    def halton_index(i, base):\n        f = 1.0\n        r = 0.0\n        while i > 0:\n            f /= base\n            r += f * (i % base)\n            i //= base\n        return r\n\n    # First few primes for bases (enough for typical dim; reuse if dim larger)\n    primes = np.array(\n        [2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n         31, 37, 41, 43, 47, 53, 59, 61, 67, 71],\n        dtype=int,\n    )\n    bases = primes[: max(1, min(dim, len(primes)))]\n    if dim > len(bases):\n        # Repeat bases if dim > number of primes (still better than pure random)\n        bases = np.resize(bases, dim)\n\n    start_index = rng.randint(1, 10_000)\n\n    for k in range(global_budget):\n        if evals_used >= budget:\n            break\n        idx = start_index + k + 1\n        u = np.empty(dim, dtype=float)\n        for d in range(dim):\n            u[d] = halton_index(idx, int(bases[d]))\n        x = low + u * span\n        y = objective_function(x)\n        evals_used += 1\n        if y < y_best:\n            x_best, y_best = x, y\n\n    # --- Local search: simplified CMA-ES style covariance adaptation ---\n    if local_budget > 0 and evals_used < budget:\n        # Effective population size: small but at least 4\n        lam = max(4, min(4 + dim // 2, local_budget // 2 or 1))\n        # Number of parents\n        mu = max(1, lam // 2)\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights /= weights.sum()\n        mu_eff = 1.0 / np.sum(weights ** 2)\n\n        # Strategy parameters\n        sigma = 0.3  # initial global step-size in normalized space\n        c_c = (4.0 + mu_eff / dim) / (dim + 4.0 + 2.0 * mu_eff / dim)\n        c_1 = 2.0 / ((dim + 1.3) ** 2 + mu_eff)\n        c_mu = min(1.0 - c_1, 2.0 * (mu_eff - 2.0 + 1.0 / mu_eff) / ((dim + 2.0) ** 2 + mu_eff))\n        c_sigma = (mu_eff + 2.0) / (dim + mu_eff + 5.0)\n        d_sigma = 1.0 + 2.0 * max(0.0, np.sqrt((mu_eff - 1.0) / (dim + 1.0)) - 1.0) + c_sigma\n\n        # Initialize paths and covariance\n        p_sigma = np.zeros(dim)\n        p_c = np.zeros(dim)\n        C = np.eye(dim)\n\n        # Normalize current best to [0,1]^dim for internal representation\n        m = (x_best - low) / span\n        m = np.clip(m, 0.0, 1.0)\n\n        max_iters = max(1, local_budget // lam)\n        for _ in range(max_iters):\n            if evals_used >= budget:\n                break\n\n            # Eigen decomposition (safeguarded)\n            try:\n                D2, B = np.linalg.eigh(C)\n                D2 = np.maximum(D2, 1e-14)\n            except np.linalg.LinAlgError:\n                C = np.diag(np.maximum(np.diag(C), 1e-14))\n                D2, B = np.linalg.eigh(C)\n                D2 = np.maximum(D2, 1e-14)\n            D = np.sqrt(D2)\n            # Sampling\n            z = rng.randn(lam, dim)\n            y_pop = z @ (B * D).T  # shape (lam, dim)\n            m_repeated = m.reshape(1, -1)\n            x_pop = m_repeated + sigma * y_pop\n            # Map back to original space\n            x_pop = low + np.clip(x_pop, 0.0, 1.0) * span\n\n            ys = []\n            xs = []\n            for i in range(lam):\n                if evals_used >= budget:\n                    break\n                x_i = x_pop[i]\n                f_i = objective_function(x_i)\n                evals_used += 1\n                ys.append(f_i)\n                xs.append(x_i)\n                if f_i < y_best:\n                    x_best, y_best = x_i, f_i\n\n            if len(xs) < mu:\n                break  # budget exhausted\n\n            ys = np.array(ys)\n            xs = np.array(xs)\n\n            # Sort by fitness\n            idx = np.argsort(ys)\n            xs = xs[idx]\n            ys = ys[idx]\n\n            # Update mean in normalized space\n            x_parents = xs[:mu]\n            m_old = m.copy()\n            m = (x_parents - low) / span\n            m = np.clip(m, 0.0, 1.0)\n            m = np.dot(weights, m)\n\n            # Evolution paths\n            y_w = ((m - m_old) / sigma)\n            # Transform into isotropic coordinate system\n            inv_sqrt_C = B @ np.diag(1.0 / D) @ B.T\n            p_sigma = (1.0 - c_sigma) * p_sigma + np.sqrt(c_sigma * (2.0 - c_sigma) * mu_eff) * (\n                inv_sqrt_C @ y_w\n            )\n\n            norm_p_sigma = np.linalg.norm(p_sigma)\n            h_sigma = 1.0 if norm_p_sigma / np.sqrt(\n                1.0 - (1.0 - c_sigma) ** (2.0 * (evals_used / max(lam, 1)))\n            ) < (1.4 + 2.0 / (dim + 1.0)) else 0.0\n\n            p_c = (1.0 - c_c) * p_c + h_sigma * np.sqrt(c_c * (2.0 - c_c) * mu_eff) * y_w\n\n            # Covariance update\n            y_parents = ((x_parents - low) / span) - m_old\n            y_parents /= max(sigma, 1e-12)\n            rank_mu = sum(\n                w * np.outer(y, y) for w, y in zip(weights, y_parents[:mu])\n            )\n            C = (\n                (1.0 - c_1 - c_mu) * C\n                + c_1 * (np.outer(p_c, p_c) + (1.0 - h_sigma) * c_c * (2.0 - c_c) * C)\n                + c_mu * rank_mu\n            )\n\n            # Step-size control\n            sigma *= np.exp((c_sigma / d_sigma) * (norm_p_sigma / np.sqrt(dim) - 1.0))\n\n            # Safeguards\n            if sigma < 1e-8:\n                sigma = 1e-8\n            if sigma > 3.0:\n                sigma = 3.0\n\n        # Final candidate is best found\n    return x_best",
    "X": "0.45540909629477133 0.9983523298287384 0.13378150822343132 0.0 1.0 0.0"
}