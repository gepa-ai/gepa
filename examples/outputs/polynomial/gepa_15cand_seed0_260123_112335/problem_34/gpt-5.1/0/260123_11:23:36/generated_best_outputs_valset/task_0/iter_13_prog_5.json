{
    "score": -7.6943251568557836,
    "Input": "McCourt28",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with a hybrid global-local search.\n\n    Improvements over previous version:\n    - More robust global search: Latin Hypercube Sampling (LHS) with fallback random.\n    - Clear budget split: global (incl. warm start coverage) + local refinement.\n    - Use prev_best_x both as evaluation and as a center for early local search.\n    - Dimension-aware scaling and step control for local search.\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    assert bounds.shape == (dim, 2)\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n    span[span == 0] = 1.0\n\n    def clip_to_bounds(x):\n        return np.clip(x, lower, upper)\n\n    eval_count = 0\n    best_x = None\n    best_y = np.inf\n\n    def evaluate(x):\n        nonlocal eval_count, best_x, best_y\n        if eval_count >= budget:\n            return np.inf\n        x = clip_to_bounds(np.asarray(x, dtype=float).reshape(-1))\n        y = objective_function(x)\n        eval_count += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    if budget <= 0:\n        return (lower + upper) / 2.0\n\n    # Warm start from previous best if dimension matches\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            evaluate(x0)\n\n    if eval_count >= budget:\n        if best_x is None:\n            best_x = (lower + upper) / 2.0\n        return best_x\n\n    remaining = budget - eval_count\n\n    # Decide fraction for global sampling; higher dim \u2192 more global\n    if dim <= 3:\n        global_frac = 0.5\n    elif dim <= 10:\n        global_frac = 0.6\n    else:\n        global_frac = 0.7\n\n    # Ensure enough global samples but keep room for local refinement\n    global_evals = int(global_frac * remaining)\n    global_evals = max(min(40 * dim, remaining // 2), min(20 * dim, remaining))\n    global_evals = min(global_evals, remaining - 1) if remaining > 1 else remaining\n\n    # ------------------------------------------------------------------\n    # Global search: Latin Hypercube Sampling for robustness\n    # ------------------------------------------------------------------\n    def lhs_sample(n, d):\n        # Latin Hypercube in [0,1]^d\n        if n <= 0:\n            return np.empty((0, d), dtype=float)\n        cut = np.linspace(0.0, 1.0, n + 1)\n        u = np.random.rand(n, d)\n        a = cut[:n]\n        b = cut[1:n + 1]\n        rdpoints = np.zeros((n, d))\n        for j in range(d):\n            rdpoints[:, j] = u[:, j] * (b - a) + a\n            np.random.shuffle(rdpoints[:, j])\n        return rdpoints\n\n    if global_evals > 0:\n        u = lhs_sample(global_evals, dim)\n        if u.shape[0] != global_evals:  # safety fallback\n            u = np.random.rand(global_evals, dim)\n        candidates = lower + u * (upper - lower)\n\n        # If we have a previously known best_x, bias a few global points around it\n        if best_x is not None and global_evals > 4:\n            k = max(2, global_evals // 10)\n            idx = np.random.choice(global_evals, size=k, replace=False)\n            local_span = 0.25 * span\n            for i in idx:\n                candidates[i] = clip_to_bounds(\n                    best_x + np.random.uniform(-local_span, local_span, size=dim)\n                )\n\n        for i in range(global_evals):\n            if eval_count >= budget:\n                break\n            evaluate(candidates[i])\n\n    if eval_count >= budget:\n        return best_x if best_x is not None else (lower + upper) / 2.0\n\n    # ------------------------------------------------------------------\n    # Ensure we have a starting point for local search\n    # ------------------------------------------------------------------\n    if best_x is None:\n        # Use center or a random point if possible\n        x_init = (lower + upper) / 2.0\n        evaluate(x_init)\n        if eval_count >= budget:\n            return best_x if best_x is not None else x_init\n\n    remaining = budget - eval_count\n    if remaining <= 0:\n        return best_x\n\n    # ------------------------------------------------------------------\n    # Local search: adaptive Gaussian / coordinate search\n    # ------------------------------------------------------------------\n    # Dimension-aware base sigma: smaller for high-dim\n    dim_factor = 1.0 / np.sqrt(max(dim, 1))\n    base_sigma = 0.3 * span * dim_factor\n    base_sigma[base_sigma == 0] = 0.1\n    sigma = base_sigma.copy()\n\n    global_scale = 1.0\n    no_improve = 0\n    last_best_y = best_y\n\n    adapt_interval = max(5 * dim, 20)\n    stagnation_restart = 3\n\n    # Precompute dimension ordering by span (largest span first)\n    d_order = np.argsort(-span)\n\n    for i in range(remaining):\n        if eval_count >= budget:\n            break\n\n        # Periodic adaptation of global scale\n        if i > 0 and i % adapt_interval == 0:\n            if best_y >= last_best_y - 1e-12:\n                global_scale *= 0.5\n                no_improve += 1\n            else:\n                global_scale *= 1.25\n                no_improve = 0\n            global_scale = float(np.clip(global_scale, 0.02, 2.0))\n            last_best_y = best_y\n\n        # Stagnation handling: restart exploration\n        if no_improve >= stagnation_restart and eval_count < budget:\n            # One uniform random and one wide Gaussian around current best\n            restart1 = np.random.uniform(lower, upper)\n            restart2 = best_x + np.random.normal(scale=0.5 * span, size=dim)\n            restart2 = clip_to_bounds(restart2)\n            evaluate(restart1)\n            if eval_count < budget:\n                evaluate(restart2)\n            no_improve = 0\n            continue\n\n        # Propose a new candidate\n        if dim == 1 or i % 3 != 0:\n            # Full-dimensional Gaussian perturbation\n            step_sigma = sigma * global_scale\n            step_sigma = np.maximum(step_sigma, 1e-8 * span)\n            perturb = np.random.normal(scale=step_sigma, size=dim)\n            x_new = best_x + perturb\n        else:\n            # Coordinate-wise move along the more sensitive dimensions first\n            x_new = best_x.copy()\n            d_idx = d_order[i % dim]\n            step_sigma = sigma[d_idx] * global_scale\n            if step_sigma <= 0:\n                step_sigma = 1e-8 * span[d_idx]\n            step = np.random.normal(scale=step_sigma)\n            x_new[d_idx] += step\n\n        x_new = clip_to_bounds(x_new)\n        old_best_y = best_y\n        y_new = evaluate(x_new)\n        if y_new < old_best_y - 1e-12:\n            no_improve = 0\n        else:\n            no_improve += 1\n\n    if best_x is None:\n        best_x = (lower + upper) / 2.0\n    return best_x",
    "X": "0.4494037188842143 0.06651080099886915 0.9082446279397673 0.27072520213993534"
}