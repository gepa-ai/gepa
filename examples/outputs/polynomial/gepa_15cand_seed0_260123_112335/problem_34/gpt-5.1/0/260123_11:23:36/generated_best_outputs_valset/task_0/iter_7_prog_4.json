{
    "score": -7.694325124944093,
    "Input": "McCourt28",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with a hybrid global-local search.\n\n    Strategy:\n    1. Warm start: evaluate prev_best_x if given.\n    2. Global search:\n       - Sobol-like low-discrepancy sampling (via simple Owen-scrambled Halton).\n       - Budget split based on dimension and remaining evaluations.\n    3. Local search:\n       - Adaptive Gaussian perturbations around the current best.\n       - Occasional global restarts when stagnating.\n       - Simple covariance adaptation (isotropic) to better shape step sizes.\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    assert bounds.shape == (dim, 2)\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n\n    def clip_to_bounds(x):\n        return np.clip(x, lower, upper)\n\n    eval_count = 0\n    best_x = None\n    best_y = np.inf\n\n    def evaluate(x):\n        nonlocal eval_count, best_x, best_y\n        if eval_count >= budget:\n            return np.inf\n        x = clip_to_bounds(np.asarray(x, dtype=float).reshape(-1))\n        y = objective_function(x)\n        eval_count += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    if budget <= 0:\n        return (lower + upper) / 2.0\n\n    # Warm start from previous best if dimension matches\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            evaluate(x0)\n\n    if eval_count >= budget:\n        if best_x is None:\n            best_x = (lower + upper) / 2.0\n        return best_x\n\n    # ------------------------------------------------------------------\n    # Global search: low-discrepancy sampling (Halton with random base shift)\n    # ------------------------------------------------------------------\n    remaining = budget - eval_count\n\n    if dim <= 3:\n        global_frac = 0.5\n    elif dim <= 10:\n        global_frac = 0.6\n    else:\n        global_frac = 0.7\n\n    global_evals = int(global_frac * remaining)\n    global_evals = max(global_evals, min(30 * dim, remaining))\n    global_evals = min(global_evals, remaining)\n\n    def halton_sequence(n, d, scramble=True):\n        \"\"\"Generate n points in [0,1]^d using a simple Halton sequence.\"\"\"\n        # First d primes\n        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n                  31, 37, 41, 43, 47, 53, 59, 61, 67, 71]\n        if d > len(primes):\n            # Fallback to random if dimension is very high\n            return np.random.rand(n, d)\n        bases = primes[:d]\n\n        def van_der_corput(num, base, offset=0.0):\n            vdc, denom = 0.0, 1.0\n            num = num + 1  # start at index 1\n            while num > 0:\n                num, remainder = divmod(num, base)\n                denom *= base\n                vdc += remainder / denom\n            return (vdc + offset) % 1.0\n\n        offsets = np.random.rand(d) if scramble else np.zeros(d)\n        seq = np.empty((n, d), dtype=float)\n        for j in range(d):\n            b = bases[j]\n            off = offsets[j]\n            for i in range(n):\n                seq[i, j] = van_der_corput(i, b, off)\n        return seq\n\n    if global_evals > 0:\n        n_samples = global_evals\n        # Use Halton for better uniformity than random, especially in low dim\n        u = halton_sequence(n_samples, dim, scramble=True)\n        candidates = lower + u * span\n\n        for i in range(n_samples):\n            if eval_count >= budget:\n                break\n            evaluate(candidates[i])\n\n    if eval_count >= budget:\n        return best_x if best_x is not None else (lower + upper) / 2.0\n\n    # ------------------------------------------------------------------\n    # Local search phase\n    # ------------------------------------------------------------------\n    if best_x is None:\n        best_x = (lower + upper) / 2.0\n        evaluate(best_x)\n        if eval_count >= budget:\n            return best_x\n\n    remaining = budget - eval_count\n    if remaining <= 0:\n        return best_x\n\n    # Adaptive Gaussian search parameters\n    base_sigma = 0.2 * span\n    base_sigma[base_sigma == 0] = 0.1\n    sigma = base_sigma.copy()\n\n    no_improve = 0\n    last_best_y = best_y\n    adapt_interval = max(5 * dim, 15)\n\n    # For simple covariance adaptation (scalar factor)\n    global_scale = 1.0\n\n    for i in range(remaining):\n        if eval_count >= budget:\n            break\n\n        if i > 0 and i % adapt_interval == 0:\n            if best_y >= last_best_y - 1e-12:\n                global_scale *= 0.5\n                no_improve += 1\n            else:\n                global_scale *= 1.2\n                no_improve = 0\n            global_scale = np.clip(global_scale, 0.05, 1.5)\n            last_best_y = best_y\n\n        # Occasional diversification when stagnating\n        if no_improve >= 3 and eval_count < budget:\n            # Try a global restart near the best but with larger spread,\n            # plus a purely random point.\n            restart1 = np.random.uniform(lower, upper)\n            restart2 = best_x + np.random.normal(scale=0.5 * span, size=dim)\n            restart2 = clip_to_bounds(restart2)\n            evaluate(restart1)\n            if eval_count < budget:\n                evaluate(restart2)\n            no_improve = 0\n            continue\n\n        # Alternate between full-dimensional Gaussian step and coordinate move\n        if i % 3 != 0 or dim == 1:\n            # Full-dimensional Gaussian perturbation\n            step_sigma = sigma * global_scale\n            perturb = np.random.normal(scale=step_sigma, size=dim)\n            x_new = best_x + perturb\n        else:\n            # Coordinate-wise focused move along dimension sorted by span\n            d_order = np.argsort(-span)\n            d = d_order[i % dim]\n            x_new = best_x.copy()\n            step = np.random.normal(scale=sigma[d] * global_scale)\n            x_new[d] += step\n\n        x_new = clip_to_bounds(x_new)\n        old_best_y = best_y\n        y_new = evaluate(x_new)\n        if y_new < old_best_y - 1e-12:\n            no_improve = 0\n\n    if best_x is None:\n        best_x = (lower + upper) / 2.0\n    return best_x",
    "X": "0.4494037188842143 0.06651080099886915 0.9083798702940449 0.27072520213993534"
}