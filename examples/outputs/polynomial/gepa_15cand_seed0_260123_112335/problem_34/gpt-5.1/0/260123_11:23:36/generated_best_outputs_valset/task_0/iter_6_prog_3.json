{
    "score": -7.694324532623301,
    "Input": "McCourt28",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with a hybrid global-local search.\n\n    Strategy:\n    1. Warm start: evaluate prev_best_x if given.\n    2. Global search:\n       - Latin-hypercube-like low-discrepancy sampling.\n       - Budget split based on dimension and remaining evaluations.\n    3. Local search:\n       - Adaptive Gaussian perturbations around the current best.\n       - Coordinate-wise refinement for robustness in small budgets.\n       - Occasional random restarts when stagnating.\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    assert bounds.shape == (dim, 2)\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    span = upper - lower\n\n    def clip_to_bounds(x):\n        return np.clip(x, lower, upper)\n\n    eval_count = 0\n    best_x = None\n    best_y = np.inf\n\n    def evaluate(x):\n        nonlocal eval_count, best_x, best_y\n        if eval_count >= budget:\n            return np.inf\n        x = clip_to_bounds(np.asarray(x, dtype=float).reshape(-1))\n        y = objective_function(x)\n        eval_count += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    if budget <= 0:\n        return (lower + upper) / 2.0\n\n    # Warm start from previous best if dimension matches\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            evaluate(x0)\n\n    if eval_count >= budget:\n        if best_x is None:\n            best_x = (lower + upper) / 2.0\n        return best_x\n\n    remaining = budget - eval_count\n\n    # Determine global vs local budget split\n    # Prefer more global exploration in higher dimensions.\n    if dim <= 3:\n        global_frac = 0.4\n    elif dim <= 10:\n        global_frac = 0.5\n    else:\n        global_frac = 0.65\n\n    global_evals = int(global_frac * remaining)\n    # Ensure a minimum number of global samples\n    global_evals = max(global_evals, min(20 * dim, remaining))\n    global_evals = min(global_evals, remaining)\n\n    # --- Global search: Latin-hypercube-like sampling ---\n    if global_evals > 0:\n        n_samples = global_evals\n\n        # LHS: in each dim, divide [0,1] into n_samples intervals and shuffle\n        u = (np.arange(n_samples) + np.random.rand(n_samples)) / n_samples\n        points = np.empty((n_samples, dim), dtype=float)\n        for d in range(dim):\n            np.random.shuffle(u)\n            points[:, d] = u\n        candidates = lower + points * span\n\n        for i in range(n_samples):\n            if eval_count >= budget:\n                break\n            evaluate(candidates[i])\n\n    if eval_count >= budget:\n        return best_x if best_x is not None else (lower + upper) / 2.0\n\n    # --- Local search phase ---\n    if best_x is None:\n        # If nothing evaluated (e.g., bounds degenerate), fallback to center\n        best_x = (lower + upper) / 2.0\n        evaluate(best_x)\n        if eval_count >= budget:\n            return best_x\n\n    remaining = budget - eval_count\n    if remaining <= 0:\n        return best_x\n\n    # Adaptive Gaussian search parameters\n    base_sigma = 0.2 * span\n    # Avoid zero span leading to zero step; pick small but non-zero\n    base_sigma[base_sigma == 0] = 0.1\n    sigma = base_sigma.copy()\n\n    no_improve = 0\n    last_best_y = best_y\n\n    # Define how often to adapt step size\n    adapt_interval = max(5 * dim, 15)\n\n    # We'll combine global-style local perturbations and coordinate refinement\n    for i in range(remaining):\n        if eval_count >= budget:\n            break\n\n        # Check for stagnation and adapt sigma\n        if i > 0 and i % adapt_interval == 0:\n            if best_y >= last_best_y - 1e-12:\n                sigma *= 0.5\n                sigma = np.maximum(sigma, 0.01 * base_sigma)\n                no_improve += 1\n            else:\n                sigma *= 1.1\n                sigma = np.minimum(sigma, base_sigma)\n                no_improve = 0\n            last_best_y = best_y\n\n        # Occasional diversification when stagnating\n        if no_improve >= 3 and eval_count < budget:\n            restart_x = np.random.uniform(lower, upper)\n            evaluate(restart_x)\n            no_improve = 0\n            continue\n\n        # Alternate between full-dimensional Gaussian step and coordinate move\n        if i % 2 == 0 or dim == 1:\n            # Full-dimensional Gaussian perturbation\n            perturb = np.random.normal(scale=sigma, size=dim)\n            x_new = best_x + perturb\n        else:\n            # Coordinate-wise perturbation along a random dimension\n            d = np.random.randint(0, dim)\n            x_new = best_x.copy()\n            step = np.random.normal(scale=sigma[d])\n            x_new[d] += step\n\n        x_new = clip_to_bounds(x_new)\n        old_best_y = best_y\n        y_new = evaluate(x_new)\n        if y_new < old_best_y - 1e-12:\n            no_improve = 0\n\n    if best_x is None:\n        best_x = (lower + upper) / 2.0\n    return best_x",
    "X": "0.4494175693115653 0.06693631134540516 0.9083798702940449 0.27072520213993534"
}