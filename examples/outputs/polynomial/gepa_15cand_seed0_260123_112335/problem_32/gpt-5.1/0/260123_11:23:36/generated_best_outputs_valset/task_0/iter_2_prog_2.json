{
    "score": -18.355649527034434,
    "Input": "McCourt23",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef _random_in_bounds(bounds, rng):\n    return rng.uniform(bounds[:, 0], bounds[:, 1])\n\n\ndef _clip_to_bounds(x, bounds):\n    return np.clip(x, bounds[:, 0], bounds[:, 1])\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization using a hybrid multi-start local search:\n    - Global: random sampling (optionally seeded by prev_best_x neighborhood)\n    - Local: adaptive Gaussian perturbation around best-so-far candidates\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config.get(\"budget\", 1))\n\n    # Safety checks\n    if bounds.shape[0] != dim:\n        bounds = bounds.reshape(dim, 2)\n\n    # Edge case: no budget\n    if budget <= 0:\n        return (bounds[:, 0] + bounds[:, 1]) / 2.0\n\n    def eval_x(x):\n        return objective_function(x)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # -------- Initialization with optional warm start --------\n    # Try prev_best_x once if valid\n    if prev_best_x is not None and evals_used < budget:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            x0 = _clip_to_bounds(x0, bounds)\n            y0 = eval_x(x0)\n            evals_used += 1\n            best_x, best_y = x0, y0\n        except Exception:\n            best_x, best_y = None, np.inf\n\n    # If still no incumbent, pick a random point\n    if best_x is None and evals_used < budget:\n        x = _random_in_bounds(bounds, rng)\n        y = eval_x(x)\n        evals_used += 1\n        best_x, best_y = x, y\n\n    if evals_used >= budget:\n        return np.asarray(best_x, dtype=float)\n\n    remaining = budget - evals_used\n\n    # -------- Global sampling phase (multi-start) --------\n    # Use ~50% of remaining budget for global search (at least 2 if possible)\n    global_evals = max(2, int(0.5 * remaining))\n    global_evals = min(global_evals, remaining)\n    local_budget = remaining - global_evals\n\n    # Ensure at least some budget for local search\n    if local_budget < 0:\n        local_budget = 0\n\n    # Track a small pool of good global candidates for local refinement\n    pool_size = min(5, max(1, global_evals // 4))\n    pool_x = []\n    pool_y = []\n\n    # Define box sizes once\n    box_sizes = bounds[:, 1] - bounds[:, 0]\n    box_sizes[box_sizes == 0.0] = 1.0\n\n    # If we have a warm start, occasionally sample near it during global phase\n    use_prev_neighborhood = prev_best_x is not None\n\n    for i in range(global_evals):\n        if evals_used >= budget:\n            break\n\n        if use_prev_neighborhood and (i % 4 == 0):\n            # Sample from a relatively wide Gaussian around best_x, then clip\n            step = 0.5 * box_sizes\n            cand = best_x + rng.normal(loc=0.0, scale=step, size=dim)\n            x = _clip_to_bounds(cand, bounds)\n        else:\n            x = _random_in_bounds(bounds, rng)\n\n        y = eval_x(x)\n        evals_used += 1\n\n        if y < best_y:\n            best_x, best_y = x, y\n\n        # Maintain pool of best global candidates\n        if len(pool_x) < pool_size:\n            pool_x.append(x)\n            pool_y.append(y)\n        else:\n            worst_idx = int(np.argmax(pool_y))\n            if y < pool_y[worst_idx]:\n                pool_x[worst_idx] = x\n                pool_y[worst_idx] = y\n\n    if evals_used >= budget or local_budget <= 0:\n        return np.asarray(best_x, dtype=float)\n\n    # -------- Local search phase: adaptive Gaussian around top pool points --------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return np.asarray(best_x, dtype=float)\n\n    # Recompute in case global was cut short\n    local_budget = remaining\n\n    # Base step size as fraction of box size\n    base_step = 0.15 * box_sizes\n\n    # Distribute local budget among pool members\n    n_starts = max(1, len(pool_x))\n    evals_per_start = max(1, local_budget // n_starts)\n\n    # If few budget, focus only on the current best\n    if local_budget < n_starts * 2:\n        pool_x = [best_x]\n        pool_y = [best_y]\n        n_starts = 1\n        evals_per_start = local_budget\n\n    for start_idx in range(n_starts):\n        if evals_used >= budget:\n            break\n\n        center_x = pool_x[start_idx]\n        center_y = pool_y[start_idx]\n\n        step_size = base_step.copy()\n        successes = 0\n        attempts = 0\n        adapt_interval = max(5, dim)\n\n        for _ in range(evals_per_start):\n            if evals_used >= budget:\n                break\n\n            noise = rng.normal(loc=0.0, scale=step_size, size=dim)\n            cand = center_x + noise\n            cand = _clip_to_bounds(cand, bounds)\n            y = eval_x(cand)\n            evals_used += 1\n            attempts += 1\n\n            improved_global = False\n            if y < best_y:\n                best_x, best_y = cand, y\n                improved_global = True\n\n            if y < center_y:\n                center_x, center_y = cand, y\n                successes += 1\n                if improved_global:\n                    # Slightly enlarge step where we see improvements\n                    step_size *= 1.05\n\n            if attempts >= adapt_interval:\n                success_rate = successes / float(attempts)\n                # Target success rate around 0.25\n                if success_rate < 0.15:\n                    step_size *= 0.6\n                elif success_rate > 0.35:\n                    step_size *= 1.4\n                # Bound step size to avoid collapse or explosion\n                step_size = np.maximum(step_size, 1e-8 * box_sizes)\n                step_size = np.minimum(step_size, box_sizes)\n                successes = 0\n                attempts = 0\n\n    return np.asarray(best_x, dtype=float)",
    "X": "0.7368849849248382 0.399001618869169 0.0 0.7285609453960111 0.537799089441198 0.7162461017975589"
}