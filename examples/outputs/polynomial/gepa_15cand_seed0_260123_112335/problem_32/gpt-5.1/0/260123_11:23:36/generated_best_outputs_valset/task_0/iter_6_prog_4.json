{
    "score": -18.35750126536961,
    "Input": "McCourt23",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef _random_in_bounds(bounds, rng):\n    return rng.uniform(bounds[:, 0], bounds[:, 1])\n\n\ndef _clip_to_bounds(x, bounds):\n    return np.clip(x, bounds[:, 0], bounds[:, 1])\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization using a hybrid multi-start local search + coordinate search:\n    - Global: stratified / quasi-random sampling (optionally exploiting prev_best_x)\n    - Local: adaptive Gaussian perturbation around pool of good points\n    - Refinement: simple coordinate-wise line search around final best\n\n    Keeps main structure of the previous version but:\n    - Uses slightly more global search on small budgets for robustness\n    - Adds a cheap greedy coordinate search to better exploit promising solutions\n    - Guards more carefully against degenerate bounds and shape issues\n    \"\"\"\n    # Optional reproducible seed support\n    seed = int(config.get(\"seed\", 0)) if \"seed\" in config else None\n    rng = np.random.RandomState(seed)\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config.get(\"budget\", 1))\n\n    # Safety for bounds shape\n    if bounds.ndim != 2 or bounds.shape[1] != 2:\n        bounds = np.asarray(bounds, dtype=float).reshape(dim, 2)\n\n    if bounds.shape[0] != dim:\n        dim = bounds.shape[0]\n\n    # Edge case: no budget\n    if budget <= 0:\n        # Return center of bounds\n        return (bounds[:, 0] + bounds[:, 1]) / 2.0\n\n    def eval_x(x):\n        return objective_function(x)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # -------- Initialization with optional warm start --------\n    if prev_best_x is not None and evals_used < budget:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            x0 = _clip_to_bounds(x0, bounds)\n            y0 = eval_x(x0)\n            evals_used += 1\n            best_x, best_y = x0, y0\n        except Exception:\n            best_x, best_y = None, np.inf\n\n    # If still no incumbent, pick a random point\n    if best_x is None and evals_used < budget:\n        x = _random_in_bounds(bounds, rng)\n        y = eval_x(x)\n        evals_used += 1\n        best_x, best_y = x, y\n\n    if evals_used >= budget:\n        return np.asarray(best_x, dtype=float)\n\n    remaining = budget - evals_used\n\n    # -------- Budget allocation: global vs local --------\n    # For very small budgets, favor global sampling heavily\n    if remaining <= 8:\n        global_evals = remaining\n        local_budget = 0\n    else:\n        # More nuanced split: more local search as budget grows\n        frac_local = 0.35 + 0.25 * np.tanh((remaining - 30) / 40.0)\n        frac_local = float(np.clip(frac_local, 0.3, 0.65))\n        local_budget = int(frac_local * remaining)\n        global_evals = remaining - local_budget\n\n        # Ensure at least a few global samples\n        min_global = min(6, remaining)\n        if global_evals < min_global:\n            global_evals = min_global\n            local_budget = max(0, remaining - global_evals)\n\n    if global_evals < 0:\n        global_evals = 0\n    if local_budget < 0:\n        local_budget = 0\n\n    # Track a small pool of good global candidates for local refinement\n    pool_size = min(10, max(3, global_evals // 3))\n    pool_x = []\n    pool_y = []\n\n    # Define box sizes once\n    box_sizes = bounds[:, 1] - bounds[:, 0]\n    # Avoid zero-size steps\n    zero_mask = box_sizes == 0.0\n    box_sizes[zero_mask] = 1.0\n\n    use_prev_neighborhood = prev_best_x is not None\n\n    # -------- Global sampling phase (multi-start) --------\n    if global_evals > 0:\n        # Stratified random in [0,1]^dim\n        base = (np.arange(global_evals) + 0.5) / global_evals\n        u = rng.rand(global_evals, dim) * (1.0 / global_evals)\n        u += base[:, None]\n        u = np.mod(u, 1.0)\n\n        for i in range(global_evals):\n            if evals_used >= budget:\n                break\n\n            # Occasionally sample around best_x if we have warm start\n            if use_prev_neighborhood and (i % 4 == 0):\n                step = 0.35 * box_sizes\n                cand = best_x + rng.normal(loc=0.0, scale=step, size=dim)\n                x = _clip_to_bounds(cand, bounds)\n            else:\n                x = bounds[:, 0] + u[i] * (bounds[:, 1] - bounds[:, 0])\n\n            y = eval_x(x)\n            evals_used += 1\n\n            if y < best_y:\n                best_x, best_y = x, y\n\n            # Maintain pool of best global candidates\n            if len(pool_x) < pool_size:\n                pool_x.append(x)\n                pool_y.append(y)\n            else:\n                worst_idx = int(np.argmax(pool_y))\n                if y < pool_y[worst_idx]:\n                    pool_x[worst_idx] = x\n                    pool_y[worst_idx] = y\n\n    if evals_used >= budget:\n        return np.asarray(best_x, dtype=float)\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return np.asarray(best_x, dtype=float)\n\n    # -------- Local search phase: adaptive Gaussian around top pool points --------\n    if local_budget > 0:\n        local_budget = min(local_budget, remaining)\n\n        # Base step size as fraction of box size\n        base_step = 0.12 * box_sizes\n\n        # Sort pool by quality and keep the best few\n        if pool_x:\n            pool_arr = np.asarray(pool_x)\n            pool_y_arr = np.asarray(pool_y)\n            order = np.argsort(pool_y_arr)\n            pool_x = [pool_arr[i] for i in order]\n            pool_y = [pool_y_arr[i] for i in order]\n        else:\n            pool_x = [best_x]\n            pool_y = [best_y]\n\n        # Number of distinct local starts (up to 6)\n        n_starts = min(len(pool_x), 6)\n        # Ensure we always include global best as first start\n        pool_x[0] = best_x\n        pool_y[0] = best_y\n\n        # Distribute local budget among starts (geometric weighting)\n        weights = np.array([0.55 ** i for i in range(n_starts)], dtype=float)\n        weights /= weights.sum()\n        evals_per_start = np.maximum(1, (weights * local_budget).astype(int))\n\n        # Fix rounding: ensure total == local_budget\n        diff = local_budget - int(evals_per_start.sum())\n        for i in range(abs(diff)):\n            idx = i % n_starts\n            if diff > 0:\n                evals_per_start[idx] += 1\n            elif evals_per_start[idx] > 1:\n                evals_per_start[idx] -= 1\n\n        adapt_interval_base = max(5, dim)\n\n        for start_idx in range(n_starts):\n            if evals_used >= budget:\n                break\n\n            n_eval_start = int(evals_per_start[start_idx])\n            if n_eval_start <= 0:\n                continue\n\n            center_x = np.array(pool_x[start_idx], dtype=float)\n            center_y = float(pool_y[start_idx])\n\n            step_size = base_step.copy()\n            successes = 0\n            attempts = 0\n            adapt_interval = adapt_interval_base\n\n            for _ in range(n_eval_start):\n                if evals_used >= budget:\n                    break\n\n                noise = rng.normal(loc=0.0, scale=step_size, size=dim)\n                cand = center_x + noise\n                cand = _clip_to_bounds(cand, bounds)\n                y = eval_x(cand)\n                evals_used += 1\n                attempts += 1\n\n                improved_global = False\n                if y < best_y:\n                    best_x, best_y = cand, y\n                    improved_global = True\n\n                if y < center_y:\n                    center_x, center_y = cand, y\n                    successes += 1\n                    if improved_global:\n                        # Slightly enlarge step where we see improvements\n                        step_size *= 1.08\n\n                if attempts >= adapt_interval:\n                    success_rate = successes / float(attempts)\n                    # Target success rate ~0.25\n                    if success_rate < 0.15:\n                        step_size *= 0.5\n                    elif success_rate > 0.40:\n                        step_size *= 1.5\n                    # Bound step size\n                    step_size = np.maximum(step_size, 1e-8 * box_sizes)\n                    step_size = np.minimum(step_size, 0.5 * box_sizes)\n                    successes = 0\n                    attempts = 0\n\n    # -------- Final cheap coordinate-wise refinement around best_x --------\n    remaining = budget - evals_used\n    if remaining > 0 and best_x is not None:\n        x_curr = np.asarray(best_x, dtype=float).copy()\n        y_curr = float(best_y)\n\n        # Use small fraction of remaining budget for coordinate search\n        coord_budget = max(1, int(0.6 * remaining))\n        per_dim = max(1, coord_budget // (2 * dim))  # 2 directions per dimension\n\n        # Step size per dimension\n        coord_step = 0.05 * box_sizes\n        coord_step = np.maximum(coord_step, 1e-8 * box_sizes)\n\n        for d in range(dim):\n            if evals_used >= budget:\n                break\n            for _ in range(per_dim):\n                if evals_used >= budget:\n                    break\n\n                for direction in (+1.0, -1.0):\n                    if evals_used >= budget:\n                        break\n                    step_vec = np.zeros(dim, dtype=float)\n                    step_vec[d] = direction * coord_step[d]\n                    cand = x_curr + step_vec\n                    cand = _clip_to_bounds(cand, bounds)\n                    y = eval_x(cand)\n                    evals_used += 1\n\n                    if y < y_curr:\n                        x_curr, y_curr = cand, y\n                        if y < best_y:\n                            best_x, best_y = cand, y\n\n        best_x = x_curr\n        best_y = y_curr\n\n    return np.asarray(best_x, dtype=float)",
    "X": "0.7267965418256818 0.39592453363912694 0.0 0.7267992308639436 0.5306268829561711 0.7033763809209183"
}