{
    "score": -18.357502460573695,
    "Input": "McCourt23",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef _random_in_bounds(bounds, rng):\n    return rng.uniform(bounds[:, 0], bounds[:, 1])\n\n\ndef _clip_to_bounds(x, bounds):\n    return np.clip(x, bounds[:, 0], bounds[:, 1])\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Blackbox minimization using a hybrid global sampling + adaptive local search.\n\n    This version:\n    - Keeps DE-like global search and adaptive local search\n    - Adds cheap low-discrepancy global sampling for very small budgets\n    - Adds a quick restart/perturbation of prev_best_x to avoid trapping\n    - Simplifies and stabilizes coordinate refinement\n    - Tightens dimension/budget dependent heuristics\n    \"\"\"\n    # -------- Setup and basic parsing --------\n    seed = int(config.get(\"seed\", 0)) if \"seed\" in config else None\n    rng = np.random.RandomState(seed)\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config.get(\"budget\", 1))\n\n    # Safety for bounds shape\n    if bounds.ndim != 2 or bounds.shape[1] != 2:\n        bounds = np.asarray(bounds, dtype=float).reshape(dim, 2)\n    if bounds.shape[0] != dim:\n        dim = bounds.shape[0]\n\n    # Edge case: no budget -> return center of bounds\n    if budget <= 0:\n        return (bounds[:, 0] + bounds[:, 1]) / 2.0\n\n    box_sizes = bounds[:, 1] - bounds[:, 0]\n    zero_mask = box_sizes == 0.0\n    box_sizes[zero_mask] = 1.0  # avoid zeros in step-size calculations\n\n    def eval_x(x):\n        return objective_function(x)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # -------- Initialization with optional warm start --------\n    if prev_best_x is not None and evals_used < budget:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            x0 = _clip_to_bounds(x0, bounds)\n            y0 = eval_x(x0)\n            evals_used += 1\n            best_x, best_y = x0, float(y0)\n\n            # Small local random perturbations around prev_best_x to escape shallow traps\n            remaining_for_perturb = min(4, budget - evals_used)\n            if remaining_for_perturb > 0:\n                step = 0.05 * box_sizes\n                step = np.maximum(step, 1e-8 * box_sizes)\n                for _ in range(remaining_for_perturb):\n                    if evals_used >= budget:\n                        break\n                    noise = rng.normal(0.0, step, size=dim)\n                    cand = _clip_to_bounds(x0 + noise, bounds)\n                    y = eval_x(cand)\n                    evals_used += 1\n                    y = float(y)\n                    if y < best_y:\n                        best_x, best_y = cand.copy(), y\n        except Exception:\n            best_x, best_y = None, np.inf\n\n    # Fallback to center if no valid warm start or no improvement\n    if best_x is None and evals_used < budget:\n        x_center = (bounds[:, 0] + bounds[:, 1]) / 2.0\n        y_center = eval_x(x_center)\n        evals_used += 1\n        best_x, best_y = x_center, float(y_center)\n\n    if evals_used >= budget:\n        return np.asarray(best_x, dtype=float)\n\n    remaining = budget - evals_used\n\n    # -------- Very small budget strategy: quasi-random sampling only --------\n    # For extremely small budgets, complex DE / local phases are ineffective.\n    if remaining <= 6:\n        # Use a simple low-discrepancy-like grid/jitter sampling\n        n_samples = remaining\n        for k in range(n_samples):\n            if evals_used >= budget:\n                break\n            # Halton-like scalar sequence mapped to dim\n            t = (k + 0.5) / n_samples\n            base = 2\n            halton = []\n            f = 1.0 / base\n            tt = t\n            while len(halton) < dim:\n                tt *= base\n                digit = int(tt)\n                tt -= digit\n                halton.append(digit * f)\n                f /= base\n                base += 1\n            halton = np.array(halton[:dim])\n            cand = bounds[:, 0] + halton * (bounds[:, 1] - bounds[:, 0])\n            cand = _clip_to_bounds(cand, bounds)\n            y = eval_x(cand)\n            evals_used += 1\n            y = float(y)\n            if y < best_y:\n                best_x, best_y = cand.copy(), y\n\n        return np.asarray(best_x, dtype=float)\n\n    # -------- Budget allocation: global vs local --------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return np.asarray(best_x, dtype=float)\n\n    # Slightly more global search, but still adaptive\n    # More local search if budget is large vs dim\n    budget_per_dim = remaining / float(max(dim, 1))\n    if budget_per_dim < 3.0:\n        frac_local = 0.15\n    elif budget_per_dim < 10.0:\n        frac_local = 0.25\n    else:\n        frac_local = 0.35 + 0.25 * np.tanh((remaining - 80) / 80.0)\n        frac_local = float(np.clip(frac_local, 0.25, 0.60))\n\n    local_budget = int(frac_local * remaining)\n    global_evals = remaining - local_budget\n\n    # Ensure minimum global sampling\n    min_global = min(max(10, 3 * dim), remaining)\n    if global_evals < min_global:\n        global_evals = min_global\n        local_budget = max(0, remaining - global_evals)\n\n    if global_evals < 0:\n        global_evals = 0\n    if local_budget < 0:\n        local_budget = 0\n\n    # -------- Global phase: DE-like population evolution --------\n    pool_x = []\n    pool_y = []\n\n    if global_evals > 0 and evals_used < budget:\n        # Population size scaled with dimension and global budget\n        # Keep pop_size moderate to allow multiple DE generations\n        pop_size = min(max(6, 4 * dim), global_evals // 2 if global_evals >= 4 else global_evals)\n        pop_size = max(4, pop_size)\n        pop_size = min(pop_size, global_evals)  # at least one eval per member\n\n        # In very high dim with tiny budget, just do sampling\n        if pop_size <= 0:\n            pop_size = max(1, min(global_evals, dim * 2))\n\n        pop = np.empty((pop_size, dim), dtype=float)\n\n        # Use current best as one elite in the population\n        pop[0] = best_x\n\n        # Latin-hypercube-ish initialization for the rest\n        if pop_size > 1:\n            u = (np.arange(pop_size - 1) + rng.rand(pop_size - 1)) / (pop_size - 1)\n            u = u[:, None]\n            jitter = rng.rand(pop_size - 1, dim) / (pop_size - 1)\n            u = (u + jitter) % 1.0\n            pop[1:] = bounds[:, 0] + u * (bounds[:, 1] - bounds[:, 0])\n\n        # Evaluate initial population\n        pop_y = np.empty(pop_size, dtype=float)\n        for i in range(pop_size):\n            if evals_used >= budget or global_evals <= 0:\n                break\n            y = eval_x(pop[i])\n            evals_used += 1\n            global_evals -= 1\n            pop_y[i] = float(y)\n            if y < best_y:\n                best_x, best_y = pop[i].copy(), float(y)\n\n        # Keep initial population in pool\n        pool_x = [pop[i].copy() for i in range(pop_size)]\n        pool_y = [float(pop_y[i]) for i in range(pop_size)]\n\n        if evals_used < budget and global_evals > 0:\n            # DE parameters tuned to be slightly more explorative on first generations\n            F_base = 0.6\n            CR_base = 0.9\n\n            # Limit number of generations to use global_evals roughly evenly\n            # Assume ~pop_size evals per generation\n            while global_evals > 0 and evals_used < budget:\n                # Optionally adapt F / CR based on progress\n                F = F_base\n                CR = CR_base\n                # simple adaptation: smaller F in high dim\n                if dim > 30:\n                    F = 0.4\n                    CR = 0.85\n\n                for i in range(pop_size):\n                    if global_evals <= 0 or evals_used >= budget:\n                        break\n\n                    # Mutation: choose r1, r2, r3 distinct from i\n                    idxs = np.arange(pop_size)\n                    rng.shuffle(idxs)\n                    # ensure we have at least 4 distinct indices including i\n                    if pop_size >= 4:\n                        candidates = [j for j in idxs if j != i]\n                        if len(candidates) < 3:\n                            candidates = [j for j in range(pop_size) if j != i]\n                        r1, r2, r3 = rng.choice(candidates, size=3, replace=False)\n                    else:\n                        # fall back to random sampling if too small population\n                        mutant = _random_in_bounds(bounds, rng)\n                        trial = mutant\n                        trial = _clip_to_bounds(trial, bounds)\n                        y_trial = eval_x(trial)\n                        evals_used += 1\n                        global_evals -= 1\n                        y_trial = float(y_trial)\n                        if y_trial < best_y:\n                            best_x, best_y = trial.copy(), y_trial\n                        pool_x.append(trial.copy())\n                        pool_y.append(y_trial)\n                        continue\n\n                    mutant = pop[r1] + F * (pop[r2] - pop[r3])\n\n                    # Crossover\n                    cross_mask = rng.rand(dim) < CR\n                    if not np.any(cross_mask):\n                        cross_mask[rng.randint(dim)] = True\n                    trial = np.where(cross_mask, mutant, pop[i])\n                    trial = _clip_to_bounds(trial, bounds)\n\n                    y_trial = eval_x(trial)\n                    evals_used += 1\n                    global_evals -= 1\n                    y_trial = float(y_trial)\n\n                    if y_trial <= pop_y[i]:\n                        pop[i] = trial\n                        pop_y[i] = y_trial\n\n                    if y_trial < best_y:\n                        best_x, best_y = trial.copy(), y_trial\n\n                    # Maintain pool of good candidates (limited size to avoid blow-up)\n                    pool_x.append(trial.copy())\n                    pool_y.append(y_trial)\n                    if len(pool_x) > 5 * pop_size:\n                        arr_y = np.asarray(pool_y)\n                        order = np.argsort(arr_y)\n                        keep = min(3 * pop_size, len(order))\n                        pool_x = [pool_x[j] for j in order[:keep]]\n                        pool_y = [float(arr_y[j]) for j in order[:keep]]\n\n                    if global_evals <= 0 or evals_used >= budget:\n                        break\n\n    if evals_used >= budget:\n        return np.asarray(best_x, dtype=float)\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return np.asarray(best_x, dtype=float)\n\n    # -------- Prepare pool for local search --------\n    if not pool_x:\n        pool_x = [best_x]\n        pool_y = [float(best_y)]\n    else:\n        pool_arr = np.asarray(pool_x)\n        pool_y_arr = np.asarray(pool_y, dtype=float)\n        order = np.argsort(pool_y_arr)\n        pool_x = [pool_arr[i] for i in order]\n        pool_y = [float(pool_y_arr[i]) for i in order]\n\n    # Force inclusion of global best at index 0\n    pool_x[0] = np.asarray(best_x, dtype=float)\n    pool_y[0] = float(best_y)\n\n    # -------- Local search phase: adaptive Gaussian around top pool points --------\n    if local_budget > 0 and evals_used < budget:\n        remaining = budget - evals_used\n        local_budget = min(local_budget, remaining)\n\n        # Base step relative to box size; smaller in high-dim\n        if dim <= 10:\n            scale_factor = 0.12\n        elif dim <= 30:\n            scale_factor = 0.08\n        else:\n            scale_factor = 0.05\n        base_step = scale_factor * box_sizes\n\n        # Limit number of local starts to keep per-start intensity reasonable\n        n_starts_max = max(3, 2 * (1 + dim // 25))\n        n_starts = min(len(pool_x), n_starts_max)\n\n        # Allocate more local budget to better starts via geometric weights\n        weights = np.array([0.55 ** i for i in range(n_starts)], dtype=float)\n        weights /= weights.sum()\n        evals_per_start = np.maximum(1, (weights * local_budget).astype(int))\n\n        diff = local_budget - int(evals_per_start.sum())\n        for i in range(abs(diff)):\n            idx = i % n_starts\n            if diff > 0:\n                evals_per_start[idx] += 1\n            elif evals_per_start[idx] > 1:\n                evals_per_start[idx] -= 1\n\n        adapt_interval_base = max(5, dim)\n\n        for start_idx in range(n_starts):\n            if evals_used >= budget:\n                break\n\n            n_eval_start = int(evals_per_start[start_idx])\n            if n_eval_start <= 0:\n                continue\n\n            center_x = np.array(pool_x[start_idx], dtype=float)\n            center_y = float(pool_y[start_idx])\n\n            step_size = base_step.copy()\n            step_size = np.maximum(step_size, 1e-8 * box_sizes)\n            step_size = np.minimum(step_size, 0.5 * box_sizes)\n\n            successes = 0\n            attempts = 0\n            adapt_interval = adapt_interval_base\n\n            for _ in range(n_eval_start):\n                if evals_used >= budget:\n                    break\n\n                noise = rng.normal(loc=0.0, scale=step_size, size=dim)\n                cand = center_x + noise\n                cand = _clip_to_bounds(cand, bounds)\n                y = eval_x(cand)\n                evals_used += 1\n                y = float(y)\n                attempts += 1\n\n                improved_global = False\n                if y < best_y:\n                    best_x, best_y = cand.copy(), y\n                    improved_global = True\n\n                if y < center_y:\n                    center_x, center_y = cand, y\n                    successes += 1\n                    if improved_global:\n                        step_size *= 1.05\n\n                if attempts >= adapt_interval:\n                    success_rate = successes / float(max(attempts, 1))\n                    if success_rate < 0.18:\n                        step_size *= 0.5\n                    elif success_rate > 0.45:\n                        step_size *= 1.4\n                    step_size = np.maximum(step_size, 1e-8 * box_sizes)\n                    step_size = np.minimum(step_size, 0.5 * box_sizes)\n                    successes = 0\n                    attempts = 0\n\n    # -------- Final coordinate-wise refinement around best_x --------\n    remaining = budget - evals_used\n    if remaining > 0 and best_x is not None:\n        x_curr = np.asarray(best_x, dtype=float).copy()\n        y_curr = float(best_y)\n\n        coord_budget = remaining\n        if coord_budget <= 0:\n            return np.asarray(best_x, dtype=float)\n\n        # Per-dimension passes limited to keep budget use controlled\n        per_dim_passes = max(1, min(3, coord_budget // (2 * dim))) if coord_budget >= 2 * dim else 1\n\n        coord_step = 0.03 * box_sizes\n        coord_step = np.maximum(coord_step, 1e-8 * box_sizes)\n\n        for _ in range(per_dim_passes):\n            if evals_used >= budget:\n                break\n            for d in range(dim):\n                if evals_used >= budget:\n                    break\n                for direction in (+1.0, -1.0):\n                    if evals_used >= budget:\n                        break\n                    step_vec = np.zeros(dim, dtype=float)\n                    step_vec[d] = direction * coord_step[d]\n                    cand = x_curr + step_vec\n                    cand = _clip_to_bounds(cand, bounds)\n                    y = eval_x(cand)\n                    evals_used += 1\n                    y = float(y)\n                    if y < y_curr:\n                        x_curr, y_curr = cand, y\n                        if y < best_y:\n                            best_x, best_y = cand.copy(), y\n\n        best_x = x_curr\n        best_y = y_curr\n\n    return np.asarray(best_x, dtype=float)",
    "X": "0.7267949187940442 0.3959241922769366 0.0 0.726794919504668 0.5306351842455859 0.7033826622664415"
}