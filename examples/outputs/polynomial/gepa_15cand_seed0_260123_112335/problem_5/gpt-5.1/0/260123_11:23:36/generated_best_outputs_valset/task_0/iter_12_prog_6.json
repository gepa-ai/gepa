{
    "score": 7.779576249859547e-57,
    "Input": "Csendes",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid global-local blackbox minimization with adaptive budget allocation.\n\n    Strategy (kept and refined from previous version):\n    - Warm-start from prev_best_x when available and compatible\n    - Global search with quasi-random (low-discrepancy-like) exploration\n      and sampling around current best\n    - Local search via adaptive coordinate-wise pattern search with random\n      perturbations\n    - Budget and dimension aware: adapts exploration / exploitation split\n      and step sizes\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", len(bounds)))\n    budget = int(config.get(\"budget\", 1))\n\n    # Basic guards\n    if dim <= 0 or budget <= 0:\n        mid = bounds[:, 0] + 0.5 * (bounds[:, 1] - bounds[:, 0])\n        return np.asarray(mid, dtype=float)\n\n    lower = bounds[:, 0].astype(float)\n    upper = bounds[:, 1].astype(float)\n    span = upper - lower\n\n    # Handle zero-span dimensions robustly\n    fixed_mask = span <= 0.0\n    span_safe = np.where(fixed_mask, 1.0, span)\n\n    def clamp(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return best_y if best_y is not None else np.inf\n        y = objective_function(x)\n        evals_used += 1\n        if best_x is None or y < best_y:\n            best_x = np.array(x, copy=True)\n            best_y = float(y)\n        return y\n\n    # ---- Initialization: warm start + random / LHS-like starts ----\n\n    # 1) Warm-start from prev_best_x if compatible\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape[0] == dim:\n            x0 = clamp(x0)\n            eval_point(x0)\n\n    # 2) Additional initial points\n    remaining = budget - evals_used\n    if remaining > 0:\n        # Slightly denser initial coverage to quickly find good basins,\n        # especially useful for multimodal problems like Csendes\n        if budget < 40:\n            init_trials = min(remaining, 5)\n        else:\n            init_trials = min(max(8, budget // 25), 20)\n\n        n_init = init_trials\n        if n_init > 0:\n            cut = np.linspace(0.0, 1.0, n_init + 1)\n            u = np.random.rand(n_init, dim)\n            a = cut[:n_init]\n            b = cut[1:n_init + 1]\n            u = a[:, None] + (b - a)[:, None] * u\n            for d in range(dim):\n                np.random.shuffle(u[:, d])\n            xs = lower + u * span_safe\n            for k in range(n_init):\n                if evals_used >= budget:\n                    break\n                eval_point(xs[k])\n\n    if best_x is None:\n        # Fallback: midpoint\n        x_mid = clamp(lower + 0.5 * span)\n        eval_point(x_mid)\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n\n    # ---- Budget split: global vs local ----\n    # Slightly more global search for higher dim or very small budget\n    if remaining < 40 or dim > 20:\n        global_frac = 0.7\n    else:\n        global_frac = 0.55\n\n    global_budget = max(1, int(global_frac * remaining))\n    global_budget = min(global_budget, remaining)\n    local_budget = remaining - global_budget\n\n    # ---- Quasi-random utilities ----\n    def van_der_corput(n, base=2):\n        seq = np.empty(n, dtype=float)\n        for i in range(n):\n            v = i\n            denom = 1.0\n            x = 0.0\n            while v:\n                v, r = divmod(v, base)\n                denom *= base\n                x += r / denom\n            seq[i] = x\n        return seq\n\n    def quasi_random_points(n_points, dim_):\n        if n_points <= 0:\n            return np.empty((0, dim_), dtype=float)\n        base_sequence = van_der_corput(n_points + 16, 2)[8:8 + n_points]\n        pts = np.empty((n_points, dim_), dtype=float)\n        for d in range(dim_):\n            perm = np.random.permutation(n_points)\n            pts[:, d] = base_sequence[perm]\n        jitter = (np.random.rand(n_points, dim_) - 0.5) / (4.0 * max(n_points, 1))\n        u = np.clip(pts + jitter, 0.0, 1.0)\n        return lower + u * span_safe\n\n    # ---- Global search: exploration + refinement ----\n    n_global = global_budget\n    # Increase refinement share a bit for multimodal but smooth landscapes\n    refine_budget = max(0, int(0.35 * n_global))\n    pure_explore_budget = max(0, n_global - refine_budget)\n\n    # 1) Pure exploration\n    if pure_explore_budget > 0 and evals_used < budget:\n        xs = quasi_random_points(pure_explore_budget, dim)\n        for i in range(pure_explore_budget):\n            if evals_used >= budget:\n                break\n            eval_point(xs[i])\n\n    # 2) Refinement around current best\n    if refine_budget > 0 and evals_used < budget and best_x is not None:\n        # Smaller base sigma to better exploit very sharp minima like Csendes\n        base_sigma = 0.12 * span_safe\n        base_sigma[fixed_mask] = 0.0\n        for t in range(refine_budget):\n            if evals_used >= budget:\n                break\n            frac = 1.0 - (t / max(refine_budget - 1, 1))\n            sigma = np.maximum(base_sigma * (0.4 + 0.6 * frac), 1e-16)\n            noise = np.random.randn(dim) * sigma\n            cand = clamp(best_x + noise)\n            eval_point(cand)\n\n    if evals_used >= budget or local_budget <= 0:\n        return best_x\n\n    # ---- Local search: adaptive coordinate-wise search ----\n    remaining = budget - evals_used\n\n    # Step size selection: make initial steps smaller to avoid overshooting\n    # near narrow minima and allow finer-grained search\n    if remaining < 40:\n        step_frac = 0.18\n    elif dim <= 5:\n        step_frac = 0.16\n    else:\n        step_frac = 0.12\n\n    step = step_frac * span_safe\n    step[fixed_mask] = 0.0\n    min_step = 1e-10 * np.maximum(span_safe, 1.0)\n\n    # Local pattern search parameters\n    per_iter_cost = max(2 * dim + 3, 1)\n    max_iters = max(1, min(60, local_budget // per_iter_cost))\n\n    it = 0\n    no_improve_iters = 0\n\n    while evals_used < budget and it < max_iters and np.any(step > min_step):\n        it += 1\n        improved = False\n        current_best_x = best_x.copy()\n\n        order = np.arange(dim)\n        np.random.shuffle(order)\n\n        for j in order:\n            if evals_used >= budget:\n                break\n            if fixed_mask[j]:\n                continue\n\n            # +/- step moves\n            for direction in (-1.0, 1.0):\n                if evals_used >= budget:\n                    break\n                cand = current_best_x.copy()\n                cand[j] = cand[j] + direction * step[j]\n                cand = clamp(cand)\n                y = eval_point(cand)\n                if y < best_y:\n                    improved = True\n                    current_best_x = best_x.copy()\n\n            if evals_used >= budget:\n                break\n\n            # Additional random moves within step scale on this coordinate\n            if step[j] > 0:\n                for _ in range(2):\n                    if evals_used >= budget:\n                        break\n                    rnd = (2.0 * np.random.rand() - 1.0) * step[j]\n                    cand = current_best_x.copy()\n                    cand[j] = cand[j] + rnd\n                    cand = clamp(cand)\n                    y = eval_point(cand)\n                    if y < best_y:\n                        improved = True\n                        current_best_x = best_x.copy()\n\n        if evals_used >= budget:\n            break\n\n        # Occasional full-dimensional Gaussian step to escape locals\n        full_noise = np.random.randn(dim) * (0.35 * step)\n        full_noise[fixed_mask] = 0.0\n        cand = clamp(best_x + full_noise)\n        y = eval_point(cand)\n        if y < best_y:\n            improved = True\n\n        # Step size adaptation\n        if not improved:\n            no_improve_iters += 1\n            # Gentle decay to allow fine convergence to tiny values\n            if no_improve_iters <= 3:\n                decay = 0.7\n            elif no_improve_iters <= 7:\n                decay = 0.5\n            else:\n                decay = 0.35\n            step *= decay\n        else:\n            no_improve_iters = 0\n            # Mild increase to promote local exploration while converging\n            step *= 1.05\n\n        step = np.maximum(step, min_step)\n\n    return best_x",
    "X": "-3.897732994734483e-10 1.0089165993638733e-10"
}