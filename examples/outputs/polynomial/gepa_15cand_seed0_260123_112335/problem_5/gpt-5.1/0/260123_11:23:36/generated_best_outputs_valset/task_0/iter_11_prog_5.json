{
    "score": 2.43014945828335e-50,
    "Input": "Csendes",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid global-local blackbox minimization with adaptive budget allocation.\n\n    Strategy:\n    - Careful initialization (uses prev_best_x if available, plus random / quasi-random starts)\n    - Global search with quasi-random exploration + focused refinement\n    - Local search via adaptive coordinate-wise stochastic search\n    - Automatically adapts to dimensionality and budget size\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", len(bounds)))\n    budget = int(config.get(\"budget\", 1))\n\n    # Basic guards\n    if dim <= 0 or budget <= 0:\n        mid = bounds[:, 0] + 0.5 * (bounds[:, 1] - bounds[:, 0])\n        return np.asarray(mid, dtype=float)\n\n    lower = bounds[:, 0].astype(float)\n    upper = bounds[:, 1].astype(float)\n    span = upper - lower\n\n    # Handle zero-span dimensions robustly\n    fixed_mask = span <= 0.0\n    span_safe = span.copy()\n    span_safe[fixed_mask] = 1.0\n\n    def clamp(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            # Should not be called when budget exhausted, but guard anyway\n            return best_y if best_y is not None else np.inf\n        y = objective_function(x)\n        evals_used += 1\n        if best_x is None or y < best_y:\n            best_x = np.array(x, copy=True)\n            best_y = float(y)\n        return y\n\n    # ---- Initialization: warm start + random starts ----\n    # 1) prev_best_x if compatible\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape[0] == dim:\n            x0 = clamp(x0)\n            eval_point(x0)\n\n    # 2) Random / quasi-random initial points (proportional to budget, capped)\n    remaining = budget - evals_used\n    if remaining > 0:\n        if budget < 30:\n            init_trials = min(remaining, 3)\n        else:\n            init_trials = min(max(5, budget // 30), 15)\n\n        # Use a simple Latin-hypercube-like sampling for initialization\n        n_init = init_trials\n        if n_init > 0:\n            # Generate stratified samples in [0,1]^dim\n            cut = np.linspace(0.0, 1.0, n_init + 1)\n            u = np.random.rand(n_init, dim)\n            a = cut[:n_init]\n            b = cut[1:n_init + 1]\n            u = a[:, None] + (b - a)[:, None] * u\n            # Shuffle each dimension independently\n            for d in range(dim):\n                np.random.shuffle(u[:, d])\n            xs = lower + u * span_safe\n            for k in range(n_init):\n                if evals_used >= budget:\n                    break\n                eval_point(xs[k])\n\n    if best_x is None:\n        # Fallback: midpoint\n        x_mid = lower + 0.5 * span\n        x_mid = clamp(x_mid)\n        eval_point(x_mid)\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n\n    # ---- Budget split: global vs local ----\n    # Slightly more global search for higher dim or very small budget\n    if remaining < 40 or dim > 20:\n        global_frac = 0.7\n    else:\n        global_frac = 0.55\n\n    global_budget = max(1, int(global_frac * remaining))\n    global_budget = min(global_budget, remaining)\n    local_budget = remaining - global_budget\n\n    # ---- Quasi-random utilities ----\n    def van_der_corput(n, base=2):\n        # Includes i=0 term which is 0; we will offset indices for better dispersion.\n        seq = np.empty(n, dtype=float)\n        for i in range(n):\n            v = i\n            denom = 1.0\n            x = 0.0\n            while v:\n                v, r = divmod(v, base)\n                denom *= base\n                x += r / denom\n            seq[i] = x\n        return seq\n\n    def quasi_random_points(n_points, dim_):\n        if n_points <= 0:\n            return np.empty((0, dim_), dtype=float)\n        # Skip the very first few low-discrepancy points to avoid clustering at 0\n        base_sequence = van_der_corput(n_points + 16, 2)[8:8 + n_points]\n        pts = np.empty((n_points, dim_), dtype=float)\n        for d in range(dim_):\n            perm = np.random.permutation(n_points)\n            pts[:, d] = base_sequence[perm]\n        # Small jitter to break grid structure\n        jitter = (np.random.rand(n_points, dim_) - 0.5) / (4.0 * max(n_points, 1))\n        u = np.clip(pts + jitter, 0.0, 1.0)\n        return lower + u * span_safe\n\n    # ---- Global search: exploration + refinement ----\n    n_global = global_budget\n    refine_budget = max(0, int(0.25 * n_global))\n    pure_explore_budget = max(0, n_global - refine_budget)\n\n    # 1) Pure exploration\n    if pure_explore_budget > 0 and evals_used < budget:\n        xs = quasi_random_points(pure_explore_budget, dim)\n        for i in range(pure_explore_budget):\n            if evals_used >= budget:\n                break\n            eval_point(xs[i])\n\n    # 2) Refinement around current best (Gaussian sampling, gradually shrinking)\n    if refine_budget > 0 and evals_used < budget and best_x is not None:\n        base_sigma = 0.18 * span_safe\n        base_sigma[fixed_mask] = 0.0\n        # Decrease sigma linearly over refinement iterations\n        for t in range(refine_budget):\n            if evals_used >= budget:\n                break\n            frac = 1.0 - (t / max(refine_budget - 1, 1))\n            sigma = np.maximum(base_sigma * (0.5 + 0.5 * frac), 1e-12)\n            noise = np.random.randn(dim) * sigma\n            cand = clamp(best_x + noise)\n            eval_point(cand)\n\n    if evals_used >= budget or local_budget <= 0:\n        return best_x\n\n    # ---- Local search: adaptive coordinate-wise search ----\n    remaining = budget - evals_used\n\n    # Initial step size: depends on remaining budget and dimension\n    if remaining < 40:\n        step_frac = 0.25\n    elif dim <= 5:\n        step_frac = 0.20\n    else:\n        step_frac = 0.15\n    step = step_frac * span_safe\n    step[fixed_mask] = 0.0\n    min_step = 1e-8 * np.maximum(span_safe, 1.0)\n\n    # Number of iterations; each iteration ~ 2*dim + 2 evaluations\n    per_iter_cost = max(2 * dim + 2, 1)\n    max_iters = max(1, min(40, local_budget // per_iter_cost))\n\n    it = 0\n    no_improve_iters = 0\n\n    while evals_used < budget and it < max_iters and np.any(step > min_step):\n        it += 1\n        improved = False\n\n        # Randomize coordinate order\n        order = np.arange(dim)\n        np.random.shuffle(order)\n\n        for j in order:\n            if evals_used >= budget:\n                break\n            if fixed_mask[j]:\n                continue\n\n            # +/- step\n            for direction in (-1.0, 1.0):\n                if evals_used >= budget:\n                    break\n                cand = np.array(best_x, copy=True)\n                cand[j] = cand[j] + direction * step[j]\n                cand = clamp(cand)\n                y = eval_point(cand)\n                if y < best_y:\n                    improved = True\n\n            if evals_used >= budget:\n                break\n\n            # Random within step range on this coordinate (three-way sampling)\n            if step[j] > 0:\n                for _ in range(2):\n                    if evals_used >= budget:\n                        break\n                    rnd = (2.0 * np.random.rand() - 1.0) * step[j]\n                    cand = np.array(best_x, copy=True)\n                    cand[j] = cand[j] + rnd\n                    cand = clamp(cand)\n                    y = eval_point(cand)\n                    if y < best_y:\n                        improved = True\n\n        if evals_used >= budget:\n            break\n\n        # Occasional full-dimensional Gaussian step to escape locals\n        full_noise = np.random.randn(dim) * (0.4 * step)\n        full_noise[fixed_mask] = 0.0\n        cand = clamp(best_x + full_noise)\n        y = eval_point(cand)\n        if y < best_y:\n            improved = True\n\n        # Adapt step size with a bit of momentum\n        if not improved:\n            no_improve_iters += 1\n            decay = 0.5 if no_improve_iters <= 3 else 0.3\n            step *= decay\n        else:\n            no_improve_iters = 0\n            step *= 1.08\n        step = np.maximum(step, min_step)\n\n    return best_x",
    "X": "1.5233042963531162e-09 4.842506526659554e-09"
}