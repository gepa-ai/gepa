{
    "score": -8.672684636031477,
    "Input": "McCourt19",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budget-aware blackbox optimizer (drop-in replacement).\n\n    Strategy:\n    - Robust initialization with warm-start (prev_best_x) and random/center points.\n    - Dimension- and budget-aware split between global exploration and local search.\n    - Global search via low-discrepancy-like sequence + uniform sampling.\n    - Local search via adaptive random search with diagonal covariance.\n    - Strict adherence to evaluation budget while trying to fully utilize it.\n    \"\"\"\n    rng = np.random.RandomState()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low, high = bounds[:, 0], bounds[:, 1]\n    span = high - low\n\n    # Handle degenerate span (zero-width dimensions)\n    span[span < 1e-12] = 0.0\n\n    def clamp(x):\n        return np.minimum(np.maximum(x, low), high)\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    # Helper: safely evaluate and maintain global best\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = np.asarray(x, dtype=float).reshape(dim)\n        x = clamp(x)\n        try:\n            y = float(objective_function(x))\n        except Exception:\n            # Objective failure: count evaluation defensively\n            evals_used += 1\n            return None\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # Degenerate budget: no available evaluations\n    if budget <= 0:\n        if prev_best_x is not None:\n            try:\n                x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n                return clamp(x0)\n            except Exception:\n                pass\n        # Fallback: center of box\n        return ((low + high) * 0.5).astype(float)\n\n    # ================== Initialization / warm start ================== #\n    # 1) Try prev_best_x as a candidate\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            eval_point(x0)\n        except Exception:\n            pass\n\n    # 2) Ensure at least one point evaluated: center of search box\n    if evals_used < budget and best_x is None:\n        x0 = ((low + high) * 0.5).astype(float)\n        eval_point(x0)\n\n    # 3) A few random points for basic diversity when budget allows\n    remaining = budget - evals_used\n    if remaining > 0:\n        # Slightly larger init sample to improve robustness\n        n_init_rand = min(max(2, 2 * dim), remaining)\n        for _ in range(n_init_rand):\n            if evals_used >= budget:\n                break\n            xr = rng.uniform(low, high)\n            eval_point(xr)\n\n    if evals_used >= budget:\n        return best_x.astype(float)\n\n    remaining = budget - evals_used\n\n    # ================== Very small budget: global-only search ================== #\n    # Pure global exploration when very few evaluations remain\n    tiny_threshold = max(6, 3 * dim)\n    if remaining <= tiny_threshold:\n        # Low-discrepancy-like sequence with mild jitter\n        golden_ratio_conj = (np.sqrt(5.0) - 1.0) / 2.0\n        base = rng.uniform(0.0, 1.0, size=dim)\n        increments = np.mod(\n            golden_ratio_conj * (np.sqrt(np.arange(1, dim + 1)) * 1.31 + 0.27), 1.0\n        )\n        for i in range(remaining):\n            if evals_used >= budget:\n                break\n            t = (base + increments * (i + 1)) % 1.0\n            jitter = rng.uniform(-5e-4, 5e-4, size=dim)\n            u = np.mod(t + jitter, 1.0)\n            x = low + u * span\n            eval_point(x)\n        return best_x.astype(float)\n\n    # ================== Budget split: global vs local ================== #\n    # Favor global search a bit more for high dimensions\n    if dim <= 4:\n        global_frac = 0.55\n    elif dim <= 12:\n        global_frac = 0.65\n    else:\n        global_frac = 0.7\n\n    remaining = budget - evals_used\n    # Guard against pathological remaining/budget interactions\n    global_budget = int(global_frac * remaining)\n    # Ensure some meaningful global search, but not all\n    global_budget = max(global_budget, min(remaining - 1, dim + 5))\n    global_budget = min(global_budget, remaining - 1) if remaining > 1 else remaining\n    if global_budget < 0:\n        global_budget = max(0, remaining - 1)\n    local_budget = remaining - global_budget\n\n    # In case the rounding made local_budget 0, reserve at least 1 call\n    if local_budget <= 0 and remaining > 1:\n        local_budget = 1\n        global_budget = remaining - 1\n\n    # ================== Global search ================== #\n    start_evals_global = evals_used\n    n_global = global_budget\n\n    golden_ratio_conj = (np.sqrt(5.0) - 1.0) / 2.0\n    increments = np.mod(\n        golden_ratio_conj * (np.sqrt(np.arange(1, dim + 1)) * 1.37 + 0.11), 1.0\n    )\n    base = rng.uniform(0.0, 1.0, size=dim)\n\n    ld_fraction = 0.7\n    n_ld = int(ld_fraction * n_global)\n    n_ld = min(n_ld, n_global)\n\n    # Low-discrepancy-like points\n    for i in range(n_ld):\n        if evals_used >= budget:\n            break\n        t = (base + increments * (i + 1)) % 1.0\n        jitter_scale = 4 * max(1, n_ld)\n        jitter = rng.uniform(-1.0 / jitter_scale, 1.0 / jitter_scale, size=dim)\n        u = np.mod(t + jitter, 1.0)\n        x = low + u * span\n        eval_point(x)\n\n    # Remaining global budget: uniform random samples\n    remaining_global = n_global - (evals_used - start_evals_global)\n    for _ in range(max(0, remaining_global)):\n        if evals_used >= budget:\n            break\n        x = rng.uniform(low, high)\n        eval_point(x)\n\n    # If no budget for local search or no best_x, terminate\n    if evals_used >= budget or local_budget <= 0 or best_x is None:\n        return best_x.astype(float)\n\n    # ================== Local refinement ================== #\n    x_center = best_x.copy()\n\n    # Base step sizes as a moderate fraction of span; handle near-zero spans.\n    base_step = np.maximum(0.2 * span, 1e-3)\n    min_step = np.where(span > 0.0, np.maximum(1e-4 * span, 1e-6), 0.0)\n    max_step = np.where(span > 0.0, np.maximum(0.5 * span, 1e-2), 0.0)\n\n    # Conservative initial step sizes to avoid overshooting in narrow basins\n    step_sizes = np.minimum(base_step, np.maximum(0.05 * span, 5e-4))\n\n    n_local = local_budget\n    # Split more towards fine phase for small budgets\n    if n_local < 20:\n        n_coarse = int(0.4 * n_local)\n    else:\n        n_coarse = int(0.6 * n_local)\n    n_coarse = max(0, min(n_coarse, n_local))\n    n_fine = n_local - n_coarse\n\n    cov_diag = np.where(span > 0, (0.5 * step_sizes) ** 2, 0.0)\n\n    def local_phase(n_iter, shrink_target, heavy_tail=False):\n        nonlocal x_center, step_sizes, cov_diag, best_x, best_y, evals_used\n        if n_iter <= 0:\n            return\n        success_count = 0\n        no_improve_streak = 0\n        for i in range(n_iter):\n            if evals_used >= budget:\n                break\n\n            progress = (i + 1) / max(1.0, n_iter)\n            frac = shrink_target + (1.0 - shrink_target) * (1.0 - progress)\n            current_step = np.clip(step_sizes * frac, min_step, max_step)\n\n            if heavy_tail:\n                direction = rng.standard_t(df=3, size=dim)\n            else:\n                direction = rng.normal(size=dim)\n\n            # scale by diagonal covariance (shape only)\n            direction *= np.sqrt(np.maximum(cov_diag, 1e-16))\n            norm = np.linalg.norm(direction)\n            if norm == 0.0:\n                direction = np.ones(dim, dtype=float) / np.sqrt(max(1, dim))\n            else:\n                direction /= norm\n\n            # Sampling magnitude from beta gives more local moves but occasional larger ones\n            if heavy_tail:\n                mag = rng.beta(0.4, 1.8)\n            else:\n                mag = rng.beta(0.8, 3.0)\n\n            delta = direction * (mag * current_step)\n            x_new = x_center + delta\n            y_new = eval_point(x_new)\n            if y_new is None:\n                break\n\n            improved = y_new <= best_y\n            if improved:\n                # recenter at global best to track best basin\n                x_center = best_x.copy()\n                success_count += 1\n                no_improve_streak = 0\n                # Increase step sizes mildly and adapt covariance\n                step_sizes = np.clip(step_sizes * 1.05, min_step, max_step)\n                cov_diag = 0.9 * cov_diag + 0.1 * (delta ** 2)\n            else:\n                no_improve_streak += 1\n                step_sizes = np.clip(step_sizes * 0.97, min_step, max_step)\n\n            # If stagnating, adjust radius to escape or refine\n            if (i + 1) % max(5, dim) == 0:\n                if success_count == 0:\n                    # No success: expand search radius cautiously\n                    step_sizes = np.clip(step_sizes * 1.12, min_step, max_step)\n                else:\n                    # Some success: gently shrink radius to refine\n                    step_sizes = np.clip(step_sizes * 0.985, min_step, max_step)\n                success_count = 0\n\n            # Occasional recentering to global best to avoid drift\n            if no_improve_streak >= max(10, 2 * dim):\n                x_center = best_x.copy()\n                no_improve_streak = 0\n                step_sizes = np.maximum(step_sizes * 0.7, min_step)\n\n    # Coarse phase: larger, heavy-tailed steps to explore basin\n    local_phase(n_coarse, shrink_target=0.7, heavy_tail=True)\n\n    if best_x is not None:\n        x_center = best_x.copy()\n\n    # Fine phase: smaller, more local steps\n    local_phase(n_fine, shrink_target=0.25, heavy_tail=False)\n\n    return best_x.astype(float)",
    "X": "0.4000010010607861 0.7999999958594304"
}