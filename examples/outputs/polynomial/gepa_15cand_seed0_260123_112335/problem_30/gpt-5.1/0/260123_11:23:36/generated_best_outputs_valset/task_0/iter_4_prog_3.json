{
    "score": -8.672610390822205,
    "Input": "McCourt19",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budget-aware blackbox optimizer.\n\n    Hybrid strategy:\n    - Robust initialization using prev_best_x (if available) and random samples.\n    - Global exploration via low-discrepancy sequence mixed with uniform random.\n    - Local refinement using covariance-adaptive random search around current best.\n    - Adaptive allocation of global/local budget based on dimension and budget.\n    - Strict respect of the evaluation budget while using it as fully as possible.\n    \"\"\"\n    rng = np.random.RandomState()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low, high = bounds[:, 0], bounds[:, 1]\n    span = high - low\n\n    def clamp(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = clamp(x)\n        y = objective_function(x)\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_y = y\n            best_x = x\n        return y\n\n    # Handle degenerate budget quickly\n    if budget <= 0:\n        if prev_best_x is not None:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            return clamp(x0)\n        return ((low + high) * 0.5).astype(float)\n\n    # --- Initialization / warm start --------------------------------------- #\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            eval_point(x0)\n        except Exception:\n            # If prev_best_x is malformed, just ignore it\n            pass\n\n    if evals_used < budget and best_x is None:\n        # Ensure at least one point evaluated\n        x0 = rng.uniform(low, high)\n        eval_point(x0)\n\n    if evals_used >= budget:\n        return best_x.astype(float)\n\n    remaining = budget - evals_used\n\n    # --- Very small budget: pure random sampling --------------------------- #\n    # When budget is tiny, overhead of structure is not worthwhile.\n    if remaining <= max(4, 2 * dim):\n        for _ in range(remaining):\n            x = rng.uniform(low, high)\n            eval_point(x)\n        return best_x.astype(float)\n\n    # --- Budget split: global vs local ------------------------------------- #\n    # More global search for high dimensions or tiny budgets.\n    if dim <= 4:\n        global_frac = 0.55\n    elif dim <= 12:\n        global_frac = 0.6\n    else:\n        global_frac = 0.7\n\n    global_budget = int(global_frac * remaining)\n    global_budget = max(global_budget, dim + 5)\n    global_budget = min(global_budget, remaining)\n    local_budget = remaining - global_budget\n\n    # --- Global search: low-discrepancy + uniform mix ---------------------- #\n    n_global = global_budget\n\n    # Use a simple additive recurrence sequence (Kronecker-type) for coverage.\n    alpha = (np.sqrt(5.0) - 1.0) / 2.0  # inverse golden ratio\n    increments = (alpha * (np.arange(1, dim + 1) * 1.37 + 0.11)) % 1.0\n    base = rng.uniform(0.0, 1.0, size=dim)\n\n    # Fraction of points from low-discrepancy, rest pure random\n    ld_fraction = 0.7\n    n_ld = int(ld_fraction * n_global)\n\n    # Generate low-discrepancy points\n    for i in range(n_ld):\n        if evals_used >= budget:\n            break\n        t = (base + increments * i) % 1.0\n        # Small jitter to avoid grid artifacts\n        jitter = rng.uniform(-1.0 / (4 * n_ld + 1e-9),\n                             1.0 / (4 * n_ld + 1e-9), size=dim)\n        u = (t + jitter) % 1.0\n        x = low + u * span\n        eval_point(x)\n\n    # Remaining global points as uniform random\n    for _ in range(n_global - min(n_ld, evals_used - (budget - remaining))):\n        if evals_used >= budget:\n            break\n        x = rng.uniform(low, high)\n        eval_point(x)\n\n    if evals_used >= budget or local_budget <= 0 or best_x is None:\n        return best_x.astype(float)\n\n    # --- Local refinement: covariance-adaptive random search --------------- #\n    # We maintain a diagonal covariance (per-coordinate step sizes) adapted\n    # from accepted moves to roughly match scale of improvement directions.\n    x_center = best_x.copy()\n\n    # Base step: moderate fraction of span; handle near-zero spans.\n    base_step = np.maximum(0.2 * span, 1e-3)\n    min_step = np.maximum(1e-4 * span, 1e-6)\n    max_step = np.maximum(0.5 * span, 1e-2)\n\n    step_sizes = base_step.copy()\n\n    n_local = local_budget\n    # Split local budget into coarse and fine search\n    n_coarse = int(0.5 * n_local)\n    n_fine = n_local - n_coarse\n\n    def local_phase(n_iter, shrink_target, heavy_tail=False):\n        nonlocal x_center, step_sizes\n        for i in range(n_iter):\n            if evals_used >= budget:\n                break\n\n            # Smoothly shrink step sizes towards shrink_target fraction\n            frac = shrink_target + (1.0 - shrink_target) * (1.0 - (i + 1) / max(1.0, n_iter))\n            current_step = np.clip(step_sizes * frac, min_step, max_step)\n\n            # Draw direction\n            if heavy_tail:\n                # Student-t-like heavy-tailed steps for escape\n                direction = rng.standard_t(df=3, size=dim)\n            else:\n                direction = rng.normal(size=dim)\n            norm = np.linalg.norm(direction)\n            if norm == 0.0:\n                direction = np.ones(dim) / np.sqrt(dim)\n            else:\n                direction /= norm\n\n            # Step magnitude biased to small values but allowing some large jumps\n            if heavy_tail:\n                mag = rng.beta(0.5, 1.5)\n            else:\n                mag = rng.beta(1.0, 3.0)\n\n            delta = direction * (mag * current_step)\n            x_new = x_center + delta\n            y_new = eval_point(x_new)\n            if y_new is None:\n                break\n\n            # Simple success-based adaptation: if accepted, slightly increase\n            if y_new <= best_y:\n                x_center = best_x  # best_x already updated inside eval_point\n                # Increase step sizes mildly in successful directions\n                step_sizes = np.clip(step_sizes * 1.05, min_step, max_step)\n            else:\n                # Mild decrease when unsuccessful\n                step_sizes = np.clip(step_sizes * 0.97, min_step, max_step)\n\n    # Coarse phase: larger, heavy-tailed steps to explore basin\n    local_phase(n_coarse, shrink_target=0.6, heavy_tail=True)\n\n    # Move center to best found so far before fine search\n    if best_x is not None:\n        x_center = best_x.copy()\n\n    # Fine phase: smaller, more local steps\n    local_phase(n_fine, shrink_target=0.2, heavy_tail=False)\n\n    return best_x.astype(float)",
    "X": "0.3999941046613725 0.8000024997362151"
}