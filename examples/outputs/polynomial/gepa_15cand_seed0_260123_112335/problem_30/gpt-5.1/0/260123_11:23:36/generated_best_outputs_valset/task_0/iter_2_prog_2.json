{
    "score": -8.670048802727884,
    "Input": "McCourt19",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budget-aware blackbox optimizer.\n\n    Strategy:\n    - Robust initialization using prev_best_x (if available) and random samples.\n    - Global exploration via quasi-random (scrambled) Sobol-like sequence built\n      from a simple low-discrepancy construction plus random jitter.\n    - Local refinement with adaptive step-size random search around the best\n      solution found so far.\n    - Explicit use of the full available budget while strictly respecting it.\n    \"\"\"\n    rng = np.random.RandomState()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low, high = bounds[:, 0], bounds[:, 1]\n    span = high - low\n\n    def clamp(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    # Helper to evaluate a point safely within budget\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = clamp(x)\n        y = objective_function(x)\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_y = y\n            best_x = x\n        return y\n\n    if budget <= 0:\n        # No evaluations allowed; fall back to clamped prev_best_x or center\n        if prev_best_x is not None:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            return clamp(x0)\n        return ((low + high) * 0.5).astype(float)\n\n    # ---- Warm start from previous best, if available ---------------------- #\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n        eval_point(x0)\n\n    # Ensure at least one evaluated point if possible\n    if evals_used < budget and best_x is None:\n        x0 = rng.uniform(low, high)\n        eval_point(x0)\n\n    if evals_used >= budget:\n        return best_x.astype(float)\n\n    remaining = budget - evals_used\n\n    # --- Very small budget: pure random search ----------------------------- #\n    if remaining <= max(5, 2 * dim):\n        for _ in range(remaining):\n            x = rng.uniform(low, high)\n            eval_point(x)\n        return best_x.astype(float)\n\n    # --- Budget split: global vs local ------------------------------------- #\n    # Slightly favor global search for higher dimensions\n    global_frac = 0.65 if dim > 8 else 0.6\n    global_budget = int(global_frac * remaining)\n    global_budget = max(global_budget, dim + 5)\n    global_budget = min(global_budget, remaining)\n    local_budget = remaining - global_budget\n\n    # --- Global search: low-discrepancy + jitter --------------------------- #\n    n_global = global_budget\n\n    # Simple low-discrepancy sequence: use additive recurrence with\n    # golden-ratio-based direction numbers, then random permutation per dim.\n    # This is not a true Sobol but gives better coverage than iid random.\n    alpha = (np.sqrt(5.0) - 1.0) / 2.0  # ~0.618\n    # Use different incommensurate increments per dimension\n    increments = (alpha * (np.arange(1, dim + 1) * 1.37 + 0.11)) % 1.0\n    base = rng.uniform(0.0, 1.0, size=dim)\n\n    # Pre-generate a random permutation per dimension to break structure\n    perms = [rng.permutation(n_global) for _ in range(dim)]\n\n    for i in range(n_global):\n        # Low-discrepancy point in [0,1]^dim\n        t = (base + increments * i) % 1.0\n        # Scramble via permutations and jitter for each dimension\n        jitter = rng.uniform(-1.0 / (2 * n_global), 1.0 / (2 * n_global), size=dim)\n        u = (t + jitter) % 1.0\n\n        # Mild dimension-wise permutation: mix the ordering in each coordinate\n        for d in range(dim):\n            u[d] = ((perms[d][i] + 0.5) / n_global + u[d]) * 0.5 % 1.0\n\n        x = low + u * span\n        eval_point(x)\n        if evals_used >= budget:\n            return best_x.astype(float)\n\n    # --- Local refinement -------------------------------------------------- #\n    if local_budget <= 0 or best_x is None:\n        return best_x.astype(float)\n\n    x_center = best_x.copy()\n\n    # Initial step size: 25% of span, but at least a small absolute value\n    base_step = np.maximum(0.25 * span, 1e-3)\n    min_step = np.maximum(1e-4 * span, 1e-6)\n\n    # Split local budget into phases: exploration then exploitation\n    n_local = local_budget\n    n_explore = int(0.6 * n_local)\n    n_exploit = n_local - n_explore\n\n    # --- Exploratory local phase: broader steps ---------------------------- #\n    for i in range(n_explore):\n        if evals_used >= budget:\n            break\n\n        # Geometric decay; slower in exploratory phase\n        decay = 0.7 ** (i / max(1.0, n_explore - 1))\n        step = np.maximum(base_step * decay, min_step)\n\n        direction = rng.normal(size=dim)\n        norm = np.linalg.norm(direction)\n        if norm == 0:\n            direction = np.ones(dim) / np.sqrt(dim)\n        else:\n            direction /= norm\n\n        # Random magnitude scalar in [0,1]\n        mag = rng.uniform(0.0, 1.0)\n        delta = direction * (mag * step)\n\n        x_new = x_center + delta\n        y_new = eval_point(x_new)\n        if y_new is not None and y_new <= best_y:\n            x_center = best_x  # best_x already updated in eval_point\n\n    # --- Exploitative local phase: finer steps around current best -------- #\n    x_center = best_x.copy()\n    for i in range(n_exploit):\n        if evals_used >= budget:\n            break\n\n        # Faster decay for fine search\n        decay = 0.5 ** (i / max(1.0, n_exploit - 1))\n        step = np.maximum(base_step * decay, min_step)\n\n        direction = rng.normal(size=dim)\n        norm = np.linalg.norm(direction)\n        if norm == 0:\n            direction = np.ones(dim) / np.sqrt(dim)\n        else:\n            direction /= norm\n\n        # Use smaller, biased-to-small steps\n        mag = rng.beta(0.5, 2.0)  # many small, some larger\n        delta = direction * (mag * step)\n\n        x_new = x_center + delta\n        y_new = eval_point(x_new)\n        if y_new is not None and y_new <= best_y:\n            x_center = best_x  # best_x already updated\n\n    return best_x.astype(float)",
    "X": "0.39983489301706804 0.8001106517819796"
}