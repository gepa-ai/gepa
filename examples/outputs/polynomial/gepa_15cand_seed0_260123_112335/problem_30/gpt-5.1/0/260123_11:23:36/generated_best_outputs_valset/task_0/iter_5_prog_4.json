{
    "score": -8.6726193690616,
    "Input": "McCourt19",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budget-aware blackbox optimizer.\n\n    Hybrid strategy:\n    - Robust initialization using prev_best_x (if available) and random samples.\n    - Global exploration via low-discrepancy sequence mixed with uniform random.\n    - Local refinement using covariance-adaptive random search around current best.\n    - Adaptive allocation of global/local budget based on dimension and budget.\n    - Strict respect of the evaluation budget while using it as fully as possible.\n    \"\"\"\n    rng = np.random.RandomState()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low, high = bounds[:, 0], bounds[:, 1]\n    span = high - low\n\n    def clamp(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    # Helper: safely evaluate and maintain global best\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = clamp(x)\n        y = objective_function(x)\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_y = y\n            best_x = x\n        return y\n\n    # Handle degenerate budget quickly\n    if budget <= 0:\n        if prev_best_x is not None:\n            try:\n                x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n                return clamp(x0)\n            except Exception:\n                pass\n        return ((low + high) * 0.5).astype(float)\n\n    # --- Initialization / warm start --------------------------------------- #\n    # Use prev_best_x if available; if not improving, random will overwrite.\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            eval_point(x0)\n        except Exception:\n            pass\n\n    if evals_used < budget and best_x is None:\n        # Ensure at least one point evaluated\n        x0 = rng.uniform(low, high)\n        eval_point(x0)\n\n    if evals_used >= budget:\n        return best_x.astype(float)\n\n    remaining = budget - evals_used\n\n    # --- Very small budget: pure random sampling --------------------------- #\n    if remaining <= max(4, 2 * dim):\n        for _ in range(remaining):\n            if evals_used >= budget:\n                break\n            x = rng.uniform(low, high)\n            eval_point(x)\n        return best_x.astype(float)\n\n    # --- Budget split: global vs local ------------------------------------- #\n    # Slightly more emphasis on global search for robustness.\n    if dim <= 4:\n        global_frac = 0.6\n    elif dim <= 12:\n        global_frac = 0.65\n    else:\n        global_frac = 0.7\n\n    global_budget = int(global_frac * remaining)\n    global_budget = max(global_budget, dim + 5)\n    global_budget = min(global_budget, remaining)\n    local_budget = remaining - global_budget\n\n    # Record current eval count before global phase to avoid convoluted math\n    start_evals_global = evals_used\n\n    # --- Global search: low-discrepancy + uniform mix ---------------------- #\n    n_global = global_budget\n\n    # Use a simple additive recurrence sequence (Kronecker-type) for coverage.\n    alpha = (np.sqrt(5.0) - 1.0) / 2.0  # inverse golden ratio\n    increments = (alpha * (np.arange(1, dim + 1) * 1.37 + 0.11)) % 1.0\n    base = rng.uniform(0.0, 1.0, size=dim)\n\n    # Fraction of points from low-discrepancy, rest pure random\n    ld_fraction = 0.7\n    n_ld = int(ld_fraction * n_global)\n    n_ld = min(n_ld, n_global)\n\n    # Generate low-discrepancy points\n    for i in range(n_ld):\n        if evals_used >= budget:\n            break\n        t = (base + increments * i) % 1.0\n        # Small jitter to avoid grid artifacts\n        jitter_scale = 4 * max(1, n_ld)\n        jitter = rng.uniform(-1.0 / jitter_scale, 1.0 / jitter_scale, size=dim)\n        u = (t + jitter) % 1.0\n        x = low + u * span\n        eval_point(x)\n\n    # Remaining global points as uniform random\n    remaining_global = n_global - (evals_used - start_evals_global)\n    for _ in range(max(0, remaining_global)):\n        if evals_used >= budget:\n            break\n        x = rng.uniform(low, high)\n        eval_point(x)\n\n    if evals_used >= budget or local_budget <= 0 or best_x is None:\n        return best_x.astype(float)\n\n    # --- Local refinement: covariance-adaptive random search --------------- #\n    x_center = best_x.copy()\n\n    # Base step sizes as a moderate fraction of span; handle near-zero spans.\n    base_step = np.maximum(0.15 * span, 1e-3)\n    min_step = np.maximum(1e-4 * span, 1e-6)\n    max_step = np.maximum(0.5 * span, 1e-2)\n\n    step_sizes = base_step.copy()\n\n    n_local = local_budget\n    # Split local budget into coarse and fine search\n    n_coarse = int(0.4 * n_local)\n    n_fine = n_local - n_coarse\n\n    def local_phase(n_iter, shrink_target, heavy_tail=False):\n        nonlocal x_center, step_sizes\n        # Track recent success rate for adaptive aggressiveness\n        success_count = 0\n        for i in range(n_iter):\n            if evals_used >= budget:\n                break\n\n            # Adaptive shrink factor based on progress within phase\n            progress = (i + 1) / max(1.0, n_iter)\n            frac = shrink_target + (1.0 - shrink_target) * (1.0 - progress)\n            current_step = np.clip(step_sizes * frac, min_step, max_step)\n\n            # Draw direction\n            if heavy_tail:\n                direction = rng.standard_t(df=3, size=dim)\n            else:\n                direction = rng.normal(size=dim)\n            norm = np.linalg.norm(direction)\n            if norm == 0.0:\n                direction = np.ones(dim) / np.sqrt(dim)\n            else:\n                direction /= norm\n\n            # Step magnitude biased to small values but allowing some large jumps\n            if heavy_tail:\n                mag = rng.beta(0.5, 1.5)\n            else:\n                mag = rng.beta(0.8, 3.0)\n\n            delta = direction * (mag * current_step)\n            x_new = x_center + delta\n            y_new = eval_point(x_new)\n            if y_new is None:\n                break\n\n            # Success-based adaptation\n            if y_new <= best_y:\n                # best_x already updated inside eval_point\n                x_center = best_x\n                success_count += 1\n                # Increase step sizes mildly in successful directions\n                step_sizes = np.clip(step_sizes * 1.06, min_step, max_step)\n            else:\n                # Mild decrease when unsuccessful\n                step_sizes = np.clip(step_sizes * 0.97, min_step, max_step)\n\n            # If no success for a while, slightly expand search radius to escape\n            if (i + 1) % max(5, dim) == 0:\n                if success_count == 0:\n                    step_sizes = np.clip(step_sizes * 1.1, min_step, max_step)\n                success_count = 0\n\n    # Coarse phase: larger, heavy-tailed steps to explore basin\n    local_phase(n_coarse, shrink_target=0.7, heavy_tail=True)\n\n    if best_x is not None:\n        x_center = best_x.copy()\n\n    # Fine phase: smaller, more local steps\n    local_phase(n_fine, shrink_target=0.25, heavy_tail=False)\n\n    return best_x.astype(float)",
    "X": "0.39999470108499513 0.8000021540329034"
}