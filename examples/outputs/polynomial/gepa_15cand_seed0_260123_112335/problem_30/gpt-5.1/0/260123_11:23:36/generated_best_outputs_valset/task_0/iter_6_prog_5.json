{
    "score": -8.672670707568344,
    "Input": "McCourt19",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budget-aware blackbox optimizer.\n\n    Hybrid strategy:\n    - Robust initialization using prev_best_x (if available) and random samples.\n    - Global exploration via Sobol-like low-discrepancy sequence mixed with uniform random.\n    - Local refinement using covariance-adaptive random search around current best.\n    - Adaptive allocation of global/local budget based on dimension and budget.\n    - Strict respect of the evaluation budget while using it as fully as possible.\n    \"\"\"\n    rng = np.random.RandomState()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low, high = bounds[:, 0], bounds[:, 1]\n    span = high - low\n\n    # Handle degenerate span (zero-width dimensions)\n    span[span == 0.0] = 0.0\n\n    def clamp(x):\n        return np.minimum(np.maximum(x, low), high)\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    # Helper: safely evaluate and maintain global best\n    def eval_point(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = clamp(x)\n        try:\n            y = objective_function(x)\n        except Exception:\n            # In case objective_function fails on invalid shapes etc., skip\n            return None\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_y = y\n            best_x = x\n        return y\n\n    # Handle degenerate budget quickly\n    if budget <= 0:\n        if prev_best_x is not None:\n            try:\n                x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n                return clamp(x0)\n            except Exception:\n                pass\n        # fall back to center of box\n        return ((low + high) * 0.5).astype(float)\n\n    # --- Initialization / warm start --------------------------------------- #\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            eval_point(x0)\n        except Exception:\n            pass\n\n    if evals_used < budget and best_x is None:\n        # Ensure at least one point evaluated\n        x0 = rng.uniform(low, high)\n        eval_point(x0)\n\n    if evals_used >= budget:\n        return best_x.astype(float)\n\n    remaining = budget - evals_used\n\n    # --- Very small budget: quasi-random + random mix ---------------------- #\n    if remaining <= max(6, 3 * dim):\n        # simple low-discrepancy sequence: additive recurrence with scrambling\n        alpha = (np.sqrt(5.0) - 1.0) / 2.0\n        base = rng.uniform(0.0, 1.0, size=dim)\n        increments = (alpha * (np.arange(1, dim + 1) * 1.11 + 0.37)) % 1.0\n        for i in range(remaining):\n            if evals_used >= budget:\n                break\n            t = (base + increments * (i + 1)) % 1.0\n            jitter = rng.uniform(-1e-3, 1e-3, size=dim)\n            u = np.mod(t + jitter, 1.0)\n            x = low + u * span\n            eval_point(x)\n        return best_x.astype(float)\n\n    # --- Budget split: global vs local ------------------------------------- #\n    # Slightly more emphasis on global search for robustness.\n    if dim <= 4:\n        global_frac = 0.6\n    elif dim <= 12:\n        global_frac = 0.65\n    else:\n        global_frac = 0.7\n\n    global_budget = int(global_frac * remaining)\n    global_budget = max(global_budget, dim + 5)\n    global_budget = min(global_budget, remaining)\n    local_budget = remaining - global_budget\n\n    # --- Global search: Sobol-like + uniform mix --------------------------- #\n    start_evals_global = evals_used\n    n_global = global_budget\n\n    # Kronecker-type low-discrepancy sequence, dimension-robust\n    alpha = (np.sqrt(5.0) - 1.0) / 2.0\n    # use incommensurate increments per dimension\n    increments = np.mod(alpha * (np.sqrt(np.arange(1, dim + 1)) * 1.37 + 0.11), 1.0)\n    base = rng.uniform(0.0, 1.0, size=dim)\n\n    ld_fraction = 0.75\n    n_ld = int(ld_fraction * n_global)\n    n_ld = min(n_ld, n_global)\n\n    for i in range(n_ld):\n        if evals_used >= budget:\n            break\n        t = (base + increments * (i + 1)) % 1.0\n        jitter_scale = 8 * max(1, n_ld)\n        jitter = rng.uniform(-1.0 / jitter_scale, 1.0 / jitter_scale, size=dim)\n        u = np.mod(t + jitter, 1.0)\n        x = low + u * span\n        eval_point(x)\n\n    remaining_global = n_global - (evals_used - start_evals_global)\n    for _ in range(max(0, remaining_global)):\n        if evals_used >= budget:\n            break\n        x = rng.uniform(low, high)\n        eval_point(x)\n\n    if evals_used >= budget or local_budget <= 0 or best_x is None:\n        return best_x.astype(float)\n\n    # --- Local refinement: covariance-adaptive random search --------------- #\n    x_center = best_x.copy()\n\n    # Base step sizes as a moderate fraction of span; handle near-zero spans.\n    base_step = np.maximum(0.2 * span, 1e-3)\n    min_step = np.where(span > 0.0, np.maximum(1e-4 * span, 1e-6), 0.0)\n    max_step = np.where(span > 0.0, np.maximum(0.5 * span, 1e-2), 0.0)\n\n    step_sizes = base_step.copy()\n\n    n_local = local_budget\n    n_coarse = int(0.5 * n_local)\n    n_fine = n_local - n_coarse\n\n    # simple diagonal covariance estimate\n    cov_diag = (step_sizes ** 2).copy()\n\n    def local_phase(n_iter, shrink_target, heavy_tail=False):\n        nonlocal x_center, step_sizes, cov_diag\n        success_count = 0\n        no_improve_streak = 0\n        for i in range(n_iter):\n            if evals_used >= budget:\n                break\n\n            progress = (i + 1) / max(1.0, n_iter)\n            frac = shrink_target + (1.0 - shrink_target) * (1.0 - progress)\n            current_step = np.clip(step_sizes * frac, min_step, max_step)\n\n            if heavy_tail:\n                direction = rng.standard_t(df=3, size=dim)\n            else:\n                direction = rng.normal(size=dim)\n            # scale by diagonal covariance\n            direction *= np.sqrt(np.maximum(cov_diag, 1e-16))\n            norm = np.linalg.norm(direction)\n            if norm == 0.0:\n                direction = np.ones(dim) / np.sqrt(dim)\n            else:\n                direction /= norm\n\n            if heavy_tail:\n                mag = rng.beta(0.5, 1.5)\n            else:\n                mag = rng.beta(0.8, 3.0)\n\n            delta = direction * (mag * current_step)\n            x_new = x_center + delta\n            y_new = eval_point(x_new)\n            if y_new is None:\n                break\n\n            if y_new <= best_y:\n                x_center = best_x\n                success_count += 1\n                no_improve_streak = 0\n                # Increase step sizes mildly and adapt covariance\n                step_sizes = np.clip(step_sizes * 1.08, min_step, max_step)\n                cov_diag = 0.9 * cov_diag + 0.1 * (delta ** 2)\n            else:\n                no_improve_streak += 1\n                step_sizes = np.clip(step_sizes * 0.97, min_step, max_step)\n\n            # If stagnating, slightly expand radius to escape\n            if (i + 1) % max(5, dim) == 0:\n                if success_count == 0:\n                    step_sizes = np.clip(step_sizes * 1.15, min_step, max_step)\n                success_count = 0\n\n            # Occasional recentring to global best to avoid drift\n            if no_improve_streak >= max(10, 2 * dim):\n                x_center = best_x.copy()\n                no_improve_streak = 0\n                step_sizes = np.maximum(step_sizes * 0.7, min_step)\n\n    # Coarse phase: larger, heavy-tailed steps to explore basin\n    local_phase(n_coarse, shrink_target=0.7, heavy_tail=True)\n\n    if best_x is not None:\n        x_center = best_x.copy()\n\n    # Fine phase: smaller, more local steps\n    local_phase(n_fine, shrink_target=0.25, heavy_tail=False)\n\n    return best_x.astype(float)",
    "X": "0.4000029023131745 0.8000004704062977"
}