{
    "score": -1.5534932288725631,
    "Input": "McCourt26",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid global-local derivative-free optimizer.\n    - Global phase: low-discrepancy-like random sampling + local refinement of elites\n    - Local phase: adaptive pattern search around current best\n    Uses full evaluation budget and leverages warm-start if available.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low, high = bounds[:, 0], bounds[:, 1]\n    span = high - low\n    span[span <= 0] = 1.0  # guard against degenerate bounds\n\n    remaining = budget\n\n    # --- Helper: ensure we always respect budget in any loop ---\n    def eval_point(x, x_best, y_best):\n        nonlocal remaining\n        if remaining <= 0:\n            return x_best, y_best, True\n        y = objective_function(x)\n        remaining -= 1\n        if (y_best is None) or (y < y_best):\n            return x, y, remaining <= 0\n        return x_best, y_best, remaining <= 0\n\n    # --- Initialization with optional warm-start ---\n    x_best = None\n    y_best = None\n\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.shape[0] == dim:\n            x0 = np.clip(x0, low, high)\n            x_best, y_best, stop = eval_point(x0, x_best, y_best)\n            if stop:\n                return x_best\n\n    if x_best is None:\n        # If no valid warm start or budget exhausted before, sample a fresh point\n        x0 = rng.uniform(low, high)\n        x_best, y_best, stop = eval_point(x0, x_best, y_best)\n        if stop:\n            return x_best\n\n    # --- Budget split: global vs local ---\n    # Ensure both phases get some evaluations for moderate/large budgets.\n    if remaining <= 0:\n        return x_best\n\n    min_local = max(5 * dim, 20)\n    min_global = max(10 * dim, 20)\n\n    if remaining <= min_local:\n        global_evals = 0\n    else:\n        # Try to keep a nontrivial global phase while reserving local phase\n        global_evals = min(remaining - min_local, max(min_global, remaining // 2))\n\n    global_evals = max(0, min(global_evals, remaining))\n\n    # --- Global search with elite local refinement ---\n    if global_evals > 0:\n        # Number of elites to refine locally from global samples\n        n_elite = max(2, min(5, global_evals // max(5 * dim, 10)))\n        # Keep track of top candidates\n        elite_X = []\n        elite_y = []\n\n        batch_size = min(64, max(8, global_evals // 4))\n        total_samples = global_evals\n\n        def update_elites(x, y):\n            nonlocal elite_X, elite_y\n            if len(elite_X) < n_elite:\n                elite_X.append(x.copy())\n                elite_y.append(y)\n            else:\n                worst_idx = int(np.argmax(elite_y))\n                if y < elite_y[worst_idx]:\n                    elite_X[worst_idx] = x.copy()\n                    elite_y[worst_idx] = y\n\n        while remaining > 0 and total_samples > 0:\n            current_batch = min(batch_size, total_samples, remaining)\n            if current_batch <= 0:\n                break\n\n            # Random uniform samples; could be improved later with LHS or Sobol\n            X = rng.uniform(low, high, size=(current_batch, dim))\n            for x in X:\n                x_best, y_best, _ = eval_point(x, x_best, y_best)\n                # Use latest evaluation (y is y_best only if improved)\n                # To track the actual objective, we re-evaluate to avoid ambiguity\n                # but that would waste budget; instead, we keep a local 'y' by\n                # re-running objective only when x is new: not possible here.\n                # So we locally compute y before updating elites.\n                # This costs one more eval, so instead we compute y first:\n                # To avoid this overhead, maintain a second evaluation path:\n\n            # To avoid double evaluations, re-run the batch with explicit evals\n            # but with careful budget accounting:\n            # We switch strategy: explicitly evaluate here and bypass eval_point.\n            # This is safe as we control remaining.\n            # Roll back the last batch's use of eval_point; instead, we implement\n            # explicit evaluation below and not call eval_point in this loop.\n            # For clarity and correctness without complex state rewinding, we\n            # re-implement the batch evaluation from scratch and never use the\n            # above loop. (The above loop is effectively a no-op.)\n            break\n\n        # Correct global phase implementation (no misuse of eval_point):\n\n        while remaining > 0 and total_samples > 0:\n            current_batch = min(batch_size, total_samples, remaining)\n            if current_batch <= 0:\n                break\n\n            X = rng.uniform(low, high, size=(current_batch, dim))\n            for x in X:\n                if remaining <= 0:\n                    break\n                y = objective_function(x)\n                remaining -= 1\n                if (y_best is None) or (y < y_best):\n                    x_best, y_best = x, y\n                update_elites(x, y)\n                total_samples -= 1\n                if total_samples <= 0 or remaining <= 0:\n                    break\n\n        # Local refinement of elites with tiny budgets each\n        if remaining > 0 and elite_X:\n            # Allocate a small local budget to each elite (at least 1 eval)\n            per_elite = max(1, remaining // (2 * len(elite_X)))\n            per_elite = max(per_elite, 2 * dim if remaining > 4 * dim else 1)\n            for ex in elite_X:\n                if remaining <= 0:\n                    break\n                # Simple one-step coordinate scan around elite\n                step = 0.1 * span\n                step[step == 0] = 0.1\n                x_cand = ex.copy()\n                for i in range(dim):\n                    if remaining <= 0:\n                        break\n                    for direction in (+1.0, -1.0):\n                        if remaining <= 0:\n                            break\n                        x_try = x_cand.copy()\n                        x_try[i] = np.clip(x_try[i] + direction * step[i], low[i], high[i])\n                        y = objective_function(x_try)\n                        remaining -= 1\n                        if y < y_best:\n                            x_best, y_best = x_try, y\n                if remaining <= 0:\n                    break\n\n    if remaining <= 0:\n        return x_best\n\n    # --- Main local search: adaptive pattern search around x_best ---\n    step = 0.25 * span\n    step[step == 0] = 0.1\n\n    no_improve_iters = 0\n    max_no_improve = 5  # iterations of full coordinate sweeps without improvement\n\n    while remaining > 0:\n        improved = False\n        for i in range(dim):\n            if remaining <= 0:\n                break\n\n            # Try positive and negative directions\n            for direction in (+1.0, -1.0):\n                if remaining <= 0:\n                    break\n\n                x_candidate = x_best.copy()\n                x_candidate[i] = np.clip(\n                    x_candidate[i] + direction * step[i], low[i], high[i]\n                )\n                y = objective_function(x_candidate)\n                remaining -= 1\n                if y < y_best:\n                    x_best, y_best = x_candidate, y\n                    improved = True\n\n        if improved:\n            no_improve_iters = 0\n        else:\n            no_improve_iters += 1\n            step *= 0.5\n\n        # Stop local pattern search if step is too small or no improvement for a while\n        if np.all(step < span * 1e-6) or no_improve_iters >= max_no_improve:\n            break\n\n    # --- Use any leftover budget for stochastic refinement around best ---\n    while remaining > 0:\n        # Mix of small and moderate Gaussian perturbations\n        scale = 0.05 * span if rng.random() < 0.7 else 0.2 * span\n        perturb = rng.normal(scale=scale, size=dim)\n        x_candidate = np.clip(x_best + perturb, low, high)\n        y = objective_function(x_candidate)\n        remaining -= 1\n        if y < y_best:\n            x_best, y_best = x_candidate, y\n\n    return np.asarray(x_best, dtype=float).reshape(dim,)",
    "X": "0.5000008994517325 0.8000003455795183 0.29999884315967407"
}