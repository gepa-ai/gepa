{
    "score": -6.108324077366665,
    "Input": "Alpine02",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budget-aware evolutionary local search.\n\n    Strategy:\n    1. Use a small Sobol-like random initialization (quasi-random via stratified sampling).\n    2. Treat prev_best_x as an additional candidate if provided.\n    3. Run a CMA-ES\u2013inspired evolution strategy with adaptive step-size\n       and simple boundary handling (reflection).\n    4. Use the entire evaluation budget.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    span[span <= 0] = 1.0  # safeguard\n\n    # Helper: project into bounds by reflection\n    def reflect_into_bounds(x):\n        x_ref = x.copy()\n        for i in range(dim):\n            if span[i] <= 0:\n                x_ref[i] = low[i]\n                continue\n            # reflect until in range\n            while x_ref[i] < low[i] or x_ref[i] > high[i]:\n                if x_ref[i] < low[i]:\n                    x_ref[i] = low[i] + (low[i] - x_ref[i])\n                if x_ref[i] > high[i]:\n                    x_ref[i] = high[i] - (x_ref[i] - high[i])\n        return x_ref\n\n    evals_used = 0\n\n    # Evaluate a point with budget check\n    def eval_point(x):\n        nonlocal evals_used\n        if evals_used >= budget:\n            # Should not be called when budget exhausted\n            return np.inf\n        y = objective_function(x)\n        evals_used += 1\n        return y\n\n    # Initialization: small stratified sample + prev_best_x if any\n    init_evals = min(max(5, 2 * dim), max(1, budget // 5))\n    candidates = []\n\n    # Stratified sampling per dimension\n    for i in range(init_evals):\n        frac = (i + 0.5) / init_evals\n        # jittered stratified sample in each dimension\n        xi = low + span * np.clip(\n            rng.normal(loc=frac, scale=0.15, size=dim), 0.0, 1.0\n        )\n        xi = reflect_into_bounds(xi)\n        yi = eval_point(xi)\n        candidates.append((yi, xi))\n        if evals_used >= budget:\n            break\n\n    # Include warm start if provided and within budget\n    if prev_best_x is not None and evals_used < budget:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = reflect_into_bounds(x0)\n            y0 = eval_point(x0)\n            candidates.append((y0, x0))\n\n    if not candidates:\n        # Fallback: single random point\n        x = low + span * rng.random(dim)\n        return x\n\n    # Determine current best\n    candidates.sort(key=lambda t: t[0])\n    best_y, best_x = candidates[0]\n\n    if evals_used >= budget:\n        return best_x\n\n    # ES parameters\n    remaining = budget - evals_used\n    # population size scales gently with dim but bounded\n    lam = min(max(4, 2 * dim), max(10, remaining // 2))\n    mu = max(2, lam // 2)\n\n    # step-size: fraction of domain span\n    sigma = 0.2 * np.mean(span)\n    if sigma <= 0:\n        return best_x\n\n    # main loop\n    # allocate remaining evaluations to generations\n    # while-loop rather than fixed generations to respect budget tightly\n    while evals_used < budget:\n        # number of offspring this generation\n        remaining = budget - evals_used\n        if remaining <= 0:\n            break\n        this_lam = min(lam, remaining)\n\n        # simple multivariate normal around best_x, isotropic variance\n        offspring = []\n        for _ in range(this_lam):\n            z = rng.standard_normal(dim)\n            x_off = best_x + sigma * z\n            x_off = reflect_into_bounds(x_off)\n            y_off = eval_point(x_off)\n            offspring.append((y_off, x_off))\n            if evals_used >= budget:\n                break\n\n        # update best\n        for y_off, x_off in offspring:\n            if y_off < best_y:\n                best_y, best_x = y_off, x_off\n\n        # selection & step-size adaptation\n        offspring.sort(key=lambda t: t[0])\n        top = offspring[: min(mu, len(offspring))]\n        if top:\n            xs = np.array([t[1] for t in top])\n            mean_top = xs.mean(axis=0)\n            # distance from best to mean of elites\n            step = np.linalg.norm(mean_top - best_x) / (np.sqrt(dim) + 1e-12)\n\n            # adapt sigma: if elites concentrate near best, shrink; else expand slightly\n            if step < 0.1 * sigma:\n                sigma *= 0.82\n            else:\n                sigma *= 1.05\n            # keep sigma within reasonable portion of domain\n            max_sigma = 0.5 * np.mean(span)\n            min_sigma = 1e-8 * np.mean(span + 1.0)\n            sigma = float(np.clip(sigma, min_sigma, max_sigma))\n\n        if sigma <= 1e-12:\n            # stagnation; reinitialize around best with larger sigma once\n            sigma = 0.1 * np.mean(span)\n\n    return best_x",
    "X": "4.76968635094183 7.84862110807871"
}