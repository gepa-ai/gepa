{
    "score": -6.129503891130689,
    "Input": "Alpine02",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budget-aware hybrid global + local search.\n\n    Improvements over previous version:\n    - Simpler and more robust initialization: LHS-like sampling done once.\n    - More reliable use of remaining budget: dynamic allocation between global ES and local search.\n    - Gradient-free local search (coordinate-wise adaptive search) to better exploit good incumbents.\n    - Cleaner covariance adaptation and boundary handling while preserving warm-start and budget usage.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    # Safeguard for degenerate bounds\n    span[span <= 0] = 0.0\n\n    # ---------- Helpers ----------\n\n    def reflect_into_bounds(x):\n        \"\"\"Reflect x into [low, high] box constraints.\"\"\"\n        x_ref = np.asarray(x, dtype=float).copy()\n        for i in range(dim):\n            li, hi = low[i], high[i]\n            if li >= hi:\n                x_ref[i] = li\n                continue\n            while x_ref[i] < li or x_ref[i] > hi:\n                if x_ref[i] < li:\n                    x_ref[i] = li + (li - x_ref[i])\n                if x_ref[i] > hi:\n                    x_ref[i] = hi - (x_ref[i] - hi)\n        return x_ref\n\n    evals_used = 0\n\n    def eval_point(x):\n        nonlocal evals_used\n        if evals_used >= budget:\n            return np.inf\n        y = objective_function(x)\n        evals_used += 1\n        return y\n\n    # ---------- Initialization / Global Sampling ----------\n\n    if budget <= 0:\n        # No evaluations allowed; return some feasible point\n        x0 = np.clip(low, low, high)\n        return x0\n\n    # Allocate ~30% of budget to global sampling (but at least some reasonable number)\n    init_evals = int(0.3 * budget)\n    init_evals = max(init_evals, min(5 * dim, budget))\n    init_evals = min(init_evals, budget)\n\n    candidates = []\n\n    if init_evals > 0:\n        # Latin hypercube style sampling\n        # For each dimension, split [0, 1] into init_evals intervals and permute them.\n        u_base = (np.arange(init_evals) + rng.random(init_evals)) / init_evals\n        u_base = np.clip(u_base, 0.0, 1.0)\n\n        perms = [rng.permutation(init_evals) for _ in range(dim)]\n        for i in range(init_evals):\n            u = np.array(\n                [(perms[d][i] + rng.random()) / init_evals for d in range(dim)]\n            )\n            u = np.clip(u, 0.0, 1.0)\n            xi = low + span * u\n            xi = reflect_into_bounds(xi)\n            yi = eval_point(xi)\n            candidates.append((yi, xi))\n            if evals_used >= budget:\n                break\n\n    # Include warm start if provided\n    if prev_best_x is not None and evals_used < budget:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = reflect_into_bounds(x0)\n            y0 = eval_point(x0)\n            candidates.append((y0, x0))\n\n    # If for some reason we have no evaluated candidates, create one\n    if not candidates:\n        x = low + (high - low) * rng.random(dim)\n        x = reflect_into_bounds(x)\n        return x\n\n    # Determine current best\n    candidates.sort(key=lambda t: t[0])\n    best_y, best_x = candidates[0]\n\n    if evals_used >= budget:\n        return best_x\n\n    # ---------- Global CMA-ES\u2013like Evolution Strategy ----------\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Use roughly half of remaining evals for ES, half for local search (but adapt to small budgets)\n    global_budget = max(remaining // 2, min(10, remaining))\n    local_budget = remaining - global_budget\n\n    # If too small for meaningful ES, skip directly to local search\n    if global_budget < 4:\n        global_budget = 0\n        local_budget = remaining\n\n    # CMA-ES-like only if we have at least minimal budget\n    if global_budget > 0:\n        # Population size: depends on dimension and remaining ES budget\n        lam = max(4, 4 + int(3 * np.log(dim + 1)))\n        lam = min(lam, max(4, global_budget // 2))  # ensure a couple of generations\n        mu = max(2, lam // 2)\n\n        # Weights for recombination\n        weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        mu_eff = 1.0 / np.sum(weights**2)\n\n        # Step-size as fraction of average non-degenerate span\n        effective_span = np.where(high > low, high - low, 1.0)\n        mean_span = float(np.mean(effective_span))\n        if mean_span <= 0:\n            mean_span = 1.0\n        sigma = 0.3 * mean_span\n\n        # Initial covariance\n        C = np.eye(dim)\n\n        # Evolution path for sigma (simplified)\n        c_sigma = (mu_eff + 2) / (dim + mu_eff + 5)\n        d_sigma = 1 + 2 * max(0, np.sqrt((mu_eff - 1) / (dim + 1)) - 1) + c_sigma\n        p_sigma = np.zeros(dim)\n\n        # Estimate number of generations with given global_budget\n        # Each generation uses ~lam evaluations\n        max_generations = max(1, global_budget // lam)\n\n        generations = 0\n        while evals_used < budget and generations < max_generations:\n            remaining_global = global_budget - (generations * lam)\n            if remaining_global <= 0:\n                break\n\n            this_lam = min(lam, remaining_global, budget - evals_used)\n            if this_lam <= 0:\n                break\n\n            try:\n                A = np.linalg.cholesky(C)\n            except np.linalg.LinAlgError:\n                C = np.eye(dim)\n                A = np.eye(dim)\n\n            zs = rng.standard_normal((this_lam, dim))\n            xs = best_x + sigma * (zs @ A.T)\n\n            for i in range(this_lam):\n                xs[i] = reflect_into_bounds(xs[i])\n\n            ys = []\n            for i in range(this_lam):\n                y = eval_point(xs[i])\n                ys.append(y)\n                if y < best_y:\n                    best_y = y\n                    best_x = xs[i].copy()\n                if evals_used >= budget:\n                    break\n\n            ys = np.array(ys)\n            if ys.size == 0:\n                break\n\n            # Selection\n            idx = np.argsort(ys)\n            elite_idx = idx[: min(mu, len(idx))]\n            x_elite = xs[elite_idx]\n            z_elite = zs[elite_idx]\n\n            # Weighted means\n            w_elite = weights[: x_elite.shape[0]]\n            mean_z = np.dot(w_elite, z_elite)\n\n            # Update step-size path\n            p_sigma = (1 - c_sigma) * p_sigma + np.sqrt(\n                c_sigma * (2 - c_sigma) * mu_eff\n            ) * mean_z\n\n            # CSA step-size adaptation\n            norm_p = np.linalg.norm(p_sigma)\n            expected_norm = np.sqrt(dim) * (\n                1 - 1 / (4 * dim) + 1 / (21 * dim**2)\n            )\n            sigma *= np.exp((norm_p / expected_norm - 1) * c_sigma / d_sigma)\n\n            # Clamp sigma\n            max_sigma = 0.5 * mean_span\n            min_sigma = 1e-9 * (mean_span + 1.0)\n            sigma = float(np.clip(sigma, min_sigma, max_sigma))\n\n            # Covariance adaptation (rank-mu)\n            # Directions relative to current best\n            y_c = x_elite - best_x\n            if y_c.size > 0 and sigma > 0:\n                y_c /= max(sigma, 1e-20)\n                C_update = np.zeros_like(C)\n                for w, yc in zip(w_elite, y_c):\n                    C_update += w * np.outer(yc, yc)\n\n                c_cov = 2.0 / ((dim + 1.3) ** 2 + mu_eff)\n                C = (1 - c_cov) * C + c_cov * C_update\n\n                # Numerical safeguard\n                C = 0.5 * (C + C.T)\n                C += 1e-12 * np.eye(dim)\n\n            # restart if sigma extremely tiny\n            if sigma <= min_sigma * 1.01 and evals_used < budget:\n                sigma = 0.1 * mean_span\n                C = np.eye(dim)\n                p_sigma[:] = 0.0\n\n            generations += 1\n\n    # ---------- Local Search (Coordinate-wise Adaptive Search) ----------\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Use all remaining budget for local search\n    # Start with step size proportional to span but not too large\n    step = 0.1 * np.where(high > low, high - low, 1.0)\n    step[step <= 0] = 0.1\n\n    x = best_x.copy()\n    fx = best_y\n\n    # Simple coordinate descent-like search with shrinking steps\n    # Each \"sweep\" tries positive and negative direction for each coordinate\n    while evals_used < budget:\n        improved = False\n        for d in range(dim):\n            if evals_used >= budget:\n                break\n            sd = step[d]\n            if sd <= 0:\n                continue\n\n            # Try positive move\n            x_pos = x.copy()\n            x_pos[d] += sd\n            x_pos = reflect_into_bounds(x_pos)\n            f_pos = eval_point(x_pos)\n            if f_pos < fx:\n                fx = f_pos\n                x = x_pos\n                improved = True\n                continue  # try same direction again next sweep\n\n            if evals_used >= budget:\n                break\n\n            # Try negative move\n            x_neg = x.copy()\n            x_neg[d] -= sd\n            x_neg = reflect_into_bounds(x_neg)\n            f_neg = eval_point(x_neg)\n            if f_neg < fx:\n                fx = f_neg\n                x = x_neg\n                improved = True\n\n            if evals_used >= budget:\n                break\n\n        # Update global best if local improved\n        if fx < best_y:\n            best_y = fx\n            best_x = x.copy()\n\n        # If no improvement in this sweep, shrink step sizes\n        if not improved:\n            step *= 0.5\n            # If steps are extremely small, stop local search\n            if np.all(step < 1e-12):\n                break\n\n    return best_x",
    "X": "4.815842316604414 7.917052671966543"
}