{
    "score": -6.129503891130647,
    "Input": "Alpine02",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budget-aware hybrid global + local search.\n\n    Strategy:\n    1. Global search via low-discrepancy-like stratified sampling (Sobol-ish) and\n       adaptive covariance ES (CMA-ES\u2013inspired).\n    2. Exploit prev_best_x as an elite warm-start if provided.\n    3. Adaptive population size and step-size tied to remaining budget and dimension.\n    4. Simple but robust boundary handling by reflection.\n    5. Use (almost) the entire evaluation budget.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    span[span <= 0] = 1.0  # safeguard for degenerate bounds\n\n    # Helper: project into bounds by reflection (vectorized-ish)\n    def reflect_into_bounds(x):\n        x_ref = np.asarray(x, dtype=float).copy()\n        for i in range(dim):\n            if span[i] <= 0:\n                x_ref[i] = low[i]\n                continue\n            # reflect until in range\n            li, hi = low[i], high[i]\n            while x_ref[i] < li or x_ref[i] > hi:\n                if x_ref[i] < li:\n                    x_ref[i] = li + (li - x_ref[i])\n                if x_ref[i] > hi:\n                    x_ref[i] = hi - (x_ref[i] - hi)\n        return x_ref\n\n    evals_used = 0\n\n    def eval_point(x):\n        nonlocal evals_used\n        if evals_used >= budget:\n            return np.inf\n        y = objective_function(x)\n        evals_used += 1\n        return y\n\n    # ---------------- Initialization ----------------\n    # Allocate about 20\u201330% of budget to global initialization, but at least 5 and at most 50 + 5*dim\n    init_evals = min(\n        max(5, 3 * dim),\n        max(1, int(0.3 * budget)),\n        50 + 5 * dim,\n    )\n\n    candidates = []\n\n    # Stratified sampling using jittered grid in each dimension\n    if init_evals > 0:\n        # Draw base points in [0,1]\n        # Use Latin-hypercube style: each point gets a unique stratum per dim\n        base = (np.arange(init_evals) + rng.random(init_evals)) / init_evals\n        base = np.clip(base, 0.0, 1.0)\n        for i in range(init_evals):\n            # scramble per-dimension to decorrelate\n            perms = [rng.permutation(init_evals) for _ in range(dim)]\n            u = np.array(\n                [(perms[d][i] + rng.random()) / init_evals for d in range(dim)]\n            )\n            u = np.clip(u, 0.0, 1.0)\n            xi = low + span * u\n            xi = reflect_into_bounds(xi)\n            yi = eval_point(xi)\n            candidates.append((yi, xi))\n            if evals_used >= budget:\n                break\n\n    # Include warm start if provided and within budget\n    if prev_best_x is not None and evals_used < budget:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = reflect_into_bounds(x0)\n            y0 = eval_point(x0)\n            candidates.append((y0, x0))\n\n    # Fallback if no evaluations could be made\n    if not candidates:\n        x = low + span * rng.random(dim)\n        return x\n\n    # Determine current best\n    candidates.sort(key=lambda t: t[0])\n    best_y, best_x = candidates[0]\n\n    if evals_used >= budget:\n        return best_x\n\n    # ---------------- ES (CMA-inspired) parameters ----------------\n    remaining = budget - evals_used\n\n    # Population size: scale with dimension & remaining budget\n    lam = min(max(4, 4 + int(3 * np.log(dim + 1))), max(10, remaining // 3))\n    mu = max(2, lam // 2)\n\n    # Weights for recombination\n    weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n    weights = weights / np.sum(weights)\n    mu_eff = 1.0 / np.sum(weights**2)\n\n    # Step-size: fraction of average span\n    mean_span = float(np.mean(span))\n    if mean_span <= 0:\n        return best_x\n    sigma = 0.3 * mean_span\n\n    # Initial covariance: isotropic\n    C = np.eye(dim)\n\n    # Evolution path for sigma (simplified)\n    c_sigma = (mu_eff + 2) / (dim + mu_eff + 5)\n    d_sigma = 1 + 2 * max(0, np.sqrt((mu_eff - 1) / (dim + 1)) - 1) + c_sigma\n    p_sigma = np.zeros(dim)\n\n    # number of generations roughly\n    # ensure at least a couple of iterations if budget allows\n    while evals_used < budget:\n        remaining = budget - evals_used\n        if remaining <= 0:\n            break\n        this_lam = min(lam, remaining)\n\n        # Sample offspring ~ N(best_x, sigma^2 C)\n        try:\n            A = np.linalg.cholesky(C)\n        except np.linalg.LinAlgError:\n            # If covariance is not PD, reset to identity\n            C = np.eye(dim)\n            A = np.eye(dim)\n\n        zs = rng.standard_normal((this_lam, dim))\n        xs = best_x + sigma * (zs @ A.T)\n\n        # Reflect into bounds\n        for i in range(this_lam):\n            xs[i] = reflect_into_bounds(xs[i])\n\n        ys = []\n        for i in range(this_lam):\n            y = eval_point(xs[i])\n            ys.append(y)\n            if y < best_y:\n                best_y = y\n                best_x = xs[i].copy()\n            if evals_used >= budget:\n                break\n\n        ys = np.array(ys)\n        # If no valid evaluations happened, exit\n        if ys.size == 0:\n            break\n\n        # Selection\n        idx = np.argsort(ys)\n        elite_idx = idx[: min(mu, len(idx))]\n        x_elite = xs[elite_idx]\n        z_elite = zs[elite_idx]\n\n        # Weighted mean in x and z spaces\n        mean_x = np.dot(weights[: x_elite.shape[0]], x_elite)\n        mean_z = np.dot(weights[: z_elite.shape[0]], z_elite)\n\n        # Update step-size path\n        p_sigma = (1 - c_sigma) * p_sigma + np.sqrt(\n            c_sigma * (2 - c_sigma) * mu_eff\n        ) * mean_z\n\n        # Step-size adaptation (CSA-like)\n        norm_p = np.linalg.norm(p_sigma)\n        expected_norm = np.sqrt(dim) * (1 - 1 / (4 * dim) + 1 / (21 * dim**2))\n        sigma *= np.exp((norm_p / expected_norm - 1) * c_sigma / d_sigma)\n\n        # Clamp sigma\n        max_sigma = 0.5 * mean_span\n        min_sigma = 1e-9 * (mean_span + 1.0)\n        sigma = float(np.clip(sigma, min_sigma, max_sigma))\n\n        # Covariance adaptation (rank-mu)\n        y_c = x_elite - best_x  # directions relative to current best\n        if y_c.size > 0:\n            # Normalize directions by sigma\n            y_c /= max(sigma, 1e-20)\n            # Weighted covariance update\n            C_update = np.zeros_like(C)\n            for w, yc in zip(weights[: y_c.shape[0]], y_c):\n                C_update += w * np.outer(yc, yc)\n            # Simple damping to keep PD and stable\n            c_cov = 2.0 / ((dim + 1.3) ** 2 + mu_eff)\n            C = (1 - c_cov) * C + c_cov * C_update\n\n            # Numerical safeguard: keep symmetric and add small ridge\n            C = 0.5 * (C + C.T)\n            C += 1e-12 * np.eye(dim)\n\n        # If sigma gets extremely tiny, perform a mild restart around best\n        if sigma <= min_sigma * 1.01 and evals_used < budget:\n            sigma = 0.1 * mean_span\n            C = np.eye(dim)\n\n    return best_x",
    "X": "4.81584239111022 7.917052768824091"
}