{
    "score": -1.0082028087379462,
    "Input": "OddSquare",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budget\u2011aware black\u2011box optimizer: hybrid global + local search\n    using an evolutionary population plus adaptive local refinement.\n\n    This version slightly rebalances exploration/exploitation, makes better\n    use of very small budgets, and handles degenerate / tiny search spaces\n    more robustly, while preserving the overall structure of the previous\n    implementation.\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config.get(\"budget\", 1))\n\n    low = bounds[:, 0].astype(float)\n    high = bounds[:, 1].astype(float)\n    span = high - low\n\n    # Handle zero or negative budget: return center of bounds\n    if budget <= 0:\n        return ((low + high) / 2.0).reshape(dim)\n\n    rng = np.random.default_rng()\n    evals_used = 0\n\n    # Helper to clip and ensure correct shape\n    def project(x):\n        x = np.asarray(x, dtype=float).reshape(dim)\n        return np.clip(x, low, high)\n\n    # --- Initialization / warm start ---\n    x_best = None\n    y_best = None\n\n    # If prev_best_x is provided, do not reevaluate it if it's clearly out of bounds;\n    # project and still evaluate once to integrate into current objective.\n    if prev_best_x is not None and evals_used < budget:\n        x0 = project(prev_best_x)\n        y0 = objective_function(x0)\n        evals_used += 1\n        x_best, y_best = x0.copy(), y0\n\n    # If no warm start or budget still allows: ensure at least one random sample\n    if (x_best is None) and (evals_used < budget):\n        x0 = rng.uniform(low, high)\n        y0 = objective_function(x0)\n        evals_used += 1\n        x_best, y_best = x0.copy(), y0\n\n    # If budget already exhausted\n    if evals_used >= budget:\n        return x_best.reshape(dim)\n\n    remaining_budget = budget - evals_used\n    if remaining_budget <= 0:\n        return x_best.reshape(dim)\n\n    # Very small budgets: just do a simple local random search around best\n    if remaining_budget <= 3:\n        base_step = 0.3\n        for i in range(remaining_budget):\n            if evals_used >= budget:\n                break\n            frac = base_step * (0.2 + 0.8 * (1.0 - i / max(1, remaining_budget - 1)))\n            step = frac * (span + 1e-12)  # protect zero span\n            noise = rng.normal(0.0, 1.0, size=dim)\n            x_candidate = x_best + noise * step\n            x_candidate = np.clip(x_candidate, low, high)\n            y = objective_function(x_candidate)\n            evals_used += 1\n            if y < y_best:\n                x_best, y_best = x_candidate, y\n        return x_best.reshape(dim)\n\n    # --- Global evolutionary search phase ---\n    # Use ~60% of remaining budget for global search, but keep at least some for local search\n    global_budget = max(1, int(0.6 * remaining_budget))\n    global_budget = min(global_budget, remaining_budget - 1) if remaining_budget > 4 else remaining_budget\n\n    # Population size: scale with budget and dimension, but keep moderate\n    if remaining_budget <= 12:\n        pop_size = min(6, remaining_budget)\n    else:\n        target = max(8, remaining_budget // 4)\n        pop_size = int(np.clip(target, 8, 40))\n        pop_size = min(pop_size, remaining_budget)  # can't exceed remaining evaluations in extreme cases\n\n    # Initialize population: mixture of around-best and random\n    pop = np.empty((pop_size, dim), dtype=float)\n    half = pop_size // 2\n    noise_scale = 0.25\n    for i in range(half):\n        noise = rng.normal(0.0, 1.0, size=dim)\n        candidate = x_best + noise * (noise_scale * (span + 1e-12))\n        pop[i] = np.clip(candidate, low, high)\n    if half < pop_size:\n        pop[half:] = rng.uniform(low, high, size=(pop_size - half, dim))\n\n    fitness = np.empty(pop_size, dtype=float)\n\n    # Evaluate initial population\n    for i in range(pop_size):\n        if evals_used >= budget:\n            break\n        # Avoid re-evaluating x_best if it matches an individual exactly\n        if np.allclose(pop[i], x_best) and y_best is not None:\n            y = y_best\n        else:\n            y = objective_function(pop[i])\n            evals_used += 1\n        fitness[i] = y\n        if (y_best is None) or (y < y_best):\n            x_best, y_best = pop[i].copy(), y\n\n    if evals_used >= budget:\n        return x_best.reshape(dim)\n\n    evals_for_global = min(global_budget, budget - evals_used)\n    if evals_for_global <= 0:\n        # Go directly to local refinement\n        remaining = budget - evals_used\n        if remaining <= 0:\n            return x_best.reshape(dim)\n\n        steps = remaining\n        base_step = 0.25\n        for i in range(steps):\n            if evals_used >= budget:\n                break\n            progress = i / max(1, steps - 1)\n            frac = base_step * (0.05 + 0.95 * (1.0 - progress))\n            step = frac * (span + 1e-12)\n\n            if i % 2 == 0 or dim == 1:\n                noise = rng.normal(0.0, 1.0, size=dim)\n                x_candidate = x_best + noise * step\n            else:\n                x_candidate = x_best.copy()\n                coord = rng.integers(0, dim)\n                step_coord = step[coord]\n                x_candidate[coord] += rng.normal(0.0, step_coord)\n\n            x_candidate = np.clip(x_candidate, low, high)\n            y = objective_function(x_candidate)\n            evals_used += 1\n            if y < y_best:\n                x_best, y_best = x_candidate, y\n        return x_best.reshape(dim)\n\n    # Conservative upper bound: each generation uses ~pop_size evaluations\n    max_generations = max(1, evals_for_global // max(1, pop_size))\n    base_mut_scale = 0.25\n\n    for gen in range(max_generations):\n        if evals_used >= budget:\n            break\n\n        # Rank population\n        order = np.argsort(fitness)\n        pop = pop[order]\n        fitness = fitness[order]\n\n        elite_frac = 0.35\n        n_elite = max(1, int(elite_frac * pop_size))\n        elites = pop[:n_elite]\n        elites_fit = fitness[:n_elite]\n\n        if elites_fit[0] < y_best:\n            x_best, y_best = elites[0].copy(), elites_fit[0]\n\n        new_pop = np.empty_like(pop)\n        new_pop[:n_elite] = elites\n\n        def select_parent():\n            # Tournament selection among elites\n            idx1 = rng.integers(0, n_elite)\n            idx2 = rng.integers(0, n_elite)\n            return elites[idx1] if elites_fit[idx1] < elites_fit[idx2] else elites[idx2]\n\n        gen_frac = gen / max(1, max_generations - 1)\n        mut_scale = base_mut_scale * (0.5 + 0.5 * (1.0 - gen_frac))\n\n        # Create offspring\n        for i in range(n_elite, pop_size):\n            if rng.random() < 0.85:\n                # Blend crossover\n                p1 = select_parent()\n                p2 = select_parent()\n                alpha = rng.random(dim)\n                child = alpha * p1 + (1.0 - alpha) * p2\n            else:\n                # Mutation from best elite or random elite\n                base = elites[0] if rng.random() < 0.6 else select_parent()\n                child = base.copy()\n\n            noise = rng.normal(0.0, 1.0, size=dim)\n            child = child + noise * (mut_scale * (span + 1e-12))\n            new_pop[i] = np.clip(child, low, high)\n\n        pop = new_pop\n\n        # Evaluate population\n        for i in range(pop_size):\n            if evals_used >= budget:\n                break\n            # Reuse elite fitness where possible\n            if i < n_elite:\n                fitness[i] = elites_fit[i]\n                if fitness[i] < y_best:\n                    x_best, y_best = pop[i].copy(), fitness[i]\n                continue\n\n            y = objective_function(pop[i])\n            evals_used += 1\n            fitness[i] = y\n            if y < y_best:\n                x_best, y_best = pop[i].copy(), y\n\n    if evals_used >= budget:\n        return x_best.reshape(dim)\n\n    # --- Local refinement phase around best found ---\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best.reshape(dim)\n\n    steps = remaining\n    base_step = 0.2  # fraction of span\n    for i in range(steps):\n        if evals_used >= budget:\n            break\n        progress = i / max(1, steps - 1)\n        frac = base_step * (0.05 + 0.95 * (1.0 - progress))\n        step = frac * (span + 1e-12)\n\n        if i % 2 == 0 or dim == 1:\n            noise = rng.normal(0.0, 1.0, size=dim)\n            x_candidate = x_best + noise * step\n        else:\n            x_candidate = x_best.copy()\n            coord = rng.integers(0, dim)\n            step_coord = step[coord]\n            x_candidate[coord] += rng.normal(0.0, step_coord)\n\n        x_candidate = np.clip(x_candidate, low, high)\n        y = objective_function(x_candidate)\n        evals_used += 1\n        if y < y_best:\n            x_best, y_best = x_candidate, y\n\n    return x_best.reshape(dim)",
    "X": "1.0829268397638976 1.381305722614379"
}