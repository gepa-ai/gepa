{
    "score": -0.9999999999973936,
    "Input": "LennardJones6",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    bounds = np.array(config['bounds'], dtype=float)\n    dim = int(config.get('dim', bounds.shape[0]))\n    budget = int(config.get('budget', 1))\n    low, high = bounds[:, 0], bounds[:, 1]\n\n    def clip(x):\n        return np.clip(x, low, high)\n\n    # If no evaluations allowed, return center of bounds\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    rng = np.random.default_rng()\n    evals_used = 0\n\n    # --- Initialization / warm start ---\n    best_x = None\n    best_y = None\n\n    # Use prev_best_x if valid\n    if prev_best_x is not None and evals_used < budget:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape == (dim,):\n            x0 = clip(x0)\n            y0 = objective_function(x0)\n            evals_used += 1\n            best_x, best_y = x0, y0\n\n    # Fallback to a random start\n    if best_x is None and evals_used < budget:\n        x0 = rng.uniform(low, high)\n        y0 = objective_function(x0)\n        evals_used += 1\n        best_x, best_y = x0, y0\n\n    if evals_used >= budget:\n        return np.array(best_x, dtype=float)\n\n    # Helper: random candidate within bounds\n    def random_candidate():\n        return rng.uniform(low, high)\n\n    # --- Adaptive global search (CMA-ES\u2013like / ES) ---\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return np.array(best_x, dtype=float)\n\n    # Allocate ~70% of remaining budget to global search (ES-style),\n    # keep the rest for local refinement.\n    global_budget = max(1, int(0.7 * remaining))\n    local_budget = remaining - global_budget\n\n    # Population size scales with dim but with caps for small/large budgets\n    if dim <= 5:\n        base_lambda = 8\n    else:\n        base_lambda = 4 + int(3 * np.log(dim + 1))\n    lam = max(4, min(base_lambda, max(4, global_budget // 2)))\n\n    # Initial global step size: fraction of range\n    range_scale = high - low\n    range_scale[range_scale == 0.0] = 1.0\n    sigma = 0.3 * range_scale\n\n    x_mean = best_x.copy()\n    y_mean = best_y\n\n    # Precompute weights for recombination (favor best)\n    # Use a simple decreasing linear weight scheme\n    ranks = np.arange(1, lam + 1)\n    weights = (lam + 1 - ranks).astype(float)\n    weights /= weights.sum()\n\n    # Global ES loop\n    while evals_used < budget and global_budget > 0:\n        # Number of offspring this generation\n        lam_gen = min(lam, global_budget)\n\n        # Sample population\n        population = []\n        scores = []\n\n        for _ in range(lam_gen):\n            if evals_used >= budget or global_budget <= 0:\n                break\n            z = rng.standard_normal(dim)\n            x = x_mean + sigma * z\n            # With small probability, do a uniform restart sample to avoid stagnation\n            if rng.random() < 0.05:\n                x = random_candidate()\n            x = clip(x)\n            y = objective_function(x)\n            evals_used += 1\n            global_budget -= 1\n\n            population.append(x)\n            scores.append(y)\n\n            if y < best_y:\n                best_x, best_y = x, y\n\n        if len(population) == 0:\n            break\n\n        population = np.asarray(population)\n        scores = np.asarray(scores)\n\n        # Sort by fitness (ascending)\n        idx = np.argsort(scores)\n        population = population[idx]\n        scores = scores[idx]\n\n        # Update mean with weighted recombination of best individuals\n        k = min(len(population), lam)\n        w = weights[:k]\n        x_new_mean = np.sum(population[:k] * w[:, None], axis=0)\n        y_new_mean = scores[:k].dot(w)\n\n        # Adapt step size: if mean improved, slightly increase; otherwise decrease\n        if y_new_mean < y_mean:\n            x_mean, y_mean = x_new_mean, y_new_mean\n            sigma *= 1.05\n        else:\n            sigma *= 0.7\n\n        # Keep sigma within reasonable bounds\n        sigma = np.clip(sigma, 1e-6 * range_scale, 0.5 * range_scale)\n\n        # Occasional random re-centering if no improvement for many evaluations\n        if evals_used > 0 and evals_used % max(10 * dim, 20) == 0:\n            # If best_y has not changed much compared to mean, inject exploration\n            x_mean = 0.5 * x_mean + 0.5 * random_candidate()\n\n    if evals_used >= budget or local_budget <= 0:\n        return np.array(best_x, dtype=float)\n\n    # --- Local search: adaptive coordinate search starting from best_x ---\n    remaining = min(local_budget, budget - evals_used)\n    if remaining <= 0:\n        return np.array(best_x, dtype=float)\n\n    x_current = best_x.copy()\n    y_current = best_y\n\n    # Coordinate step sizes start smaller than global sigma\n    step = 0.1 * range_scale\n    step[step == 0.0] = 1.0\n\n    # Max sweeps using approximate 2 evals per dimension\n    max_sweeps = max(1, remaining // max(1, 2 * dim))\n\n    for _ in range(max_sweeps):\n        if evals_used >= budget:\n            break\n        improved = False\n\n        # Randomize coordinate order for robustness\n        coords = np.arange(dim)\n        rng.shuffle(coords)\n\n        for d in coords:\n            if evals_used >= budget:\n                break\n\n            # Try negative direction\n            x_try = x_current.copy()\n            x_try[d] = x_try[d] - step[d]\n            x_try = clip(x_try)\n            y_try = objective_function(x_try)\n            evals_used += 1\n\n            if y_try < y_current:\n                x_current, y_current = x_try, y_try\n                if y_current < best_y:\n                    best_x, best_y = x_current, y_current\n                improved = True\n                continue\n\n            if evals_used >= budget:\n                break\n\n            # Try positive direction\n            x_try = x_current.copy()\n            x_try[d] = x_try[d] + step[d]\n            x_try = clip(x_try)\n            y_try = objective_function(x_try)\n            evals_used += 1\n\n            if y_try < y_current:\n                x_current, y_current = x_try, y_try\n                if y_current < best_y:\n                    best_x, best_y = x_current, y_current\n                improved = True\n\n        # Adapt step sizes\n        if not improved:\n            step *= 0.5\n            if np.all(step < range_scale * 1e-5):\n                break\n        else:\n            step *= 1.1\n            step = np.minimum(step, 0.25 * range_scale)\n\n    return np.array(best_x, dtype=float)",
    "X": "-0.9397429303910362 -0.09464751255289033 2.1930728755173554 -1.1561081802380513 0.5585439373315854 1.4674506988486808"
}