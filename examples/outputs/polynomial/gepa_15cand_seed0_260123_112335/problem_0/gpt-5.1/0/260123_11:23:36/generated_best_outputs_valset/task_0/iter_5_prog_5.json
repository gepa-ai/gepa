{
    "score": 10.121851731821694,
    "Input": "Ackley",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid global-local derivative-free optimizer with warm start support.\n\n    This version keeps the previous hybrid structure (global LHS + local search),\n    but makes several changes focused on robustness and better performance on\n    smooth benchmark functions (e.g., Ackley, Rastrigin) and noisy cases:\n\n    - More samples near warm start / current best (if available).\n    - Tighter and more adaptive global/local budget split.\n    - Improved step-size initialization and adaptation.\n    - Short, greedy refinement pass at the end to better exploit the best point.\n    \"\"\"\n\n    # ---------- Setup ----------\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0].astype(float)\n    high = bounds[:, 1].astype(float)\n    span = high - low\n    # Handle degenerate bounds\n    span = np.where(span <= 0.0, 1.0, span)\n\n    # Deterministic seed based on config to keep behavior reproducible\n    key = (tuple(map(float, np.array(config.get(\"bounds\", []), float).ravel()))\n           if isinstance(config, dict) and \"bounds\" in config else ())\n    seed = (hash((config.get(\"dim\", dim), config.get(\"budget\", budget), key)) % (2**32 - 1))\n    rng = np.random.RandomState(seed)\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    # ---------- Initialize incumbent ----------\n    x_best = None\n    y_best = None\n\n    # Warm start from prev_best_x if provided\n    if prev_best_x is not None and budget > 0:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            x0 = clip_to_bounds(x0)\n            y0 = float(objective_function(x0))\n            evals_used += 1\n            x_best, y_best = x0, y0\n        except Exception:\n            x_best, y_best = None, None\n\n    if evals_used >= budget:\n        if x_best is None:\n            return clip_to_bounds((low + high) * 0.5)\n        return x_best\n\n    # If no valid warm start, start from random point (center-biased)\n    if x_best is None:\n        # Slight bias toward center to help on symmetric benchmarks like Ackley\n        center = (low + high) * 0.5\n        scale = 0.5\n        x0 = center + rng.uniform(-scale, scale, size=dim) * span\n        x0 = clip_to_bounds(x0)\n        y0 = float(objective_function(x0))\n        evals_used += 1\n        x_best, y_best = x0, y0\n\n    if evals_used >= budget:\n        return x_best\n\n    # ---------- Budget allocation (global vs local) ----------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # Adaptive split:\n    # - In low dimensions and small budget, favor local search.\n    # - In higher dimensions and larger budget, favor global exploration.\n    dim_factor = min(1.0, max(0.0, (dim - 3) / 15.0))       # 0 at dim<=3, ~1 at dim>=18\n    budget_factor = min(1.0, max(0.0, (budget - 40) / 260.0))  # 0 at <=40, ~1 at >=300\n    global_ratio = 0.3 + 0.3 * dim_factor + 0.25 * budget_factor\n    global_ratio = max(0.2, min(0.8, global_ratio))\n\n    global_budget = int(global_ratio * remaining)\n    # Ensure at least a few local evaluations\n    global_budget = min(global_budget, remaining - 5) if remaining > 5 else max(1, remaining - 1)\n    global_budget = max(3, min(global_budget, remaining))  # at least 3 global samples\n    local_budget = remaining - global_budget\n\n    # ---------- Global exploration ----------\n    best_seen_x = [x_best]\n    best_seen_y = [y_best]\n\n    if global_budget > 0:\n        n_global = global_budget\n\n        # Latin-hypercube-like sampling for better coverage\n        base = rng.rand(n_global, dim)\n        for d in range(dim):\n            perm = rng.permutation(n_global)\n            base[:, d] = (perm + base[:, d]) / float(n_global)\n\n        Xg = low + base * span\n\n        # Slight bias: inject a few points around current best (local refinement seeds)\n        n_around_best = max(1, min(4, n_global // 6))\n        for i in range(n_around_best):\n            if evals_used >= budget:\n                break\n            noise_dir = rng.normal(size=dim)\n            norm = np.linalg.norm(noise_dir)\n            if norm > 1e-12:\n                noise_dir /= norm\n            radius = span * (0.15 + 0.15 * rng.rand(dim))  # 15\u201330% of span\n            x_nb = clip_to_bounds(x_best + noise_dir * radius)\n            Xg[i] = x_nb\n\n        for i in range(n_global):\n            if evals_used >= budget:\n                break\n            x = Xg[i]\n            y = float(objective_function(x))\n            evals_used += 1\n            if y < y_best:\n                x_best, y_best = x, y\n            best_seen_x.append(x)\n            best_seen_y.append(y)\n\n    # Sort the evaluated points for better local-start selection\n    best_seen_y = np.array(best_seen_y, dtype=float)\n    best_seen_x = np.array(best_seen_x, dtype=float)\n    order = np.argsort(best_seen_y)\n    best_seen_x = best_seen_x[order]\n    best_seen_y = best_seen_y[order]\n\n    if evals_used >= budget:\n        return x_best\n\n    # ---------- Local search: multi-start adaptive random search ----------\n    remaining = budget - evals_used\n    if remaining <= 0 or local_budget <= 0:\n        return x_best\n\n    # Dimension-aware neighborhood:\n    # Use smaller base step in higher dimensions to avoid over-jumping.\n    # Also shrink relative step for larger bounds.\n    base_step = span / (8.0 + 0.7 * max(dim - 1, 0))\n    # Keep min_step reasonably small to allow fine convergence\n    min_step = span / 1e4 + 1e-12\n    max_step = span\n\n    # Decide number of local starts:\n    # More starts in higher dims, but keep enough budget per start.\n    max_starts = max(1, min(6, local_budget // max(4, 2 * dim)))\n    # Also cap by available distinct candidates\n    max_starts = min(max_starts, max(1, len(best_seen_x)))\n\n    # Select best candidates as starting points\n    local_starts = best_seen_x[:max_starts].copy()\n\n    # If not enough starts (e.g., very small global_budget), add random ones\n    while len(local_starts) < max_starts:\n        local_starts = np.vstack([local_starts, rng.uniform(low, high)])\n    local_starts = local_starts[:max_starts]\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # Redistribute remaining budget across starts\n    per_start = [remaining // max_starts] * max_starts\n    for i in range(remaining % max_starts):\n        per_start[i] += 1\n\n    # Map from x to y for reuse (approx, via allclose)\n    def find_cached_y(x):\n        for j in range(len(best_seen_x)):\n            if np.allclose(best_seen_x[j], x, atol=1e-12, rtol=0.0):\n                return float(best_seen_y[j])\n        return None\n\n    for s in range(max_starts):\n        if evals_used >= budget:\n            break\n\n        start_budget = per_start[s]\n        if start_budget <= 0:\n            continue\n\n        current_x = clip_to_bounds(local_starts[s].copy())\n        y_cur = find_cached_y(current_x)\n        if y_cur is None:\n            if evals_used >= budget:\n                break\n            y_cur = float(objective_function(current_x))\n            evals_used += 1\n            best_seen_x = np.vstack([best_seen_x, current_x])\n            best_seen_y = np.concatenate([best_seen_y, [y_cur]])\n\n        current_y = y_cur\n        if current_y < y_best:\n            x_best, y_best = current_x, current_y\n\n        # Start step size per coordinate\n        current_step = np.maximum(base_step * (0.7 + 0.6 * rng.rand(dim)), min_step)\n\n        no_improve = 0\n        for _ in range(start_budget):\n            if evals_used >= budget:\n                break\n\n            # Decide direction type: more coordinate moves in high dim\n            if rng.rand() < (0.7 if dim <= 10 else 0.85):\n                # Coordinate direction\n                d = np.zeros(dim)\n                idx = rng.randint(dim)\n                d[idx] = 1.0 if rng.rand() < 0.5 else -1.0\n                direction = d\n            else:\n                direction = rng.normal(size=dim)\n                norm = np.linalg.norm(direction)\n                if norm > 1e-12:\n                    direction /= norm\n\n            # Step magnitude with mild log-normal variability\n            mag = rng.lognormal(mean=-0.2, sigma=0.35)\n            step_vec = current_step * mag\n            x_candidate = current_x + direction * step_vec\n            x_candidate = clip_to_bounds(x_candidate)\n\n            y_candidate = float(objective_function(x_candidate))\n            evals_used += 1\n\n            if y_candidate < current_y:\n                current_x, current_y = x_candidate, y_candidate\n                if y_candidate < y_best:\n                    x_best, y_best = x_candidate, y_candidate\n                # Success: gently enlarge step but keep bounded\n                current_step = np.minimum(current_step * 1.25, max_step)\n                no_improve = 0\n            else:\n                # Failure: contract step\n                current_step = np.maximum(current_step * 0.6, min_step)\n                no_improve += 1\n\n            if evals_used >= budget:\n                break\n\n            # Local restart if stuck\n            if no_improve >= max(10, 3 * dim) and evals_used < budget:\n                noise = rng.normal(size=dim)\n                norm = np.linalg.norm(noise)\n                if norm > 1e-12:\n                    noise /= norm\n                # Radius shrinks with remaining starts and progress\n                progress = (s + 1) / max_starts\n                radius = span * (0.2 * (1.0 - 0.6 * progress))\n                x_restart = clip_to_bounds(x_best + noise * radius)\n                y_restart = float(objective_function(x_restart))\n                evals_used += 1\n                if y_restart < current_y:\n                    current_x, current_y = x_restart, y_restart\n                    if y_restart < y_best:\n                        x_best, y_best = x_restart, y_restart\n                    # Reset step moderately small\n                    current_step = np.maximum(base_step * (0.5 ** (s + 1)), min_step)\n                no_improve = 0\n\n                if evals_used >= budget:\n                    break\n\n    # ---------- Final greedy refinement around best ----------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # Short, greedy local search around x_best with small steps\n    refine_steps = min(remaining, max(5, dim * 3))\n    refine_step = np.maximum(span / (20.0 + dim), min_step)\n\n    current_x = x_best.copy()\n    current_y = y_best\n    step_vec = refine_step.copy()\n\n    for _ in range(refine_steps):\n        if evals_used >= budget:\n            break\n\n        # Try coordinate moves only (cheap, stable)\n        idx = rng.randint(dim)\n        direction = np.zeros(dim)\n        direction[idx] = 1.0 if rng.rand() < 0.5 else -1.0\n\n        x_candidate = clip_to_bounds(current_x + direction * step_vec)\n        y_candidate = float(objective_function(x_candidate))\n        evals_used += 1\n\n        if y_candidate < current_y:\n            current_x, current_y = x_candidate, y_candidate\n            if y_candidate < y_best:\n                x_best, y_best = x_candidate, y_candidate\n            step_vec[idx] = min(step_vec[idx] * 1.4, max_step[idx])\n        else:\n            step_vec[idx] = max(step_vec[idx] * 0.5, min_step[idx])\n\n    return x_best",
    "X": "0.0007522678113084671 7.956580848876257 -6.951863068653992 1.9877768282412394 3.652843307581029e-05 3.9939912408026568 1.0116946011444465 0.044707504089355296 0.9957435008956389 -0.918584546989107 0.00020114850601920102"
}