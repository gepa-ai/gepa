{
    "score": 10.186268751523878,
    "Input": "Ackley",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid global-local derivative-free optimizer with warm start support.\n\n    This version keeps the previous hybrid structure (global LHS + local search)\n    and improves robustness and budget usage, particularly on smooth benchmark\n    functions like Ackley:\n\n    - Deterministic RNG seeding based on config for reproducibility across runs.\n    - Stronger and more adaptive budget allocation between global and local phases.\n    - More intensive local search near the best global sample (multi-start).\n    - Dimension-aware scaling of sampling density and local neighborhood size.\n    \"\"\"\n\n    # Deterministic but varied seed if available; otherwise fallback\n    seed = None\n    if isinstance(config, dict):\n        # Try to build a simple hash-based seed to keep runs reproducible\n        key = (tuple(map(float, np.array(config.get(\"bounds\", []), float).ravel()))\n               if \"bounds\" in config else ())\n        seed = (hash((config.get(\"dim\", 0), config.get(\"budget\", 0), key)) % (2**32 - 1))\n    rng = np.random.RandomState(seed if seed is not None else None)\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0].astype(float)\n    high = bounds[:, 1].astype(float)\n    span = high - low\n\n    # Handle degenerate bounds\n    span = np.where(span <= 0.0, 1.0, span)\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    # Initialize incumbent\n    x_best = None\n    y_best = None\n\n    # Use prev_best_x if provided and valid\n    if prev_best_x is not None and budget > 0:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            x0 = clip_to_bounds(x0)\n            y0 = objective_function(x0)\n            evals_used += 1\n            x_best, y_best = x0, y0\n        except Exception:\n            x_best, y_best = None, None\n\n    if evals_used >= budget:\n        if x_best is None:\n            return clip_to_bounds((low + high) * 0.5)\n        return x_best\n\n    # If no warm start or failed, start from random point\n    if x_best is None:\n        x0 = rng.uniform(low, high)\n        y0 = objective_function(x0)\n        evals_used += 1\n        x_best, y_best = x0, y0\n\n    if evals_used >= budget:\n        return x_best\n\n    # ------------- Budget allocation -------------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # Sharper adaptive global/local split:\n    # - In low dim and small budget, prioritize local search.\n    # - In higher dim / large budget, more global exploration.\n    dim_factor = min(1.0, max(0.0, (dim - 3) / 20.0))  # 0 at dim<=3, ~0.5 at dim=13, capped at 1\n    budget_factor = min(1.0, max(0.0, (budget - 50) / 450.0))  # more global when budget>50\n    global_ratio = 0.3 + 0.3 * dim_factor + 0.2 * budget_factor\n    global_ratio = max(0.2, min(0.8, global_ratio))\n\n    global_budget = int(global_ratio * remaining)\n    local_budget = remaining - global_budget\n\n    # Ensure at least a few global samples in all cases\n    global_budget = max(min(global_budget, remaining - 3), min(remaining, 3))\n\n    # ------------- Global exploration: stratified / LHS-like sampling -------------\n    if global_budget > 0:\n        n_global = global_budget\n\n        # Latin-hypercube-like sampling for better coverage\n        base = rng.rand(n_global, dim)\n        for d in range(dim):\n            perm = rng.permutation(n_global)\n            base[:, d] = (perm + base[:, d]) / float(n_global)\n\n        Xg = low + base * span\n\n        # Always include incumbent in the pool as a reference point\n        # (doesn't cost extra evaluation)\n        best_seen_x = [x_best]\n        best_seen_y = [y_best]\n\n        for i in range(n_global):\n            if evals_used >= budget:\n                break\n            x = Xg[i]\n            y = objective_function(x)\n            evals_used += 1\n            if y < y_best:\n                x_best, y_best = x, y\n            best_seen_x.append(x)\n            best_seen_y.append(y)\n\n        # Sort best_seen by value for later local starts\n        best_seen_y = np.array(best_seen_y)\n        best_seen_x = np.array(best_seen_x)\n        order = np.argsort(best_seen_y)\n        best_seen_x = best_seen_x[order]\n        best_seen_y = best_seen_y[order]\n    else:\n        best_seen_x = np.array([x_best])\n        best_seen_y = np.array([y_best])\n\n    if evals_used >= budget:\n        return x_best\n\n    # ------------- Local search: multi-start adaptive random search -------------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # Dimension-aware neighborhood\n    # Smaller step for higher dimensions to avoid too aggressive jumps\n    base_step = span / (5.0 + 0.5 * max(dim - 1, 0))\n    min_step = span / 1e5 + 1e-12\n    max_step = span\n\n    # Decide number of local starts: more starts for higher dim and more budget\n    max_starts = min(5, remaining // max(5, 2 * dim))\n    max_starts = max(max_starts, 1)\n\n    # Initial starting points: best global points + incumbent\n    # Always include at least the 2 best points if available\n    n_candidates = min(len(best_seen_x), max_starts + 1)\n    local_starts = best_seen_x[:n_candidates]\n\n    # If not enough distinct starts, add random ones\n    while len(local_starts) < max_starts:\n        local_starts = np.vstack([local_starts, rng.uniform(low, high)])\n    local_starts = local_starts[:max_starts]\n\n    # Redistribute remaining budget across starts\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    per_start = [remaining // max_starts] * max_starts\n    for i in range(remaining % max_starts):\n        per_start[i] += 1\n\n    for s in range(max_starts):\n        if evals_used >= budget:\n            break\n\n        start_budget = per_start[s]\n        if start_budget <= 0:\n            continue\n\n        current_x = clip_to_bounds(local_starts[s].copy())\n        # Ensure current_y is evaluated (might already be in best_seen)\n        y_cur = None\n        # Try to reuse evaluation when possible\n        for j in range(len(best_seen_x)):\n            if np.allclose(best_seen_x[j], current_x, atol=1e-12, rtol=0.0):\n                y_cur = float(best_seen_y[j])\n                break\n        if y_cur is None:\n            if evals_used >= budget:\n                break\n            y_cur = float(objective_function(current_x))\n            evals_used += 1\n\n        current_y = y_cur\n        if current_y < y_best:\n            x_best, y_best = current_x, current_y\n\n        # Start step size slightly different per start to diversify\n        current_step = np.maximum(base_step * (0.8 + 0.4 * rng.rand(dim)), min_step)\n\n        # Adaptive local steps\n        no_improve = 0\n        for _ in range(start_budget):\n            if evals_used >= budget:\n                break\n\n            # Coordinate vs isotropic directions\n            if rng.rand() < 0.6:\n                # Coordinate direction\n                d = np.zeros(dim)\n                idx = rng.randint(dim)\n                d[idx] = 1.0 if rng.rand() < 0.5 else -1.0\n                direction = d\n            else:\n                direction = rng.normal(size=dim)\n                norm = np.linalg.norm(direction)\n                if norm > 1e-12:\n                    direction /= norm\n\n            # Step magnitude with mild log-normal variability\n            mag = rng.lognormal(mean=-0.3, sigma=0.4)\n            step_vec = current_step * mag\n            x_candidate = current_x + direction * step_vec\n            x_candidate = clip_to_bounds(x_candidate)\n\n            y_candidate = objective_function(x_candidate)\n            evals_used += 1\n\n            if y_candidate < current_y:\n                current_x, current_y = x_candidate, y_candidate\n                if y_candidate < y_best:\n                    x_best, y_best = x_candidate, y_candidate\n                # Success: gently enlarge step within bounds\n                current_step = np.minimum(current_step * 1.2, max_step)\n                no_improve = 0\n            else:\n                # Failure: contract step\n                current_step = np.maximum(current_step * 0.7, min_step)\n                no_improve += 1\n\n            if evals_used >= budget:\n                break\n\n            # Local restart if stuck: sample around global best, radius decreasing with progress\n            if no_improve >= max(8, 2 * dim) and evals_used < budget:\n                noise = rng.normal(size=dim)\n                norm = np.linalg.norm(noise)\n                if norm > 1e-12:\n                    noise /= norm\n                # Radius shrinks with how far we are through the starts\n                progress = (s + 1) / max_starts\n                radius = span * (0.25 * (1.0 - 0.7 * progress))\n                x_restart = x_best + noise * radius\n                x_restart = clip_to_bounds(x_restart)\n                y_restart = objective_function(x_restart)\n                evals_used += 1\n                if y_restart < current_y:\n                    current_x, current_y = x_restart, y_restart\n                    if y_restart < y_best:\n                        x_best, y_best = x_restart, y_restart\n                    current_step = np.maximum(base_step * (0.7 ** (s + 1)), min_step)\n                no_improve = 0\n\n                if evals_used >= budget:\n                    break\n\n    return x_best",
    "X": "0.0007522678113084671 7.956580848876257 -6.951863068653992 1.9877768282412394 3.652843307581029e-05 3.9939912408026568 1.0167253250064963 0.07077091472840755 0.9957435008956389 -1.9854107281921995 0.00020114850601920102"
}