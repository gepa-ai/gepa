{
    "score": 10.188651061011667,
    "Input": "Ackley",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid global-local derivative-free optimizer with warm start support.\n\n    Improvements over previous version:\n    - More robust and scalable global search using quasi-Latin hypercube sampling.\n    - Budget-aware allocation between global and local phases (adaptive to dim/budget).\n    - Simple restart-aware local search that can escape poor warm starts.\n    - Slightly more aggressive but controlled local refinement on promising regions.\n    \"\"\"\n\n    rng = np.random.RandomState()\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0].astype(float)\n    high = bounds[:, 1].astype(float)\n    span = high - low\n\n    # Handle degenerate bounds\n    span = np.where(span <= 0.0, 1.0, span)\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    # Initialize incumbent\n    x_best = None\n    y_best = None\n\n    # Use prev_best_x if provided\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            x0 = clip_to_bounds(x0)\n            y0 = objective_function(x0)\n            evals_used += 1\n            x_best, y_best = x0, y0\n        except Exception:\n            # If anything goes wrong with warm start, fall back to fresh start\n            x_best, y_best = None, None\n\n    if evals_used >= budget:\n        # If nothing evaluated correctly, fall back to center of the box\n        if x_best is None:\n            return clip_to_bounds((low + high) * 0.5)\n        return x_best\n\n    # If no warm start or failed, start from random point\n    if x_best is None:\n        x0 = rng.uniform(low, high)\n        y0 = objective_function(x0)\n        evals_used += 1\n        x_best, y_best = x0, y0\n\n    if evals_used >= budget:\n        return x_best\n\n    # --------- Budget allocation ---------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # Adaptive global/local split: more global search for higher dim or larger budget\n    # Base ratio 0.4, increase toward 0.6 for high dim\n    dim_factor = min(1.0, max(0.0, (dim - 5) / 20.0))  # 0 at dim<=5, ~0.5 at dim=15, capped at 1\n    global_ratio = 0.4 + 0.2 * dim_factor\n    global_budget = int(global_ratio * budget) - evals_used\n    global_budget = max(0, min(global_budget, remaining))\n    local_budget = remaining - global_budget\n\n    # --------- Global exploration: stratified / LHS-like sampling ---------\n    # We avoid exponential grids; instead use stratified sampling per dimension.\n    if global_budget > 0:\n        n_global = global_budget\n\n        # Create per-dimension stratification indices (Latin hypercube style)\n        # Each dim: permutation of 0..n_global-1, jittered within cell.\n        # For high dim, this is still linear in n_global*dim.\n        # This is more robust than previous pseudo-grid in high dimension.\n        base = rng.rand(n_global, dim)\n        # Scale by stratification indices along each dimension independently\n        for d in range(dim):\n            perm = rng.permutation(n_global)\n            base[:, d] = (perm + base[:, d]) / float(n_global)\n\n        # Map to bounds\n        Xg = low + base * span\n\n        for i in range(n_global):\n            if evals_used >= budget:\n                break\n            x = Xg[i]\n            y = objective_function(x)\n            evals_used += 1\n            if y < y_best:\n                x_best, y_best = x, y\n\n    if evals_used >= budget:\n        return x_best\n\n    # --------- Local search: adaptive random search with restarts ---------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # Start step size: moderate fraction of span\n    base_step = span / 4.0\n    min_step = span / 1e5 + 1e-12\n    max_step = span\n\n    # Number of local \"rounds\" (each round can optionally restart)\n    # Keep small to preserve coherence but allow adaptation\n    rounds = min(5, max(1, remaining // max(6, 2 * dim)))\n\n    # Distribute remaining budget across rounds\n    per_round = [remaining // rounds] * rounds\n    for i in range(remaining % rounds):\n        per_round[i] += 1\n\n    # Local search state\n    current_x = x_best.copy()\n    current_y = float(y_best)\n    current_step = base_step.copy()\n\n    for r in range(rounds):\n        if evals_used >= budget:\n            break\n\n        round_budget = per_round[r]\n        if round_budget <= 0:\n            continue\n\n        # Slight annealing across rounds: reduce base step\n        if r > 0:\n            current_step = np.maximum(current_step * 0.7, min_step)\n\n        # Track best improvement within this round\n        round_best_y = current_y\n        no_improve_count = 0\n\n        for _ in range(round_budget):\n            if evals_used >= budget:\n                break\n\n            # Mix of coordinate directions and isotropic perturbations\n            if rng.rand() < 0.5:\n                # Coordinate direction\n                d = np.zeros(dim)\n                idx = rng.randint(dim)\n                d[idx] = 1.0 if rng.rand() < 0.5 else -1.0\n                direction = d\n            else:\n                direction = rng.normal(size=dim)\n                norm = np.linalg.norm(direction)\n                if norm > 1e-12:\n                    direction /= norm\n\n            # Sample step magnitude with mild log-normal spread\n            mag = rng.lognormal(mean=-0.2, sigma=0.5)\n            step_vec = current_step * mag\n            x_candidate = current_x + direction * step_vec\n            x_candidate = clip_to_bounds(x_candidate)\n\n            y_candidate = objective_function(x_candidate)\n            evals_used += 1\n\n            if y_candidate < current_y:\n                # Accept move\n                current_x, current_y = x_candidate, y_candidate\n                if y_candidate < y_best:\n                    x_best, y_best = x_candidate, y_candidate\n                # Slightly expand step on success (but bounded)\n                current_step = np.minimum(current_step * 1.25, max_step)\n                round_best_y = min(round_best_y, y_candidate)\n                no_improve_count = 0\n            else:\n                # Contract step on failure\n                current_step = np.maximum(current_step * 0.7, min_step)\n                no_improve_count += 1\n\n            if evals_used >= budget:\n                break\n\n            # Early mini-restart if stuck: sample around global best, not just current\n            # This helps in case warm start is poor or we got trapped.\n            if no_improve_count >= max(10, dim * 2) and evals_used < budget:\n                # Propose a new base near x_best with moderate noise\n                noise = rng.normal(size=dim)\n                norm = np.linalg.norm(noise)\n                if norm > 1e-12:\n                    noise /= norm\n                # Radius shrinks with round index\n                radius = span / (3.0 + r)\n                x_restart = x_best + noise * radius\n                x_restart = clip_to_bounds(x_restart)\n                y_restart = objective_function(x_restart)\n                evals_used += 1\n                if y_restart < current_y:\n                    current_x, current_y = x_restart, y_restart\n                    if y_restart < y_best:\n                        x_best, y_best = x_restart, y_restart\n                    current_step = np.maximum(base_step * (0.7 ** (r + 1)), min_step)\n                    round_best_y = min(round_best_y, y_restart)\n                no_improve_count = 0\n                if evals_used >= budget:\n                    break\n\n    return x_best",
    "X": "1.360462930654504e-07 7.958330020107732 -6.952831960180409 1.988800736366111 -0.00022932882965115073 3.9975698265606168 1.0192513086923314 0.07287075656642243 0.9934009704717752 -1.9876710884213504 -0.00026775646717237144"
}