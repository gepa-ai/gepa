{
    "score": 4.033982868875356,
    "Input": "Ackley",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid global (LHS + evolutionary) and local (adaptive random / coordinate search)\n    optimizer with warm start support.\n\n    This version keeps the global+local hybrid structure but:\n    - Strengthens global exploration for rugged functions (e.g., Ackley)\n    - Uses a smoother, more robust budget split\n    - Adds a focused final intensification around the incumbent\n    - Simplifies a few step-size heuristics for more stable behavior\n    \"\"\"\n\n    # ---------- Setup ----------\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0].astype(float)\n    high = bounds[:, 1].astype(float)\n    span = high - low\n    span = np.where(span <= 0.0, 1.0, span)  # handle degenerate bounds\n\n    if budget <= 0:\n        # No evaluations allowed; return center of the box\n        return (low + high) * 0.5\n\n    # Deterministic seed based on config for reproducibility, but guard against\n    # pathological hash behavior by also mixing in a random seed if provided\n    key = (tuple(map(float, np.array(config.get(\"bounds\", []), float).ravel()))\n           if isinstance(config, dict) and \"bounds\" in config else ())\n    base_seed = (hash((config.get(\"dim\", dim), config.get(\"budget\", budget), key)) %\n                 (2**32 - 1))\n    extra_seed = int(config.get(\"seed\", 0)) % (2**32 - 1)\n    seed = (base_seed ^ extra_seed) % (2**32 - 1)\n    rng = np.random.RandomState(seed)\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    # ---------- Initialize incumbent ----------\n    x_best = None\n    y_best = None\n\n    # Warm start from prev_best_x if provided and valid\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            x0 = clip_to_bounds(x0)\n            y0 = float(objective_function(x0))\n            evals_used += 1\n            x_best, y_best = x0, y0\n        except Exception:\n            x_best, y_best = None, None\n\n    if evals_used >= budget:\n        if x_best is None:\n            return clip_to_bounds((low + high) * 0.5)\n        return x_best\n\n    # If no valid warm start, start from center-biased random point\n    if x_best is None:\n        center = (low + high) * 0.5\n        # Slightly larger spread to better escape central plateaus on Ackley\n        scale = 0.6\n        x0 = center + rng.uniform(-scale, scale, size=dim) * span\n        x0 = clip_to_bounds(x0)\n        y0 = float(objective_function(x0))\n        evals_used += 1\n        x_best, y_best = x0, y0\n\n    if evals_used >= budget:\n        return x_best\n\n    # Global cache (simple numeric hash)\n    cache = {}\n\n    def key_from_x(x):\n        return tuple(np.round(x, decimals=12))\n\n    cache[key_from_x(x_best)] = y_best\n\n    # Track good points for local starts\n    best_seen_x = [x_best]\n    best_seen_y = [y_best]\n\n    # ---------- Budget allocation (global vs local) ----------\n    remaining_total = budget - evals_used\n    if remaining_total <= 0:\n        return x_best\n\n    # Very small budgets: do simple local refinement only\n    if remaining_total <= 5:\n        base_step = span / (12.0 + 0.3 * max(dim - 1, 0))\n        min_step = span / 4e4 + 1e-12\n        step_vec = np.maximum(base_step, min_step)\n        current_x = x_best.copy()\n        current_y = y_best\n        for _ in range(remaining_total):\n            if evals_used >= budget:\n                break\n            idx = rng.randint(dim)\n            direction = np.zeros(dim)\n            direction[idx] = 1.0 if rng.rand() < 0.5 else -1.0\n            x_candidate = clip_to_bounds(current_x + direction * step_vec)\n            k = key_from_x(x_candidate)\n            y_candidate = cache.get(k, None)\n            if y_candidate is None:\n                y_candidate = float(objective_function(x_candidate))\n                evals_used += 1\n                cache[k] = y_candidate\n            if y_candidate < current_y:\n                current_x, current_y = x_candidate, y_candidate\n                if y_candidate < y_best:\n                    x_best, y_best = x_candidate, y_candidate\n                step_vec[idx] = step_vec[idx] * 1.5\n            else:\n                step_vec[idx] = np.maximum(step_vec[idx] * 0.5, min_step[idx])\n        return x_best\n\n    # Heuristic global vs local ratio: bias more to global for rugged/multimodal\n    dim_factor = min(1.0, max(0.0, (dim - 2) / 18.0))\n    budget_factor = min(1.0, max(0.0, (budget - 40) / 260.0))\n    # Slightly stronger global search than previous version\n    global_ratio = 0.60 + 0.20 * dim_factor + 0.10 * budget_factor\n    global_ratio = max(0.40, min(0.80, global_ratio))\n\n    global_budget = int(round(global_ratio * remaining_total))\n\n    # Ensure some evaluations for local search\n    if remaining_total > 25:\n        global_budget = min(global_budget, remaining_total - 12)\n        global_budget = max(8, global_budget)\n    else:\n        global_budget = max(5, remaining_total - 6)\n\n    global_budget = min(global_budget, remaining_total)\n    local_budget = remaining_total - global_budget\n\n    # ---------- Global exploration: LHS + compact evolutionary phase ----------\n    if global_budget > 0:\n        if global_budget <= 6:\n            n_lhs = global_budget\n            evo_budget = 0\n        else:\n            # A bit more LHS to improve coverage for rugged functions\n            n_lhs = max(4, int(0.70 * global_budget))\n            n_lhs = min(n_lhs, global_budget)\n            evo_budget = global_budget - n_lhs\n\n        # Latin-hypercube-like sampling (n_lhs points)\n        n_global = n_lhs\n        base = rng.rand(n_global, dim)\n        for d in range(dim):\n            perm = rng.permutation(n_global)\n            base[:, d] = (perm + base[:, d]) / float(n_global)\n        Xg = low + base * span\n\n        # Inject points around current best to exploit incumbent\n        n_around_best = max(1, min(12, n_global // 3))\n        for i in range(n_around_best):\n            noise_dir = rng.normal(size=dim)\n            norm = np.linalg.norm(noise_dir)\n            if norm > 1e-12:\n                noise_dir /= norm\n            # Slightly wider neighborhood for better basin-hopping\n            radius = span * (0.06 + 0.16 * rng.rand(dim))  # 6\u201322% of span\n            x_nb = clip_to_bounds(x_best + noise_dir * radius)\n            Xg[i] = x_nb\n\n        # Evaluate global points\n        Yg = np.empty(n_global, dtype=float)\n        for i in range(n_global):\n            if evals_used >= budget:\n                Yg[i] = np.inf\n                continue\n            x = clip_to_bounds(Xg[i])\n            k = key_from_x(x)\n            y = cache.get(k, None)\n            if y is None:\n                y = float(objective_function(x))\n                evals_used += 1\n                cache[k] = y\n            Yg[i] = y\n            best_seen_x.append(x)\n            best_seen_y.append(y)\n            if y < y_best:\n                x_best, y_best = x, y\n\n        if evals_used >= budget:\n            return x_best\n\n        # Compact evolutionary refinement\n        evo_evals_left = max(0, evo_budget)\n        if evo_evals_left > 0 and n_global > 2 and evals_used < budget:\n            elite_frac = 0.35\n            elite_size = max(2, int(elite_frac * n_global))\n            elite_size = min(elite_size, n_global)\n            elite_idx = np.argsort(Yg)[:elite_size]\n            elite_X = Xg[elite_idx].copy()\n            elite_Y = Yg[elite_idx].copy()\n\n            if elite_size > 1:\n                elite_span = np.max(elite_X, axis=0) - np.min(elite_X, axis=0)\n            else:\n                elite_span = span * 0.25\n            # Slightly smaller step to avoid overshooting narrow basins\n            evo_step = np.maximum(elite_span * 0.30, span * 0.02)\n\n            while evo_evals_left > 0 and evals_used < budget:\n                parent_ids = rng.choice(elite_size, size=2, replace=True)\n                p1, p2 = elite_X[parent_ids[0]], elite_X[parent_ids[1]]\n                alpha = rng.rand()\n                child = alpha * p1 + (1.0 - alpha) * p2\n\n                # Gaussian mutation, slightly tempered\n                mutation = rng.normal(scale=0.30, size=dim) * evo_step\n                child = clip_to_bounds(child + mutation)\n\n                k = key_from_x(child)\n                y = cache.get(k, None)\n                if y is None:\n                    y = float(objective_function(child))\n                    evals_used += 1\n                    cache[k] = y\n                evo_evals_left -= 1\n\n                best_seen_x.append(child)\n                best_seen_y.append(y)\n                if y < y_best:\n                    x_best, y_best = child, y\n\n                worst_idx = np.argmax(elite_Y)\n                if y < elite_Y[worst_idx]:\n                    elite_Y[worst_idx] = y\n                    elite_X[worst_idx] = child\n\n    if evals_used >= budget:\n        return x_best\n\n    # ---------- Local search: multi-start adaptive random search ----------\n    remaining = budget - evals_used\n    if remaining <= 0 or local_budget <= 0:\n        return x_best\n\n    local_budget = min(local_budget, remaining)\n    if local_budget <= 0:\n        return x_best\n\n    best_seen_y_arr = np.array(best_seen_y, dtype=float)\n    best_seen_x_arr = np.array(best_seen_x, dtype=float)\n    order = np.argsort(best_seen_y_arr)\n    best_seen_x_arr = best_seen_x_arr[order]\n    best_seen_y_arr = best_seen_y_arr[order]\n\n    # Slightly smaller base_step for more careful local moves\n    base_step = span / (16.0 + 0.35 * max(dim - 1, 0))\n    min_step = span / 4e4 + 1e-12\n    max_step = span\n\n    # Limit number of starts to avoid fragmenting budget too much\n    min_per_start = max(4, 2 * dim, 10)\n    max_starts = max(1, min(4, local_budget // min_per_start))\n    max_starts = min(max_starts, max(1, len(best_seen_x_arr)))\n    local_starts = best_seen_x_arr[:max_starts].copy()\n\n    # Add random starts if not enough good points\n    while len(local_starts) < max_starts:\n        rnd = rng.uniform(low, high)\n        local_starts = np.vstack([local_starts, rnd])\n    local_starts = local_starts[:max_starts]\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # Distribute remaining budget across starts, slightly favoring best ones\n    per_start = [remaining // max_starts] * max_starts\n    for i in range(remaining % max_starts):\n        per_start[i] += 1\n    for i in range(max_starts):\n        per_start[i] = max(6, per_start[i])\n\n    for s in range(max_starts):\n        if evals_used >= budget:\n            break\n\n        start_budget = per_start[s]\n        remaining = budget - evals_used\n        if remaining <= 0:\n            break\n        start_budget = min(start_budget, remaining)\n        if start_budget <= 0:\n            continue\n\n        current_x = clip_to_bounds(local_starts[s].copy())\n        k = key_from_x(current_x)\n        y_cur = cache.get(k, None)\n        if y_cur is None:\n            if evals_used >= budget:\n                break\n            y_cur = float(objective_function(current_x))\n            evals_used += 1\n            cache[k] = y_cur\n\n        current_y = y_cur\n        if current_y < y_best:\n            x_best, y_best = current_x, current_y\n\n        current_step = np.maximum(\n            base_step * (0.85 + 0.6 * rng.rand(dim)), min_step\n        )\n        no_improve = 0\n        no_improve_thres = max(10, 3 * dim)\n\n        for _ in range(start_budget):\n            if evals_used >= budget:\n                break\n\n            # Coordinate moves preferred in higher dims\n            if rng.rand() < (0.72 if dim <= 10 else 0.92):\n                d = np.zeros(dim)\n                idx = rng.randint(dim)\n                d[idx] = 1.0 if rng.rand() < 0.5 else -1.0\n                direction = d\n            else:\n                direction = rng.normal(size=dim)\n                norm = np.linalg.norm(direction)\n                if norm > 1e-12:\n                    direction /= norm\n\n            # Slightly smaller lognormal magnitude to avoid huge jumps\n            mag = rng.lognormal(mean=-0.20, sigma=0.30)\n            step_vec = current_step * mag\n            x_candidate = clip_to_bounds(current_x + direction * step_vec)\n\n            k = key_from_x(x_candidate)\n            y_candidate = cache.get(k, None)\n            if y_candidate is None:\n                y_candidate = float(objective_function(x_candidate))\n                evals_used += 1\n                cache[k] = y_candidate\n\n            if y_candidate < current_y:\n                current_x, current_y = x_candidate, y_candidate\n                if y_candidate < y_best:\n                    x_best, y_best = x_candidate, y_candidate\n                current_step = np.minimum(current_step * 1.25, max_step)\n                no_improve = 0\n            else:\n                current_step = np.maximum(current_step * 0.72, min_step)\n                no_improve += 1\n\n            if evals_used >= budget:\n                break\n\n            # Local restart if stuck: bias toward global-best neighborhood\n            if no_improve >= no_improve_thres and evals_used < budget:\n                noise = rng.normal(size=dim)\n                norm = np.linalg.norm(noise)\n                if norm > 1e-12:\n                    noise /= norm\n                progress = (s + 1) / max_starts\n                # Slightly reduced radius schedule to focus around x_best\n                radius = span * (0.18 * (1.0 - 0.5 * progress))\n                x_restart = clip_to_bounds(x_best + noise * radius)\n\n                k_r = key_from_x(x_restart)\n                y_restart = cache.get(k_r, None)\n                if y_restart is None:\n                    y_restart = float(objective_function(x_restart))\n                    evals_used += 1\n                    cache[k_r] = y_restart\n\n                if y_restart < current_y:\n                    current_x, current_y = x_restart, y_restart\n                    if y_restart < y_best:\n                        x_best, y_best = x_restart, y_restart\n                    current_step = np.maximum(\n                        base_step * (0.40 ** (s + 1)), min_step\n                    )\n                no_improve = 0\n\n                if evals_used >= budget:\n                    break\n\n    # ---------- Final greedy coordinate refinement + focused intensification ----------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return x_best\n\n    # Keep coordinate refinement but also add shrinking neighborhood moves\n    refine_steps = min(remaining, max(14, dim * 6))\n    min_step = span / 4e4 + 1e-12\n    refine_step = np.maximum(span / (30.0 + dim), min_step)\n\n    current_x = x_best.copy()\n    current_y = y_best\n    step_vec = refine_step.copy()\n\n    for i in range(refine_steps):\n        if evals_used >= budget:\n            break\n\n        # With small probability, try a tiny multidimensional move\n        if rng.rand() < 0.20:\n            direction = rng.normal(size=dim)\n            norm = np.linalg.norm(direction)\n            if norm > 1e-12:\n                direction /= norm\n            scale = 0.5 ** (1 + i / max(1, refine_steps))\n            step = step_vec * scale\n            x_candidate = clip_to_bounds(current_x + direction * step)\n        else:\n            idx = rng.randint(dim)\n            direction = np.zeros(dim)\n            direction[idx] = 1.0 if rng.rand() < 0.5 else -1.0\n            x_candidate = clip_to_bounds(current_x + direction * step_vec)\n\n        k = key_from_x(x_candidate)\n        y_candidate = cache.get(k, None)\n        if y_candidate is None:\n            y_candidate = float(objective_function(x_candidate))\n            evals_used += 1\n            cache[k] = y_candidate\n\n        if y_candidate < current_y:\n            current_x, current_y = x_candidate, y_candidate\n            if y_candidate < y_best:\n                x_best, y_best = x_candidate, y_candidate\n            # Gentle expansion, but still bounded\n            step_vec = np.minimum(step_vec * 1.2, span)\n        else:\n            step_vec = np.maximum(step_vec * 0.60, min_step)\n\n    return x_best",
    "X": "-0.924884346928192 1.2819790929326114 -1.9344429023269025 0.06240821923947348 -0.9403912696370086 -0.0734517255085611 0.00854839710212648 -1.0044274065249197 -1.008802187387002 -1.0220610780024202 0.9938835314917017"
}