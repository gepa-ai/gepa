{
    "score": 2.105973435243634,
    "Input": "Easom",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with hybrid global + local search.\n\n    Strategy:\n    1. Robust space-filling global exploration (low-discrepancy + random jitter).\n       - Incorporate prev_best_x if provided.\n       - More emphasis on global search for multimodal functions.\n    2. Local evolutionary refinement around current best.\n       - Population-based Gaussian search.\n       - Success-based step-size adaptation.\n       - Always honor total evaluation budget.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Fallback if no budget: return center of bounds\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    # Internal evaluation accounting\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    def eval_candidate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        y = objective_function(x)\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # Halton-like low-discrepancy sequence\n    def _van_der_corput(n, base):\n        vdc, denom = 0.0, 1.0\n        while n:\n            n, remainder = divmod(n, base)\n            denom *= base\n            vdc += remainder / denom\n        return vdc\n\n    def halton_points(num, d, seed=0):\n        # Small prime bases\n        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n                  31, 37, 41, 43, 47, 53, 59, 61, 67, 71]\n        if d > len(primes):\n            # Fallback: uniform\n            return rng.random((num, d))\n        pts = np.empty((num, d), dtype=float)\n        start = seed + 1\n        for j in range(d):\n            base = primes[j]\n            for i in range(num):\n                pts[i, j] = _van_der_corput(start + i, base)\n        return pts\n\n    # -------- 1) Global exploration --------\n    # Easom-like problems are extremely non-convex; emphasize global search.\n    if budget <= 5:\n        global_evals = budget\n    else:\n        # Allocate 60% of budget (clamped) to global, leaving at least a small local phase\n        global_evals = max(4, min(budget - 3, int(0.6 * budget)))\n\n    # Evaluate warm start first if available\n    if prev_best_x is not None:\n        try:\n            x0 = np.array(prev_best_x, dtype=float).reshape(dim)\n            x0 = clip_to_bounds(x0)\n            eval_candidate(x0)\n        except Exception:\n            # Ignore malformed prev_best_x\n            pass\n\n    remaining_global = max(0, global_evals - evals_used)\n\n    if remaining_global > 0 and evals_used < budget:\n        # Use Halton points mapped to bounds with small Gaussian jitter to avoid lattice effects\n        halton_seed = int(rng.integers(0, 10_000_000))\n        base_samples = halton_points(remaining_global, dim, seed=halton_seed)\n\n        # Add mild jitter that shrinks in higher dimensions\n        jitter_scale = 0.03 / np.sqrt(max(1, dim))\n        jitter = rng.normal(0.0, jitter_scale, size=base_samples.shape)\n        samples = np.clip(base_samples + jitter, 0.0, 1.0)\n\n        xs = low + samples * span\n        for i in range(remaining_global):\n            if evals_used >= global_evals or evals_used >= budget:\n                break\n            eval_candidate(xs[i])\n\n    # Pathological case: still nothing evaluated\n    if best_x is None:\n        x_rand = rng.uniform(low, high)\n        eval_candidate(x_rand)\n\n    if evals_used >= budget or best_x is None:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # -------- 2) Local evolutionary refinement --------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    center = best_x.copy()\n\n    # Population size: small but proportional to dimension, ensure at least ~3 generations\n    max_pop = max(4, 4 * dim)\n    pop_size = min(max_pop, max(4, remaining // 3))\n\n    # Initial step size: ~1/4 of box width, with safeguard for zero span dims\n    base_sigma = span / 4.0\n    base_sigma[base_sigma == 0.0] = 1.0\n    sigma = base_sigma.copy()\n\n    # Success-based step-size adaptation (1/5th success rule\u2013like)\n    target_success_rate = 0.2\n    adapt_factor_inc = 1.5\n    adapt_factor_dec = 0.7\n\n    # Limit for sigma relative scale\n    sigma_min = base_sigma * 1e-4\n    sigma_max = base_sigma * 10.0\n\n    while evals_used < budget:\n        n_to_sample = min(pop_size, budget - evals_used)\n        if n_to_sample <= 0:\n            break\n\n        new_bests = 0\n\n        # First candidate: exploitative small perturbation of current best\n        x0 = clip_to_bounds(center + rng.normal(0.0, sigma / 5.0))\n        y_before = best_y\n        eval_candidate(x0)\n        if best_y is not None and (y_before is None or best_y < y_before):\n            new_bests += 1\n\n        # Rest of the population: mixture of exploration and exploitation\n        for _ in range(n_to_sample - 1):\n            if evals_used >= budget:\n                break\n\n            if rng.random() < 0.7:\n                # Exploit: local Gaussian\n                perturb = rng.normal(0.0, sigma)\n                x = center + perturb\n            else:\n                # Occasional broader move: use scaled uniform within bounds\n                step = (rng.random(dim) - 0.5) * 2.0\n                x = center + step * sigma * 3.0\n\n            x = clip_to_bounds(x)\n            y_before = best_y\n            eval_candidate(x)\n            if best_y is not None and (y_before is None or best_y < y_before):\n                new_bests += 1\n\n        if best_x is not None:\n            center = best_x.copy()\n\n        # Adapt sigma based on success ratio\n        success_rate = new_bests / max(1, n_to_sample)\n        if success_rate > target_success_rate * 1.1:\n            sigma *= adapt_factor_inc\n        elif success_rate < target_success_rate * 0.5:\n            sigma *= adapt_factor_dec\n\n        sigma = np.clip(sigma, sigma_min, sigma_max)\n\n        # If very little budget remains, break and fall through to greedy steps\n        if budget - evals_used < 3:\n            break\n\n    # Greedy local refinement with remaining evaluations\n    while evals_used < budget:\n        # Shrink steps near the end\n        frac_left = (budget - evals_used) / max(1, budget)\n        step_scale = max(0.1, frac_left)\n        step = rng.normal(0.0, sigma * step_scale)\n        x = clip_to_bounds(center + step)\n        eval_candidate(x)\n        if best_x is not None:\n            center = best_x.copy()\n\n    return best_x if best_x is not None else (low + high) / 2.0",
    "X": "-0.13948320342210324 0.34294219434785 -0.10119959716926666 0.16094292178380581"
}