{
    "score": 0.0056507630967996825,
    "Input": "Easom",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with hybrid global + local search.\n    This version slightly simplifies and stabilizes the previous variant,\n    keeps the global/local structure, and adds a few robustness tweaks:\n    - Ensures full or near-full budget usage in all branches\n    - More robust handling of degenerate bounds and prev_best_x shape\n    - Slightly stronger global coverage and final needle refinement\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Degenerate bounds: return the only feasible point\n    if np.all(span == 0) or budget <= 0:\n        return (low + high) / 2.0\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    def eval_candidate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = np.asarray(x, dtype=float).reshape(dim)\n        x = clip_to_bounds(x)\n        y = objective_function(x)\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # Halton helpers\n    def _van_der_corput(n, base):\n        vdc, denom = 0.0, 1.0\n        while n:\n            n, remainder = divmod(n, base)\n            denom *= base\n            vdc += remainder / denom\n        return vdc\n\n    def halton_points(num, d, seed=0):\n        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n                  31, 37, 41, 43, 47, 53, 59, 61, 67, 71]\n        if num <= 0:\n            return np.empty((0, d), dtype=float)\n        if d > len(primes):\n            return rng.random((num, d))\n        pts = np.empty((num, d), dtype=float)\n        start = seed + 1\n        for j in range(d):\n            base = primes[j]\n            for i in range(num):\n                pts[i, j] = _van_der_corput(start + i, base)\n        return pts\n\n    # -------- 0) Very small budgets: simple strategy --------\n    if budget <= 5:\n        # Warm start\n        if prev_best_x is not None and evals_used < budget:\n            try:\n                x0 = np.array(prev_best_x, dtype=float).reshape(dim)\n                eval_candidate(x0)\n            except Exception:\n                pass\n\n        # Center\n        if evals_used < budget:\n            center = (low + high) / 2.0\n            eval_candidate(center)\n\n        # Spread samples (Halton + random) to cover space\n        remaining = budget - evals_used\n        if remaining > 0:\n            n_halton = remaining // 2\n            n_rand = remaining - n_halton\n\n            if n_halton > 0:\n                hs = halton_points(n_halton, dim, seed=int(rng.integers(1, 10_000_000)))\n                xs = low + hs * span\n                for x in xs:\n                    if evals_used >= budget:\n                        break\n                    eval_candidate(x)\n\n            if n_rand > 0 and evals_used < budget:\n                xs = rng.uniform(low, high, size=(n_rand, dim))\n                for x in xs:\n                    if evals_used >= budget:\n                        break\n                    eval_candidate(x)\n\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # -------- 1) Global exploration --------\n    # Slight bias to more global work on modest budgets\n    if budget <= 12:\n        global_evals = max(4, int(0.5 * budget))\n    elif budget <= 40:\n        global_evals = max(8, min(budget - 6, int(0.6 * budget)))\n    else:\n        global_evals = max(12, min(budget - 12, int(0.65 * budget)))\n\n    global_evals = min(global_evals, budget)\n\n    # 1a) Warm start and small cloud around it\n    if prev_best_x is not None and evals_used < global_evals:\n        try:\n            x0 = np.array(prev_best_x, dtype=float).reshape(dim)\n            eval_candidate(x0)\n\n            if evals_used < global_evals:\n                warm_span = np.where(span > 0, span, 1.0)\n                rel = 0.01 if budget <= 30 else 0.03\n                radius = np.maximum(rel * warm_span, 1e-4)\n\n                n_cloud = min(4, global_evals - evals_used)\n                for _ in range(n_cloud):\n                    if evals_used >= global_evals or evals_used >= budget:\n                        break\n                    z = rng.normal(0.0, 0.5, size=dim)\n                    x = x0 + z * radius\n                    eval_candidate(x)\n        except Exception:\n            pass\n\n    # 1b) Center point\n    if evals_used < global_evals:\n        center_point = (low + high) / 2.0\n        eval_candidate(center_point)\n\n    # 1c) Central Gaussian search around domain center\n    if evals_used < global_evals and budget >= 6:\n        center = (low + high) / 2.0\n\n        if dim <= 3:\n            if budget <= 20:\n                base_rel_radius = 0.003\n            elif budget <= 50:\n                base_rel_radius = 0.006\n            else:\n                base_rel_radius = 0.008\n        else:\n            base_rel_radius = 0.02\n\n        radius = np.maximum(base_rel_radius * span, 1e-3)\n\n        max_center_samples = 10 if dim <= 5 else 8\n        n_center_samples = min(\n            max_center_samples,\n            max(0, global_evals - evals_used)\n        )\n\n        for i in range(n_center_samples):\n            if evals_used >= global_evals or evals_used >= budget:\n                break\n            if i < n_center_samples // 2:\n                z = rng.normal(0.0, 0.2, size=dim)\n            else:\n                z = rng.normal(0.0, 0.4, size=dim)\n            x = center + z * radius\n            eval_candidate(x)\n\n    # 1d) Remaining global samples: Halton + biased random\n    remaining_global = max(0, global_evals - evals_used)\n    if remaining_global > 0 and evals_used < budget:\n        n_halton = int(0.6 * remaining_global)\n        n_rand = remaining_global - n_halton\n\n        xs_list = []\n\n        if n_halton > 0:\n            halton_seed = int(rng.integers(0, 10_000_000))\n            base_samples = halton_points(n_halton, dim, seed=halton_seed)\n            jitter_scale = 0.02 / np.sqrt(max(1, dim))\n            jitter = rng.normal(0.0, jitter_scale, size=base_samples.shape)\n            samples = np.clip(base_samples + jitter, 0.0, 1.0)\n            xs_list.append(low + samples * span)\n\n        if n_rand > 0:\n            # Slight edge bias to catch boundary minima\n            beta_samples = rng.beta(1.7, 1.7, size=(n_rand, dim))\n            xs_list.append(low + beta_samples * span)\n\n        if xs_list:\n            xs = np.vstack(xs_list)\n            rng.shuffle(xs, axis=0)\n            for i in range(xs.shape[0]):\n                if evals_used >= global_evals or evals_used >= budget:\n                    break\n                eval_candidate(xs[i])\n\n    # Fallback if nothing evaluated\n    if best_x is None:\n        x_rand = rng.uniform(low, high)\n        eval_candidate(x_rand)\n\n    if evals_used >= budget or best_x is None:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # -------- 2) Local evolutionary refinement --------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    center = best_x.copy()\n\n    max_pop = max(8, 4 * dim)\n    pop_size = min(max_pop, max(4, remaining // 5))\n\n    base_sigma = span / 6.0\n    base_sigma[base_sigma == 0.0] = 1.0\n    sigma = base_sigma.copy()\n\n    target_success_rate = 0.25\n    adapt_factor_inc = 1.6\n    adapt_factor_dec = 0.6\n\n    sigma_min = base_sigma * 1e-8\n    sigma_max = base_sigma * 10.0\n\n    archive_size = 5\n    archive_x = [best_x.copy()]\n    archive_y = [best_y]\n\n    def update_archive(x, y):\n        if len(archive_x) < archive_size:\n            archive_x.append(x.copy())\n            archive_y.append(y)\n        else:\n            worst_idx = int(np.argmax(archive_y))\n            if y < archive_y[worst_idx]:\n                archive_x[worst_idx] = x.copy()\n                archive_y[worst_idx] = y\n\n    update_archive(best_x, best_y)\n\n    stagnant_generations = 0\n    max_stagnant_generations = 5\n\n    # Reserve a small fraction (at least 3 evals) for final needle refinement\n    def reserved_for_final(total_budget):\n        return max(3, int(0.05 * total_budget))\n\n    final_reserve = reserved_for_final(budget)\n\n    while evals_used < budget - final_reserve:\n        remaining_local = budget - final_reserve - evals_used\n        if remaining_local <= 0:\n            break\n\n        n_to_sample = min(pop_size, remaining_local)\n        if n_to_sample <= 0:\n            break\n\n        new_bests = 0\n        y_before_gen = best_y\n\n        # Exploit around current center (small step)\n        x0 = center + rng.normal(0.0, sigma / 10.0, size=dim)\n        y_before = best_y\n        eval_candidate(x0)\n        if best_y is not None and (y_before is None or best_y < y_before):\n            new_bests += 1\n            update_archive(best_x, best_y)\n\n        # Population samples\n        for _ in range(n_to_sample - 1):\n            if evals_used >= budget - final_reserve:\n                break\n\n            r = rng.random()\n            if r < 0.6:\n                perturb = rng.normal(0.0, sigma, size=dim)\n                x = center + perturb\n            elif r < 0.88 and len(archive_x) > 0:\n                idx = int(rng.integers(0, len(archive_x)))\n                base = archive_x[idx]\n                scale = rng.uniform(0.15, 0.9)\n                perturb = rng.normal(0.0, sigma * scale, size=dim)\n                x = base + perturb\n            else:\n                step = (rng.random(dim) - 0.5) * 2.0\n                x = center + step * sigma * 3.0\n\n            y_before = best_y\n            eval_candidate(x)\n            if best_y is not None and (y_before is None or best_y < y_before):\n                new_bests += 1\n                update_archive(best_x, best_y)\n\n        if best_x is not None:\n            center = best_x.copy()\n\n        success_rate = new_bests / max(1, n_to_sample)\n        if success_rate > target_success_rate * 1.1:\n            sigma *= adapt_factor_inc\n        elif success_rate < target_success_rate * 0.5:\n            sigma *= adapt_factor_dec\n\n        sigma = np.clip(sigma, sigma_min, sigma_max)\n\n        if best_y is not None and (y_before_gen is not None) and best_y < y_before_gen:\n            stagnant_generations = 0\n        else:\n            stagnant_generations += 1\n\n        if stagnant_generations >= max_stagnant_generations and len(archive_x) > 0:\n            best_idx = int(np.argmin(archive_y))\n            center = archive_x[best_idx].copy()\n            sigma = np.maximum(sigma * 0.4, sigma_min)\n            stagnant_generations = 0\n\n        if budget - evals_used <= final_reserve:\n            break\n\n    # -------- 3) Final greedy needle-in-haystack refinement --------\n    if best_x is not None:\n        center = best_x.copy()\n    else:\n        center = (low + high) / 2.0\n\n    # Use a steadily shrinking step size to zoom in on sharp minima\n    while evals_used < budget:\n        remaining_frac = (budget - evals_used) / max(1, budget)\n        # Shrink fast but keep a tiny floor for very sharp functions\n        step_scale = max(1e-5, remaining_frac ** 2)\n        # Use current sigma as base scale; if sigma went too small, fall back to base_sigma\n        eff_sigma = np.where(sigma > sigma_min, sigma, base_sigma)\n        step = rng.normal(0.0, eff_sigma * step_scale, size=dim)\n        x = center + step\n        y_before = best_y\n        eval_candidate(x)\n        if best_x is not None and (y_before is None or best_y < y_before):\n            center = best_x.copy()\n\n    return best_x if best_x is not None else (low + high) / 2.0",
    "X": "-0.0022517976505890542 -0.0013939207315339908 -0.000826016372782142 1.3137596974321046e-06"
}