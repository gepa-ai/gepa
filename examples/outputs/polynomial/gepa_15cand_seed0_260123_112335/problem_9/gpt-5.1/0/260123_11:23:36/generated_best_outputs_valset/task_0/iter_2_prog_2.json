{
    "score": 3.1326455956112764,
    "Input": "Easom",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with hybrid global + local search.\n\n    Strategy (CMA-ES\u2013inspired, but simplified and budget-aware):\n    1. Use part of the budget for space-filling global exploration.\n       - Quasi-random Sobol-like sampling (via scrambled Halton-style fallback).\n       - Incorporate prev_best_x as a seed sample if provided.\n    2. Use the remaining budget for an adaptive Gaussian evolutionary search\n       around the current best:\n       - Population-based sampling per generation.\n       - Step-size (sigma) adapted based on success rate.\n       - Always honor total evaluation budget.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.array(config['bounds'], dtype=float)\n    dim = int(config['dim'])\n    budget = int(config['budget'])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Fallback if no budget: return center of bounds\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    # Internal evaluation accounting\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    def eval_candidate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        y = objective_function(x)\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # Halton-like low-discrepancy sequence (scrambled base-2,3,5,...)\n    def _van_der_corput(n, base):\n        vdc, denom = 0.0, 1.0\n        while n:\n            n, remainder = divmod(n, base)\n            denom *= base\n            vdc += remainder / denom\n        return vdc\n\n    def halton_points(num, dim, seed=0):\n        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n        if dim > len(primes):\n            # Fallback to uniform if dimension too large\n            return rng.random((num, dim))\n        pts = np.empty((num, dim), dtype=float)\n        start = seed + 1\n        for d in range(dim):\n            base = primes[d]\n            for i in range(num):\n                pts[i, d] = _van_der_corput(start + i, base)\n        return pts\n\n    # 1) Global exploration: allocate 40% of budget (min 2, max budget-1)\n    if budget <= 3:\n        global_evals = budget  # very small budgets: no real separation\n    else:\n        global_evals = max(2, min(budget - 1, int(0.4 * budget)))\n\n    # Evaluate warm start first if available\n    if prev_best_x is not None:\n        x0 = np.array(prev_best_x, dtype=float).reshape(dim)\n        x0 = clip_to_bounds(x0)\n        eval_candidate(x0)\n\n    # Global sampling using low-discrepancy sequence mapped to bounds\n    remaining_global = max(0, global_evals - evals_used)\n    if remaining_global > 0 and evals_used < budget:\n        # Include current best (if any) plus halton points\n        halton_seed = rng.integers(0, 10_000)\n        samples = halton_points(remaining_global, dim, seed=halton_seed)\n        xs = low + samples * span\n        for i in range(remaining_global):\n            if evals_used >= global_evals or evals_used >= budget:\n                break\n            eval_candidate(xs[i])\n\n    # If nothing evaluated yet (pathological), sample one random point\n    if best_x is None:\n        x_rand = rng.uniform(low, high)\n        eval_candidate(x_rand)\n\n    # If no budget left, return best found\n    if evals_used >= budget or best_x is None:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # 2) Local evolutionary refinement around best_x\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    center = best_x.copy()\n\n    # Population size: scale mildly with dimension but cap for small budgets\n    # Aim for at least 2 generations\n    max_pop = max(4, 2 * dim)\n    pop_size = min(max_pop, max(3, remaining // 2))\n\n    # Initial step size: 1/3 of box width, with safeguard\n    base_sigma = span / 3.0\n    base_sigma[base_sigma == 0.0] = 1.0\n    sigma = base_sigma.copy()\n\n    # Success-based step-size adaptation\n    target_success_rate = 0.3\n    adapt_factor_inc = 1.2\n    adapt_factor_dec = 0.8\n\n    # We adjust sigma based on per-generation success\n    while evals_used < budget:\n        # Generate population around current center\n        # Always include current best to preserve elitism\n        n_to_sample = min(pop_size, budget - evals_used)\n\n        new_bests = 0\n        # First candidate: small perturbation of current best\n        x0 = clip_to_bounds(center + rng.normal(0.0, sigma / 4.0))\n        y0_before = best_y\n        eval_candidate(x0)\n        if best_y is not None and (y0_before is None or best_y < y0_before):\n            new_bests += 1\n\n        # Remaining individuals\n        for _ in range(n_to_sample - 1):\n            if evals_used >= budget:\n                break\n            perturb = rng.normal(0.0, sigma)\n            x = clip_to_bounds(center + perturb)\n            y_before = best_y\n            eval_candidate(x)\n            if best_y is not None and (y_before is None or best_y < y_before):\n                new_bests += 1\n\n        # Update center\n        if best_x is not None:\n            center = best_x.copy()\n\n        # Adapt sigma based on success ratio in this \"generation\"\n        if n_to_sample > 0:\n            success_rate = new_bests / n_to_sample\n            if success_rate > target_success_rate:\n                sigma *= adapt_factor_inc\n            elif success_rate < target_success_rate / 2:\n                sigma *= adapt_factor_dec\n\n        # Prevent sigma from collapsing or exploding\n        sigma = np.clip(sigma, base_sigma * 1e-3, base_sigma * 10.0)\n\n        # If very little budget remains, break and let final loop handle it\n        if budget - evals_used < 2:\n            break\n\n    # If evaluations remain (due to small pop_size rounding), do greedy local steps\n    while evals_used < budget:\n        step = rng.normal(0.0, sigma / 2.0)\n        x = clip_to_bounds(center + step)\n        eval_candidate(x)\n        if best_x is not None:\n            center = best_x.copy()\n\n    return best_x if best_x is not None else (low + high) / 2.0",
    "X": "0.952486309379405 0.9797103358120517 0.9688861976897003 0.00013852347745628717"
}