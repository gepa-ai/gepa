{
    "score": 0.036935393617743006,
    "Input": "Easom",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with hybrid global + local search.\n\n    Strategy:\n    1. Strong global exploration using a mix of low-discrepancy (Halton),\n       random, and warm-start seeding (if prev_best_x is available).\n    2. Dedicated central-region search very early (for Easom-like functions).\n    3. Local evolutionary refinement around the best found point with\n       success-based step-size control and adaptive population size.\n    4. Simple restart logic if local search appears stagnant.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    # Internal evaluation accounting\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    def eval_candidate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = clip_to_bounds(x)\n        y = objective_function(x)\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # Halton-like low-discrepancy sequence\n    def _van_der_corput(n, base):\n        vdc, denom = 0.0, 1.0\n        while n:\n            n, remainder = divmod(n, base)\n            denom *= base\n            vdc += remainder / denom\n        return vdc\n\n    def halton_points(num, d, seed=0):\n        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n                  31, 37, 41, 43, 47, 53, 59, 61, 67, 71]\n        if d > len(primes):\n            return rng.random((num, d))\n        pts = np.empty((num, d), dtype=float)\n        start = seed + 1\n        for j in range(d):\n            base = primes[j]\n            for i in range(num):\n                pts[i, j] = _van_der_corput(start + i, base)\n        return pts\n\n    # -------- 1) Global exploration --------\n    # For highly multimodal landscapes, strong global emphasis helps.\n    # Allocate ~65% of budget for global exploration, but keep at least 8 evals\n    # for local refinement when budget is moderate/large.\n    if budget <= 10:\n        global_evals = budget\n    else:\n        global_evals = max(6, min(budget - 8, int(0.65 * budget)))\n\n    # 1a) Evaluate warm start first if available\n    if prev_best_x is not None and evals_used < budget:\n        try:\n            x0 = np.array(prev_best_x, dtype=float).reshape(dim)\n            eval_candidate(x0)\n        except Exception:\n            pass\n\n    # 1b) Deterministic mid-point as a robust baseline if budget allows\n    if evals_used < budget and budget >= 3:\n        center_point = (low + high) / 2.0\n        eval_candidate(center_point)\n\n    # 1c) Dedicated central search: a small cloud around the center\n    # This is particularly helpful for Easom-like functions where the\n    # global optimum lies exactly at the center of the domain.\n    if evals_used < budget and budget >= 10:\n        # Use a tight radius around the center in absolute coordinates\n        center = (low + high) / 2.0\n        # Use an absolute radius that is 1% of span, but at least 1e-2\n        radius = np.maximum(0.01 * span, 1e-2)\n        n_center_samples = min(6, global_evals - evals_used)\n        for _ in range(n_center_samples):\n            if evals_used >= budget:\n                break\n            z = rng.normal(0.0, 0.3, size=dim)  # 0.3*radius ~ tighter than before\n            x = center + z * radius\n            eval_candidate(x)\n\n    # 1d) Remaining global samples\n    remaining_global = max(0, global_evals - evals_used)\n    if remaining_global > 0 and evals_used < budget:\n        n_halton = int(0.6 * remaining_global)\n        n_rand = remaining_global - n_halton\n\n        xs_list = []\n\n        if n_halton > 0:\n            halton_seed = int(rng.integers(0, 10_000_000))\n            base_samples = halton_points(n_halton, dim, seed=halton_seed)\n            jitter_scale = 0.02 / np.sqrt(max(1, dim))\n            jitter = rng.normal(0.0, jitter_scale, size=base_samples.shape)\n            samples = np.clip(base_samples + jitter, 0.0, 1.0)\n            xs_list.append(low + samples * span)\n\n        if n_rand > 0:\n            # Beta(2,2) to bias toward center but still cover whole domain\n            beta_samples = rng.beta(2.0, 2.0, size=(n_rand, dim))\n            xs_list.append(low + beta_samples * span)\n\n        if xs_list:\n            xs = np.vstack(xs_list)\n            rng.shuffle(xs, axis=0)\n            for i in range(xs.shape[0]):\n                if evals_used >= global_evals or evals_used >= budget:\n                    break\n                eval_candidate(xs[i])\n\n    # If still nothing evaluated (pathological), do one random sample\n    if best_x is None:\n        x_rand = rng.uniform(low, high)\n        eval_candidate(x_rand)\n\n    if evals_used >= budget or best_x is None:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # -------- 2) Local evolutionary refinement --------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    center = best_x.copy()\n\n    # Population size: proportional to dimension, but keep at least ~6 generations\n    max_pop = max(8, 4 * dim)\n    pop_size = min(max_pop, max(4, remaining // 6))\n\n    # Initial step size: slightly more conservative in rugged landscapes.\n    base_sigma = span / 6.0\n    base_sigma[base_sigma == 0.0] = 1.0\n    sigma = base_sigma.copy()\n\n    target_success_rate = 0.25\n    adapt_factor_inc = 1.5\n    adapt_factor_dec = 0.65\n\n    sigma_min = base_sigma * 1e-8\n    sigma_max = base_sigma * 10.0\n\n    # Small archive of best points for restarts\n    archive_size = 5\n    archive_x = [best_x.copy()]\n    archive_y = [best_y]\n\n    def update_archive(x, y):\n        if len(archive_x) < archive_size:\n            archive_x.append(x.copy())\n            archive_y.append(y)\n        else:\n            worst_idx = int(np.argmax(archive_y))\n            if y < archive_y[worst_idx]:\n                archive_x[worst_idx] = x.copy()\n                archive_y[worst_idx] = y\n\n    update_archive(best_x, best_y)\n\n    stagnant_generations = 0\n    max_stagnant_generations = 6\n\n    while evals_used < budget:\n        remaining_local = budget - evals_used\n        if remaining_local <= 0:\n            break\n\n        n_to_sample = min(pop_size, remaining_local)\n        if n_to_sample <= 0:\n            break\n\n        new_bests = 0\n        y_before_gen = best_y\n\n        # Ensure exploitation via a small-step probe around center\n        x0 = center + rng.normal(0.0, sigma / 8.0, size=dim)\n        y_before = best_y\n        eval_candidate(x0)\n        if best_y is not None and (y_before is None or best_y < y_before):\n            new_bests += 1\n            update_archive(best_x, best_y)\n\n        # Remaining candidates: mixture of local, archive-based, and broader moves\n        for _ in range(n_to_sample - 1):\n            if evals_used >= budget:\n                break\n\n            r = rng.random()\n            if r < 0.6:\n                # Local Gaussian exploration around center\n                perturb = rng.normal(0.0, sigma, size=dim)\n                x = center + perturb\n            elif r < 0.85 and len(archive_x) > 0:\n                # Restart from one of the archive elites with moderate noise\n                idx = int(rng.integers(0, len(archive_x)))\n                base = archive_x[idx]\n                scale = rng.uniform(0.2, 1.0)\n                perturb = rng.normal(0.0, sigma * scale, size=dim)\n                x = base + perturb\n            else:\n                # Occasional broader uniform move in a sigma-scaled box\n                step = (rng.random(dim) - 0.5) * 2.0\n                x = center + step * sigma * 3.0\n\n            y_before = best_y\n            eval_candidate(x)\n            if best_y is not None and (y_before is None or best_y < y_before):\n                new_bests += 1\n                update_archive(best_x, best_y)\n\n        if best_x is not None:\n            center = best_x.copy()\n\n        # Adapt sigma based on success ratio within this generation\n        success_rate = new_bests / max(1, n_to_sample)\n        if success_rate > target_success_rate * 1.1:\n            sigma *= adapt_factor_inc\n        elif success_rate < target_success_rate * 0.5:\n            sigma *= adapt_factor_dec\n\n        sigma = np.clip(sigma, sigma_min, sigma_max)\n\n        # Stagnation detection and restart\n        if best_y is not None and (y_before_gen is not None) and best_y < y_before_gen:\n            stagnant_generations = 0\n        else:\n            stagnant_generations += 1\n\n        if stagnant_generations >= max_stagnant_generations and len(archive_x) > 0:\n            # Restart near the best archive point but with reduced sigma\n            best_idx = int(np.argmin(archive_y))\n            center = archive_x[best_idx].copy()\n            sigma = np.maximum(sigma * 0.5, sigma_min)\n            stagnant_generations = 0\n\n        # If very little budget remains, break and fall through to greedy steps\n        if budget - evals_used < 5:\n            break\n\n    # -------- 3) Greedy local refinement with remaining evaluations --------\n    while evals_used < budget:\n        frac_left = (budget - evals_used) / max(1, budget)\n        step_scale = max(0.02, frac_left ** 2)\n        step = rng.normal(0.0, sigma * step_scale, size=dim)\n        x = center + step\n        y_before = best_y\n        eval_candidate(x)\n        if best_x is not None and (y_before is None or best_y < y_before):\n            center = best_x.copy()\n\n    return best_x if best_x is not None else (low + high) / 2.0",
    "X": "-0.012139365593537843 -0.010295922853188678 0.004811232325548621 3.6568241747660796e-05"
}