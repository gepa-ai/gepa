{
    "score": 0.009474087643323248,
    "Input": "Easom",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with hybrid global + local search.\n\n    This version:\n    - Keeps the global/local hybrid structure\n    - Strengthens handling of very small budgets\n    - Introduces a dedicated \"needle-in-a-haystack\" mode for extremely sharp optima\n      (e.g., Easom) by biasing search around best-so-far with shrinking radius\n    - Uses prev_best_x more aggressively as a warm start\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    def eval_candidate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        x = clip_to_bounds(x)\n        y = objective_function(x)\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # Halton / low discrepancy helpers\n    def _van_der_corput(n, base):\n        vdc, denom = 0.0, 1.0\n        while n:\n            n, remainder = divmod(n, base)\n            denom *= base\n            vdc += remainder / denom\n        return vdc\n\n    def halton_points(num, d, seed=0):\n        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n                  31, 37, 41, 43, 47, 53, 59, 61, 67, 71]\n        if d > len(primes):\n            return rng.random((num, d))\n        pts = np.empty((num, d), dtype=float)\n        start = seed + 1\n        for j in range(d):\n            base = primes[j]\n            for i in range(num):\n                pts[i, j] = _van_der_corput(start + i, base)\n        return pts\n\n    # -------- 0) Very small budgets: simplified strategy --------\n    # For extremely low budgets, avoid complex machinery and just\n    # do a few well-spaced samples plus warm start.\n    if budget <= 5:\n        # 0a) Warm start\n        if prev_best_x is not None:\n            try:\n                x0 = np.array(prev_best_x, dtype=float).reshape(dim)\n                eval_candidate(x0)\n            except Exception:\n                pass\n\n        # 0b) Center\n        if evals_used < budget:\n            center = (low + high) / 2.0\n            eval_candidate(center)\n\n        # 0c) Spread samples (Halton + random)\n        remaining = budget - evals_used\n        if remaining > 0:\n            n_halton = remaining // 2\n            n_rand = remaining - n_halton\n\n            if n_halton > 0:\n                hs = halton_points(n_halton, dim, seed=int(rng.integers(1, 10_000_000)))\n                xs = low + hs * span\n                for x in xs:\n                    if evals_used >= budget:\n                        break\n                    eval_candidate(x)\n\n            if evals_used < budget and n_rand > 0:\n                xs = rng.uniform(low, high, size=(n_rand, dim))\n                for x in xs:\n                    if evals_used >= budget:\n                        break\n                    eval_candidate(x)\n\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # -------- 1) Global exploration --------\n    # Adjust split: a bit more global for tough needle-like problems.\n    if budget <= 12:\n        global_evals = max(4, int(0.5 * budget))\n    elif budget <= 40:\n        global_evals = max(8, min(budget - 8, int(0.6 * budget)))\n    else:\n        global_evals = max(12, min(budget - 15, int(0.65 * budget)))\n\n    global_evals = min(global_evals, budget)\n\n    # 1a) Warm start: evaluate prev_best_x first and also create a small cloud around it\n    if prev_best_x is not None and evals_used < budget:\n        try:\n            x0 = np.array(prev_best_x, dtype=float).reshape(dim)\n            eval_candidate(x0)\n\n            # Small refinement cloud around warm start (for needle-like minima continuation)\n            if evals_used < global_evals:\n                warm_span = np.where(span > 0, span, 1.0)\n                # Tight radius relative to span; smaller if budget is small\n                if budget <= 30:\n                    rel = 0.01\n                else:\n                    rel = 0.03\n                radius = rel * warm_span\n                radius = np.maximum(radius, 1e-4)\n\n                n_cloud = min(4, global_evals - evals_used)\n                for _ in range(n_cloud):\n                    if evals_used >= global_evals or evals_used >= budget:\n                        break\n                    z = rng.normal(0.0, 0.5, size=dim)\n                    x = x0 + z * radius\n                    eval_candidate(x)\n        except Exception:\n            pass\n\n    # 1b) Center point\n    if evals_used < budget:\n        center_point = (low + high) / 2.0\n        eval_candidate(center_point)\n\n    # 1c) Dedicated central search (kept, but slightly more aggressive radius for higher dims)\n    if evals_used < budget and budget >= 6:\n        center = (low + high) / 2.0\n\n        base_rel_radius = 0.01\n        if dim <= 3:\n            if budget <= 20:\n                base_rel_radius = 0.003  # slightly larger than before to improve hit chance\n            elif budget <= 50:\n                base_rel_radius = 0.006\n        else:\n            # For higher dims, radius needs to be a bit larger\n            base_rel_radius = 0.02\n\n        radius = np.maximum(base_rel_radius * span, 1e-3)\n\n        max_center_samples = 10 if dim <= 5 else 8\n        n_center_samples = min(\n            max_center_samples,\n            max(0, global_evals - evals_used)\n        )\n\n        for i in range(n_center_samples):\n            if evals_used >= budget:\n                break\n            if i < n_center_samples // 2:\n                z = rng.normal(0.0, 0.2, size=dim)  # tight\n            else:\n                z = rng.normal(0.0, 0.4, size=dim)  # moderately tight\n            x = center + z * radius\n            eval_candidate(x)\n\n    # 1d) Remaining global samples: Halton + random\n    remaining_global = max(0, global_evals - evals_used)\n    if remaining_global > 0 and evals_used < budget:\n        n_halton = int(0.6 * remaining_global)\n        n_rand = remaining_global - n_halton\n\n        xs_list = []\n\n        if n_halton > 0:\n            halton_seed = int(rng.integers(0, 10_000_000))\n            base_samples = halton_points(n_halton, dim, seed=halton_seed)\n            jitter_scale = 0.02 / np.sqrt(max(1, dim))\n            jitter = rng.normal(0.0, jitter_scale, size=base_samples.shape)\n            samples = np.clip(base_samples + jitter, 0.0, 1.0)\n            xs_list.append(low + samples * span)\n\n        if n_rand > 0:\n            # Slightly heavier tails than Beta(2,2) to increase edge hits\n            beta_samples = rng.beta(1.7, 1.7, size=(n_rand, dim))\n            xs_list.append(low + beta_samples * span)\n\n        if xs_list:\n            xs = np.vstack(xs_list)\n            rng.shuffle(xs, axis=0)\n            for i in range(xs.shape[0]):\n                if evals_used >= global_evals or evals_used >= budget:\n                    break\n                eval_candidate(xs[i])\n\n    # Fallback random sample if nothing evaluated\n    if best_x is None:\n        x_rand = rng.uniform(low, high)\n        eval_candidate(x_rand)\n\n    if evals_used >= budget or best_x is None:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # -------- 2) Local evolutionary refinement --------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    center = best_x.copy()\n\n    max_pop = max(8, 4 * dim)\n    pop_size = min(max_pop, max(4, remaining // 5))\n\n    base_sigma = span / 6.0\n    base_sigma[base_sigma == 0.0] = 1.0\n    sigma = base_sigma.copy()\n\n    target_success_rate = 0.25\n    adapt_factor_inc = 1.6\n    adapt_factor_dec = 0.6\n\n    sigma_min = base_sigma * 1e-8\n    sigma_max = base_sigma * 10.0\n\n    archive_size = 5\n    archive_x = [best_x.copy()]\n    archive_y = [best_y]\n\n    def update_archive(x, y):\n        if len(archive_x) < archive_size:\n            archive_x.append(x.copy())\n            archive_y.append(y)\n        else:\n            worst_idx = int(np.argmax(archive_y))\n            if y < archive_y[worst_idx]:\n                archive_x[worst_idx] = x.copy()\n                archive_y[worst_idx] = y\n\n    update_archive(best_x, best_y)\n\n    stagnant_generations = 0\n    max_stagnant_generations = 5\n\n    while evals_used < budget:\n        remaining_local = budget - evals_used\n        if remaining_local <= 0:\n            break\n\n        n_to_sample = min(pop_size, remaining_local)\n        if n_to_sample <= 0:\n            break\n\n        new_bests = 0\n        y_before_gen = best_y\n\n        # Exploit around current center with a small step\n        x0 = center + rng.normal(0.0, sigma / 10.0, size=dim)\n        y_before = best_y\n        eval_candidate(x0)\n        if best_y is not None and (y_before is None or best_y < y_before):\n            new_bests += 1\n            update_archive(best_x, best_y)\n\n        # Remaining candidates\n        for _ in range(n_to_sample - 1):\n            if evals_used >= budget:\n                break\n\n            r = rng.random()\n            if r < 0.6:\n                # Main local perturbation\n                perturb = rng.normal(0.0, sigma, size=dim)\n                x = center + perturb\n            elif r < 0.88 and len(archive_x) > 0:\n                # Archive-based exploitation\n                idx = int(rng.integers(0, len(archive_x)))\n                base = archive_x[idx]\n                scale = rng.uniform(0.15, 0.9)\n                perturb = rng.normal(0.0, sigma * scale, size=dim)\n                x = base + perturb\n            else:\n                # Occasional bold step\n                step = (rng.random(dim) - 0.5) * 2.0\n                x = center + step * sigma * 3.0\n\n            y_before = best_y\n            eval_candidate(x)\n            if best_y is not None and (y_before is None or best_y < y_before):\n                new_bests += 1\n                update_archive(best_x, best_y)\n\n        if best_x is not None:\n            center = best_x.copy()\n\n        success_rate = new_bests / max(1, n_to_sample)\n        if success_rate > target_success_rate * 1.1:\n            sigma *= adapt_factor_inc\n        elif success_rate < target_success_rate * 0.5:\n            sigma *= adapt_factor_dec\n\n        sigma = np.clip(sigma, sigma_min, sigma_max)\n\n        if best_y is not None and (y_before_gen is not None) and best_y < y_before_gen:\n            stagnant_generations = 0\n        else:\n            stagnant_generations += 1\n\n        # \"Needle\" safeguard: if stagnant, shrink sigma and re-center at best archive point\n        if stagnant_generations >= max_stagnant_generations and len(archive_x) > 0:\n            best_idx = int(np.argmin(archive_y))\n            center = archive_x[best_idx].copy()\n            sigma = np.maximum(sigma * 0.4, sigma_min)\n            stagnant_generations = 0\n\n        if budget - evals_used < 4:\n            break\n\n    # -------- 3) Greedy local refinement / needle-in-haystack mode --------\n    # Final phase: aggressively shrink radius around best-so-far to hit very sharp minima.\n    while evals_used < budget:\n        frac_left = (budget - evals_used) / max(1, budget)\n        # Quadratic decay of step size; lower bound extremely small for very sharp functions.\n        step_scale = max(1e-4, frac_left ** 2)\n        step = rng.normal(0.0, sigma * step_scale, size=dim)\n        x = center + step\n        y_before = best_y\n        eval_candidate(x)\n        if best_x is not None and (y_before is None or best_y < y_before):\n            center = best_x.copy()\n\n    return best_x if best_x is not None else (low + high) / 2.0",
    "X": "-0.0036305476110316997 0.0012555127069535975 0.0001682857758902108 0.0025182315392501867"
}