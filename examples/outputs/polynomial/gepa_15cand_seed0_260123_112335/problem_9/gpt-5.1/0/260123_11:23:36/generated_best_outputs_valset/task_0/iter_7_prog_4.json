{
    "score": 0.21450551814268382,
    "Input": "Easom",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with hybrid global + local search.\n\n    Strategy:\n    1. Global exploration using a mix of low-discrepancy (Halton), random,\n       and warm-start seeding (if prev_best_x is available).\n    2. Local evolutionary refinement around the best found point with\n       success-based step-size control and adaptive population size.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    # Internal evaluation accounting\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    def eval_candidate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        y = objective_function(x)\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # Halton-like low-discrepancy sequence\n    def _van_der_corput(n, base):\n        vdc, denom = 0.0, 1.0\n        while n:\n            n, remainder = divmod(n, base)\n            denom *= base\n            vdc += remainder / denom\n        return vdc\n\n    def halton_points(num, d, seed=0):\n        # Small prime bases\n        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n                  31, 37, 41, 43, 47, 53, 59, 61, 67, 71]\n        if d > len(primes):\n            # Fallback: uniform\n            return rng.random((num, d))\n        pts = np.empty((num, d), dtype=float)\n        start = seed + 1\n        for j in range(d):\n            base = primes[j]\n            for i in range(num):\n                pts[i, j] = _van_der_corput(start + i, base)\n        return pts\n\n    # -------- 1) Global exploration --------\n    # Better balance: allocate around half for global, guarantee meaningful local phase.\n    if budget <= 5:\n        global_evals = budget\n    else:\n        global_evals = max(4, min(budget - 5, int(0.5 * budget)))\n\n    # 1a) Evaluate warm start first if available\n    if prev_best_x is not None:\n        try:\n            x0 = np.array(prev_best_x, dtype=float).reshape(dim)\n            x0 = clip_to_bounds(x0)\n            eval_candidate(x0)\n        except Exception:\n            pass\n\n    # 1b) Add deterministic mid-point as a robust baseline if budget allows\n    if evals_used < budget and budget >= 3:\n        center_point = (low + high) / 2.0\n        eval_candidate(center_point)\n\n    # 1c) Remaining global samples\n    remaining_global = max(0, global_evals - evals_used)\n    if remaining_global > 0 and evals_used < budget:\n        # Split between low-discrepancy and random to avoid structure artefacts\n        n_halton = remaining_global // 2\n        n_rand = remaining_global - n_halton\n\n        xs_list = []\n\n        if n_halton > 0:\n            halton_seed = int(rng.integers(0, 10_000_000))\n            base_samples = halton_points(n_halton, dim, seed=halton_seed)\n            jitter_scale = 0.05 / np.sqrt(max(1, dim))\n            jitter = rng.normal(0.0, jitter_scale, size=base_samples.shape)\n            samples = np.clip(base_samples + jitter, 0.0, 1.0)\n            xs_list.append(low + samples * span)\n\n        if n_rand > 0:\n            rand_samples = rng.random((n_rand, dim))\n            xs_list.append(low + rand_samples * span)\n\n        if xs_list:\n            xs = np.vstack(xs_list)\n            rng.shuffle(xs, axis=0)\n            for i in range(xs.shape[0]):\n                if evals_used >= global_evals or evals_used >= budget:\n                    break\n                eval_candidate(xs[i])\n\n    # If still nothing evaluated (pathological), do one random sample\n    if best_x is None:\n        x_rand = rng.uniform(low, high)\n        eval_candidate(x_rand)\n\n    if evals_used >= budget or best_x is None:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # -------- 2) Local evolutionary refinement --------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    center = best_x.copy()\n\n    # Population size: proportional to dimension, but keep at least ~5 generations\n    max_pop = max(6, 4 * dim)\n    pop_size = min(max_pop, max(4, remaining // 5))\n\n    # Initial step size: ~1/3 of box width, with safeguard for zero span dims\n    base_sigma = span / 3.0\n    base_sigma[base_sigma == 0.0] = 1.0\n    sigma = base_sigma.copy()\n\n    # Success-based step-size adaptation parameters\n    target_success_rate = 0.25\n    adapt_factor_inc = 1.6\n    adapt_factor_dec = 0.65\n\n    sigma_min = base_sigma * 1e-6\n    sigma_max = base_sigma * 10.0\n\n    # Keep a small archive of best points to occasionally restart from\n    archive_size = 4\n    archive_x = [best_x.copy()]\n    archive_y = [best_y]\n\n    def update_archive(x, y):\n        if len(archive_x) < archive_size:\n            archive_x.append(x.copy())\n            archive_y.append(y)\n        else:\n            worst_idx = int(np.argmax(archive_y))\n            if y < archive_y[worst_idx]:\n                archive_x[worst_idx] = x.copy()\n                archive_y[worst_idx] = y\n\n    update_archive(best_x, best_y)\n\n    while evals_used < budget:\n        n_to_sample = min(pop_size, budget - evals_used)\n        if n_to_sample <= 0:\n            break\n\n        new_bests = 0\n        y_before_gen = best_y\n\n        # Ensure we exploit around current best at least once\n        x0 = clip_to_bounds(center + rng.normal(0.0, sigma / 5.0))\n        y_before = best_y\n        eval_candidate(x0)\n        if best_y is not None and (y_before is None or best_y < y_before):\n            new_bests += 1\n            update_archive(best_x, best_y)\n\n        # Remaining candidates: mixture\n        for _ in range(n_to_sample - 1):\n            if evals_used >= budget:\n                break\n\n            r = rng.random()\n            if r < 0.6:\n                # Local Gaussian exploration around center\n                perturb = rng.normal(0.0, sigma)\n                x = center + perturb\n            elif r < 0.85 and len(archive_x) > 0:\n                # Restart from one of the archive elites\n                idx = int(rng.integers(0, len(archive_x)))\n                base = archive_x[idx]\n                scale = rng.uniform(0.3, 1.0)\n                perturb = rng.normal(0.0, sigma * scale)\n                x = base + perturb\n            else:\n                # Occasional broader uniform move in a sigma-scaled box\n                step = (rng.random(dim) - 0.5) * 2.0\n                x = center + step * sigma * 3.0\n\n            x = clip_to_bounds(x)\n            y_before = best_y\n            eval_candidate(x)\n            if best_y is not None and (y_before is None or best_y < y_before):\n                new_bests += 1\n                update_archive(best_x, best_y)\n\n        if best_x is not None:\n            center = best_x.copy()\n\n        # Adapt sigma based on success ratio within this generation\n        success_rate = new_bests / max(1, n_to_sample)\n        if success_rate > target_success_rate * 1.1:\n            sigma *= adapt_factor_inc\n        elif success_rate < target_success_rate * 0.5:\n            sigma *= adapt_factor_dec\n\n        sigma = np.clip(sigma, sigma_min, sigma_max)\n\n        # If the generation did not improve at all, slightly re-center on an archive elite\n        if new_bests == 0 and len(archive_x) > 0:\n            idx = int(rng.integers(0, len(archive_x)))\n            center = archive_x[idx].copy()\n\n        # If very little budget remains, break and fall through to greedy steps\n        if budget - evals_used < 4:\n            break\n\n    # Greedy local refinement with remaining evaluations\n    while evals_used < budget:\n        frac_left = (budget - evals_used) / max(1, budget)\n        # Reduce step aggressively near the end\n        step_scale = max(0.05, frac_left**2)\n        step = rng.normal(0.0, sigma * step_scale)\n        x = clip_to_bounds(center + step)\n        y_before = best_y\n        eval_candidate(x)\n        if best_x is not None and (y_before is None or best_y < y_before):\n            center = best_x.copy()\n\n    return best_x if best_x is not None else (low + high) / 2.0",
    "X": "0.03924300358468848 0.027289795774807328 0.03361685706516694 -0.04324086938087701"
}