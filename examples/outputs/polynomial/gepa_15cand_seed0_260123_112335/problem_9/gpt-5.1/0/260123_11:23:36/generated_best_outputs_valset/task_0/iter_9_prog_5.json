{
    "score": 0.10010806951050144,
    "Input": "Easom",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with hybrid global + local search.\n\n    Strategy:\n    1. Global exploration using a mix of low-discrepancy (Halton), random,\n       and warm-start seeding (if prev_best_x is available).\n    2. Local evolutionary refinement around the best found point with\n       success-based step-size control and adaptive population size.\n    3. Extra emphasis on wide global search for highly multimodal problems\n       (e.g., Easom) by biasing initial samples toward central regions\n       but covering the whole domain.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    # Internal evaluation accounting\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    def eval_candidate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        y = objective_function(x)\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # Halton-like low-discrepancy sequence\n    def _van_der_corput(n, base):\n        vdc, denom = 0.0, 1.0\n        while n:\n            n, remainder = divmod(n, base)\n            denom *= base\n            vdc += remainder / denom\n        return vdc\n\n    def halton_points(num, d, seed=0):\n        # Small prime bases\n        primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29,\n                  31, 37, 41, 43, 47, 53, 59, 61, 67, 71]\n        if d > len(primes):\n            # Fallback: uniform\n            return rng.random((num, d))\n        pts = np.empty((num, d), dtype=float)\n        start = seed + 1\n        for j in range(d):\n            base = primes[j]\n            for i in range(num):\n                pts[i, j] = _van_der_corput(start + i, base)\n        return pts\n\n    # -------- 1) Global exploration --------\n    # For highly multimodal landscapes, stronger global emphasis helps.\n    # Use ~60% of budget for global exploration (more than before),\n    # but ensure some evaluations remain for local refinement.\n    if budget <= 7:\n        global_evals = budget\n    else:\n        global_evals = max(5, min(budget - 7, int(0.6 * budget)))\n\n    # 1a) Evaluate warm start first if available\n    if prev_best_x is not None:\n        try:\n            x0 = np.array(prev_best_x, dtype=float).reshape(dim)\n            x0 = clip_to_bounds(x0)\n            eval_candidate(x0)\n        except Exception:\n            pass\n\n    # 1b) Add deterministic mid-point as a robust baseline if budget allows\n    if evals_used < budget and budget >= 3:\n        center_point = (low + high) / 2.0\n        eval_candidate(center_point)\n\n    # 1c) Explicitly sample a few points very close to the center (for Easom-like)\n    # Only if budget suffices and we haven't spent many global evaluations yet.\n    if evals_used < budget and budget >= 10:\n        n_center_samples = min(3, global_evals - evals_used)\n        if n_center_samples > 0:\n            center = (low + high) / 2.0\n            # Very small radius in normalized space; ensures we probe near the\n            # central global optimum of many benchmark functions such as Easom.\n            for _ in range(n_center_samples):\n                if evals_used >= budget:\n                    break\n                # Normalized small Gaussian, then scaled to box\n                z = rng.normal(0.0, 0.02, size=dim)\n                x = center + z * span\n                x = clip_to_bounds(x)\n                eval_candidate(x)\n\n    # 1d) Remaining global samples\n    remaining_global = max(0, global_evals - evals_used)\n    if remaining_global > 0 and evals_used < budget:\n        # Split between low-discrepancy and random to avoid structure artefacts\n        n_halton = int(0.6 * remaining_global)\n        n_rand = remaining_global - n_halton\n\n        xs_list = []\n\n        if n_halton > 0:\n            halton_seed = int(rng.integers(0, 10_000_000))\n            base_samples = halton_points(n_halton, dim, seed=halton_seed)\n            # Mild jitter to avoid lattice artefacts but not too large\n            jitter_scale = 0.02 / np.sqrt(max(1, dim))\n            jitter = rng.normal(0.0, jitter_scale, size=base_samples.shape)\n            samples = np.clip(base_samples + jitter, 0.0, 1.0)\n            xs_list.append(low + samples * span)\n\n        if n_rand > 0:\n            # Bias random samples a bit toward the central region but still cover entire domain.\n            # Draw from Beta(2,2) in [0,1], which is peaked at 0.5, then scale.\n            beta_samples = rng.beta(2.0, 2.0, size=(n_rand, dim))\n            xs_list.append(low + beta_samples * span)\n\n        if xs_list:\n            xs = np.vstack(xs_list)\n            rng.shuffle(xs, axis=0)\n            for i in range(xs.shape[0]):\n                if evals_used >= global_evals or evals_used >= budget:\n                    break\n                eval_candidate(xs[i])\n\n    # If still nothing evaluated (pathological), do one random sample\n    if best_x is None:\n        x_rand = rng.uniform(low, high)\n        eval_candidate(x_rand)\n\n    if evals_used >= budget or best_x is None:\n        return best_x if best_x is not None else (low + high) / 2.0\n\n    # -------- 2) Local evolutionary refinement --------\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    center = best_x.copy()\n\n    # Population size: proportional to dimension, but keep at least ~6 generations\n    max_pop = max(6, 4 * dim)\n    pop_size = min(max_pop, max(4, remaining // 6))\n\n    # Initial step size: smaller than before to be more conservative in rugged landscapes.\n    base_sigma = span / 5.0\n    base_sigma[base_sigma == 0.0] = 1.0\n    sigma = base_sigma.copy()\n\n    # Success-based step-size adaptation parameters (slightly milder)\n    target_success_rate = 0.25\n    adapt_factor_inc = 1.4\n    adapt_factor_dec = 0.7\n\n    sigma_min = base_sigma * 1e-7\n    sigma_max = base_sigma * 10.0\n\n    # Keep a small archive of best points to occasionally restart from\n    archive_size = 4\n    archive_x = [best_x.copy()]\n    archive_y = [best_y]\n\n    def update_archive(x, y):\n        if len(archive_x) < archive_size:\n            archive_x.append(x.copy())\n            archive_y.append(y)\n        else:\n            worst_idx = int(np.argmax(archive_y))\n            if y < archive_y[worst_idx]:\n                archive_x[worst_idx] = x.copy()\n                archive_y[worst_idx] = y\n\n    update_archive(best_x, best_y)\n\n    while evals_used < budget:\n        n_to_sample = min(pop_size, budget - evals_used)\n        if n_to_sample <= 0:\n            break\n\n        new_bests = 0\n        y_before_gen = best_y\n\n        # Ensure we exploit around current best at least once (small step)\n        x0 = clip_to_bounds(center + rng.normal(0.0, sigma / 7.0))\n        y_before = best_y\n        eval_candidate(x0)\n        if best_y is not None and (y_before is None or best_y < y_before):\n            new_bests += 1\n            update_archive(best_x, best_y)\n\n        # Remaining candidates: mixture\n        for _ in range(n_to_sample - 1):\n            if evals_used >= budget:\n                break\n\n            r = rng.random()\n            if r < 0.55:\n                # Local Gaussian exploration around center\n                perturb = rng.normal(0.0, sigma)\n                x = center + perturb\n            elif r < 0.85 and len(archive_x) > 0:\n                # Restart from one of the archive elites\n                idx = int(rng.integers(0, len(archive_x)))\n                base = archive_x[idx]\n                scale = rng.uniform(0.3, 1.0)\n                perturb = rng.normal(0.0, sigma * scale)\n                x = base + perturb\n            else:\n                # Occasional broader uniform move in a sigma-scaled box\n                step = (rng.random(dim) - 0.5) * 2.0\n                x = center + step * sigma * 3.0\n\n            x = clip_to_bounds(x)\n            y_before = best_y\n            eval_candidate(x)\n            if best_y is not None and (y_before is None or best_y < y_before):\n                new_bests += 1\n                update_archive(best_x, best_y)\n\n        if best_x is not None:\n            center = best_x.copy()\n\n        # Adapt sigma based on success ratio within this generation\n        success_rate = new_bests / max(1, n_to_sample)\n        if success_rate > target_success_rate * 1.1:\n            sigma *= adapt_factor_inc\n        elif success_rate < target_success_rate * 0.5:\n            sigma *= adapt_factor_dec\n\n        sigma = np.clip(sigma, sigma_min, sigma_max)\n\n        # If the generation did not improve at all, slightly re-center on an archive elite\n        if new_bests == 0 and len(archive_x) > 0:\n            idx = int(rng.integers(0, len(archive_x)))\n            center = archive_x[idx].copy()\n\n        # If very little budget remains, break and fall through to greedy steps\n        if budget - evals_used < 5:\n            break\n\n    # Greedy local refinement with remaining evaluations\n    while evals_used < budget:\n        frac_left = (budget - evals_used) / max(1, budget)\n        # Reduce step aggressively near the end\n        step_scale = max(0.03, frac_left**2)\n        step = rng.normal(0.0, sigma * step_scale)\n        x = clip_to_bounds(center + step)\n        y_before = best_y\n        eval_candidate(x)\n        if best_x is not None and (y_before is None or best_y < y_before):\n            center = best_x.copy()\n\n    return best_x if best_x is not None else (low + high) / 2.0",
    "X": "0.029492488955496957 0.00414370855608439 0.0239812230064487 -0.010497776624552403"
}