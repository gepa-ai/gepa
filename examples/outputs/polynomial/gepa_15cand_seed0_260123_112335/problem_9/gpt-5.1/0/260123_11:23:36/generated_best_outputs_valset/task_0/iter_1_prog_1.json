{
    "score": 8.433384887382365,
    "Input": "Easom",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Basic black-box minimization using random search with optional local\n    refinement around a warm start (prev_best_x).\n\n    Strategy:\n    1. Use a fraction of the budget for global random search over bounds.\n    2. If prev_best_x is provided and feasible, refine around it with\n       shrinking Gaussian perturbations.\n    3. Always respect the total evaluation budget.\n    \"\"\"\n    rng = np.random.default_rng()\n    bounds = np.array(config['bounds'], dtype=float)\n    dim = int(config['dim'])\n    budget = int(config['budget'])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Ensure we always have at least 1 evaluation\n    if budget <= 0:\n        # Fallback: just return center of bounds\n        return (low + high) / 2.0\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    def eval_candidate(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return None\n        y = objective_function(x)\n        evals_used += 1\n        if (best_y is None) or (y < best_y):\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # Helper: project into bounds\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    # 1) Global random search on full box\n    # Allocate ~70% of budget for global exploration\n    global_evals = max(1, int(budget * 0.7))\n\n    # If we have a warm start, evaluate it first (counts against budget)\n    if prev_best_x is not None:\n        x0 = np.array(prev_best_x, dtype=float).reshape(dim)\n        x0 = clip_to_bounds(x0)\n        if evals_used < budget:\n            eval_candidate(x0)\n\n    # Global random exploration\n    while evals_used < global_evals and evals_used < budget:\n        x = rng.uniform(low, high)\n        eval_candidate(x)\n\n    # 2) Local refinement around best found so far (if any budget left)\n    remaining = budget - evals_used\n    if remaining <= 0 or best_x is None:\n        # If for some reason nothing was evaluated (budget maybe 0),\n        # fall back to a random point\n        if best_x is None:\n            best_x = rng.uniform(low, high)\n        return best_x\n\n    # Local search: Gaussian perturbations with decreasing scale\n    # Allocate all remaining evaluations to local search\n    center = best_x.copy()\n    # Initial step size: a fraction of the box width\n    base_sigma = span / 4.0\n    base_sigma[base_sigma == 0.0] = 1.0  # fallback if any zero-width dim\n\n    for i in range(remaining):\n        # Decrease step size over time: from base_sigma to ~0\n        t = (i + 1) / (remaining + 1)\n        sigma = base_sigma * (1.0 - 0.8 * t)  # keep some exploration\n        perturbation = rng.normal(loc=0.0, scale=sigma)\n        x_candidate = clip_to_bounds(center + perturbation)\n        eval_candidate(x_candidate)\n        # Update center towards the best found so far to intensify search\n        if best_x is not None:\n            center = best_x\n\n    return best_x if best_x is not None else rng.uniform(low, high)",
    "X": "2.988495016154828 1.772070325218671 -0.6944637418686497 2.2808903648529273"
}