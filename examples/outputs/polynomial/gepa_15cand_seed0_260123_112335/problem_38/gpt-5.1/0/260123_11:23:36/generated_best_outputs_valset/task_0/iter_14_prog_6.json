{
    "score": -2.2839498384747583,
    "Input": "Mishra06",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budget-aware black-box minimization with hybrid global-local search.\n\n    Main structure:\n    - Robust initialization (prev_best_x, center, random / LHS points).\n    - Global phase: adaptive evolutionary search with simple covariance shaping.\n    - Local phase: short coordinate-wise search around incumbent.\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config.get(\"budget\", 1))\n\n    lower, upper = bounds[:, 0], bounds[:, 1]\n    span = upper - lower\n    span = np.where(span == 0.0, 1.0, span)\n\n    seed = config.get(\"seed\", None)\n    rng = np.random.RandomState(seed) if seed is not None else np.random.RandomState()\n\n    def project(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # ---------- robust evaluation wrapper ----------\n    def safe_eval(x):\n        nonlocal evals_used, best_x, best_y\n        if evals_used >= budget:\n            return np.inf\n        try:\n            y = float(objective_function(x))\n        except Exception:\n            y = np.inf\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # ---------- initial design (with LHS-style spread) ----------\n    init_candidates = []\n\n    # warm start\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim and np.all(np.isfinite(x0)):\n            init_candidates.append(project(x0))\n\n    center = lower + 0.5 * span\n    init_candidates.append(project(center))\n\n    # budget-aware number of initial samples\n    # keep at most ~20% of budget for seeding but not more than 12\n    if budget <= 5:\n        n_init_rand = max(0, budget - len(init_candidates))\n    else:\n        n_init_rand = min(max(0, int(0.2 * budget) - len(init_candidates)), 12)\n\n    if n_init_rand > 0:\n        # LHS-like: stratify each dimension independently\n        # Generate uniform [0,1] stratified samples then scale\n        u = (rng.rand(n_init_rand, dim) + rng.permutation(n_init_rand)[:, None]) / max(\n            1, n_init_rand\n        )\n        u = np.clip(u, 0.0, 1.0)\n        lhs_points = lower + u * (upper - lower)\n        for i in range(n_init_rand):\n            init_candidates.append(lhs_points[i])\n\n    # one Gaussian around center if budget permits\n    if len(init_candidates) < budget:\n        init_candidates.append(project(center + rng.normal(0.0, 0.25, size=dim) * span))\n\n    for x0 in init_candidates:\n        if evals_used >= budget:\n            break\n        safe_eval(x0)\n\n    if best_x is None:\n        return center.astype(float)\n    if evals_used >= budget:\n        return best_x.astype(float)\n\n    remaining = budget - evals_used\n\n    # ---------- phase split ----------\n    if remaining <= 6:\n        global_evals = remaining\n        local_evals = 0\n    else:\n        # more global on multimodal; ensure some local refinement\n        global_evals = int(0.75 * remaining)\n        global_evals = max(5, min(global_evals, remaining - 3))\n        local_evals = remaining - global_evals\n\n    # ---------- global: adaptive evolutionary search ----------\n    base_scale = 0.15 * span / max(1.0, np.sqrt(dim))\n    base_scale = np.where(base_scale == 0.0, 1.0, base_scale)\n\n    archive_size = max(6, min(16, global_evals))\n    archive_x = [best_x.copy()]\n    archive_y = [best_y]\n\n    def update_archive(x, y):\n        if not np.isfinite(y):\n            return\n        if len(archive_x) < archive_size:\n            archive_x.append(x.copy())\n            archive_y.append(y)\n        else:\n            idx_worst = int(np.argmax(archive_y))\n            if y < archive_y[idx_worst]:\n                archive_x[idx_worst] = x.copy()\n                archive_y[idx_worst] = y\n\n    update_archive(best_x, best_y)\n\n    # maintain simple diagonal covariance estimate for mutations\n    diag_var = (0.25 * span) ** 2\n\n    success_count = 0\n    trial_count = 0\n\n    for _ in range(global_evals):\n        if evals_used >= budget:\n            break\n\n        # choose base (biased to better archive members)\n        if len(archive_x) > 1 and rng.rand() < 0.8:\n            i, j = rng.randint(0, len(archive_x)), rng.randint(0, len(archive_x))\n            base = archive_x[i] if archive_y[i] < archive_y[j] else archive_x[j]\n        else:\n            base = best_x\n\n        r = rng.rand()\n\n        if r < 0.5:\n            # Gaussian mutation with adaptive scale and diag covariance\n            if trial_count >= 12:\n                rate = success_count / float(trial_count)\n                adapt = 1.0\n                if rate > 0.3:\n                    adapt = 1.5\n                elif rate < 0.15:\n                    adapt = 0.6\n            else:\n                adapt = 1.0\n            scale = base_scale * adapt\n            noise = rng.normal(0.0, 1.0, size=dim) * np.sqrt(\n                np.maximum(diag_var, 1e-16)\n            )\n            cand = base + noise * (scale / (np.sqrt(diag_var) + 1e-12))\n        elif r < 0.8:\n            # global uniform exploration\n            cand = rng.uniform(lower, upper, size=dim)\n        else:\n            # differential-like move; occasionally use best-to-rand\n            if len(archive_x) >= 3:\n                a, b = rng.choice(len(archive_x), size=2, replace=False)\n                xa, xb = archive_x[a], archive_x[b]\n                F = 0.5 + 0.5 * rng.rand()\n                if rng.rand() < 0.5:\n                    cand = base + F * (xa - xb)\n                else:\n                    xr = rng.uniform(lower, upper, size=dim)\n                    cand = best_x + F * (xr - base)\n            else:\n                xr = rng.uniform(lower, upper, size=dim)\n                cand = best_x + 0.7 * (xr - best_x)\n\n        cand = project(cand)\n        y = safe_eval(cand)\n        trial_count += 1\n        if y < best_y:\n            success_count += 1\n\n        update_archive(cand, y)\n\n        # update diagonal variance estimate with exponential moving average\n        if np.isfinite(y):\n            diff = cand - best_x\n            diag_var = 0.9 * diag_var + 0.1 * (diff * diff)\n\n    if evals_used >= budget or local_evals <= 0:\n        return best_x.astype(float)\n\n    # ---------- local: coordinate-wise refinement ----------\n    x = best_x.copy()\n\n    step = 0.02 * span / max(1.0, np.sqrt(dim))\n    step = np.where(step <= 0.0, 1e-4, step)\n    min_step = 1e-10 * span\n    min_step = np.where(min_step <= 0.0, 1e-9, min_step)\n    max_step = 0.25 * span\n\n    for _ in range(local_evals):\n        if evals_used >= budget:\n            break\n\n        if dim == 1:\n            idxs = [0]\n        else:\n            if rng.rand() < 0.9:\n                k = 1\n            else:\n                k = min(3, dim)\n            idxs = rng.choice(dim, size=k, replace=False)\n\n        cand = x.copy()\n        for i in idxs:\n            direction = -1.0 if rng.rand() < 0.5 else 1.0\n            mag = step[i] * (0.2 + 0.8 * rng.rand())\n            cand[i] += direction * mag\n\n        cand = project(cand)\n        y = safe_eval(cand)\n\n        if y < best_y:\n            x = cand\n            for i in idxs:\n                step[i] = min(step[i] * 1.3, max_step[i])\n        else:\n            for i in idxs:\n                step[i] *= 0.5\n                if step[i] < min_step[i]:\n                    step[i] = min_step[i]\n\n    return best_x.astype(float)",
    "X": "2.886307231554306 1.8232603293892946"
}