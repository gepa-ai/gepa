{
    "score": -2.283949838474755,
    "Input": "Mishra06",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with budget-aware hybrid global-local search.\n\n    Strategy:\n    - Careful seeding (includes prev_best_x, center of bounds, and uniform random).\n    - Global search: adaptive evolutionary / CMA-like sampling around an incumbent\n      plus broad uniform exploration.\n    - Local search: coordinate-wise adaptive steps.\n    - Always respect evaluation budget and return best solution found.\n    \"\"\"\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config.get(\"budget\", 1))\n\n    lower, upper = bounds[:, 0], bounds[:, 1]\n    span = upper - lower\n    # Avoid zero span issues\n    span = np.where(span == 0.0, 1.0, span)\n\n    rng = np.random.RandomState()\n\n    def project(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    # ---------- Initial seeding (use up to 3 evaluations if possible) ----------\n    # Candidate list to try in order: prev_best_x, box center, random\n    init_candidates = []\n\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            init_candidates.append(project(x0))\n\n    # Center of the box often good for symmetric problems\n    center = lower + 0.5 * span\n    init_candidates.append(project(center))\n\n    # One purely random point\n    init_candidates.append(rng.uniform(lower, upper, size=dim))\n\n    for x0 in init_candidates:\n        if evals_used >= budget:\n            break\n        y0 = objective_function(x0)\n        evals_used += 1\n        if y0 < best_y:\n            best_x, best_y = x0, y0\n\n    if best_x is None:\n        # In pathological case with budget == 0 (should not happen, but be safe)\n        # or no valid candidates, just return center.\n        return center.astype(float)\n\n    if evals_used >= budget:\n        return best_x.astype(float)\n\n    remaining = budget - evals_used\n\n    # ---------- Phase split: global vs local ----------\n    if remaining <= 5:\n        global_evals = remaining\n        local_evals = 0\n    else:\n        # More global exploration but keep local refinement\n        global_evals = int(0.6 * remaining)\n        global_evals = max(4, min(global_evals, remaining - 2))\n        local_evals = remaining - global_evals\n\n    # ---------- Global search: mixed evolutionary sampling ----------\n    # Maintain a simple \"population\" notion around current best to intensify search\n    # while still exploring broadly.\n    # Set baseline mutation scale relative to span and dimension.\n    base_scale = 0.2 * span / max(1.0, np.sqrt(dim))\n    base_scale = np.where(base_scale == 0.0, 1.0, base_scale)\n\n    # Track a rough success rate to adapt step-size\n    success_count = 0\n    trial_count = 0\n\n    for g in range(global_evals):\n        if evals_used >= budget:\n            break\n\n        r = rng.rand()\n        if r < 0.5:\n            # Best-centered Gaussian mutation (CMA-like)\n            # Scale adapts with rough success rate in last iterations.\n            adapt_factor = 1.0\n            if trial_count >= 10:\n                rate = success_count / float(trial_count)\n                # If many successes -> increase step, else shrink\n                if rate > 0.3:\n                    adapt_factor = 1.4\n                elif rate < 0.15:\n                    adapt_factor = 0.7\n            scale = base_scale * adapt_factor\n            noise = rng.normal(0.0, 1.0, size=dim) * scale\n            cand = best_x + noise\n        elif r < 0.85:\n            # Pure global uniform exploration\n            cand = rng.uniform(lower, upper, size=dim)\n        else:\n            # Directional move between best and a random point (DE-like)\n            xr = rng.uniform(lower, upper, size=dim)\n            F = 0.8\n            cand = project(best_x + F * (xr - best_x))\n\n        cand = project(cand)\n        y = objective_function(cand)\n        evals_used += 1\n        trial_count += 1\n\n        if y < best_y:\n            best_x, best_y = cand, y\n            success_count += 1\n\n    if evals_used >= budget or local_evals <= 0:\n        return best_x.astype(float)\n\n    # ---------- Local search: adaptive coordinate search ----------\n    x = best_x.copy()\n\n    # Initial step sizes based on span; smaller for high dimensions.\n    step = 0.05 * span / max(1.0, np.sqrt(dim))\n    step = np.where(step <= 0.0, 1e-3, step)\n\n    min_step = 1e-8 * span\n    min_step = np.where(min_step <= 0.0, 1e-8, min_step)\n    max_step = 0.5 * span\n\n    for _ in range(local_evals):\n        if evals_used >= budget:\n            break\n\n        # For high dimension, focus mostly on single-coordinate moves\n        if dim == 1:\n            idxs = [0]\n        else:\n            if rng.rand() < 0.8:\n                k = 1\n            else:\n                k = min(2, dim)\n            idxs = rng.choice(dim, size=k, replace=False)\n\n        cand = x.copy()\n        for i in idxs:\n            direction = rng.choice([-1.0, 1.0])\n            mag = step[i] * (0.3 + 0.9 * rng.rand())\n            cand[i] += direction * mag\n\n        cand = project(cand)\n        y = objective_function(cand)\n        evals_used += 1\n\n        if y < best_y:\n            best_y = y\n            best_x = cand\n            x = cand\n            # Slightly increase step on improved coordinates\n            for i in idxs:\n                step[i] = min(step[i] * 1.3, max_step[i])\n        else:\n            # Reduce step on failed coordinates\n            for i in idxs:\n                step[i] *= 0.5\n                if step[i] < min_step[i]:\n                    step[i] = min_step[i]\n\n    return best_x.astype(float)",
    "X": "2.8863072392091493 1.8232603482575098"
}