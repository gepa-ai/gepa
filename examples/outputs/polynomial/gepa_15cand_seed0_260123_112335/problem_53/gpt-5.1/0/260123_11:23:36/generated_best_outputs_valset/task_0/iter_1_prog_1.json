{
    "score": -195.71177836238166,
    "Input": "StyblinskiTang",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Simple derivative-free optimizer: global random search + local hill climbing.\n\n    Strategy:\n    1. Use a fraction of the budget for global random search (Latin-hypercube-ish).\n    2. Use the remaining budget for local search around the best point found.\n    3. If prev_best_x is provided and inside bounds, include it as a starting point.\n    \"\"\"\n    rng = np.random.RandomState()  # independent RNG\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    width = upper - lower\n\n    def clamp(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n\n    # Helper: evaluate point and track best\n    best_x = None\n    best_y = np.inf\n\n    def evaluate(x):\n        nonlocal best_x, best_y, evals_used\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # Use prev_best_x if available and valid\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = clamp(x0)\n            evaluate(x0)\n\n    if evals_used >= budget:\n        return best_x if best_x is not None else clamp(lower)\n\n    # Decide split between global and local search\n    if dim <= 5:\n        global_frac = 0.4\n    elif dim <= 20:\n        global_frac = 0.6\n    else:\n        global_frac = 0.75\n\n    global_budget = max(1, int(budget * global_frac) - evals_used)\n    local_budget = max(0, budget - evals_used - global_budget)\n\n    # --- Global random search (quasi space-filling via simple stratification) ---\n    if global_budget > 0:\n        # Stratified sampling per dimension\n        # Create distinct strata indices for each sample per dimension\n        n = global_budget\n        # Pre-generate positions in [0,1] with jitter\n        base = (np.arange(n) + rng.rand(n)) / n  # shape (n,)\n        for i in range(n):\n            # For each sample, permute strata across dims\n            u = np.empty(dim)\n            for d in range(dim):\n                # Simple scramble: circular shift based on dim and sample index\n                idx = (i + d) % n\n                u[d] = base[idx]\n            x = lower + u * width\n            evaluate(x)\n            if evals_used >= budget:\n                return best_x\n\n    # If nothing evaluated yet (no prev_best_x and zero global_budget), create one\n    if best_x is None:\n        x = lower + rng.rand(dim) * width\n        evaluate(x)\n        if evals_used >= budget:\n            return best_x\n\n    # --- Local stochastic hill climbing around best_x ---\n    if local_budget <= 0:\n        return best_x\n\n    # Adaptive step sizes based on bounds\n    # Start with step size as a fraction of the width\n    initial_step = 0.2 * width\n    min_step = 1e-6 * np.maximum(1.0, np.abs(width))\n\n    x_current = best_x.copy()\n    step = initial_step.copy()\n\n    remaining = budget - evals_used\n    # Simple loop; each iteration uses 1 evaluation\n    while remaining > 0:\n        # Propose a neighbor using Gaussian perturbation per dimension\n        perturb = rng.randn(dim) * step\n        x_candidate = clamp(x_current + perturb)\n        y_candidate = evaluate(x_candidate)\n        remaining = budget - evals_used\n        if y_candidate < best_y:\n            # Improvement: move center to candidate, optionally expand steps slightly\n            x_current = x_candidate\n            step = np.minimum(step * 1.2, width)\n        else:\n            # No improvement: shrink step sizes to refine search\n            step = np.maximum(step * 0.7, min_step)\n\n        if remaining <= 0:\n            break\n\n    return best_x",
    "X": "-2.953636413742182 -2.920048980901561 -2.922166739831954 -2.8449384177219192 -2.884972671754575"
}