{
    "score": -195.8308285058186,
    "Input": "StyblinskiTang",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Derivative-free optimizer: robust hybrid global + local search with warm start.\n\n    Strategy:\n    1. Space-filling random initialization for global search.\n    2. Diagonal-CMA-ES\u2013like global search for a large share of budget.\n    3. Strong, adaptive local stochastic search for remaining budget.\n    4. Proper use of prev_best_x to seed search when available.\n    5. Handles low budgets and small/large dimensions robustly.\n    \"\"\"\n\n    # ---------- Basic setup ----------\n    rng = np.random.RandomState()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    width = upper - lower\n    width = np.where(width <= 0, 1.0, width)\n\n    def clamp(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    def evaluate(x):\n        nonlocal best_x, best_y, evals_used\n        if evals_used >= budget:\n            return best_y\n        y = objective_function(x)\n        evals_used += 1\n        if best_x is None or y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    # ---------- Edge cases ----------\n    if budget <= 0:\n        return clamp(lower)\n\n    if dim <= 0:\n        # pathological config, just call once on center if budget>0\n        x0 = clamp(lower)\n        evaluate(x0)\n        return best_x\n\n    # ---------- Budget split: global vs local ----------\n    # Slightly bias more towards global when budget is larger, local when small\n    if dim <= 5:\n        global_frac = 0.6\n    elif dim <= 20:\n        global_frac = 0.7\n    else:\n        global_frac = 0.8\n\n    if budget < 30:\n        global_frac = 0.5\n    if budget < 15:\n        global_frac = 0.3\n\n    global_budget = int(budget * global_frac)\n    global_budget = max(0, min(global_budget, budget - 1))\n    local_budget = budget - global_budget\n\n    # ---------- Warm start ----------\n    if prev_best_x is not None and evals_used < budget:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = clamp(x0)\n            evaluate(x0)\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x if best_x is not None else clamp(lower)\n\n    global_budget = min(global_budget, remaining)\n    local_budget = max(0, budget - evals_used - global_budget)\n\n    # ---------- Very low budget: fall back to pure sampling ----------\n    if budget <= 5:\n        # Already may have used 1 eval for warm start\n        n_extra = budget - evals_used\n        for _ in range(n_extra):\n            x = lower + rng.rand(dim) * width\n            evaluate(x)\n        return best_x if best_x is not None else clamp(lower)\n\n    # ---------- Initial global random / LHS exploration ----------\n    if global_budget > 0 and evals_used < budget:\n        # ensure non-trivial fraction of global budget goes to init\n        init_samples = min(max(5, global_budget // 3), 80, budget - evals_used)\n        if init_samples > 0:\n            # Latin-hypercube-like stratified sampling per dimension\n            grid = (np.arange(init_samples) + 0.5) / init_samples\n            z = None\n            for d in range(dim):\n                rng.shuffle(grid)\n                col = grid.reshape(-1, 1)\n                if z is None:\n                    z = col\n                else:\n                    z = np.hstack([z, col])\n            # small jitter\n            z = (z + (rng.rand(init_samples, dim) - 0.5) / init_samples) % 1.0\n            for i in range(init_samples):\n                if evals_used >= budget:\n                    break\n                x = clamp(lower + z[i] * width)\n                evaluate(x)\n\n    if evals_used >= budget:\n        return best_x if best_x is not None else clamp(lower)\n\n    # ---------- Global evolutionary search (diagonal CMA-ES like) ----------\n    remaining_global = min(global_budget, budget - evals_used)\n    if remaining_global > 0:\n        # Population size\n        lam = int(4 + np.floor(3 * np.log(dim))) if dim > 0 else 4\n        lam = max(4, lam)\n        if remaining_global < 4:\n            lam = remaining_global\n        else:\n            lam = min(lam, max(8, remaining_global // 3))\n        lam = max(2, lam)\n        mu = max(1, lam // 2)\n\n        weights = np.log(np.arange(1, mu + 1) + 0.5)\n        weights = weights[::-1]\n        weights = weights / np.sum(weights)\n        mueff = 1.0 / np.sum(weights ** 2)\n\n        eff_dim = max(dim, 1)\n        cc = (4 + mueff / eff_dim) / (eff_dim + 4 + 2 * mueff / eff_dim)\n        c1 = 2 / ((eff_dim + 1.3) ** 2 + mueff)\n        cmu = min(1 - c1, 2 * (mueff - 2 + 1 / mueff) / ((eff_dim + 2) ** 2 + mueff))\n        damps = 1 + 2 * max(0, np.sqrt((mueff - 1) / (eff_dim + 1)) - 1) + cc\n\n        pc = np.zeros(dim)\n        ps = np.zeros(dim)\n        diagC = np.ones(dim)\n\n        # Start mean at best found so far if available; otherwise central random\n        if best_x is not None:\n            mean = best_x.copy()\n        else:\n            center = lower + 0.5 * width\n            mean = clamp(center + 0.2 * width * rng.randn(dim))\n\n        # Moderate initial sigma, scaled with dimension\n        sigma = 0.3 if dim <= 5 else 0.2\n\n        counteval_start = evals_used\n        chi_n = np.sqrt(eff_dim) * (1 - 1. / (4 * eff_dim) + 1. / (21 * eff_dim ** 2))\n\n        generation = 0\n        cma_no_improve = 0\n        # online tracking of best value seen inside CMA\n        cma_best_y = best_y\n\n        # allow a small margin of global budget for diversity even when stagnating\n        while evals_used < budget and (evals_used - counteval_start) < remaining_global:\n            remaining_global_inner = remaining_global - (evals_used - counteval_start)\n            if remaining_global_inner <= 0:\n                break\n\n            lam_actual = min(lam, remaining_global_inner)\n            if lam_actual <= 0:\n                break\n\n            arz = rng.randn(lam_actual, dim)\n            # sample in normalized coordinates, then scale by width\n            steps = arz * (sigma * np.sqrt(diagC))\n            arx = mean + steps * width\n            arx = clamp(arx)\n\n            fitness = np.empty(lam_actual)\n            for k in range(lam_actual):\n                if evals_used >= budget:\n                    fitness[k] = np.inf\n                else:\n                    fitness[k] = evaluate(arx[k])\n\n            if lam_actual == 0:\n                break\n\n            idx = np.argsort(fitness)\n            arx = arx[idx]\n            arz = arz[idx]\n            fitness = fitness[idx]\n\n            # update internal CMA best tracker\n            if fitness[0] < cma_best_y:\n                cma_best_y = fitness[0]\n\n            mu_eff = min(mu, lam_actual)\n            w = weights[:mu_eff]\n            w = w / w.sum()\n            xold = mean.copy()\n            mean = np.sum(arx[:mu_eff] * w[:, None], axis=0)\n\n            # Stagnation detection: if CMA internal best not improving\n            if cma_best_y >= best_y - 1e-10:\n                cma_no_improve += 1\n            else:\n                cma_no_improve = 0\n                best_y = min(best_y, cma_best_y)\n\n            # Update evolution paths and diagonal covariance\n            if eff_dim > 0:\n                z_mean = np.sum(arz[:mu_eff] * w[:, None], axis=0)\n                ps = (1 - cc) * ps + np.sqrt(cc * (2 - cc) * mueff) * z_mean\n\n                norm_ps = np.linalg.norm(ps)\n                hsig_cond = norm_ps / np.sqrt(1 - (1 - cc) ** (2 * (generation + 1)))\n                hsig = 1 if hsig_cond / chi_n < (1.4 + 2 / (eff_dim + 1)) else 0\n\n                diff = (mean - xold) / (sigma * width + 1e-12)\n                pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * diff\n\n                delta_hsig = (1 - hsig) * cc * (2 - cc)\n                # covariance update in diagonal form\n                diagC = (1 + c1 * delta_hsig - c1 - cmu * np.sum(w)) * diagC\n                diagC += c1 * pc ** 2\n                for i in range(mu_eff):\n                    yi = (arx[i] - xold) / (sigma * width + 1e-12)\n                    diagC += cmu * w[i] * yi ** 2\n\n                # numerical safety\n                diagC = np.clip(diagC, 1e-12, 1e5)\n\n                # Step-size control (larger damping for stability)\n                sigma = sigma * np.exp((norm_ps / chi_n - 1) * (cc / (damps * 1.5)))\n                sigma = np.clip(sigma, 5e-5, 2.0)\n\n            generation += 1\n\n            # Early termination if CMA-ES stagnates and enough budget remains\n            # to switch to more exploitative local search\n            if cma_no_improve >= 8:\n                remaining_total = budget - evals_used\n                if remaining_total > max(10, 2 * dim):\n                    break\n                else:\n                    cma_no_improve = 0\n\n            if evals_used >= budget:\n                break\n\n        # Safety: if CMA never evaluated a valid point (extremely unlikely)\n        if best_x is None and evals_used < budget:\n            x = lower + rng.rand(dim) * width\n            evaluate(x)\n\n    if evals_used >= budget:\n        return best_x if best_x is not None else clamp(lower)\n\n    # ---------- Local stochastic hill-climbing / search ----------\n    if best_x is None and evals_used < budget:\n        x = lower + rng.rand(dim) * width\n        evaluate(x)\n        if evals_used >= budget:\n            return best_x if best_x is not None else clamp(lower)\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x if best_x is not None else clamp(lower)\n\n    x_current = best_x.copy()\n\n    # Step sizes: start moderately small but dimension-aware\n    base_scale = 0.04 if dim <= 5 else 0.03\n    step = base_scale * width\n    min_step = 1e-6 * np.maximum(1.0, np.abs(width))\n    max_step = 0.4 * width\n\n    no_improve_count = 0\n    long_jump_threshold = 14 if dim <= 10 else 8\n\n    coord_prob = 0.45\n\n    # Local incumbent for more exploitation\n    local_best_x = x_current.copy()\n    local_best_y = best_y\n\n    while evals_used < budget:\n        # Coordinate-wise or full-dimensional perturbation\n        if rng.rand() < coord_prob:\n            # Prefer sparse coordinate moves in higher dimensions\n            mask_prob = 0.35 if dim > 5 else 0.7\n            mask = rng.rand(dim) < mask_prob\n            if not np.any(mask):\n                mask[rng.randint(dim)] = True\n            perturb = np.zeros(dim)\n            perturb[mask] = rng.randn(mask.sum()) * step[mask]\n        else:\n            perturb = rng.randn(dim) * step\n\n        x_candidate = clamp(x_current + perturb)\n        y_candidate = evaluate(x_candidate)\n\n        improved_global = y_candidate < best_y - 1e-12\n        improved_local = y_candidate < local_best_y - 1e-12\n\n        if improved_global:\n            best_y = y_candidate\n            best_x = x_candidate.copy()\n\n        if improved_local:\n            local_best_y = y_candidate\n            local_best_x = x_candidate.copy()\n            x_current = x_candidate\n            step = np.minimum(step * 1.25, max_step)\n            no_improve_count = 0\n        else:\n            step = np.maximum(step * 0.7, min_step)\n            no_improve_count += 1\n\n            # Occasional long-distance jumps to escape local minima\n            if no_improve_count >= long_jump_threshold and evals_used < budget:\n                long_scale = 0.25 if dim <= 10 else 0.18\n                long_step = long_scale * width\n\n                base_choice = rng.rand()\n                if base_choice < 0.5 and best_x is not None:\n                    base = best_x\n                elif base_choice < 0.8:\n                    base = local_best_x\n                elif base_choice < 0.93:\n                    # central biased exploration\n                    base = lower + 0.5 * width + 0.1 * width * rng.randn(dim)\n                else:\n                    # uniform random restart\n                    base = lower + rng.rand(dim) * width\n\n                x_try = clamp(base + rng.randn(dim) * long_step)\n                y_try = evaluate(x_try)\n\n                if y_try < best_y - 1e-12:\n                    best_y = y_try\n                    best_x = x_try.copy()\n                if y_try < local_best_y - 1e-12:\n                    local_best_y = y_try\n                    local_best_x = x_try.copy()\n                    x_current = x_try\n                    step = np.maximum(step, 0.1 * width)\n                else:\n                    # after failed long jump, keep step moderate\n                    step = np.maximum(step, 0.04 * width)\n\n                no_improve_count = 0\n\n    return best_x if best_x is not None else clamp(lower)",
    "X": "-2.903529635596612 -2.903535724020895 -2.903510630106596 -2.903529536833519 -2.903521211594337"
}