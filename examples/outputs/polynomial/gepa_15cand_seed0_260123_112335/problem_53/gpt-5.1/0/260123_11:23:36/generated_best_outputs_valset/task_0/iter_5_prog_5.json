{
    "score": -195.82963830404978,
    "Input": "StyblinskiTang",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Derivative-free optimizer: hybrid global + local search with warm start.\n\n    Strategy:\n    1. Sobol-like low-discrepancy random initialization for global search.\n    2. Robust diagonal-CMA-ES\u2013like global search for ~60\u201380% budget.\n    3. Adaptive local stochastic search for remaining budget.\n    4. Proper use of prev_best_x to seed search when available.\n    5. Handles very low budgets and small dimensions gracefully.\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    width = upper - lower\n    width = np.where(width <= 0, 1.0, width)\n\n    def clamp(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    def evaluate(x):\n        nonlocal best_x, best_y, evals_used\n        if evals_used >= budget:\n            return best_y\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y or best_x is None:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    if budget <= 0:\n        return clamp(lower)\n\n    # Decide split between global and local search based on dimension & budget\n    if dim <= 5:\n        global_frac = 0.6\n    elif dim <= 20:\n        global_frac = 0.7\n    else:\n        global_frac = 0.8\n\n    # Under tiny budgets, spend more on global exploration\n    if budget < 20:\n        global_frac = 0.8\n\n    global_budget = int(budget * global_frac)\n    global_budget = max(0, min(global_budget, budget - 1))\n    local_budget = budget - global_budget\n\n    # Warm start from previous best if provided\n    if prev_best_x is not None and evals_used < budget:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = clamp(x0)\n            evaluate(x0)\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x if best_x is not None else clamp(lower)\n\n    global_budget = min(global_budget, remaining)\n    local_budget = max(0, budget - evals_used - global_budget)\n\n    # -------------- Low-discrepancy-like random initial sampling --------------\n    # Use a small batch of quasi-random samples to robustly seed global search.\n    if global_budget > 0 and evals_used < budget:\n        # Use up to 10% of global budget (min 2, max 30) for initial exploration\n        init_samples = min(max(2, global_budget // 10), 30, budget - evals_used)\n        if init_samples > 0:\n            # Use a simple scrambled Halton/Sobol-inspired sequence via permutations\n            # without heavy dependencies.\n            for i in range(init_samples):\n                base = (i + 1) / (init_samples + 1)\n                # Dim-wise scrambling with different primes\n                primes = np.array([2, 3, 5, 7, 11, 13, 17, 19, 23, 29], dtype=float)\n                p = primes[: min(len(primes), dim)]\n                frac = (base * p[: len(p)] + rng.rand(len(p)) * 0.3) % 1.0\n                if dim > len(p):\n                    tail = rng.rand(dim - len(p))\n                    z = np.concatenate([frac, tail])\n                else:\n                    z = frac[:dim]\n                x = clamp(lower + z * width)\n                evaluate(x)\n                if evals_used >= budget:\n                    break\n\n    if evals_used >= budget:\n        return best_x if best_x is not None else clamp(lower)\n\n    # ----------------- Global evolutionary search (CMA-ES like) -----------------\n    remaining_global = min(global_budget, budget - evals_used)\n    if remaining_global > 0:\n        if dim > 0:\n            lam = int(4 + np.floor(3 * np.log(dim)))\n        else:\n            lam = 4\n        lam = max(4, lam)\n        if remaining_global < 4:\n            lam = remaining_global\n        else:\n            lam = min(lam, max(6, remaining_global // 3))\n        lam = max(1, lam)\n\n        mu = max(1, lam // 2)\n\n        weights = np.log(np.arange(1, mu + 1) + 0.5)\n        weights = weights[::-1]\n        weights = weights / np.sum(weights)\n        mueff = 1.0 / np.sum(weights ** 2)\n\n        eff_dim = max(dim, 1)\n        cc = (4 + mueff / eff_dim) / (eff_dim + 4 + 2 * mueff / eff_dim)\n        c1 = 2 / ((eff_dim + 1.3) ** 2 + mueff)\n        cmu = min(1 - c1, 2 * (mueff - 2 + 1 / mueff) / ((eff_dim + 2) ** 2 + mueff))\n        damps = 1 + 2 * max(0, np.sqrt((mueff - 1) / (eff_dim + 1)) - 1) + cc\n\n        pc = np.zeros(dim)\n        ps = np.zeros(dim)\n        diagC = np.ones(dim)\n\n        # Initial mean: favor current best_x if available\n        if best_x is not None:\n            mean = best_x.copy()\n        else:\n            center = lower + 0.5 * width\n            mean = clamp(center + 0.1 * width * rng.randn(dim))\n\n        # Dimension-aware initial sigma\n        sigma = 0.3 if dim <= 5 else 0.2\n\n        counteval_start = evals_used\n        chi_n = np.sqrt(eff_dim) * (1 - 1. / (4 * eff_dim) + 1. / (21 * eff_dim ** 2))\n\n        generation = 0\n        while evals_used < budget and (evals_used - counteval_start) < remaining_global:\n            remaining_global_inner = remaining_global - (evals_used - counteval_start)\n            if remaining_global_inner <= 0:\n                break\n\n            lam_actual = min(lam, remaining_global_inner)\n            if lam_actual <= 0:\n                break\n\n            arz = rng.randn(lam_actual, dim)\n            steps = arz * (sigma * np.sqrt(diagC))\n            arx = mean + steps * width\n            arx = clamp(arx)\n\n            fitness = np.empty(lam_actual)\n            for k in range(lam_actual):\n                if evals_used >= budget:\n                    fitness[k] = np.inf\n                else:\n                    fitness[k] = evaluate(arx[k])\n\n            if lam_actual == 0:\n                break\n\n            idx = np.argsort(fitness)\n            arx = arx[idx]\n            arz = arz[idx]\n            fitness = fitness[idx]\n\n            mu_eff = min(mu, lam_actual)\n            w = weights[:mu_eff]\n            w = w / w.sum()\n            xold = mean.copy()\n            mean = np.sum(arx[:mu_eff] * w[:, None], axis=0)\n\n            if eff_dim > 0:\n                z_mean = np.sum(arz[:mu_eff] * w[:, None], axis=0)\n                ps = (1 - cc) * ps + np.sqrt(cc * (2 - cc) * mueff) * z_mean\n\n                norm_ps = np.linalg.norm(ps)\n                hsig_cond = norm_ps / np.sqrt(1 - (1 - cc) ** (2 * (generation + 1)))\n                hsig = 1 if hsig_cond / chi_n < (1.4 + 2 / (eff_dim + 1)) else 0\n\n                diff = (mean - xold) / (sigma * width + 1e-12)\n                pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * diff\n\n                delta_hsig = (1 - hsig) * cc * (2 - cc)\n                diagC = (1 + c1 * delta_hsig - c1 - cmu * np.sum(w)) * diagC\n                diagC += c1 * pc ** 2\n                for i in range(mu_eff):\n                    yi = (arx[i] - xold) / (sigma * width + 1e-12)\n                    diagC += cmu * w[i] * yi ** 2\n\n                diagC = np.clip(diagC, 1e-12, 1e6)\n                sigma = sigma * np.exp((norm_ps / chi_n - 1) * cc / damps)\n                sigma = np.clip(sigma, 5e-4, 1.5)\n\n            generation += 1\n\n            if evals_used >= budget:\n                break\n\n        if best_x is None and evals_used < budget:\n            x = lower + rng.rand(dim) * width\n            evaluate(x)\n\n    if evals_used >= budget:\n        return best_x if best_x is not None else clamp(lower)\n\n    # ----------------- Local stochastic hill-climbing -----------------\n    if best_x is None and evals_used < budget:\n        x = lower + rng.rand(dim) * width\n        evaluate(x)\n        if evals_used >= budget:\n            return best_x if best_x is not None else clamp(lower)\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x if best_x is not None else clamp(lower)\n\n    x_current = best_x.copy()\n\n    # Local step size setup\n    step = 0.05 * width\n    min_step = 1e-6 * np.maximum(1.0, np.abs(width))\n    max_step = 0.5 * width\n\n    # Mildly adaptive restart strategy\n    no_improve_count = 0\n    long_jump_threshold = 10 if dim <= 10 else 6\n\n    while evals_used < budget:\n        perturb = rng.randn(dim) * step\n        x_candidate = clamp(x_current + perturb)\n        y_candidate = evaluate(x_candidate)\n\n        if y_candidate < best_y:\n            x_current = x_candidate\n            step = np.minimum(step * 1.4, max_step)\n            no_improve_count = 0\n        else:\n            step = np.maximum(step * 0.65, min_step)\n            no_improve_count += 1\n\n            if no_improve_count >= long_jump_threshold and evals_used < budget:\n                # Combine global and local information for escape\n                long_scale = 0.25 if dim <= 10 else 0.2\n                long_step = long_scale * width\n                base = best_x if rng.rand() < 0.7 else (lower + rng.rand(dim) * width)\n                x_try = clamp(base + rng.randn(dim) * long_step)\n                y_try = evaluate(x_try)\n                if y_try < best_y:\n                    x_current = x_try\n                    step = np.maximum(step, 0.1 * width)\n                else:\n                    step = np.maximum(step, 0.04 * width)\n                no_improve_count = 0\n\n    return best_x if best_x is not None else clamp(lower)",
    "X": "-2.9095326484348587 -2.9076034819749714 -2.902074018260961 -2.899827076916035 -2.9041155245403423"
}