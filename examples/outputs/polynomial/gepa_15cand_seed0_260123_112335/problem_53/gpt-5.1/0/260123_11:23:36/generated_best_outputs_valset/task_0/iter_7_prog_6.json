{
    "score": -195.83039882369707,
    "Input": "StyblinskiTang",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Derivative-free optimizer: robust hybrid global + local search with warm start.\n\n    Strategy:\n    1. Space-filling random initialization for global search.\n    2. Robust diagonal-CMA-ES\u2013like global search for ~60\u201380% of budget.\n    3. Strong, adaptive local stochastic search for remaining budget.\n    4. Proper use of prev_best_x to seed search when available.\n    5. Handles low budgets and small/large dimensions robustly.\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    lower = bounds[:, 0]\n    upper = bounds[:, 1]\n    width = upper - lower\n    # Prevent zero-width dimensions from breaking scaling\n    width = np.where(width <= 0, 1.0, width)\n\n    def clamp(x):\n        return np.clip(x, lower, upper)\n\n    evals_used = 0\n    best_x = None\n    best_y = np.inf\n\n    def evaluate(x):\n        nonlocal best_x, best_y, evals_used\n        if evals_used >= budget:\n            return best_y\n        y = objective_function(x)\n        evals_used += 1\n        if best_x is None or y < best_y:\n            best_y = y\n            best_x = x.copy()\n        return y\n\n    if budget <= 0:\n        return clamp(lower)\n\n    # Decide split between global and local search based on dimension & budget\n    if dim <= 5:\n        global_frac = 0.6\n    elif dim <= 20:\n        global_frac = 0.7\n    else:\n        global_frac = 0.8\n\n    # Under tiny budgets, spend relatively more on global exploration\n    if budget < 20:\n        global_frac = 0.8\n\n    global_budget = int(budget * global_frac)\n    global_budget = max(0, min(global_budget, budget - 1))\n    local_budget = budget - global_budget\n\n    # Warm start from previous best if provided\n    if prev_best_x is not None and evals_used < budget:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = clamp(x0)\n            evaluate(x0)\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x if best_x is not None else clamp(lower)\n\n    global_budget = min(global_budget, remaining)\n    local_budget = max(0, budget - evals_used - global_budget)\n\n    # -------------------- Initial global random exploration --------------------\n    if global_budget > 0 and evals_used < budget:\n        # Use up to 20% of global budget (min 3, max 40) for initial exploration\n        init_samples = min(max(3, global_budget // 5), 40, budget - evals_used)\n        if init_samples > 0:\n            # Space-filling sampling via stratified random in each dimension\n            # This is simpler and more robust than the previous pseudo-Sobol attempt.\n            base_grid = (np.arange(init_samples) + 0.5) / init_samples\n            for i in range(init_samples):\n                # Shuffle base grid per dimension with random jitter\n                z = base_grid[i] + (rng.rand(dim) - 0.5) / init_samples\n                z = z % 1.0\n                x = clamp(lower + z * width)\n                evaluate(x)\n                if evals_used >= budget:\n                    break\n\n    if evals_used >= budget:\n        return best_x if best_x is not None else clamp(lower)\n\n    # ----------------- Global evolutionary search (CMA-ES like) ----------------\n    remaining_global = min(global_budget, budget - evals_used)\n    if remaining_global > 0:\n        # Population size\n        if dim > 0:\n            lam = int(4 + np.floor(3 * np.log(dim)))\n        else:\n            lam = 4\n        lam = max(4, lam)\n\n        # Adapt lambda to remaining budget\n        if remaining_global < 4:\n            lam = remaining_global\n        else:\n            lam = min(lam, max(6, remaining_global // 3))\n        lam = max(1, lam)\n\n        mu = max(1, lam // 2)\n\n        # Weights for recombination\n        weights = np.log(np.arange(1, mu + 1) + 0.5)\n        weights = weights[::-1]\n        weights = weights / np.sum(weights)\n        mueff = 1.0 / np.sum(weights ** 2)\n\n        eff_dim = max(dim, 1)\n        cc = (4 + mueff / eff_dim) / (eff_dim + 4 + 2 * mueff / eff_dim)\n        c1 = 2 / ((eff_dim + 1.3) ** 2 + mueff)\n        cmu = min(1 - c1, 2 * (mueff - 2 + 1 / mueff) / ((eff_dim + 2) ** 2 + mueff))\n        damps = 1 + 2 * max(0, np.sqrt((mueff - 1) / (eff_dim + 1)) - 1) + cc\n\n        pc = np.zeros(dim)\n        ps = np.zeros(dim)\n        diagC = np.ones(dim)\n\n        # Initial mean: favor current best_x if available, else domain center\n        if best_x is not None:\n            mean = best_x.copy()\n        else:\n            center = lower + 0.5 * width\n            mean = clamp(center + 0.2 * width * rng.randn(dim))\n\n        # Dimension-aware initial sigma (normalized to width)\n        sigma = 0.3 if dim <= 5 else 0.2\n\n        counteval_start = evals_used\n        chi_n = np.sqrt(eff_dim) * (1 - 1. / (4 * eff_dim) + 1. / (21 * eff_dim ** 2))\n\n        generation = 0\n        # Track stall to adaptively stop CMA-ES early if it stagnates\n        cma_no_improve = 0\n        cma_no_improve_limit = 7\n\n        while evals_used < budget and (evals_used - counteval_start) < remaining_global:\n            remaining_global_inner = remaining_global - (evals_used - counteval_start)\n            if remaining_global_inner <= 0:\n                break\n\n            lam_actual = min(lam, remaining_global_inner)\n            if lam_actual <= 0:\n                break\n\n            # Sample population\n            arz = rng.randn(lam_actual, dim)\n            steps = arz * (sigma * np.sqrt(diagC))\n            arx = mean + steps * width\n            arx = clamp(arx)\n\n            fitness = np.empty(lam_actual)\n            for k in range(lam_actual):\n                if evals_used >= budget:\n                    fitness[k] = np.inf\n                else:\n                    fitness[k] = evaluate(arx[k])\n\n            if lam_actual == 0:\n                break\n\n            idx = np.argsort(fitness)\n            arx = arx[idx]\n            arz = arz[idx]\n            fitness = fitness[idx]\n\n            mu_eff = min(mu, lam_actual)\n            w = weights[:mu_eff]\n            w = w / w.sum()\n            xold = mean.copy()\n            mean = np.sum(arx[:mu_eff] * w[:, None], axis=0)\n\n            # Track whether CMA-ES is still improving globally\n            if best_x is not None and fitness[0] >= best_y - 1e-12:\n                cma_no_improve += 1\n            else:\n                cma_no_improve = 0\n\n            if eff_dim > 0:\n                # Update evolution paths\n                z_mean = np.sum(arz[:mu_eff] * w[:, None], axis=0)\n                ps = (1 - cc) * ps + np.sqrt(cc * (2 - cc) * mueff) * z_mean\n\n                norm_ps = np.linalg.norm(ps)\n                hsig_cond = norm_ps / np.sqrt(1 - (1 - cc) ** (2 * (generation + 1)))\n                hsig = 1 if hsig_cond / chi_n < (1.4 + 2 / (eff_dim + 1)) else 0\n\n                diff = (mean - xold) / (sigma * width + 1e-12)\n                pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * diff\n\n                # Diagonal covariance update\n                delta_hsig = (1 - hsig) * cc * (2 - cc)\n                diagC = (1 + c1 * delta_hsig - c1 - cmu * np.sum(w)) * diagC\n                diagC += c1 * pc ** 2\n                for i in range(mu_eff):\n                    yi = (arx[i] - xold) / (sigma * width + 1e-12)\n                    diagC += cmu * w[i] * yi ** 2\n\n                # Clamp covariance to avoid numerical issues\n                diagC = np.clip(diagC, 1e-10, 1e6)\n\n                # Sigma adaptation\n                sigma = sigma * np.exp((norm_ps / chi_n - 1) * cc / damps)\n                sigma = np.clip(sigma, 1e-4, 1.5)\n\n            generation += 1\n\n            # Early termination if CMA-ES stagnates and we have enough budget for local search\n            if cma_no_improve >= cma_no_improve_limit and (budget - evals_used) > max(10, dim * 2):\n                break\n\n            if evals_used >= budget:\n                break\n\n        # Ensure at least one evaluated point exists\n        if best_x is None and evals_used < budget:\n            x = lower + rng.rand(dim) * width\n            evaluate(x)\n\n    if evals_used >= budget:\n        return best_x if best_x is not None else clamp(lower)\n\n    # ----------------- Local stochastic hill-climbing / search ------------------\n    if best_x is None and evals_used < budget:\n        x = lower + rng.rand(dim) * width\n        evaluate(x)\n        if evals_used >= budget:\n            return best_x if best_x is not None else clamp(lower)\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x if best_x is not None else clamp(lower)\n\n    x_current = best_x.copy()\n\n    # Local step size setup (normalized to domain width)\n    step = 0.05 * width\n    min_step = 1e-6 * np.maximum(1.0, np.abs(width))\n    max_step = 0.5 * width\n\n    # Adaptive restart / long-jump strategy\n    no_improve_count = 0\n    long_jump_threshold = 10 if dim <= 10 else 6\n\n    # Occasionally perform coordinate-wise perturbations for robustness\n    coord_prob = 0.3\n\n    while evals_used < budget:\n        # Decide between full-dimensional and coordinate-wise moves\n        if rng.rand() < coord_prob:\n            # Coordinate-wise move: change a random subset of coordinates\n            mask = rng.rand(dim) < (0.3 if dim > 5 else 0.6)\n            if not np.any(mask):\n                mask[rng.randint(dim)] = True\n            perturb = np.zeros(dim)\n            perturb[mask] = rng.randn(mask.sum()) * step[mask]\n        else:\n            # Full-dimensional Gaussian perturbation\n            perturb = rng.randn(dim) * step\n\n        x_candidate = clamp(x_current + perturb)\n        y_candidate = evaluate(x_candidate)\n\n        if y_candidate < best_y - 1e-12:\n            x_current = x_candidate\n            # Increase step size moderately on improvement\n            step = np.minimum(step * 1.35, max_step)\n            no_improve_count = 0\n        else:\n            # Decrease step size slightly on failure\n            step = np.maximum(step * 0.7, min_step)\n            no_improve_count += 1\n\n            # Occasional long jumps to escape local minima\n            if no_improve_count >= long_jump_threshold and evals_used < budget:\n                long_scale = 0.3 if dim <= 10 else 0.22\n                long_step = long_scale * width\n                base_choice = rng.rand()\n                if base_choice < 0.6:\n                    base = best_x\n                elif base_choice < 0.85:\n                    # Use slightly perturbed center of domain\n                    base = lower + 0.5 * width + 0.1 * width * rng.randn(dim)\n                else:\n                    base = lower + rng.rand(dim) * width\n                x_try = clamp(base + rng.randn(dim) * long_step)\n                y_try = evaluate(x_try)\n                if y_try < best_y - 1e-12:\n                    x_current = x_try\n                    step = np.maximum(step, 0.12 * width)\n                else:\n                    step = np.maximum(step, 0.04 * width)\n                no_improve_count = 0\n\n    return best_x if best_x is not None else clamp(lower)",
    "X": "-2.9046299824305066 -2.900802333670225 -2.902074018260961 -2.899827076916035 -2.9041155245403423"
}