{
    "score": 3.7157236950031005e-28,
    "Input": "Parsopoulos",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Budget-aware black-box minimization using hybrid global + local search.\n\n    Strategy:\n    - Robustly use prev_best_x as a warm start if available and within bounds.\n    - Global exploration via quasi-Latin-hypercube (stratified) sampling.\n    - Then perform covariance-adapted local search (simple CMA-ES style).\n    - Always respect and (nearly) exhaust the evaluation budget.\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low, high = bounds[:, 0], bounds[:, 1]\n    span = high - low\n    span[span == 0.0] = 1.0\n\n    # If no budget, return midpoint\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    evals_used = 0\n\n    # Safe warm start handling\n    best_x = None\n    best_y = None\n\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x0.size == dim:\n            x0 = clip_to_bounds(x0)\n            best_x = x0\n            best_y = objective_function(x0)\n            evals_used += 1\n\n    # If no valid warm start, start from a random point\n    if best_x is None:\n        x0 = rng.uniform(low, high)\n        best_x = x0\n        best_y = objective_function(x0)\n        evals_used += 1\n\n    if evals_used >= budget:\n        return best_x\n\n    remaining = budget - evals_used\n\n    # Split budget: 40% global, 60% local (at least a few global samples)\n    global_evals = max(2, int(0.4 * remaining))\n    global_evals = min(global_evals, remaining)\n    local_evals = remaining - global_evals\n\n    # --- Global exploration: stratified / quasi-LHS sampling ---\n    if global_evals > 0:\n        # Generate stratified samples per dimension\n        # u in [(i)/(n+1), (i+1)/(n+1)], i = 0..n-1\n        n = global_evals\n        base = (np.arange(n) + 1.0) / (n + 1.0)\n        # For each dim, permute strata\n        U = np.empty((n, dim), dtype=float)\n        for d in range(dim):\n            perm = rng.permutation(n)\n            U[:, d] = base[perm]\n        X_global = low + U * (high - low)\n\n        for i in range(n):\n            x = X_global[i]\n            y = objective_function(x)\n            evals_used += 1\n            if y < best_y:\n                best_x, best_y = x, y\n            if evals_used >= budget:\n                return best_x\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # --- Local search: simple covariance-adapting evolution strategy ---\n    # Initialize covariance as diagonal proportional to span^2\n    # Initial global step size relative to box\n    sigma = 0.3\n    # Ensure minimal span to avoid degeneracy\n    eff_span = np.maximum(span, 1e-8)\n    cov = np.diag((eff_span * 0.2) ** 2)\n\n    # Population size and selection parameters depending on budget and dim\n    # small budgets => smaller population\n    lam = max(4, min(10, 2 * dim))  # offspring size\n    mu = max(2, lam // 2)           # number of parents\n\n    # Precompute weights (positive, sum to 1)\n    weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n    weights = weights / np.sum(weights)\n    mu_eff = 1.0 / np.sum(weights**2)\n\n    # Learning rates\n    c_cov = 2.0 / (dim + np.sqrt(2 * dim))  # covariance learning rate\n    c_sigma = 0.3                           # step-size learning rate\n    min_sigma = 0.05                         # lower bound on sigma\n    max_sigma = 2.0                          # upper bound on sigma\n\n    # Use a simple evolution path for sigma adaptation\n    p_sigma = np.zeros(dim)\n\n    # Local search loop\n    while evals_used < budget:\n        # Adjust population size if remaining budget is small\n        remaining = budget - evals_used\n        if remaining <= 0:\n            break\n        cur_lam = min(lam, remaining)\n\n        # Draw offspring\n        # Compute Cholesky if cov is positive definite, else fallback to diag\n        try:\n            A = np.linalg.cholesky(cov)\n        except np.linalg.LinAlgError:\n            A = np.diag(np.sqrt(np.maximum(np.diag(cov), 1e-12)))\n\n        zs = rng.randn(cur_lam, dim)\n        xs = np.empty((cur_lam, dim), dtype=float)\n        ys = np.empty(cur_lam, dtype=float)\n\n        for i in range(cur_lam):\n            step = sigma * (A @ zs[i])\n            cand = clip_to_bounds(best_x + step)\n            xs[i] = cand\n            ys[i] = objective_function(cand)\n            evals_used += 1\n            if ys[i] < best_y:\n                best_x, best_y = cand, ys[i]\n            if evals_used >= budget:\n                break\n\n        if evals_used >= budget:\n            break\n\n        # Select best mu offspring\n        idx = np.argsort(ys)[:mu]\n        x_parents = xs[idx]\n        z_parents = zs[idx]\n\n        # New mean (around which covariance is defined)\n        new_mean = np.sum(weights[:, None] * x_parents, axis=0)\n        # Step in standardized coordinates\n        z_mean = np.sum(weights[:, None] * z_parents, axis=0)\n\n        # Update evolution path for sigma\n        p_sigma = (1 - c_sigma) * p_sigma + np.sqrt(\n            c_sigma * (2 - c_sigma) * mu_eff\n        ) * z_mean\n\n        # Update step size sigma based on path length\n        path_norm = np.linalg.norm(p_sigma) / np.sqrt(dim)\n        sigma *= np.exp(c_sigma * (path_norm - 1.0))\n        sigma = float(np.clip(sigma, min_sigma, max_sigma))\n\n        # Covariance update (rank-mu)\n        # Centered parents relative to new_mean\n        diff = x_parents - new_mean\n        cov_mu = np.zeros_like(cov)\n        for w, d in zip(weights, diff):\n            cov_mu += w * np.outer(d, d)\n\n        cov = (1 - c_cov) * cov + c_cov * cov_mu\n\n        # Move best_x slightly toward new_mean for exploitation\n        best_x = clip_to_bounds(0.5 * best_x + 0.5 * new_mean)\n\n    return best_x",
    "X": "1.5707963267948803 3.141592653589783"
}