{
    "score": -2.0218067697998046,
    "Input": "Adjiman",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid global-local derivative-free optimizer with warm-start support.\n    Uses the full evaluation budget, balancing exploration and exploitation.\n    \"\"\"\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    # avoid degenerate steps but keep awareness of frozen dims\n    frozen_mask = span == 0.0\n    safe_span = span.copy()\n    safe_span[frozen_mask] = 1.0\n\n    rng = np.random.default_rng()\n\n    def clip_to_bounds(x):\n        return np.minimum(np.maximum(x, low), high)\n\n    evals_used = 0\n\n    # Initialize best solution, prefer warm-start if valid and finite\n    if prev_best_x is not None:\n        x_best = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x_best.shape[0] != dim:\n            x_best = rng.uniform(low, high)\n        else:\n            x_best = clip_to_bounds(x_best)\n    else:\n        x_best = rng.uniform(low, high)\n\n    y_best = objective_function(x_best)\n    evals_used += 1\n    if not np.isfinite(y_best):\n        # if warm start bad, re-sample a valid starting point\n        trials = min(5, budget - evals_used)\n        for _ in range(trials):\n            if evals_used >= budget:\n                break\n            cand = rng.uniform(low, high)\n            val = objective_function(cand)\n            evals_used += 1\n            if np.isfinite(val) and (val < y_best or not np.isfinite(y_best)):\n                x_best, y_best = cand, val\n    if evals_used >= budget:\n        return x_best\n\n    remaining = budget - evals_used\n\n    # Split budget: half global, half local (but ensure both get some evals)\n    global_samples = max(5, int(0.5 * remaining))\n    global_samples = min(global_samples, remaining - 1) if remaining > 1 else remaining\n    if global_samples < 0:\n        global_samples = 0\n    local_steps = remaining - global_samples\n\n    # Global exploration: low-discrepancy-like sampling with elitist update\n    if global_samples > 0:\n        batch_size = max(4, min(global_samples, 32))\n        num_batches = (global_samples + batch_size - 1) // batch_size\n\n        for b in range(num_batches):\n            if evals_used >= budget:\n                break\n            this_batch = min(batch_size, global_samples - b * batch_size)\n\n            if dim <= 20:\n                # Latin-hypercube-like stratification via shuffled strata per dim\n                strata = (np.arange(this_batch) + rng.random(this_batch)) / this_batch\n                xs_unit = np.empty((this_batch, dim))\n                for d in range(dim):\n                    rng.shuffle(strata)\n                    xs_unit[:, d] = strata\n                xs = low + xs_unit * span\n            else:\n                xs = rng.uniform(low, high, size=(this_batch, dim))\n\n            for i in range(this_batch):\n                if evals_used >= budget:\n                    break\n                y = objective_function(xs[i])\n                evals_used += 1\n                if np.isfinite(y) and (y < y_best or not np.isfinite(y_best)):\n                    x_best, y_best = xs[i].copy(), y\n\n        if evals_used >= budget:\n            return x_best\n\n    # Recompute remaining for local search\n    remaining = budget - evals_used\n    local_steps = remaining\n    if local_steps <= 0:\n        return x_best\n\n    # Local search: adaptive step-size hill climbing with restarts\n    base_step = 0.15 * safe_span\n\n    # Number of restarts scaled with budget and dimension\n    restarts = max(1, min(8, local_steps // max(4, dim)))\n    steps_per_restart = max(2, local_steps // restarts)\n\n    for r in range(restarts):\n        if evals_used >= budget:\n            break\n\n        # Restart center: biased towards best, with shrinking radius\n        restart_radius = 0.25 * (0.6 ** r) * safe_span\n        center = clip_to_bounds(\n            x_best + rng.normal(0.0, 1.0, size=dim) * restart_radius\n        )\n\n        x_curr = center.copy()\n        y_curr = objective_function(x_curr)\n        evals_used += 1\n        if np.isfinite(y_curr) and (y_curr < y_best):\n            x_best, y_best = x_curr.copy(), y_curr\n        if evals_used >= budget:\n            break\n\n        no_improve = 0\n        step_scale = 1.0\n\n        for t in range(steps_per_restart - 1):\n            if evals_used >= budget:\n                break\n\n            # Exponential decay of step size over time plus adaptive factor\n            frac = 1.0 - (t / max(1, steps_per_restart - 1))\n            step = base_step * (0.05 + 0.95 * frac) * step_scale\n\n            # Zero-out steps for frozen dimensions to avoid useless work\n            step[frozen_mask] = 0.0\n\n            # Symmetric perturbation\n            noise = rng.normal(0.0, 1.0, size=dim)\n            x_candidate = clip_to_bounds(x_curr + noise * step)\n            y_candidate = objective_function(x_candidate)\n            evals_used += 1\n\n            if np.isfinite(y_candidate) and (y_candidate < y_curr):\n                x_curr, y_curr = x_candidate, y_candidate\n                no_improve = 0\n                step_scale *= 1.08\n                if y_curr < y_best:\n                    x_best, y_best = x_curr.copy(), y_curr\n            else:\n                no_improve += 1\n                step_scale *= 0.88\n\n            # If stuck, perform a small random jump around the global best\n            if no_improve >= max(4, dim):\n                jump_radius = 0.12 * (0.6 ** r) * safe_span\n                x_curr = clip_to_bounds(\n                    x_best + rng.normal(0.0, 1.0, size=dim) * jump_radius\n                )\n                y_curr = objective_function(x_curr)\n                evals_used += 1\n                no_improve = 0\n                step_scale = 1.0\n                if np.isfinite(y_curr) and (y_curr < y_best):\n                    x_best, y_best = x_curr.copy(), y_curr\n\n                if evals_used >= budget:\n                    break\n\n    return x_best",
    "X": "2.0 0.10586814318556965"
}