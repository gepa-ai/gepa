{
    "score": -2.0218051048878944,
    "Input": "Adjiman",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid global-local derivative-free optimizer with warm-start support.\n    Uses the full evaluation budget, balancing exploration and exploitation.\n    \"\"\"\n\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config.get(\"dim\", bounds.shape[0]))\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n    span[span == 0.0] = 1.0  # avoid degenerate steps\n\n    rng = np.random.default_rng()\n\n    def clip_to_bounds(x):\n        return np.minimum(np.maximum(x, low), high)\n\n    evals_used = 0\n\n    # Initialize best solution, prefer warm-start if valid\n    if prev_best_x is not None:\n        x_best = np.asarray(prev_best_x, dtype=float).reshape(-1)\n        if x_best.shape[0] != dim:\n            x_best = rng.uniform(low, high)\n        else:\n            x_best = clip_to_bounds(x_best)\n    else:\n        x_best = rng.uniform(low, high)\n\n    y_best = objective_function(x_best)\n    evals_used += 1\n    if evals_used >= budget:\n        return x_best\n\n    remaining = budget - evals_used\n\n    # Split budget: more toward global search for robustness\n    global_samples = max(10, int(0.5 * remaining))\n    global_samples = min(global_samples, remaining)\n    local_steps = remaining - global_samples\n\n    # Global exploration: quasi-random sampling with occasional local refinement\n    if global_samples > 0:\n        # Use simple low-discrepancy-like sampling by stratification on each dim\n        # fall back to pure random for high dim\n        batch_size = max(1, min(global_samples, 32))\n        num_batches = (global_samples + batch_size - 1) // batch_size\n\n        for b in range(num_batches):\n            if evals_used >= budget:\n                break\n            this_batch = min(batch_size, global_samples - b * batch_size)\n\n            if dim <= 10:\n                # Stratified samples in [0,1]^dim scaled to bounds\n                u = (rng.random((this_batch, dim)) + rng.permutation(this_batch)[:, None]) / max(\n                    this_batch, 1\n                )\n                xs = low + u * (high - low)\n            else:\n                xs = rng.uniform(low, high, size=(this_batch, dim))\n\n            ys = []\n            for i in range(this_batch):\n                if evals_used >= budget:\n                    break\n                y = objective_function(xs[i])\n                ys.append(y)\n                evals_used += 1\n                if y < y_best:\n                    x_best, y_best = xs[i].copy(), y\n\n        if evals_used >= budget or local_steps <= 0:\n            return x_best\n\n    # Local search: adaptive step-size hill climbing with restarts\n    remaining = budget - evals_used\n    local_steps = remaining\n    if local_steps <= 0:\n        return x_best\n\n    # Base step: moderate fraction of span\n    base_step = 0.2 * span\n\n    # Multi-start local search: periodically restart around best with smaller radius\n    restarts = max(1, min(5, local_steps // max(5, dim)))\n    steps_per_restart = max(1, local_steps // restarts)\n\n    for r in range(restarts):\n        if evals_used >= budget:\n            break\n\n        # Restart center: best plus small random offset\n        restart_radius = 0.3 * (0.5 ** r) * span\n        center = clip_to_bounds(x_best + rng.normal(0.0, 1.0, size=dim) * restart_radius)\n\n        x_curr = center.copy()\n        y_curr = objective_function(x_curr)\n        evals_used += 1\n        if y_curr < y_best:\n            x_best, y_best = x_curr.copy(), y_curr\n        if evals_used >= budget:\n            break\n\n        # Adaptive steps within this restart\n        no_improve = 0\n        step_scale = 1.0\n\n        for t in range(steps_per_restart - 1):\n            if evals_used >= budget:\n                break\n\n            # Exponential decay of step size over time plus adaptive factor\n            frac = 1.0 - (t / max(1, steps_per_restart - 1))\n            step = base_step * (0.05 + 0.95 * frac) * step_scale\n\n            # Propose symmetric perturbation\n            noise = rng.normal(0.0, 1.0, size=dim)\n            x_candidate = clip_to_bounds(x_curr + noise * step)\n            y_candidate = objective_function(x_candidate)\n            evals_used += 1\n\n            if y_candidate < y_curr:\n                x_curr, y_curr = x_candidate, y_candidate\n                no_improve = 0\n                step_scale *= 1.05  # slightly increase step after success\n                if y_curr < y_best:\n                    x_best, y_best = x_curr.copy(), y_curr\n            else:\n                no_improve += 1\n                step_scale *= 0.9  # shrink after failure\n\n            # If stuck, perform a small random jump around best\n            if no_improve >= max(5, dim):\n                jump_radius = 0.1 * span * (0.5 ** r)\n                x_curr = clip_to_bounds(x_best + rng.normal(0.0, 1.0, size=dim) * jump_radius)\n                y_curr = objective_function(x_curr)\n                evals_used += 1\n                no_improve = 0\n                step_scale = 1.0\n                if y_curr < y_best:\n                    x_best, y_best = x_curr.copy(), y_curr\n\n                if evals_used >= budget:\n                    break\n\n    return x_best",
    "X": "2.0 0.10672568090980178"
}