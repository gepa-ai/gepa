{
    "score": 0.96563309820014,
    "Input": "Xor",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Black-box minimization with a robust hybrid global-local search.\n\n    Strategy (refined):\n    1. Initialization: random + optional warm start (prev_best_x) + small perturbations.\n    2. Budget-aware global search (evolutionary-style) with adaptive mutation and\n       limited per-dimension activation for high dimensions.\n    3. Budget-aware local search around the best solution with adaptive step control.\n    4. Handles small budgets and degenerate bounds robustly.\n    5. More deterministic population sizing and budget apportionment for stability.\n    \"\"\"\n    bounds = np.asarray(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    def clip_to_bounds(x):\n        return np.clip(x, low, high)\n\n    # Handle degenerate / zero budget: return something valid and cheap\n    if budget <= 0:\n        if prev_best_x is not None:\n            try:\n                return clip_to_bounds(np.asarray(prev_best_x, dtype=float).reshape(dim))\n            except Exception:\n                pass\n        return clip_to_bounds((low + high) * 0.5).reshape(dim)\n\n    # Ensure span is nonzero in every dimension for mutation scale;\n    # zero-span dimensions are kept fixed via variable_mask.\n    safe_span = np.where(span > 0, span, 1.0)\n    variable_mask = span > 0\n    n_var = int(variable_mask.sum())\n\n    evals_used = 0\n\n    # ----------------- Initialization -----------------\n    # More stable population sizing: proportional to dim and log(budget).\n    if budget <= 5:\n        pop_size = budget\n    else:\n        # Base population grows with sqrt(dim), but capped by budget and a modest upper bound.\n        base = 4.0 * np.sqrt(max(1, dim))\n        pop_size = int(min(max(4, base), budget // 2, max(8, 5 * dim)))\n        pop_size = max(2, min(pop_size, budget))\n\n    population = []\n\n    # Warm start: insert prev_best_x as a candidate (clipped)\n    warm_added = False\n    if prev_best_x is not None:\n        try:\n            x0 = np.asarray(prev_best_x, dtype=float).reshape(dim)\n            x0 = clip_to_bounds(x0)\n            population.append(x0)\n            warm_added = True\n        except Exception:\n            warm_added = False\n\n    # Add perturbations around warm start when viable\n    if warm_added and budget >= 6 and n_var > 0:\n        n_perturb = min(6, budget - 1, max(1, pop_size // 2))\n        base = population[0]\n        for _ in range(n_perturb):\n            step_scale = 0.05\n            step = np.zeros(dim, dtype=float)\n            if dim <= 10:\n                active = variable_mask\n            else:\n                act_prob = min(1.0, 8.0 / max(1, dim))\n                active = variable_mask & (np.random.rand(dim) < act_prob)\n                if not np.any(active):\n                    idx = np.random.randint(dim)\n                    active[idx] = variable_mask[idx]\n                    if not np.any(active):\n                        active[:] = variable_mask\n            if np.any(active):\n                step[active] = np.random.normal(0.0, step_scale * safe_span[active])\n            x_new = clip_to_bounds(base + step)\n            population.append(x_new)\n\n    # Fill remaining initial population with uniform random samples\n    while len(population) < pop_size:\n        population.append(np.random.uniform(low, high, size=dim))\n\n    population = np.asarray(population, dtype=float)\n\n    # Evaluate initial population\n    fitness = np.full(len(population), np.inf, dtype=float)\n    valid_evals = 0\n    for i, x in enumerate(population):\n        if evals_used >= budget:\n            break\n        try:\n            y = float(objective_function(x))\n        except Exception:\n            y = np.inf\n        fitness[i] = y\n        evals_used += 1\n        if np.isfinite(y):\n            valid_evals += 1\n\n    # If no evaluations were finite, fall back to warm start or center\n    finite_mask = np.isfinite(fitness)\n    if not np.any(finite_mask):\n        if warm_added:\n            return clip_to_bounds(population[0]).reshape(dim)\n        return clip_to_bounds((low + high) * 0.5).reshape(dim)\n\n    best_idx = np.argmin(np.where(finite_mask, fitness, np.inf))\n    best_x = population[best_idx].copy()\n    best_y = fitness[best_idx]\n\n    if evals_used >= budget:\n        return best_x.reshape(dim)\n\n    # ----------------- Budget planning -----------------\n    remaining = budget - evals_used\n\n    # Guarantee at least 1 evaluation for both phases when possible\n    if remaining <= 3:\n        evo_budget = max(0, remaining - 1)\n        local_budget = remaining - evo_budget\n    else:\n        evo_budget = int(0.6 * remaining)\n        evo_budget = max(2, min(evo_budget, remaining - 2))\n        local_budget = remaining - evo_budget\n\n    # ----------------- Evolutionary Global Search -----------------\n    if evo_budget > 0:\n        # Number of elite parents\n        mu = max(2, min(len(population) // 2, 16))\n\n        # Batch size (offspring per generation), modest for more frequent feedback\n        batch_size = max(4, min(24, evo_budget // 2))\n\n        # Initial elites\n        order = np.argsort(fitness)\n        elites = population[order[:mu]].copy()\n        elite_fitness = fitness[order[:mu]].copy()\n\n        evo_evals = 0\n        iter_count = 0\n\n        # Global mutation scale range (relative to span)\n        max_sigma = 0.5\n        min_sigma = 0.03\n\n        while evo_evals < evo_budget and evals_used < budget:\n            t = evo_evals / max(1, evo_budget - 1)\n            sigma = max_sigma * (1.0 - t) + min_sigma * t\n\n            budget_left_for_evo = evo_budget - evo_evals\n            current_batch = min(batch_size, budget_left_for_evo, budget - evals_used)\n            if current_batch <= 0:\n                break\n\n            new_points = np.empty((current_batch, dim), dtype=float)\n\n            for k in range(current_batch):\n                r = np.random.rand()\n                if r < 0.18:\n                    # Pure random exploration\n                    x_new = np.random.uniform(low, high, size=dim)\n                elif r < 0.38 and warm_added:\n                    # Occasionally sample around warm start if provided\n                    base = population[0]\n                    step = np.zeros(dim, dtype=float)\n                    if n_var > 0:\n                        if dim <= 10:\n                            active = variable_mask\n                        else:\n                            act_prob = min(1.0, 6.0 / max(1, dim))\n                            active = variable_mask & (np.random.rand(dim) < act_prob)\n                            if not np.any(active):\n                                idx = np.random.randint(dim)\n                                active[idx] = variable_mask[idx]\n                                if not np.any(active):\n                                    active[:] = variable_mask\n                        if np.any(active):\n                            step[active] = np.random.normal(\n                                0.0, 0.08 * safe_span[active], size=active.sum()\n                            )\n                    x_new = clip_to_bounds(base + step)\n                else:\n                    # Mutate an elite\n                    idx_parent = np.random.randint(mu)\n                    parent = elites[idx_parent]\n\n                    step = np.zeros(dim, dtype=float)\n                    if n_var > 0:\n                        if dim <= 10:\n                            active = variable_mask\n                        else:\n                            act_prob = min(1.0, 8.0 / max(1, dim))\n                            active = variable_mask & (np.random.rand(dim) < act_prob)\n                            if not np.any(active):\n                                idx = np.random.randint(dim)\n                                active[idx] = variable_mask[idx]\n                                if not np.any(active):\n                                    active[:] = variable_mask\n                        if np.any(active):\n                            step[active] = np.random.normal(\n                                0.0, sigma * safe_span[active], size=active.sum()\n                            )\n                    x_new = clip_to_bounds(parent + step)\n                new_points[k] = x_new\n\n            for x_new in new_points:\n                if evals_used >= budget or evo_evals >= evo_budget:\n                    break\n\n                try:\n                    y_new = float(objective_function(x_new))\n                except Exception:\n                    y_new = np.inf\n\n                evals_used += 1\n                evo_evals += 1\n\n                if np.isfinite(y_new) and y_new < best_y:\n                    best_y = y_new\n                    best_x = x_new\n\n                # Replace worst individual if improved\n                worst_idx = np.argmax(np.where(np.isfinite(fitness), fitness, -np.inf))\n                if y_new < fitness[worst_idx]:\n                    population[worst_idx] = x_new\n                    fitness[worst_idx] = y_new\n\n            # Refresh elites periodically\n            if iter_count % 2 == 0 or iter_count == 0:\n                order = np.argsort(fitness)\n                elites = population[order[:mu]].copy()\n                elite_fitness = fitness[order[:mu]].copy()\n            iter_count += 1\n\n    if evals_used >= budget:\n        return best_x.reshape(dim)\n\n    # ----------------- Local Search Around Best -----------------\n    remaining = budget - evals_used\n    local_budget = min(local_budget, remaining)\n\n    if local_budget > 0 and np.isfinite(best_y):\n        # Adaptive random search with shrinking/expanding step size\n        max_scale = 0.25\n        min_scale = 0.002\n        current_scale = min(0.10, max_scale)\n\n        no_improve_streak = 0\n        for i in range(local_budget):\n            if evals_used >= budget:\n                break\n\n            t = i / max(1, local_budget - 1)\n            target_scale = max_scale * (1.0 - t) + min_scale * t\n            current_scale = 0.6 * current_scale + 0.4 * target_scale\n\n            # Escape step when stuck\n            if no_improve_streak >= max(4, dim):\n                step_scale = min(max_scale, current_scale * 3.0)\n                no_improve_streak = 0\n            else:\n                step_scale = current_scale\n\n            step = np.zeros(dim, dtype=float)\n            if n_var > 0:\n                if dim <= 10:\n                    active = variable_mask\n                else:\n                    act_prob = min(1.0, 5.0 / max(1, dim))\n                    active = variable_mask & (np.random.rand(dim) < act_prob)\n                    if not np.any(active):\n                        idx = np.random.randint(dim)\n                        active[idx] = variable_mask[idx]\n                        if not np.any(active):\n                            active[:] = variable_mask\n                if np.any(active):\n                    step[active] = np.random.normal(\n                        0.0, step_scale * safe_span[active], size=active.sum()\n                    )\n\n            x_new = clip_to_bounds(best_x + step)\n            try:\n                y_new = float(objective_function(x_new))\n            except Exception:\n                y_new = np.inf\n\n            evals_used += 1\n\n            if np.isfinite(y_new) and y_new < best_y:\n                best_y = y_new\n                best_x = x_new\n                current_scale *= 1.15\n                current_scale = min(current_scale, max_scale)\n                no_improve_streak = 0\n            else:\n                current_scale *= 0.9\n                current_scale = max(current_scale, min_scale)\n                no_improve_streak += 1\n\n    return best_x.reshape(dim)",
    "X": "1.0 1.0 1.0 -0.9930249957513028 0.5254839829044226 1.0 1.0 -0.998811819682497 -0.09746265959162703"
}