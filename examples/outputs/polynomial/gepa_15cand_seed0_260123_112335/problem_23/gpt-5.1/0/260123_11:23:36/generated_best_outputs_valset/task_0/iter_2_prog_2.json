{
    "score": 3.543341186509189,
    "Input": "McCourt12",
    "Prints": "",
    "Logs": "",
    "Error": "",
    "code": "import numpy as np\n\n\ndef solve(objective_function, config, prev_best_x=None):\n    \"\"\"\n    Hybrid derivative-free optimizer:\n    - Quasi-random (Sobol-like) global exploration\n    - Adaptive local search around current best\n    - Warm-start from prev_best_x when available\n    - Uses full evaluation budget\n    \"\"\"\n    rng = np.random.RandomState()\n\n    bounds = np.array(config[\"bounds\"], dtype=float)\n    dim = int(config[\"dim\"])\n    budget = int(config[\"budget\"])\n\n    low = bounds[:, 0]\n    high = bounds[:, 1]\n    span = high - low\n\n    # Degenerate budget: return center\n    if budget <= 0:\n        return (low + high) / 2.0\n\n    # Helper: clip to bounds\n    def clip_to_bounds(x):\n        return np.minimum(high, np.maximum(low, x))\n\n    # Lightweight Sobol-like sequence using bitwise operations\n    def sobol_sequence(n, d, skip=0):\n        # Precomputed direction numbers for up to 16 dimensions, 32 bits\n        # These are simple, not optimal, but better than pure random\n        V = np.zeros((32, d), dtype=np.uint32)\n        for j in range(d):\n            # Primitive polynomials (simple choices for small dims)\n            if j < 1:\n                poly = 0b11\n            elif j < 2:\n                poly = 0b111\n            elif j < 3:\n                poly = 0b1011\n            else:\n                poly = 0b10011\n            m = [1] * 5  # initial direction numbers\n            for i in range(5):\n                V[i, j] = m[i] << (31 - i)\n            for i in range(5, 32):\n                V[i, j] = V[i - 5, j] ^ (V[i - 5, j] >> 5)\n                for k in range(4):\n                    if (poly >> k) & 1:\n                        V[i, j] ^= V[i - 5 + k, j]\n        x = np.zeros(d, dtype=np.uint32)\n        result = np.empty((n, d), dtype=float)\n        for i in range(skip, skip + n):\n            lsb = (i & -i).bit_length() - 1\n            x ^= V[lsb]\n            result[i - skip] = x / 2.0**32\n        return result\n\n    evals_used = 0\n    best_x = None\n    best_y = None\n\n    # Try warm start\n    if prev_best_x is not None:\n        x0 = np.asarray(prev_best_x, dtype=float)\n        if x0.shape == (dim,):\n            x0 = clip_to_bounds(x0)\n            try:\n                y0 = objective_function(x0)\n                evals_used += 1\n                best_x, best_y = x0, y0\n            except Exception:\n                best_x, best_y = None, None\n\n    # If warm start invalid or not given, start at center of bounds\n    if best_x is None and evals_used < budget:\n        x0 = (low + high) / 2.0\n        try:\n            y0 = objective_function(x0)\n            evals_used += 1\n            best_x, best_y = x0, y0\n        except Exception:\n            best_x, best_y = None, None\n\n    # If still no valid point, do one random evaluation\n    if best_x is None and evals_used < budget:\n        x0 = rng.uniform(low, high)\n        y0 = objective_function(x0)\n        evals_used += 1\n        best_x, best_y = x0, y0\n\n    if best_x is None:\n        # Should be extremely rare: objective always failing\n        return (low + high) / 2.0\n\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Split remaining budget: more to global, some to local\n    global_budget = int(0.6 * remaining)\n    global_budget = max(1, min(global_budget, remaining - 1))\n    local_budget = remaining - global_budget\n\n    # --- Global quasi-random exploration ---\n    # Scale Sobol samples to bounds\n    sobol_pts = sobol_sequence(global_budget, dim, skip=1)\n    global_candidates = low + sobol_pts * span\n\n    for x in global_candidates:\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y, best_x = y, x\n        if evals_used >= budget:\n            return best_x\n\n    # --- Local adaptive search around current best ---\n    remaining = budget - evals_used\n    if remaining <= 0:\n        return best_x\n\n    # Use at least 1 local evaluation\n    local_budget = max(1, min(local_budget, remaining))\n\n    # Adaptive radius: starts relatively large, shrinks\n    base_radius = 0.3\n    min_radius = 1e-4\n    scale = np.where(span > 0, span, 1.0)\n\n    # Occasionally perform coordinate-wise perturbations for robustness\n    for i in range(local_budget):\n        t = i / max(1, local_budget - 1)\n        radius = base_radius * (1.0 - t) + min_radius * t\n\n        if i % 3 == 0 and dim > 1:\n            # Coordinate-wise step\n            x = best_x.copy()\n            idx = rng.randint(0, dim)\n            step = rng.normal(scale=radius) * scale[idx]\n            x[idx] = np.clip(x[idx] + step, low[idx], high[idx])\n        else:\n            noise = rng.normal(loc=0.0, scale=radius, size=dim) * scale\n            x = clip_to_bounds(best_x + noise)\n\n        y = objective_function(x)\n        evals_used += 1\n        if y < best_y:\n            best_y, best_x = y, x\n        if evals_used >= budget:\n            break\n\n    return best_x",
    "X": "0.45025813786970587 0.45020074681729794 0.008632934527262744 1.0 0.38661500923889663 0.3019812491668321 0.6252473782018905"
}