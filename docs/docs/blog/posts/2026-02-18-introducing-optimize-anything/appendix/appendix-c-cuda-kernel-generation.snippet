<span id="appendix-c-cuda-kernel-generation"></span>
??? example "CUDA Kernel Generation"

    We tackle [KernelBench](https://github.com/ScalingIntelligence/KernelBench), a benchmark of PyTorch neural-network operations where the goal is to generate a custom CUDA kernel (via Triton / C++ extensions) that is both correct and faster than the reference PyTorch implementation. A single shared prompt is optimized across multiple problems so that insights from one kernel transfer to others.

    **Candidate** — The seed candidate is a minimal one-line instruction prompt that tells the LLM to generate a CUDA kernel replacement. It provides just enough structure to define the expected output format (a complete Python file using `load_inline`). GEPA will evolve this terse instruction into a detailed prompt packed with CUDA optimization strategies, memory access patterns, and kernel design heuristics.

    ```python
    KERNEL_GEN_PROMPT = """Write a CUDA kernel to replace the given PyTorch model for better performance.
    Output a complete Python file with ModelNew using load_inline. Include all imports."""
    ```

    **Evaluator** — The evaluator compiles the LLM-generated kernel, benchmarks it against a baseline PyTorch implementation for the given problem, and returns rich **Actionable Side Information (ASI)** — the generated code, CUDA documentation consulted during generation, runtime measurements, speedup ratios, correctness status, and detailed error feedback when compilation or validation fails.

    ```python
    def evaluate(candidate, example):
        baseline = baselines[example.problem_id]
        code, cuda_docs, eval_result = run_kernel(
            candidate["kernel_gen_prompt"], example.ref_arch, lm, predictor
        )
        score = compute_score(eval_result, baseline)

        runtime = eval_result.get("PerformanceStatsMean")
        return score, {
            "score": score,
            "problem_id": example.problem_id,
            "level": example.level,
            "baseline_ms": baseline,
            "code": code,
            "cuda_docs": cuda_docs,
            "cuda_docs_post": post_docs,
            "runtime_ms": runtime,
            "speedup": baseline / runtime if runtime else None,
            "compiled_successfully": eval_result.get("CompilationSucceeded", False),
            "ran_without_error": eval_result.get("NoRuntimeErrorDuringCorrectnessCheck", False),
            "output_values_correct": eval_result.get("CorrectnessSucceeded", False),
            "error_type": eval_result.get("ErrorType"),
            "error_detail": eval_result.get("ErrorDetail"),
        }
    ```

    **Optimizer** — This is a Multi-Task Search: a `dataset` of multiple KernelBench problems means insights from optimizing one kernel transfer to others via shared prompt improvements. `RefinerConfig()` enables automatic per-evaluation refinement — after each evaluation, an LLM proposes a refined candidate based on the feedback. `background` is used to inject CUDA best practices and constraints into the optimization loop.

    ```python
    from gepa.optimize_anything import optimize_anything, GEPAConfig, EngineConfig, RefinerConfig

    optimize_anything(
        seed_candidate={"kernel_gen_prompt": KERNEL_GEN_PROMPT},
        evaluator=evaluate,
        dataset=dataset,  # multiple KernelBench problems
        config=GEPAConfig(
            engine=EngineConfig(max_metric_calls=2000, cache_evaluation=True),
            refiner=RefinerConfig(),  # auto-refine after each evaluation
        ),
        objective="Generate an LLM prompt that produces fast, correct CUDA kernels outperforming PyTorch baselines.",
        background=BACKGROUND,
    )
    ```

    **Optimized artifact** — Below is an example kernel generated by the optimized prompt: a depthwise Conv2d CUDA kernel. The optimized prompt guided the LLM to produce kernels with sophisticated CUDA optimization techniques — shared memory for weight caching, specialized fast-paths for common configurations (K=3, stride=1, padding=0), and proper memory coalescing via `__ldg` intrinsics. The kernel uses template-based tile dimensions (TILE_W=32, TILE_H=8) to maximize occupancy and minimize thread divergence. It includes a generic fallback path for arbitrary kernel sizes while maintaining a highly optimized path for the most common depthwise convolution pattern. Safety checks (CUDA error handling, contiguity verification, autograd-awareness) ensure production robustness — the kernel gracefully falls back to PyTorch when conditions aren't met.

    <section class="scrollable-code" markdown>

    ```python
    import math
    from typing import List, Optional

    import torch
    import torch.nn as nn
    from torch.utils.cpp_extension import load_inline

    # Build CUDA extension once at import
    _cpp_src = r"""
    #include <torch/extension.h>

    // Forward declaration only; implementation is in CUDA source.
    torch::Tensor depthwise_conv2d_cuda(torch::Tensor input,
                                        torch::Tensor weight,
                                        c10::optional<torch::Tensor> bias,
                                        int64_t stride,
                                        int64_t padding);
    """

    _cuda_src = r"""
    #include <torch/extension.h>
    #include <ATen/cuda/CUDAContext.h>
    #include <cuda_runtime.h>

    #define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x " must be a CUDA tensor")
    #define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")
    #define CHECK_FLOAT32(x) TORCH_CHECK(x.scalar_type() == at::kFloat, #x " must be float32")

    static inline int div_up_int(int a, int b) { return (a + b - 1) / b; }

    // Generic kernel, supports arbitrary K, stride, padding.
    // Each thread computes one output element for a single (n, c).
    template <int TILE_W, int TILE_H>
    __global__ void depthwise_conv2d_kernel_generic(
        const float* __restrict__ input,
        const float* __restrict__ weight,
        const float* __restrict__ bias,
        float* __restrict__ output,
        int N, int C, int H, int W,
        int K, int stride, int padding,
        int H_out, int W_out,
        int has_bias
    ) {
        extern __shared__ float wshared[]; // size K*K

        const int c = blockIdx.z % C;
        const int n = blockIdx.z / C;

        // Load per-channel weights into shared memory
        int t = threadIdx.y * blockDim.x + threadIdx.x;
        int threads_per_block = blockDim.x * blockDim.y;
        for (int idx = t; idx < K * K; idx += threads_per_block) {
            // weight layout [C,1,K,K] contiguous
            wshared[idx] = weight[c * (K * K) + idx];
        }
        __syncthreads();

        const int w_out = blockIdx.x * TILE_W + threadIdx.x;
        const int h_out = blockIdx.y * TILE_H + threadIdx.y;

        if (n >= N || c >= C || h_out >= H_out || w_out >= W_out) {
            return;
        }

        const int h_in_start = h_out * stride - padding;
        const int w_in_start = w_out * stride - padding;

        float sum = 0.f;

        const int64_t in_base = ((int64_t)n * C + c) * (int64_t)H * (int64_t)W;

        for (int kh = 0; kh < K; ++kh) {
            const int h_in = h_in_start + kh;
            if (h_in < 0 || h_in >= H) continue;

            const int64_t row_base = in_base + (int64_t)h_in * (int64_t)W;

            for (int kw = 0; kw < K; ++kw) {
                const int w_in = w_in_start + kw;
                if (w_in < 0 || w_in >= W) continue;

                const float v = __ldg(&input[row_base + w_in]);
                const float wv = wshared[kh * K + kw];
                sum += v * wv;
            }
        }

        if (has_bias) {
            sum += bias[c];
        }

        const int64_t out_base = ((int64_t)n * C + c) * (int64_t)H_out * (int64_t)W_out;
        output[out_base + (int64_t)h_out * (int64_t)W_out + w_out] = sum;
    }

    // Optimized kernel for K=3, stride=1, padding=0 (common depthwise case).
    template <int TILE_W, int TILE_H>
    __global__ void depthwise_conv2d_kernel_k3_s1_p0(
        const float* __restrict__ input,
        const float* __restrict__ weight,
        const float* __restrict__ bias,
        float* __restrict__ output,
        int N, int C, int H, int W, // H_out = H-2, W_out = W-2
        int has_bias
    ) {
        __shared__ float wshared[9];

        const int c = blockIdx.z % C;
        const int n = blockIdx.z / C;

        // Load 3x3 weights into shared once per block
        int t = threadIdx.y * blockDim.x + threadIdx.x;
        if (t < 9) {
            wshared[t] = weight[c * 9 + t];
        }
        __syncthreads();

        const int W_out = W - 2;
        const int H_out = H - 2;

        const int w_out = blockIdx.x * TILE_W + threadIdx.x;
        const int h_out = blockIdx.y * TILE_H + threadIdx.y;

        if (n >= N || c >= C || h_out >= H_out || w_out >= W_out) {
            return;
        }

        const int64_t in_base = ((int64_t)n * C + c) * (int64_t)H * (int64_t)W;
        const int64_t out_base = ((int64_t)n * C + c) * (int64_t)H_out * (int64_t)W_out;

        // For stride=1, padding=0, input top-left coordinate equals (h_out, w_out)
        const int h_in = h_out;
        const int w_in = w_out;

        const int64_t row0 = in_base + (int64_t)h_in * (int64_t)W + w_in;
        const int64_t row1 = row0 + (int64_t)W;
        const int64_t row2 = row1 + (int64_t)W;

        float v00 = __ldg(&input[row0 + 0]);
        float v01 = __ldg(&input[row0 + 1]);
        float v02 = __ldg(&input[row0 + 2]);

        float v10 = __ldg(&input[row1 + 0]);
        float v11 = __ldg(&input[row1 + 1]);
        float v12 = __ldg(&input[row1 + 2]);

        float v20 = __ldg(&input[row2 + 0]);
        float v21 = __ldg(&input[row2 + 1]);
        float v22 = __ldg(&input[row2 + 2]);

        float sum = 0.f;
        sum += v00 * wshared[0];
        sum += v01 * wshared[1];
        sum += v02 * wshared[2];
        sum += v10 * wshared[3];
        sum += v11 * wshared[4];
        sum += v12 * wshared[5];
        sum += v20 * wshared[6];
        sum += v21 * wshared[7];
        sum += v22 * wshared[8];

        if (has_bias) {
            sum += bias[c];
        }

        output[out_base + (int64_t)h_out * (int64_t)W_out + w_out] = sum;
    }

    torch::Tensor depthwise_conv2d_cuda(torch::Tensor input,
                                        torch::Tensor weight,
                                        c10::optional<torch::Tensor> bias_opt,
                                        int64_t stride,
                                        int64_t padding) {
        CHECK_CUDA(input);
        CHECK_CUDA(weight);
        CHECK_FLOAT32(input);
        CHECK_FLOAT32(weight);
        TORCH_CHECK(input.dim() == 4, "input must be NCHW 4D");
        TORCH_CHECK(weight.dim() == 4, "weight must be [C,1,K,K]");
        TORCH_CHECK(weight.size(1) == 1, "weight.size(1) must be 1 for depthwise");
        TORCH_CHECK(weight.size(2) == weight.size(3), "kernel must be square KxK");
        TORCH_CHECK(input.size(1) == weight.size(0), "in_channels must equal weight.size(0)");

        // Make contiguous copies as needed
        auto input_c = input.contiguous();
        auto weight_c = weight.contiguous();

        const int64_t N = input_c.size(0);
        const int64_t C = input_c.size(1);
        const int64_t H = input_c.size(2);
        const int64_t W = input_c.size(3);
        const int64_t K = weight_c.size(2);

        TORCH_CHECK(stride > 0, "stride must be > 0");
        TORCH_CHECK(padding >= 0, "padding must be >= 0");

        const int64_t H_out = (H + 2 * padding - K) / stride + 1;
        const int64_t W_out = (W + 2 * padding - K) / stride + 1;
        TORCH_CHECK(H_out >= 0 && W_out >= 0, "Invalid output size computed");

        auto out = at::empty({N, C, H_out, W_out}, input.options());

        const float* input_ptr = input_c.data_ptr<float>();
        const float* weight_ptr = weight_c.data_ptr<float>();
        float* out_ptr = out.data_ptr<float>();

        const float* bias_ptr = nullptr;
        int has_bias = 0;
        torch::Tensor bias_c;
        if (bias_opt.has_value()) {
            auto bias = bias_opt.value();
            TORCH_CHECK(bias.is_cuda(), "bias must be CUDA when provided");
            CHECK_FLOAT32(bias);
            TORCH_CHECK(bias.dim() == 1 && bias.size(0) == C, "bias must be shape [C]");
            bias_c = bias.contiguous();
            bias_ptr = bias_c.data_ptr<float>();
            has_bias = 1;
        }

        // Choose launch configuration
        dim3 block(32, 8, 1); // 256 threads, good occupancy
        dim3 grid;
        size_t shared_bytes = 0;

        cudaStream_t stream = at::cuda::getCurrentCUDAStream();

        if (K == 3 && stride == 1 && padding == 0) {
            // Optimized path
            const int Hout = static_cast<int>(H_out);
            const int Wout = static_cast<int>(W_out);

            grid = dim3(div_up_int(Wout, block.x),
                        div_up_int(Hout, block.y),
                        (unsigned int)(N * C));

            depthwise_conv2d_kernel_k3_s1_p0<32, 8><<<grid, block, 0, stream>>>(
                input_ptr, weight_ptr, bias_ptr, out_ptr,
                (int)N, (int)C, (int)H, (int)W,
                has_bias
            );
        } else {
            // Generic path
            const int Hout = static_cast<int>(H_out);
            const int Wout = static_cast<int>(W_out);

            grid = dim3(div_up_int(Wout, block.x),
                        div_up_int(Hout, block.y),
                        (unsigned int)(N * C));

            shared_bytes = (size_t)K * (size_t)K * sizeof(float);

            depthwise_conv2d_kernel_generic<32, 8><<<grid, block, shared_bytes, stream>>>(
                input_ptr, weight_ptr, bias_ptr, out_ptr,
                (int)N, (int)C, (int)H, (int)W,
                (int)K, (int)stride, (int)padding,
                (int)H_out, (int)W_out, has_bias
            );
        }

        // Check for kernel launch errors
        cudaError_t err = cudaGetLastError();
        TORCH_CHECK(err == cudaSuccess, "depthwise_conv2d kernel launch failed: ", cudaGetErrorString(err));

        return out;
    }
    """

    kb_ext = None
    try:
        kb_ext = load_inline(
            name="kb_ext",
            cpp_sources=[_cpp_src],
            cuda_sources=[_cuda_src],
            functions=["depthwise_conv2d_cuda"],
            with_cuda=True,
            extra_cuda_cflags=["-O3", "--use_fast_math"],
            extra_cflags=["-O3"],
            verbose=False,
        )
    except Exception:
        kb_ext = None


    class ModelNew(nn.Module):
        """
        Drop-in replacement for Model that accelerates depthwise Conv2d using a CUDA extension.
        Preserves the same __init__ and forward signatures.
        """

        def __init__(self, in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = False):
            super(ModelNew, self).__init__()
            # Mirror the original layer to preserve parameters/state_dict compatibility
            self.conv2d = nn.Conv2d(
                in_channels,
                in_channels,
                kernel_size,
                stride=stride,
                padding=padding,
                groups=in_channels,
                bias=bias,
            )

        def forward(self, x: torch.Tensor) -> torch.Tensor:
            """
            Performs the depthwise 2D convolution.

            Args:
                x (torch.Tensor): Input tensor of shape (N, C, H, W)

            Returns:
                torch.Tensor: Output tensor of shape (N, C, H_out, W_out)
            """
            # Heuristic and safety fallbacks
            use_ext = (
                kb_ext is not None
                and x.is_cuda
                and x.dtype == torch.float32
                and self.conv2d.weight.is_cuda
                and self.conv2d.weight.dtype == torch.float32
                and isinstance(self.conv2d.kernel_size, tuple)
                and self.conv2d.kernel_size[0] == self.conv2d.kernel_size[1]
                and isinstance(self.conv2d.stride, tuple)
                and self.conv2d.stride[0] == self.conv2d.stride[1]
                and isinstance(self.conv2d.padding, tuple)
                and self.conv2d.padding[0] == self.conv2d.padding[1]
                and not x.requires_grad  # preserve autograd correctness; fall back if grads required
            )

            if use_ext:
                N, C, H, W = x.shape
                K = self.conv2d.kernel_size[0]
                s = self.conv2d.stride[0]
                p = self.conv2d.padding[0]

                # Avoid kernel launch overhead for very small problems
                if min(H, W) < 8 or K < 1 or C < 1:
                    use_ext = False

            if use_ext:
                bias = self.conv2d.bias if self.conv2d.bias is not None else None
                return kb_ext.depthwise_conv2d_cuda(
                    x,
                    self.conv2d.weight,
                    bias,
                    int(self.conv2d.stride[0]),
                    int(self.conv2d.padding[0]),
                )
            else:
                # Fallback to reference PyTorch ops
                return self.conv2d(x)
    ```
    </section>
