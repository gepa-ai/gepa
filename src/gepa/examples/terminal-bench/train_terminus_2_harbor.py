import os
# Fix LiteLLM event loop conflicts - must be BEFORE litellm import
os.environ["LITELLM_LOG"] = "ERROR"

import argparse
import json
from pathlib import Path

import litellm
litellm.suppress_debug_info = True  # Disable async logging
from harbor.agents.terminus_2.terminus_2 import Terminus2
from harbor.registry.client import RegistryClient

from gepa import optimize
from gepa.adapters.terminal_bench_adapter.terminus_2_harbor_adapter import (
    Terminus2HarborAdapter,
    HarborTerminus2Task,
)

TEMPLATE_DIR = Path(__file__).parent / "prompt-templates"

class Terminus2Wrapper(Terminus2):
    """
    Wrapper for Terminus 2 that uses custom prompt templates.
    
    The template file will be dynamically generated by the adapter with the
    instruction prompt baked in.
    """

    def __init__(self, *args, **kwargs):
        kwargs["max_episodes"] = 500
        super().__init__(*args, **kwargs)
    
    def _get_prompt_template_path(self) -> Path:
        """Return the path to the custom prompt template that will be dynamically generated."""
        return Path(__file__).parent / "prompt-templates" / "terminus_2_harbor.txt"


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", type=str, default="gpt-4o-mini")
    parser.add_argument("--n_concurrent", type=int, default=4)
    parser.add_argument("--parser_name", type=str, default="json", choices=["json", "xml"])
    parser.add_argument("--skip_testset", action="store_true", default=False)
    args = parser.parse_args()

    # This is the ONLY part that GEPA will optimize
    # Starting from the base technical instructions from Harbor's template
    initial_instruction_prompt = """IMPORTANT: The text inside "keystrokes" will be used completely verbatim as keystrokes. Write commands exactly as you want them sent to the terminal:
- Most bash commands should end with a newline (\n) to cause them to execute
- For special key sequences, use tmux-style escape sequences:
  - C-c for Ctrl+C
  - C-d for Ctrl+D

The "duration" attribute specifies the number of seconds to wait for the command to complete (default: 1.0) before the next command will be executed. On immediate tasks (e.g., cd, ls, echo, cat) set a duration of 0.1 seconds. On commands (e.g., gcc, find, rustc) set a duration of 1.0 seconds. On slow commands (e.g., make, python3 [long running script], wget [file]) set an appropriate duration as you determine necessary.

It is better to set a smaller duration than a longer duration. It is always possible to wait again if the prior output has not finished, by running {{"keystrokes": "", "duration": 10.0}} on subsequent requests to wait longer. Never wait longer than 60 seconds; prefer to poll to see intermediate result status.

Important notes:
- Each command's keystrokes are sent exactly as written to the terminal
- Do not include extra whitespace before or after the keystrokes unless it's part of the intended command
- Extra text before or after the JSON will generate warnings but be tolerated
- The JSON must be valid - use proper escaping for quotes and special characters within strings
- Commands array can be empty if you want to wait without taking action"""

    # Harbor datasets
    # tb-lite-beta@0.0 has 39 examples: 19 for train, 20 for val
    # terminal-bench@2.0 is the full testset
    
    print("Fetching task IDs from Harbor registry...")
    registry = RegistryClient()
    
    # Get task IDs from tb-lite-beta@0.0
    lite_dataset_items = registry.download_dataset("tb-lite-beta", "0.0", overwrite=False)
    # Extract task names (directory names) from the task IDs
    lite_task_names = [item.id.get_name() for item in lite_dataset_items]
    print(f"Found {len(lite_task_names)} tasks in tb-lite-beta@0.0")
    print(f"Sample task names: {lite_task_names[:3]}")
    
    # Split into train (19) and val (20)
    excluded_task_names = ["spinning-up-rl", "mnist-learning-fix", "hdfs-deployment"]
    trainset = [
        HarborTerminus2Task(task_id=task_name, model_name=args.model_name, parser_name=args.parser_name)
        for task_name in lite_task_names if all(excluded_task_name not in task_name for excluded_task_name in excluded_task_names) # First 19 for training
    ]
    valset = [
        HarborTerminus2Task(task_id=task_name, model_name=args.model_name, parser_name=args.parser_name)
        for task_name in lite_task_names if all(excluded_task_name not in task_name for excluded_task_name in excluded_task_names) # Last 20 for validation
    ]
    
    # Get task IDs from terminal-bench@2.0 for testset
    testset_dataset_items = registry.download_dataset("terminal-bench", "2.0", overwrite=False)
    # Extract task names (directory names) from the task IDs
    testset_task_names = [item.id.get_name() for item in testset_dataset_items]
    print(f"Found {len(testset_task_names)} tasks in terminal-bench@2.0")
    
    testset = [
        HarborTerminus2Task(task_id=task_name, model_name=args.model_name, parser_name=args.parser_name)
        for task_name in testset_task_names
    ]

    reflection_lm_name = "openai/gpt-5"
    reflection_lm = (
        lambda prompt: litellm.completion(
            model=reflection_lm_name,
            messages=[{"role": "user", "content": prompt}],
            reasoning_effort="high",
        )
        .choices[0]
        .message.content
    )

    # Create separate adapters for train/val (tb-lite-beta) and test (terminal-bench)
    adapter_lite = Terminus2HarborAdapter(
        n_concurrent=args.n_concurrent,
        parser_name=args.parser_name,
        agent_import_path="train_terminus_2_harbor:Terminus2Wrapper",
        template_dir=TEMPLATE_DIR,
        default_dataset_name="tb-lite-beta@0.0",
    )
    
    adapter_full = Terminus2HarborAdapter(
        n_concurrent=args.n_concurrent,
        parser_name=args.parser_name,
        agent_import_path="train_terminus_2_harbor:Terminus2Wrapper",
        template_dir=TEMPLATE_DIR,
        default_dataset_name="terminal-bench@2.0",
    )

    # print("=" * 80)
    # print("Evaluating testset WITHOUT custom instruction prompt (baseline)...")
    # print("=" * 80)
    # testset_results_no_prompt = adapter_full.evaluate(
    #     testset, 
    #     {"instruction_prompt": ""}, 
    #     capture_traces=True,
    # )

    print("\n" + "=" * 80)
    print("Evaluating testset BEFORE optimization (with initial instruction prompt)...")
    print("=" * 80)
    if not args.skip_testset:
        testset_results_before_opt = adapter_full.evaluate(
            testset,
            {"instruction_prompt": initial_instruction_prompt},
            capture_traces=True,
            job_name=f"gepa_terminus2_testset_{args.model_name}",
        )
    else:
        testset_results_before_opt = None

    from datetime import datetime
    output_dir = Path(f"gepa_terminus_2_harbor_{args.model_name}_{datetime.now().strftime("%m%d%H%M")}")
    output_dir.mkdir(exist_ok=True)
    

    print("\n" + "=" * 80)
    print("Starting GEPA optimization...")
    print("Optimizing ONLY the instruction prompt component")
    print("Technical specifications (JSON format, keystrokes, etc.) remain fixed")
    print("=" * 80)
    
    optimized_results = optimize(
        seed_candidate={"instruction_prompt": initial_instruction_prompt},
        trainset=trainset,
        valset=valset,
        adapter=adapter_lite,  # Use lite adapter for train/val on tb-lite-beta@0.0
        reflection_lm=reflection_lm,
        use_wandb=True,
        max_metric_calls=400,
        reflection_minibatch_size=3,
        perfect_score=1,
        skip_perfect_score=False,
        run_dir=str(output_dir),
    )

    print("\n" + "=" * 80)
    print("Evaluating testset AFTER optimization...")
    print("=" * 80)
    from datetime import datetime
    testset_results_after_opt = adapter_full.evaluate(  # Use full adapter for test on terminal-bench@2.0
        testset,
        {"instruction_prompt": optimized_results.best_candidate["instruction_prompt"]},
        capture_traces=True,
        job_name=f"gepa_terminus2_testset_optimized_{args.model_name}_{datetime.now().strftime("%Y%m%d%H%M%S")}",
    )

    # Print summary
    print("\n" + "=" * 80)
    print("SUMMARY")
    print("=" * 80)
    # print(f"Testset score (no prompt):      {sum(t['success'] for t in testset_results_no_prompt.trajectories)}/{len(testset)}")
    print(f"Testset score (before opt):     {sum(t['success'] for t in testset_results_before_opt.trajectories)}/{len(testset)}")
    print(f"Testset score (after opt):      {sum(t['success'] for t in testset_results_after_opt.trajectories)}/{len(testset)}")
    print("=" * 80)

