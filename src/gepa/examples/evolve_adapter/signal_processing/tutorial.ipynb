{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Using EvolveAdapter with Signal Processing Example\n",
    "\n",
    "This tutorial demonstrates how to adapt the OpenEvolve signal processing example to work with GEPA's `EvolveAdapter`. The key changes required are providing a batch of training data and modifying your `evaluate` function to accept a **single data instance** and return an `EvaluationResult` for that data instance.\n",
    "\n",
    "> **Note:** This tutorial is also available as a standalone Python script: `tutorial.py`.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Clone the GEPA repository** to get the tutorial files:\n",
    "   ```bash\n",
    "   git clone https://github.com/gepa-ai/gepa.git\n",
    "   cd gepa\n",
    "   ```\n",
    "\n",
    "2. **Install GEPA in editable mode** (required since EvolveAdapter is in a PR branch):\n",
    "   ```bash\n",
    "   pip install -e \".[full]\"\n",
    "   ```\n",
    "\n",
    "3. **Install dependencies and input API key** using the setup cell at the start of **Step 5: Using EvolveAdapter** below.\n",
    "\n",
    "4. **Run this notebook from:** `src/gepa/examples/evolve_adapter/signal_processing/`\n",
    "\n",
    "\n",
    "   This tutorial uses example files located in `src/gepa/examples/evolve_adapter/signal_processing/`:\n",
    "   - `tutorial.ipynb` (this notebook)\n",
    "   - `tutorial.py` (standalone Python script)\n",
    "   - `tutorial_example/` (example OpenEvolve project containing `config.yaml`, `evaluator.py`, and `initial_program.py`)\n",
    "   \n",
    "   The `tutorial_example/` directory contains a complete, working example that demonstrates the required changes.\n",
    "   **Important:** Restart the kernel after installation to ensure packages are loaded correctly.\n",
    "\n",
    "## Example Project: Signal Processing\n",
    "\n",
    "We'll use the **signal processing** example from OpenEvolve that evolves real-time adaptive filtering algorithms for non-stationary time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Key Changes Required\n",
    "\n",
    "Here are the **two main changes** you need to make to your `evaluate` function:\n",
    "\n",
    "### Change 1: Function Signature\n",
    "**Before:**\n",
    "```python\n",
    "def evaluate(program_path: str) -> dict:\n",
    "```\n",
    "\n",
    "**After:**\n",
    "```python\n",
    "def evaluate(program_path: str, data_instance: Any) -> EvaluationResult:\n",
    "```\n",
    "- Add `data_instance` parameter (can be any type - tuple, dict, custom object, etc.)\n",
    "- Return `EvaluationResult` instead of `dict`\n",
    "- For the signal processing example, we use a tuple `(noisy_signal, clean_signal)`, but you can use whatever type best suits your project\n",
    "\n",
    "### Change 2: Process Single Data Instance\n",
    "**Before:** Generated test signals internally and looped over them\n",
    "\n",
    "**After:** Process the single `data_instance` provided. The adapter will call your function for each instance in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Modifying Cascade Evaluation Functions\n",
    "\n",
    "If your project uses cascade evaluation (stage1, stage2), you'll need to modify those functions similarly. See the code cell in **Step 5: Using EvolveAdapter** for examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Defining Training Dataset\n",
    "\n",
    "The training dataset is a list of data instances. Each data instance can be of **any type** (dict, tuple, string, custom object, etc.) - use whatever type best suits your original OpenEvolve project setup.\n",
    "\n",
    "For the signal processing example, each data instance is a tuple `(noisy_signal, clean_signal)`:\n",
    "\n",
    "```python\n",
    "from evaluator import generate_test_signals\n",
    "\n",
    "test_signals = generate_test_signals(5)  # Generate 5 different test signals\n",
    "trainset = [\n",
    "    signal_pair  # Each is a tuple (noisy_signal, clean_signal)\n",
    "    for signal_pair in test_signals\n",
    "]\n",
    "```\n",
    "\n",
    "**Note:** You can use just one data instance in your trainset if that's more analogous to your original OpenEvolve project setup. For example, if your original project evaluated on one fixed signal, using a single data instance mimics that setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/angelahe/Desktop/gepa\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: litellm>=1.64.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from gepa==0.0.24) (1.80.16)\n",
      "Requirement already satisfied: datasets>=2.14.6 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from gepa==0.0.24) (4.5.0)\n",
      "Requirement already satisfied: mlflow>=3.0.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from gepa==0.0.24) (3.8.1)\n",
      "Requirement already satisfied: wandb in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from gepa==0.0.24) (0.24.0)\n",
      "Requirement already satisfied: tqdm>=4.66.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from gepa==0.0.24) (4.67.1)\n",
      "Requirement already satisfied: filelock in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from datasets>=2.14.6->gepa==0.0.24) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from datasets>=2.14.6->gepa==0.0.24) (2.4.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from datasets>=2.14.6->gepa==0.0.24) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from datasets>=2.14.6->gepa==0.0.24) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from datasets>=2.14.6->gepa==0.0.24) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from datasets>=2.14.6->gepa==0.0.24) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from datasets>=2.14.6->gepa==0.0.24) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from datasets>=2.14.6->gepa==0.0.24) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from datasets>=2.14.6->gepa==0.0.24) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.6->gepa==0.0.24) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from datasets>=2.14.6->gepa==0.0.24) (1.3.2)\n",
      "Requirement already satisfied: packaging in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from datasets>=2.14.6->gepa==0.0.24) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from datasets>=2.14.6->gepa==0.0.24) (6.0.3)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm>=1.64.0->gepa==0.0.24) (3.13.3)\n",
      "Requirement already satisfied: click in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm>=1.64.0->gepa==0.0.24) (8.3.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm>=1.64.0->gepa==0.0.24) (0.14.0)\n",
      "Requirement already satisfied: grpcio!=1.68.*,!=1.69.*,!=1.70.*,!=1.71.0,!=1.71.1,!=1.72.0,!=1.72.1,!=1.73.0,>=1.62.3 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm>=1.64.0->gepa==0.0.24) (1.76.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm>=1.64.0->gepa==0.0.24) (8.7.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm>=1.64.0->gepa==0.0.24) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.23.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm>=1.64.0->gepa==0.0.24) (4.26.0)\n",
      "Requirement already satisfied: openai>=2.8.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm>=1.64.0->gepa==0.0.24) (2.15.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm>=1.64.0->gepa==0.0.24) (2.12.5)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm>=1.64.0->gepa==0.0.24) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm>=1.64.0->gepa==0.0.24) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm>=1.64.0->gepa==0.0.24) (0.22.2)\n",
      "Requirement already satisfied: mlflow-skinny==3.8.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow>=3.0.0->gepa==0.0.24) (3.8.1)\n",
      "Requirement already satisfied: mlflow-tracing==3.8.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow>=3.0.0->gepa==0.0.24) (3.8.1)\n",
      "Requirement already satisfied: Flask-CORS<7 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow>=3.0.0->gepa==0.0.24) (6.0.2)\n",
      "Requirement already satisfied: Flask<4 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow>=3.0.0->gepa==0.0.24) (3.1.2)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow>=3.0.0->gepa==0.0.24) (1.18.1)\n",
      "Requirement already satisfied: cryptography<47,>=43.0.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow>=3.0.0->gepa==0.0.24) (46.0.3)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow>=3.0.0->gepa==0.0.24) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow>=3.0.0->gepa==0.0.24) (3.4.3)\n",
      "Requirement already satisfied: gunicorn<24 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow>=3.0.0->gepa==0.0.24) (23.0.0)\n",
      "Requirement already satisfied: huey<3,>=2.5.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow>=3.0.0->gepa==0.0.24) (2.6.0)\n",
      "Requirement already satisfied: matplotlib<4 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow>=3.0.0->gepa==0.0.24) (3.10.8)\n",
      "Requirement already satisfied: scikit-learn<2 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow>=3.0.0->gepa==0.0.24) (1.8.0)\n",
      "Requirement already satisfied: scipy<2 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow>=3.0.0->gepa==0.0.24) (1.17.0)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow>=3.0.0->gepa==0.0.24) (2.0.45)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (6.2.4)\n",
      "Requirement already satisfied: cloudpickle<4 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (3.1.2)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (0.78.0)\n",
      "Requirement already satisfied: fastapi<1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (0.128.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (3.1.46)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (1.39.1)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (6.33.4)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (0.5.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (4.15.0)\n",
      "Requirement already satisfied: uvicorn<1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (0.40.0)\n",
      "Requirement already satisfied: platformdirs in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from wandb->gepa==0.0.24) (4.5.1)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from wandb->gepa==0.0.24) (2.49.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.64.0->gepa==0.0.24) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.64.0->gepa==0.0.24) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.64.0->gepa==0.0.24) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.64.0->gepa==0.0.24) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.64.0->gepa==0.0.24) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.64.0->gepa==0.0.24) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.64.0->gepa==0.0.24) (1.22.0)\n",
      "Requirement already satisfied: Mako in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from alembic!=1.10.0,<2->mlflow>=3.0.0->gepa==0.0.24) (1.3.10)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from cryptography<47,>=43.0.0->mlflow>=3.0.0->gepa==0.0.24) (2.0.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from docker<8,>=4.0.0->mlflow>=3.0.0->gepa==0.0.24) (2.6.3)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from Flask<4->mlflow>=3.0.0->gepa==0.0.24) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from Flask<4->mlflow>=3.0.0->gepa==0.0.24) (2.2.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from Flask<4->mlflow>=3.0.0->gepa==0.0.24) (3.0.3)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from Flask<4->mlflow>=3.0.0->gepa==0.0.24) (3.1.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (4.0.12)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from graphene<4->mlflow>=3.0.0->gepa==0.0.24) (3.2.7)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from graphene<4->mlflow>=3.0.0->gepa==0.0.24) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from graphene<4->mlflow>=3.0.0->gepa==0.0.24) (2.9.0.post0)\n",
      "Requirement already satisfied: anyio in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.14.6->gepa==0.0.24) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.14.6->gepa==0.0.24) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.14.6->gepa==0.0.24) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.14.6->gepa==0.0.24) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.14.6->gepa==0.0.24) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.14.6->gepa==0.0.24) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.14.6->gepa==0.0.24) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.14.6->gepa==0.0.24) (0.21.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm>=1.64.0->gepa==0.0.24) (3.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm>=1.64.0->gepa==0.0.24) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm>=1.64.0->gepa==0.0.24) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm>=1.64.0->gepa==0.0.24) (0.30.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from matplotlib<4->mlflow>=3.0.0->gepa==0.0.24) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from matplotlib<4->mlflow>=3.0.0->gepa==0.0.24) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from matplotlib<4->mlflow>=3.0.0->gepa==0.0.24) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from matplotlib<4->mlflow>=3.0.0->gepa==0.0.24) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from matplotlib<4->mlflow>=3.0.0->gepa==0.0.24) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from matplotlib<4->mlflow>=3.0.0->gepa==0.0.24) (3.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from openai>=2.8.0->litellm>=1.64.0->gepa==0.0.24) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from openai>=2.8.0->litellm>=1.64.0->gepa==0.0.24) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from openai>=2.8.0->litellm>=1.64.0->gepa==0.0.24) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from pandas->datasets>=2.14.6->gepa==0.0.24) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from pandas->datasets>=2.14.6->gepa==0.0.24) (2025.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm>=1.64.0->gepa==0.0.24) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm>=1.64.0->gepa==0.0.24) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm>=1.64.0->gepa==0.0.24) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.14.6->gepa==0.0.24) (3.4.4)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=3.0.0->gepa==0.0.24) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=3.0.0->gepa==0.0.24) (3.6.0)\n",
      "Requirement already satisfied: greenlet>=1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=3.0.0->gepa==0.0.24) (3.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from tiktoken>=0.7.0->litellm>=1.64.0->gepa==0.0.24) (2026.1.15)\n",
      "Requirement already satisfied: pycparser in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography<47,>=43.0.0->mlflow>=3.0.0->gepa==0.0.24) (2.23)\n",
      "Requirement already satisfied: google-auth~=2.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (2.47.0)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (0.0.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (5.0.2)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (0.60b1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow>=3.0.0->gepa==0.0.24) (1.17.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (4.9.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=3.0.0->gepa==0.0.24) (0.6.2)\n",
      "Building wheels for collected packages: gepa\n",
      "  Building editable for gepa (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gepa: filename=gepa-0.0.24-0.editable-py3-none-any.whl size=13500 sha256=d5dc5373f37da000d6e61d2bdff1193c728aba2050e6c8c33ab45e23ae919ab0\n",
      "  Stored in directory: /private/var/folders/l_/glwc3twx7yd30d436jfkk3_00000gn/T/pip-ephem-wheel-cache-d3m6lcjn/wheels/bd/67/b3/b2d0d0c35bae524180c6a7cd56189ab7e2aee9c47a79c049a9\n",
      "Successfully built gepa\n",
      "Installing collected packages: gepa\n",
      "  Attempting uninstall: gepa\n",
      "    Found existing installation: gepa 0.0.24\n",
      "    Uninstalling gepa-0.0.24:\n",
      "      Successfully uninstalled gepa-0.0.24\n",
      "Successfully installed gepa-0.0.24\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install GEPA in editable mode (run from tutorial directory)\n",
    "%pip install -e \"../../../../../[full]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gepa in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (0.0.24)\n",
      "Requirement already satisfied: openevolve in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (0.2.25)\n",
      "Requirement already satisfied: numpy in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: scipy in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (1.17.0)\n",
      "Requirement already satisfied: pyyaml in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (6.0.3)\n",
      "Requirement already satisfied: litellm in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (1.80.16)\n",
      "Requirement already satisfied: openai>=1.0.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from openevolve) (2.15.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from openevolve) (4.67.1)\n",
      "Requirement already satisfied: flask in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from openevolve) (3.1.2)\n",
      "Requirement already satisfied: dacite>=1.9.2 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from openevolve) (1.9.2)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm) (3.13.3)\n",
      "Requirement already satisfied: click in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm) (8.3.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm) (0.14.0)\n",
      "Requirement already satisfied: grpcio!=1.68.*,!=1.69.*,!=1.70.*,!=1.71.0,!=1.71.1,!=1.72.0,!=1.72.1,!=1.73.0,>=1.62.3 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm) (1.76.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm) (8.7.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.23.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm) (4.26.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm) (2.12.5)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from litellm) (0.22.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (1.22.0)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from grpcio!=1.68.*,!=1.69.*,!=1.70.*,!=1.71.0,!=1.71.1,!=1.72.0,!=1.72.1,!=1.73.0,>=1.62.3->litellm) (4.15.0)\n",
      "Requirement already satisfied: anyio in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm) (0.30.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from openai>=1.0.0->openevolve) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from openai>=1.0.0->openevolve) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from openai>=1.0.0->openevolve) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from tiktoken>=0.7.0->litellm) (2026.1.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from tiktoken>=0.7.0->litellm) (2.32.5)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from flask->openevolve) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from flask->openevolve) (2.2.0)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from flask->openevolve) (3.1.5)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from tokenizers->litellm) (1.3.2)\n",
      "Requirement already satisfied: filelock in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (25.0)\n",
      "Requirement already satisfied: shellingham in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (0.21.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/angelahe/Desktop/gepa/myenv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.6.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "%pip install gepa openevolve numpy scipy pyyaml litellm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below and input your API key when prompted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = input(\"API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from gepa import optimize\n",
    "from gepa.adapters.evolve_adapter.evolve_adapter import EvolveAdapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Using EvolveAdapter\n",
    "\n",
    "Now let's set up and run the optimization. The code is split into multiple cells for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your modified OpenEvolve project directory\n",
    "# This should contain: config.yaml, evaluator.py, initial_program.py\n",
    "# For this example, we use the tutorial_example directory in the same folder as this notebook\n",
    "project_path = Path(\"tutorial_example\")\n",
    "\n",
    "# Import signal generation function from evaluator\n",
    "import importlib.util\n",
    "\n",
    "evaluator_path = project_path / \"evaluator.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"evaluator\", evaluator_path)\n",
    "if spec is None or spec.loader is None:\n",
    "    raise ImportError(f\"Could not load evaluator module from {evaluator_path}\")\n",
    "evaluator_module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(evaluator_module)\n",
    "generate_test_signals = evaluator_module.generate_test_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter created for project: tutorial_example\n"
     ]
    }
   ],
   "source": [
    "# Create the adapter\n",
    "adapter = EvolveAdapter(path=project_path)\n",
    "print(f\"Adapter created for project: {project_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training dataset with 5 signal pairs\n",
      "First signal pair: noisy_signal length=500, clean_signal length=500\n"
     ]
    }
   ],
   "source": [
    "# Define training data (signal pairs)\n",
    "# Each data_instance is a tuple (noisy_signal, clean_signal)\n",
    "test_signals = generate_test_signals(5)  # Generate 5 different test signals\n",
    "trainset = [\n",
    "    signal_pair  # Each is a tuple (noisy_signal, clean_signal)\n",
    "    for signal_pair in test_signals\n",
    "]\n",
    "\n",
    "print(f\"Created training dataset with {len(trainset)} signal pairs\")\n",
    "print(f\"First signal pair: noisy_signal length={len(trainset[0][0])}, clean_signal length={len(trainset[0][1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded initial program (6001 characters)\n"
     ]
    }
   ],
   "source": [
    "# Read initial program\n",
    "with open(project_path / \"initial_program.py\") as f:\n",
    "    initial_program = f.read()\n",
    "\n",
    "# Define seed candidate (the program to evolve)\n",
    "seed_candidate = {\"program\": initial_program}\n",
    "\n",
    "print(f\"Loaded initial program ({len(initial_program)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Base program full valset score: 0.39265168271364936 over 5 / 5 examples\n",
      "Iteration 1: Selected program 0 score: 0.39265168271364936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch item 0 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 1 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 2 failed to meet stage 1 threshold\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Proposed new text for program: \"\"\"\n",
      "    Enhanced version with trend preservation using a more sophisticated approach.\n",
      "\n",
      "    This version aims to improve upon the weighted moving average by incorporating\n",
      "    adaptive filtering concepts and potentially multi-scale analysis to better\n",
      "    handle non-stationarity and minimize the multi-objective criteria.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    # --- Advanced Filtering Techniques ---\n",
      "    # We will use a combination of techniques to address the multi-objective optimization.\n",
      "    # For improved slope change minimization, lag error, tracking accuracy, and false reversal penalty,\n",
      "    # a Kalman filter is a good candidate for its ability to model system dynamics and noise.\n",
      "    # We'll also consider a polynomial fit within the window for better trend tracking.\n",
      "\n",
      "    # Initialize Kalman Filter parameters (example for a simple linear model)\n",
      "    # State vector: [position, velocity]\n",
      "    # Measurement: current sample\n",
      "    dt = 1.0  # Time step, assuming samples are equally spaced\n",
      "    # State transition matrix (A)\n",
      "    # x_k+1 = x_k + v_k * dt\n",
      "    # v_k+1 = v_k\n",
      "    A = np.array([[1, dt], [0, 1]])\n",
      "    # Measurement matrix (H)\n",
      "    # z_k = x_k\n",
      "    H = np.array([[1, 0]])\n",
      "    # Process noise covariance (Q) - models uncertainty in the system dynamics\n",
      "    # Assume some uncertainty in velocity\n",
      "    q_pos = 0.01\n",
      "    q_vel = 0.01\n",
      "    Q = np.array([[q_pos, 0], [0, q_vel]]) * (dt**2) # Scale by dt^2 as per some formulations\n",
      "    # Measurement noise covariance (R) - models uncertainty in the sensor reading\n",
      "    R = np.array([[0.1]])  # Assuming measurement noise variance is 0.1\n",
      "\n",
      "    # Initial state estimate and covariance\n",
      "    x_hat = np.array([[x[0]], [0]])  # Initial position and velocity\n",
      "    P = np.array([[1, 0], [0, 1]])  # Initial uncertainty\n",
      "\n",
      "    # Polynomial fitting parameters\n",
      "    poly_degree = 2  # Quadratic fit for trend\n",
      "\n",
      "    for i in range(output_length):\n",
      "        window = x[i : i + window_size]\n",
      "\n",
      "        # --- Kalman Filter Step ---\n",
      "        # Predict\n",
      "        x_hat_minus = A @ x_hat\n",
      "        P_minus = A @ P @ A.T + Q\n",
      "\n",
      "        # Update (using the current sample as measurement)\n",
      "        z = np.array([[window[-1]]])  # Use the latest sample in the window\n",
      "        K = P_minus @ H.T @ np.linalg.inv(H @ P_minus @ H.T + R)\n",
      "        x_hat = x_hat_minus + K @ (z - H @ x_hat_minus)\n",
      "        P = (np.eye(2) - K @ H) @ P_minus\n",
      "\n",
      "        # The filtered output from Kalman filter (position estimate)\n",
      "        kalman_filtered_val = x_hat[0, 0]\n",
      "\n",
      "        # --- Polynomial Fitting for Trend Preservation ---\n",
      "        # Fit a polynomial to the current window to capture the underlying trend.\n",
      "        # We'll use the coefficients to predict the next point or extrapolate.\n",
      "        # For simplicity and to maintain latency, we'll fit to the window and use\n",
      "        # the predicted value from the fit as our output, or blend it.\n",
      "\n",
      "        # Fit a polynomial to the window. x-axis is relative to the window start.\n",
      "        t_window = np.arange(window_size)\n",
      "        coeffs = np.polyfit(t_window, window, poly_degree)\n",
      "        poly_func = np.poly1d(coeffs)\n",
      "\n",
      "        # Predict the value at the end of the window using the polynomial.\n",
      "        # This can be seen as an estimate of the trend.\n",
      "        trend_estimate = poly_func(window_size - 1)\n",
      "\n",
      "        # --- Combining Kalman Filter and Trend Estimate ---\n",
      "        # A simple approach is to take a weighted average.\n",
      "        # The Kalman filter is good at noise reduction and tracking dynamics.\n",
      "        # The polynomial fit is good at capturing longer-term trends.\n",
      "        # We can adjust weights based on signal characteristics or adaptively.\n",
      "        # For now, a fixed weighting can be a starting point.\n",
      "        # Let's give more weight to the Kalman filter for immediate responsiveness\n",
      "        # and the polynomial for trend accuracy.\n",
      "\n",
      "        # A more advanced approach could involve:\n",
      "        # 1. Using the Kalman filter's prediction for the next step.\n",
      "        # 2. Using the polynomial fit to correct the Kalman filter's prediction\n",
      "        #    if the trend deviates significantly from the Kalman's estimate.\n",
      "        # 3. Penalizing false reversals by looking at the second derivative of the\n",
      "        #    filtered signal or comparing slope changes between Kalman and polynomial.\n",
      "\n",
      "        # For this iteration, let's use a weighted combination.\n",
      "        # The Kalman filter's output `kalman_filtered_val` is already a smoothed estimate.\n",
      "        # We can use the polynomial's prediction `trend_estimate` to bias the Kalman output\n",
      "        # or simply average them.\n",
      "        # Let's try a weighted average, giving more weight to the Kalman for responsiveness.\n",
      "\n",
      "        # Blend Kalman output with polynomial trend estimate\n",
      "        # The weights can be tuned. For example, if the signal is very volatile,\n",
      "        # more weight on Kalman. If it's a smoother trend with noise, more on polynomial.\n",
      "        # For now, a simple blend:\n",
      "        weight_kalman = 0.7\n",
      "        weight_poly = 0.3\n",
      "        y[i] = weight_kalman * kalman_filtered_val + weight_poly * trend_estimate\n",
      "\n",
      "        # --- Refinement for False Reversal Penalty ---\n",
      "        # We can check the slope of the current filtered output and compare it\n",
      "        # to the slope of the polynomial fit or a previous filtered point.\n",
      "        # If the Kalman filter's velocity estimate (x_hat[1,0]) is positive but\n",
      "        # the polynomial trend is decreasing, it might indicate a false reversal.\n",
      "        # This is complex to implement in a single pass without lookahead or\n",
      "        # significant state management.\n",
      "\n",
      "        # A simpler approach for false reversal penalty:\n",
      "        # If the current filtered value `y[i]` is significantly different from\n",
      "        # the Kalman prediction `kalman_filtered_val` due to the polynomial blending,\n",
      "        # and the slope changes abruptly, we might want to reduce the influence of\n",
      "        # the polynomial or revert to a more conservative estimate.\n",
      "\n",
      "        # For this evolution, we will rely on the Kalman filter's inherent smoothing\n",
      "        # to reduce spurious slope changes and the polynomial to maintain trend accuracy.\n",
      "        # Further explicit false reversal penalty would require more complex state tracking.\n",
      "\n",
      "        # --- Update Kalman state for the next iteration ---\n",
      "        # The x_hat and P are already updated.\n",
      "\n",
      "    return y\n",
      "Iteration 1: New subsample score 0.0 is not better than old score 1.1606157966177166, skipping\n",
      "Iteration 2: Selected program 0 score: 0.39265168271364936\n",
      "Iteration 2: Proposed new text for program: \"\"\"\n",
      "Real-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n",
      "\n",
      "This algorithm implements a sliding window approach to filter volatile, non-stationary\n",
      "time series data while minimizing noise and preserving signal dynamics.\n",
      "It incorporates advanced techniques for multi-objective optimization.\n",
      "\"\"\"\n",
      "import numpy as np\n",
      "from scipy import signal\n",
      "from collections import deque\n",
      "from scipy.signal import filtfilt, savgol_filter\n",
      "from scipy.interpolate import UnivariateSpline\n",
      "\n",
      "class AdaptiveFilter:\n",
      "    \"\"\"\n",
      "    A class to implement advanced adaptive filtering techniques for non-stationary time series.\n",
      "    It aims to optimize for slope change minimization, lag error minimization,\n",
      "    tracking accuracy, and false reversal penalty.\n",
      "    \"\"\"\n",
      "    def __init__(self, window_size=20, polynomial_order=2, smoothing_factor=0.1, wavelet_level=4):\n",
      "        \"\"\"\n",
      "        Initializes the AdaptiveFilter.\n",
      "\n",
      "        Args:\n",
      "            window_size (int): The size of the sliding window.\n",
      "            polynomial_order (int): The order of the polynomial for trend estimation.\n",
      "            smoothing_factor (float): Controls the smoothness of the spline interpolation.\n",
      "            wavelet_level (int): The level of decomposition for wavelet analysis.\n",
      "        \"\"\"\n",
      "        self.window_size = window_size\n",
      "        self.polynomial_order = polynomial_order\n",
      "        self.smoothing_factor = smoothing_factor\n",
      "        self.wavelet_level = wavelet_level\n",
      "        self.history = deque(maxlen=window_size)\n",
      "        self.filtered_signal = []\n",
      "        self.previous_trend_direction = 0 # 0: unknown, 1: increasing, -1: decreasing\n",
      "\n",
      "    def _estimate_trend(self, window):\n",
      "        \"\"\"\n",
      "        Estimates the local trend within a window using polynomial fitting.\n",
      "        \"\"\"\n",
      "        if len(window) < self.polynomial_order + 1:\n",
      "            return np.mean(window) # Fallback for small windows\n",
      "\n",
      "        t = np.arange(len(window))\n",
      "        coeffs = np.polyfit(t, window, self.polynomial_order)\n",
      "        poly = np.poly1d(coeffs)\n",
      "        return poly(t[-1]) # Estimate at the end of the window\n",
      "\n",
      "    def _smooth_with_spline(self, data):\n",
      "        \"\"\"\n",
      "        Smooths the data using spline interpolation.\n",
      "        \"\"\"\n",
      "        if len(data) < 2:\n",
      "            return data\n",
      "        t = np.arange(len(data))\n",
      "        spl = UnivariateSpline(t, data, s=self.smoothing_factor * len(data))\n",
      "        return spl(t)\n",
      "\n",
      "    def _detect_trend_change(self, current_value, previous_value):\n",
      "        \"\"\"\n",
      "        Detects a significant trend change, penalizing false reversals.\n",
      "        \"\"\"\n",
      "        if previous_value is None:\n",
      "            return 0 # No change if no previous value\n",
      "\n",
      "        current_direction = np.sign(current_value - previous_value)\n",
      "\n",
      "        if self.previous_trend_direction == 0:\n",
      "            self.previous_trend_direction = current_direction\n",
      "            return 0 # Initial direction\n",
      "\n",
      "        if current_direction != self.previous_trend_direction:\n",
      "            # Penalize false reversals: a small change in direction might be noise\n",
      "            # A more robust check would involve looking at the derivative or slope over a longer period.\n",
      "            # For now, we'll consider a change in sign as a potential reversal.\n",
      "            # The penalty is implicitly handled by how we combine metrics later.\n",
      "            self.previous_trend_direction = current_direction\n",
      "            return 1 # Indicate a reversal\n",
      "        return 0\n",
      "\n",
      "    def _predictive_enhancement(self, window):\n",
      "        \"\"\"\n",
      "        Uses polynomial extrapolation for predictive enhancement.\n",
      "        \"\"\"\n",
      "        if len(window) < self.polynomial_order + 1:\n",
      "            return window[-1] if window else 0\n",
      "\n",
      "        t = np.arange(len(window))\n",
      "        coeffs = np.polyfit(t, window, self.polynomial_order)\n",
      "        poly = np.poly1d(coeffs)\n",
      "        # Extrapolate one step ahead\n",
      "        return poly(len(window))\n",
      "\n",
      "    def process(self, x):\n",
      "        \"\"\"\n",
      "        Processes the input signal using an adaptive filtering approach.\n",
      "\n",
      "        Args:\n",
      "            x: Input signal (1D array of real-valued samples)\n",
      "\n",
      "        Returns:\n",
      "            Filtered output signal\n",
      "        \"\"\"\n",
      "        self.filtered_signal = []\n",
      "        self.history.clear()\n",
      "        self.previous_trend_direction = 0\n",
      "\n",
      "        for i, sample in enumerate(x):\n",
      "            self.history.append(sample)\n",
      "            current_window = list(self.history)\n",
      "\n",
      "            if len(current_window) < self.window_size:\n",
      "                # Not enough data to form a full window, append raw sample\n",
      "                self.filtered_signal.append(sample)\n",
      "                continue\n",
      "\n",
      "            # --- Multi-Objective Optimization Strategy ---\n",
      "\n",
      "            # 1. Trend Estimation and Smoothing\n",
      "            # Use Savitzky-Golay filter for smoothing and slope estimation\n",
      "            # This helps in reducing noise while preserving local features.\n",
      "            if len(current_window) >= 5: # Savgol filter needs at least order+1 points\n",
      "                smoothed_window = savgol_filter(current_window, window_length=min(len(current_window), self.window_size), polyorder=self.polynomial_order)\n",
      "                estimated_value = smoothed_window[-1]\n",
      "                slope = np.gradient(smoothed_window)[-1]\n",
      "            else:\n",
      "                estimated_value = np.mean(current_window)\n",
      "                slope = 0 # Cannot estimate slope reliably\n",
      "\n",
      "            # 2. Predictive Enhancement\n",
      "            # Use polynomial extrapolation to predict the next value, which can help reduce lag.\n",
      "            predicted_value = self._predictive_enhancement(current_window)\n",
      "\n",
      "            # 3. Combine estimates: Weighted average of smoothed value and prediction\n",
      "            # The weights can be adapted based on signal characteristics, but for simplicity,\n",
      "            # we'll use a fixed weighting for now. A higher weight on prediction reduces lag.\n",
      "            # A more sophisticated approach would dynamically adjust weights based on signal volatility.\n",
      "            combined_estimate = 0.6 * estimated_value + 0.4 * predicted_value\n",
      "\n",
      "            # 4. Slope Change Minimization and False Reversal Penalty\n",
      "            # We'll use the slope from the smoothed signal to detect trend changes.\n",
      "            # The penalty for false reversals is implicitly handled by the smoothing and\n",
      "            # the fact that we are looking for a consistent change in slope.\n",
      "            # A significant change in slope (e.g., derivative crossing zero) is considered.\n",
      "\n",
      "            # Add the combined estimate to the filtered signal\n",
      "            self.filtered_signal.append(combined_estimate)\n",
      "\n",
      "            # Update previous trend direction for the next iteration\n",
      "            if len(self.filtered_signal) > 1:\n",
      "                self.previous_trend_direction = np.sign(self.filtered_signal[-1] - self.filtered_signal[-2])\n",
      "            else:\n",
      "                self.previous_trend_direction = 0\n",
      "\n",
      "\n",
      "        return np.array(self.filtered_signal)\n",
      "\n",
      "\n",
      "def process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n",
      "    \"\"\"\n",
      "    Main signal processing function that applies the selected algorithm.\n",
      "\n",
      "    Args:\n",
      "        input_signal: Input time series data\n",
      "        window_size: Window size for processing\n",
      "        algorithm_type: Type of algorithm to use (\"basic\" or \"enhanced\")\n",
      "\n",
      "    Returns:\n",
      "        Filtered signal\n",
      "    \"\"\"\n",
      "    if algorithm_type == \"enhanced\":\n",
      "        # Instantiate and use the advanced adaptive filter\n",
      "        # Parameters can be tuned for better performance.\n",
      "        # For now, using defaults.\n",
      "        filter_instance = AdaptiveFilter(window_size=window_size, polynomial_order=2, smoothing_factor=0.05)\n",
      "        return filter_instance.process(input_signal)\n",
      "    else:\n",
      "        # Fallback to basic moving average if needed, though not recommended for this task\n",
      "        if len(input_signal) < window_size:\n",
      "            raise ValueError(f\"Input signal length ({len(input_signal)}) must be >= window_size ({window_size})\")\n",
      "        output_length = len(input_signal) - window_size + 1\n",
      "        y = np.zeros(output_length)\n",
      "        for i in range(output_length):\n",
      "            window = input_signal[i : i + window_size]\n",
      "            y[i] = np.mean(window)\n",
      "        return y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: New subsample score 0.87746349541573 is not better than old score 1.1044409163956326, skipping\n",
      "Iteration 3: Selected program 0 score: 0.39265168271364936\n",
      "Iteration 3: Proposed new text for program: \"\"\"\n",
      "Real-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n",
      "\n",
      "This algorithm implements a sliding window approach to filter volatile, non-stationary\n",
      "time series data while minimizing noise and preserving signal dynamics.\n",
      "It incorporates multi-objective optimization for slope change minimization,\n",
      "lag error minimization, tracking accuracy, and false reversal penalty.\n",
      "Advanced techniques like adaptive filtering (Kalman filters), multi-scale processing\n",
      "(wavelets), and predictive enhancement (polynomial fitting) are considered.\n",
      "\"\"\"\n",
      "import numpy as np\n",
      "from scipy import signal\n",
      "from collections import deque\n",
      "from scipy.signal import savgol_filter\n",
      "\n",
      "# --- Constants for Optimization ---\n",
      "# These are heuristic values and might need tuning based on specific data characteristics.\n",
      "DEFAULT_WINDOW_SIZE = 25\n",
      "SMOOTHING_FACTOR_POLYFIT = 0.05  # Controls the strength of polynomial fitting for prediction\n",
      "KALMAN_PROCESS_NOISE = 0.01       # Process noise covariance for Kalman filter\n",
      "KALMAN_MEASUREMENT_NOISE = 0.1    # Measurement noise covariance for Kalman filter\n",
      "WAVELET_DECOMPOSITION_LEVEL = 2   # Level of wavelet decomposition\n",
      "EMD_MAX_ITERATIONS = 20           # Max iterations for Empirical Mode Decomposition\n",
      "\n",
      "class AdaptiveKalmanFilter:\n",
      "    \"\"\"\n",
      "    An adaptive Kalman filter for tracking non-stationary signals.\n",
      "    It adapts its measurement noise covariance based on signal volatility.\n",
      "    \"\"\"\n",
      "    def __init__(self, initial_state, initial_covariance, process_noise, measurement_noise_initial, dt=1.0):\n",
      "        self.dt = dt\n",
      "        self.state = np.array(initial_state, dtype=np.float64)\n",
      "        self.covariance = np.array(initial_covariance, dtype=np.float64)\n",
      "        self.process_noise = np.array(process_noise, dtype=np.float64)\n",
      "        self.measurement_noise = np.array(measurement_noise_initial, dtype=np.float64)\n",
      "\n",
      "        # State transition matrix (assuming a constant velocity model for simplicity)\n",
      "        # x_k = x_{k-1} + v_{k-1}*dt\n",
      "        # v_k = v_{k-1}\n",
      "        self.state_transition_matrix = np.array([\n",
      "            [1, self.dt],\n",
      "            [0, 1]\n",
      "        ], dtype=np.float64)\n",
      "\n",
      "        # Observation model matrix (we observe the position)\n",
      "        self.observation_model = np.array([[1, 0]], dtype=np.float64)\n",
      "\n",
      "    def predict(self):\n",
      "        \"\"\"Predict the next state.\"\"\"\n",
      "        self.state = self.state_transition_matrix @ self.state\n",
      "        self.covariance = (self.state_transition_matrix @\n",
      "                           self.covariance @\n",
      "                           self.state_transition_matrix.T +\n",
      "                           self.process_noise)\n",
      "        return self.state[0] # Return the predicted position\n",
      "\n",
      "    def update(self, measurement):\n",
      "        \"\"\"Update the state with a new measurement.\"\"\"\n",
      "        # Kalman Gain\n",
      "        innovation_covariance = (self.observation_model @\n",
      "                                 self.covariance @\n",
      "                                 self.observation_model.T +\n",
      "                                 self.measurement_noise)\n",
      "        kalman_gain = (self.covariance @\n",
      "                       self.observation_model.T @\n",
      "                       np.linalg.inv(innovation_covariance))\n",
      "\n",
      "        # Update state\n",
      "        self.state = self.state + kalman_gain @ (measurement - self.observation_model @ self.state)\n",
      "\n",
      "        # Update covariance\n",
      "        self.covariance = (np.eye(self.covariance.shape[0]) -\n",
      "                           kalman_gain @ self.observation_model) @ self.covariance\n",
      "\n",
      "        # Adapt measurement noise based on residual (error between measurement and prediction)\n",
      "        residual = measurement - self.observation_model @ self.state\n",
      "        # A simple adaptation: increase noise if residual is large, decrease if small\n",
      "        # This is a very basic adaptation and can be improved.\n",
      "        self.measurement_noise = np.clip(self.measurement_noise + residual**2 * 0.1, 0.01, 1.0) # Clip to avoid instability\n",
      "\n",
      "        return self.state[0] # Return the updated position\n",
      "\n",
      "    def filter(self, measurements):\n",
      "        \"\"\"Process a sequence of measurements.\"\"\"\n",
      "        filtered_output = []\n",
      "        for measurement in measurements:\n",
      "            predicted_state = self.predict()\n",
      "            updated_state = self.update(np.array([measurement]))\n",
      "            filtered_output.append(updated_state)\n",
      "        return np.array(filtered_output)\n",
      "\n",
      "def polynomial_predictive_filter(x, window_size=20, poly_order=2):\n",
      "    \"\"\"\n",
      "    Filters the signal using polynomial fitting within a sliding window.\n",
      "    This can help in predicting trends and reducing spurious reversals.\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        return np.array([])\n",
      "\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    for i in range(output_length):\n",
      "        window = x[i : i + window_size]\n",
      "        # Fit a polynomial to the window and take the central point as the filtered value.\n",
      "        # Using polyfit with polynomial order 2 (quadratic) for a balance between smoothing and responsiveness.\n",
      "        # The 'rcond' parameter helps with numerical stability.\n",
      "        coeffs = np.polyfit(np.arange(window_size), window, poly_order, rcond=1e-10)\n",
      "        poly = np.poly1d(coeffs)\n",
      "        y[i] = poly(window_size // 2) # Predict the value at the center of the window\n",
      "\n",
      "    return y\n",
      "\n",
      "def enhanced_filter_with_trend_preservation(x, window_size=DEFAULT_WINDOW_SIZE):\n",
      "    \"\"\"\n",
      "    Enhanced version with trend preservation using a combination of techniques.\n",
      "    This version tries to balance noise reduction with responsiveness and accuracy.\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        return np.array([])\n",
      "\n",
      "    # --- Multi-objective approach ---\n",
      "\n",
      "    # 1. Polynomial fitting for initial smoothing and prediction\n",
      "    smoothed_poly = polynomial_predictive_filter(x, window_size=window_size, poly_order=2)\n",
      "    if len(smoothed_poly) == 0:\n",
      "        return np.array([])\n",
      "\n",
      "    # 2. Adaptive Kalman Filter for tracking dynamics\n",
      "    # Initialize Kalman filter with the first few points of the smoothed signal\n",
      "    initial_window_size = min(window_size, len(smoothed_poly) // 2)\n",
      "    if initial_window_size < 2:\n",
      "        initial_window_size = 2 # Ensure at least two points for initial estimation\n",
      "\n",
      "    initial_state_est = np.mean(smoothed_poly[:initial_window_size])\n",
      "    initial_velocity_est = (smoothed_poly[initial_window_size-1] - smoothed_poly[0]) / (initial_window_size-1) if initial_window_size > 1 else 0\n",
      "\n",
      "    # Initial covariance matrix (tuned for initial uncertainty)\n",
      "    initial_covariance = np.array([\n",
      "        [1.0, 0.1],\n",
      "        [0.1, 1.0]\n",
      "    ])\n",
      "\n",
      "    kalman_filter = AdaptiveKalmanFilter(\n",
      "        initial_state=[initial_state_est, initial_velocity_est],\n",
      "        initial_covariance=initial_covariance,\n",
      "        process_noise=KALMAN_PROCESS_NOISE,\n",
      "        measurement_noise_initial=KALMAN_MEASUREMENT_NOISE,\n",
      "        dt=1.0 # Assuming unit time step between samples\n",
      "    )\n",
      "\n",
      "    # Apply Kalman filter to the smoothed signal\n",
      "    # The Kalman filter will process the sequence and provide a more robust estimate.\n",
      "    # We need to re-align the Kalman filter output to match the input windowing.\n",
      "    kalman_filtered_output = []\n",
      "    # The Kalman filter's predict/update cycle inherently handles the sliding window\n",
      "    # by processing each new measurement. We'll align it to the output length.\n",
      "    # We need to re-initialize for each block if we want to strictly adhere to sliding window,\n",
      "    # but for global filtering, a single pass is better.\n",
      "    # For real-time, we'd process one sample at a time. Here, we simulate it.\n",
      "\n",
      "    # To align with the output_length of the original sliding window:\n",
      "    # The Kalman filter's output length will be the same as the input length.\n",
      "    # We need to trim it to match the output_length of the sliding window filter.\n",
      "    # For simplicity, we'll apply Kalman to the entire smoothed signal and then trim.\n",
      "    # A more accurate real-time implementation would update Kalman incrementally.\n",
      "    kalman_full_output = kalman_filter.filter(smoothed_poly)\n",
      "\n",
      "    # The output length of the original filter was len(x) - window_size + 1.\n",
      "    # The kalman_full_output has len(smoothed_poly) which is len(x) - window_size + 1.\n",
      "    # So, the lengths should match if smoothed_poly is used as input to Kalman.\n",
      "    # However, if smoothed_poly is shorter, we need to be careful.\n",
      "    # Let's ensure kalman_full_output is trimmed to the correct length.\n",
      "    output_length = len(x) - window_size + 1\n",
      "    if len(kalman_full_output) > output_length:\n",
      "        kalman_filtered_output = kalman_full_output[:output_length]\n",
      "    else:\n",
      "        kalman_filtered_output = kalman_full_output\n",
      "\n",
      "\n",
      "    # 3. Savitzky-Golay filter for additional smoothing and slope preservation\n",
      "    # Savitzky-Golay filter can preserve signal features better than simple moving average.\n",
      "    # Window length should be odd.\n",
      "    savgol_window = min(window_size if window_size % 2 != 0 else window_size - 1, len(x) - 2)\n",
      "    if savgol_window < 3: # Minimum window size for SavGol\n",
      "        savgol_window = 3\n",
      "    savgol_polyorder = min(2, savgol_window - 1) # Polynomial order for SavGol\n",
      "\n",
      "    # Apply Savitzky-Golay to the original signal (or a slightly smoothed version)\n",
      "    # to capture finer details.\n",
      "    # For real-time, this would be applied to the latest window.\n",
      "    # Here, we apply it to the whole signal and then align.\n",
      "    # The output length of SavGol is len(x) - savgol_window + 1.\n",
      "    # We need to align this with the desired output_length.\n",
      "    # A simpler approach for this evolution is to use it as a final smoothing step.\n",
      "\n",
      "    # Let's combine the Kalman output with a final smoothing step if needed.\n",
      "    # For now, we'll use the Kalman output as the primary filtered signal.\n",
      "    # The polynomial_predictive_filter already provides some smoothing and trend.\n",
      "    # The Kalman filter refines this with dynamic tracking.\n",
      "\n",
      "    # If the Kalman output is still too noisy or shows spurious reversals,\n",
      "    # we could consider a final pass with SavGol or a more aggressive polynomial fit.\n",
      "\n",
      "    # For this iteration, let's use the Kalman output as the primary filter result.\n",
      "    # The polynomial_predictive_filter is implicitly handled by the Kalman's prediction phase.\n",
      "\n",
      "    return kalman_filtered_output\n",
      "\n",
      "\n",
      "def process_signal(input_signal, window_size=DEFAULT_WINDOW_SIZE, algorithm_type=\"enhanced\"):\n",
      "    \"\"\"\n",
      "    Main signal processing function that applies the selected algorithm.\n",
      "\n",
      "    Args:\n",
      "        input_signal: Input time series data\n",
      "        window_size: Window size for processing\n",
      "        algorithm_type: Type of algorithm to use (\"basic\", \"enhanced\", \"kalman\", \"polyfit\")\n",
      "\n",
      "    Returns:\n",
      "        Filtered signal\n",
      "    \"\"\"\n",
      "    if algorithm_type == \"enhanced\":\n",
      "        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n",
      "    elif algorithm_type == \"kalman\":\n",
      "        # For direct Kalman filter usage, we need to initialize it properly\n",
      "        # and process the signal. This requires a different setup than the\n",
      "        # sliding window approach for the other filters.\n",
      "        # For now, let's assume 'enhanced' is the primary target for improvement.\n",
      "        # If 'kalman' is selected, a simpler direct Kalman implementation would be needed.\n",
      "        # For this evolution, we'll focus on enhancing the 'enhanced' path.\n",
      "        # A basic Kalman implementation for demonstration:\n",
      "        if len(input_signal) < 2:\n",
      "            return np.array([])\n",
      "\n",
      "        initial_state_est = np.mean(input_signal[:min(window_size, len(input_signal)//2)])\n",
      "        initial_velocity_est = (input_signal[min(window_size, len(input_signal)//2)-1] - input_signal[0]) / min(window_size, len(input_signal)//2) if min(window_size, len(input_signal)//2) > 1 else 0\n",
      "\n",
      "        kalman_filter = AdaptiveKalmanFilter(\n",
      "            initial_state=[initial_state_est, initial_velocity_est],\n",
      "            initial_covariance=np.array([[1.0, 0.1], [0.1, 1.0]]),\n",
      "            process_noise=KALMAN_PROCESS_NOISE,\n",
      "            measurement_noise_initial=KALMAN_MEASUREMENT_NOISE,\n",
      "            dt=1.0\n",
      "        )\n",
      "        # The filter method processes the entire sequence.\n",
      "        # We need to align its output length to the expected sliding window output.\n",
      "        # The Kalman filter's output length is len(input_signal).\n",
      "        # We need to trim it to len(input_signal) - window_size + 1.\n",
      "        kalman_full_output = kalman_filter.filter(input_signal)\n",
      "        output_length = len(input_signal) - window_size + 1\n",
      "        if len(kalman_full_output) > output_length:\n",
      "            return kalman_full_output[:output_length]\n",
      "        else:\n",
      "            return kalman_full_output # Should not happen if input_signal is long enough\n",
      "\n",
      "    elif algorithm_type == \"polyfit\":\n",
      "        return polynomial_predictive_filter(input_signal, window_size)\n",
      "    else: # Default to basic moving average if not specified or unknown\n",
      "        return adaptive_filter(input_signal, window_size)\n",
      "\n",
      "# The original adaptive_filter and enhanced_filter_with_trend_preservation\n",
      "# are kept for reference or potential fallback, but the core logic is now in\n",
      "# the enhanced_filter_with_trend_preservation function above.\n",
      "# The 'adaptive_filter' function is a simple moving average and can be considered\n",
      "# the 'basic' algorithm.\n",
      "\n",
      "def adaptive_filter(x, window_size=20):\n",
      "    \"\"\"\n",
      "    Simple Moving Average as a baseline.\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        return np.array([])\n",
      "\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    for i in range(output_length):\n",
      "        window = x[i : i + window_size]\n",
      "        y[i] = np.mean(window)\n",
      "    return y\n",
      "\n",
      "# The enhanced_filter_with_trend_preservation function above is the primary improvement.\n",
      "# The original one is commented out to avoid confusion if it's not intended to be used.\n",
      "# def enhanced_filter_with_trend_preservation_original(x, window_size=20):\n",
      "#     \"\"\"\n",
      "#     Original enhanced version with trend preservation using weighted moving average.\n",
      "#     \"\"\"\n",
      "#     if len(x) < window_size:\n",
      "#         return np.array([])\n",
      "#\n",
      "#     output_length = len(x) - window_size + 1\n",
      "#     y = np.zeros(output_length)\n",
      "#\n",
      "#     weights = np.exp(np.linspace(-2, 0, window_size))\n",
      "#     weights = weights / np.sum(weights)\n",
      "#\n",
      "#     for i in range(output_length):\n",
      "#         window = x[i : i + window_size]\n",
      "#         y[i] = np.sum(window * weights)\n",
      "#     return y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3: New subsample score 1.310354316557564 is not better than old score 1.3253188024574332, skipping\n",
      "Iteration 4: Selected program 0 score: 0.39265168271364936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch item 0 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 1 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 2 failed to meet stage 1 threshold\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4: Proposed new text for program: \"\"\"\n",
      "    Enhanced adaptive signal processing algorithm for non-stationary time series.\n",
      "\n",
      "    This algorithm aims to balance noise reduction with signal dynamics preservation\n",
      "    by employing a multi-objective optimization approach. It considers:\n",
      "    1. Slope change minimization: Reducing spurious directional reversals.\n",
      "    2. Lag error minimization: Maintaining responsiveness.\n",
      "    3. Tracking accuracy: Preserving genuine signal trends.\n",
      "    4. False reversal penalty: Avoiding noise-induced trend changes.\n",
      "\n",
      "    Advanced techniques considered for integration:\n",
      "    - Adaptive filtering (Kalman filters, particle filters)\n",
      "    - Multi-scale processing (wavelets, EMD)\n",
      "    - Predictive enhancement (polynomial fitting, neural networks)\n",
      "    - Trend detection methods\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window (W samples)\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal with length = len(x) - window_size + 1\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    # --- Proposed improvements ---\n",
      "    # The current weighted moving average is a good starting point but lacks\n",
      "    # sophisticated adaptation. We'll introduce a Kalman filter for better\n",
      "    # state estimation and tracking of non-stationary signals.\n",
      "\n",
      "    # Kalman Filter parameters (tuned for a general case, can be further optimized)\n",
      "    # State is [position, velocity]\n",
      "    dt = 1.0  # Time step (assumed constant for simplicity)\n",
      "    # Process noise covariance (Q): reflects uncertainty in the state transition\n",
      "    # Higher values allow faster adaptation to changing dynamics.\n",
      "    q_pos = 0.01  # Process noise for position\n",
      "    q_vel = 0.005 # Process noise for velocity\n",
      "    Q = np.array([[q_pos * dt**3 / 3, q_pos * dt**2 / 2],\n",
      "                  [q_pos * dt**2 / 2, q_pos * dt]])\n",
      "\n",
      "    # Measurement noise covariance (R): reflects uncertainty in the measurement\n",
      "    # This is related to the input signal's noise level.\n",
      "    R = np.array([[np.var(x) * 0.1]]) # Heuristic based on input noise\n",
      "\n",
      "    # Measurement matrix (H): relates state to measurement\n",
      "    H = np.array([[1, 0]])\n",
      "\n",
      "    # Initial state estimate (x_hat) and covariance (P)\n",
      "    # Assuming initial position is the first measurement and initial velocity is 0.\n",
      "    x_hat = np.array([[x[0]], [0.0]])\n",
      "    P = np.array([[1.0, 0.0], [0.0, 1.0]]) # Initial uncertainty\n",
      "\n",
      "    # Store filtered states\n",
      "    filtered_states = np.zeros((output_length, 2, 1))\n",
      "\n",
      "    for i in range(output_length):\n",
      "        # Predict step\n",
      "        # A = np.array([[1, dt], [0, 1]]) # State transition matrix\n",
      "        # x_hat_pred = A @ x_hat\n",
      "        # P_pred = A @ P @ A.T + Q\n",
      "\n",
      "        # For simplicity in this implementation, we'll use a simpler prediction\n",
      "        # assuming constant velocity and update it. A more robust Kalman filter\n",
      "        # would explicitly use the state transition matrix.\n",
      "        # For a simple position-velocity model:\n",
      "        # x_hat[0] = x_hat[0] + dt * x_hat[1] # Predict position\n",
      "        # P[0,0] += dt * (P[0,1] + P[1,0]) + dt**2 * P[1,1]\n",
      "        # P[0,1] += dt * P[1,1]\n",
      "        # P[1,0] += dt * P[1,1]\n",
      "        # P[1,1] += Q[1,1] # Assuming Q[1,1] is the process noise for velocity\n",
      "\n",
      "        # A more direct implementation of the standard Kalman filter equations:\n",
      "        # State transition matrix (A) for position-velocity model\n",
      "        A = np.array([[1, dt], [0, 1]])\n",
      "        x_hat_pred = A @ x_hat\n",
      "        P_pred = A @ P @ A.T + Q\n",
      "\n",
      "        # Update step\n",
      "        z = np.array([[x[i + window_size - 1]]]) # Current measurement (last element of window)\n",
      "        y_pred = H @ x_hat_pred\n",
      "        S = H @ P_pred @ H.T + R\n",
      "        K = P_pred @ H.T @ np.linalg.inv(S) # Kalman Gain\n",
      "\n",
      "        x_hat = x_hat_pred + K @ (z - y_pred)\n",
      "        P = (np.eye(P.shape[0]) - K @ H) @ P_pred\n",
      "\n",
      "        # The filtered output is the estimated position\n",
      "        y[i] = x_hat[0, 0]\n",
      "        filtered_states[i] = x_hat\n",
      "\n",
      "    # The current weighted moving average uses fixed exponential weights.\n",
      "    # The Kalman filter provides a more adaptive approach to track dynamics.\n",
      "    # For further improvement, one could consider:\n",
      "    # - Adaptive Q and R based on signal characteristics or error analysis.\n",
      "    # - Using a polynomial fit within the window for a more robust trend estimate,\n",
      "    #   and then using the Kalman filter to track deviations from this trend.\n",
      "    # - Wavelet decomposition for multi-scale analysis to handle different\n",
      "    #   frequency components of the non-stationarity.\n",
      "    # - Incorporating a penalty for large slope changes to discourage false reversals.\n",
      "\n",
      "    # Example of a simple slope change penalty (can be integrated into a cost function\n",
      "    # for more advanced optimization or directly into the filter's update logic if\n",
      "    # designed differently). For this direct implementation, we'll rely on the\n",
      "    # Kalman filter's inherent ability to track dynamics.\n",
      "\n",
      "    return y\n",
      "Iteration 4: New subsample score 0.0 is not better than old score 0.9397379105559157, skipping\n",
      "Iteration 5: Selected program 0 score: 0.39265168271364936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch item 0 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 1 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 2 failed to meet stage 1 threshold\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5: Proposed new text for program: \"\"\"\n",
      "    Improved adaptive signal processing algorithm focusing on multi-objective optimization.\n",
      "\n",
      "    This version incorporates advanced techniques to address the trade-offs between\n",
      "    noise reduction, signal dynamics preservation, lag error, and false reversal penalty.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window (W samples)\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal with length = len(x) - window_size + 1\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    # --- Multi-Objective Optimization Strategy ---\n",
      "    # We will use a combination of techniques:\n",
      "    # 1. Adaptive Filtering (Kalman Filter): For optimal state estimation and noise reduction\n",
      "    #    in a non-stationary environment.\n",
      "    # 2. Polynomial Fitting within the window: To capture local trends and reduce spurious\n",
      "    #    slope changes.\n",
      "    # 3. Trend Detection and Adjustment: To penalize false reversals and maintain tracking accuracy.\n",
      "\n",
      "    # Kalman Filter parameters (can be tuned)\n",
      "    # State transition matrix (assuming constant velocity model for simplicity,\n",
      "    # but can be adapted for more complex dynamics)\n",
      "    dt = 1.0  # Time step, assumed to be 1 for discrete samples\n",
      "    A = np.array([[1, dt], [0, 1]])\n",
      "    # Measurement matrix\n",
      "    H = np.array([[1, 0]])\n",
      "    # Process noise covariance (tune based on expected signal non-stationarity)\n",
      "    Q_std = 0.1\n",
      "    Q = np.diag([Q_std**2, Q_std**2])\n",
      "    # Measurement noise covariance (tune based on expected sensor noise)\n",
      "    R_std = 0.5  # Higher R for more noise\n",
      "    R = np.array([[R_std**2]])\n",
      "\n",
      "    # Initial state estimate (mean and covariance)\n",
      "    x_hat = np.zeros((2, 1))  # [position, velocity]\n",
      "    P = np.diag([1.0, 1.0])\n",
      "\n",
      "    # Polynomial fitting parameters\n",
      "    poly_order = 2  # Quadratic fit to capture local trends\n",
      "\n",
      "    # Trend change detection parameters\n",
      "    slope_threshold = 0.1  # Threshold for detecting significant slope change\n",
      "    false_reversal_penalty_factor = 5  # Factor to penalize false reversals\n",
      "\n",
      "    for i in range(output_length):\n",
      "        window_data = x[i : i + window_size]\n",
      "\n",
      "        # --- Kalman Filter Step ---\n",
      "        # Prediction\n",
      "        x_hat_minus = A @ x_hat\n",
      "        P_minus = A @ P @ A.T + Q\n",
      "\n",
      "        # Update (using the current sample as measurement)\n",
      "        # If window is too small for Kalman, fall back to weighted average or mean\n",
      "        if len(window_data) > 0:\n",
      "            z = np.array([[window_data[-1]]])  # Last sample in the window\n",
      "            K = P_minus @ H.T @ np.linalg.inv(H @ P_minus @ H.T + R)\n",
      "            x_hat = x_hat_minus + K @ (z - H @ x_hat_minus)\n",
      "            P = (np.identity(2) - K @ H) @ P_minus\n",
      "\n",
      "        # --- Polynomial Fitting for Trend Preservation ---\n",
      "        # Fit a polynomial to the current window to estimate the underlying trend\n",
      "        # Use indices relative to the start of the window for fitting\n",
      "        window_indices = np.arange(window_size)\n",
      "        try:\n",
      "            coeffs = np.polyfit(window_indices, window_data, poly_order)\n",
      "            poly_trend = np.polyval(coeffs, window_indices[-1]) # Trend at the end of the window\n",
      "        except np.linalg.LinAlgError:\n",
      "            # Fallback if fitting fails (e.g., all window values are the same)\n",
      "            poly_trend = np.mean(window_data)\n",
      "\n",
      "        # --- Combining Kalman and Polynomial Trend ---\n",
      "        # The Kalman filter provides a smoothed estimate, while the polynomial fit\n",
      "        # helps preserve local trends. We can combine them.\n",
      "        # A simple approach is to use the Kalman filter's position estimate,\n",
      "        # but adjust it based on the polynomial trend to reduce lag and improve accuracy.\n",
      "        # The Kalman filter's velocity estimate can also inform the polynomial fit.\n",
      "\n",
      "        # Let's use the Kalman filter's position estimate as the primary output,\n",
      "        # but incorporate the polynomial trend to address the multi-objective goals.\n",
      "        # The Kalman state `x_hat[0, 0]` is the filtered position.\n",
      "\n",
      "        # To address slope change and false reversal penalty, we can look at the\n",
      "        # estimated velocity from Kalman and the derivative of the polynomial.\n",
      "        kalman_velocity = x_hat[1, 0]\n",
      "        poly_derivative_at_end = np.polyval(np.polyder(coeffs), window_indices[-1])\n",
      "\n",
      "        # We want to penalize large differences between Kalman velocity and polynomial derivative\n",
      "        # and also penalize significant changes in the Kalman filter's velocity estimate itself.\n",
      "\n",
      "        # A more sophisticated approach would be to use a state-space model that\n",
      "        # explicitly models trend changes or uses a combined state.\n",
      "        # For now, let's use a heuristic to adjust the output.\n",
      "\n",
      "        # The Kalman filter's prediction `x_hat_minus[0, 0]` is a good candidate for the output\n",
      "        # as it has less lag than the updated `x_hat[0, 0]` from the current sample.\n",
      "        # However, this can increase lag.\n",
      "\n",
      "        # Let's try to balance lag and accuracy:\n",
      "        # Use the Kalman prediction, but adjust it based on the polynomial trend.\n",
      "        # The polynomial trend at the end of the window `poly_trend` represents the\n",
      "        # expected value based on local dynamics.\n",
      "\n",
      "        # A simple way to reduce lag is to use the predicted state, but this can\n",
      "        # increase noise if the process noise is too high.\n",
      "        # For this iteration, let's stick with the updated Kalman state for now\n",
      "        # and focus on incorporating the polynomial trend more directly.\n",
      "\n",
      "        # Consider the current Kalman state `x_hat[0, 0]` as the base filtered value.\n",
      "        filtered_value = x_hat[0, 0]\n",
      "\n",
      "        # --- Incorporating Trend Preservation and False Reversal Penalty ---\n",
      "        # We can use the polynomial fit to guide the Kalman filter or adjust its output.\n",
      "        # One way is to use the polynomial trend as a \"correction\" to the Kalman estimate,\n",
      "        # especially when the Kalman filter might be lagging or overreacting.\n",
      "\n",
      "        # If the polynomial trend is significantly different from the Kalman estimate,\n",
      "        # it might indicate a genuine trend change that the Kalman filter hasn't caught up to,\n",
      "        # or a spurious jump in the raw data.\n",
      "\n",
      "        # Let's consider the difference between the polynomial trend at the end of the window\n",
      "        # and the Kalman estimate.\n",
      "        trend_diff = poly_trend - filtered_value\n",
      "\n",
      "        # To penalize false reversals, we can check if the sign of the Kalman velocity\n",
      "        # is opposite to the sign of the polynomial derivative.\n",
      "        # If they are opposite and significant, it might be a false reversal.\n",
      "\n",
      "        # A simpler approach for now: use the polynomial trend to \"pull\" the Kalman estimate\n",
      "        # towards the local trend, reducing lag and improving accuracy.\n",
      "        # The `trend_diff` can be seen as an error signal. We can use a portion of it\n",
      "        # to adjust the Kalman estimate.\n",
      "\n",
      "        # Let's use a weighted average of the Kalman estimate and the polynomial trend.\n",
      "        # The weights can adapt or be fixed. For simplicity, let's use a fixed weight\n",
      "        # that emphasizes the Kalman estimate but incorporates the polynomial trend.\n",
      "        # A higher weight on `trend_diff` will increase responsiveness but might\n",
      "        # introduce more noise.\n",
      "\n",
      "        # We need to be careful not to over-correct and introduce oscillations.\n",
      "        # Let's use a fraction of the `trend_diff` to adjust the Kalman output.\n",
      "        # The `false_reversal_penalty_factor` can be used to dampen the effect of\n",
      "        # the `trend_diff` if it suggests a reversal that's not strongly supported.\n",
      "\n",
      "        # A more direct approach for slope change and false reversal penalty:\n",
      "        # Compare the sign of the Kalman velocity with the sign of the polynomial derivative.\n",
      "        # If `kalman_velocity` and `poly_derivative_at_end` have opposite signs and\n",
      "        # their magnitudes are above a certain threshold, it's a potential false reversal.\n",
      "        # In such cases, we might want to suppress the change or rely more on the\n",
      "        # smoothed Kalman estimate.\n",
      "\n",
      "        # For this iteration, let's focus on a simpler integration:\n",
      "        # The Kalman filter provides a smoothed estimate. We'll use its output `x_hat[0, 0]`.\n",
      "        # To improve trend preservation and reduce lag, we can consider the polynomial\n",
      "        # fit's prediction for the *next* point, and if it's significantly different\n",
      "        # from the current Kalman estimate, it suggests a need for adjustment.\n",
      "\n",
      "        # Let's try a simple approach: use the Kalman prediction `x_hat_minus[0,0]`\n",
      "        # as the output, as it inherently has less lag.\n",
      "        # However, this can be noisy if Q is large.\n",
      "        # Let's refine this by using the updated Kalman state but trying to\n",
      "        # anticipate changes using the polynomial fit.\n",
      "\n",
      "        # Consider the polynomial fit's prediction for the *current* time step based on\n",
      "        # the data *up to* the previous step. This is `poly_trend`.\n",
      "        # The Kalman filter's updated state `x_hat[0,0]` is the smoothed estimate.\n",
      "\n",
      "        # Let's combine them:\n",
      "        # The output `y[i]` will be a blend.\n",
      "        # A simple blend: `y[i] = alpha * x_hat[0,0] + (1-alpha) * poly_trend`\n",
      "        # The choice of `alpha` is crucial. A higher `alpha` means more reliance on Kalman.\n",
      "        # A lower `alpha` means more responsiveness to local trends.\n",
      "\n",
      "        # To tackle false reversals, we can check the consistency between Kalman velocity\n",
      "        # and polynomial derivative.\n",
      "        # If `kalman_velocity` and `poly_derivative_at_end` have opposite signs and\n",
      "        # their magnitudes are significant, we might reduce the influence of the\n",
      "        # `poly_trend` or `trend_diff`.\n",
      "\n",
      "        # Let's try a more direct approach for the output:\n",
      "        # The Kalman filter's prediction `x_hat_minus[0,0]` is a good starting point\n",
      "        # for reduced lag.\n",
      "        # We can then use the polynomial fit to correct this prediction,\n",
      "        # especially to preserve trends and penalize false reversals.\n",
      "\n",
      "        # Let's refine the Kalman state update to incorporate the polynomial trend.\n",
      "        # Instead of just using `z`, we can use a \"corrected\" measurement.\n",
      "        # The corrected measurement `z_corr` could be the polynomial trend at the end of the window.\n",
      "        # This would make the Kalman filter more responsive to the local trend.\n",
      "\n",
      "        # Let's try a simpler approach first: use the Kalman filter's output,\n",
      "        # and then apply a post-processing step or modify the update.\n",
      "\n",
      "        # Using `x_hat_minus[0,0]` as the output directly:\n",
      "        # This aims to reduce lag.\n",
      "        # y[i] = x_hat_minus[0, 0]\n",
      "\n",
      "        # To improve accuracy and reduce false reversals, we can use the polynomial fit.\n",
      "        # If the polynomial fit suggests a strong trend that the Kalman filter is missing,\n",
      "        # we can adjust.\n",
      "\n",
      "        # Let's consider the difference between the polynomial trend and the Kalman prediction.\n",
      "        # `poly_trend` is the polynomial's value at the end of the window.\n",
      "        # `x_hat_minus[0,0]` is the Kalman filter's prediction for the current state.\n",
      "\n",
      "        # If `abs(poly_trend - x_hat_minus[0,0])` is large, it might indicate a\n",
      "        # need for adjustment.\n",
      "\n",
      "        # To penalize false reversals, we can check the consistency of directions.\n",
      "        # If Kalman velocity is positive and polynomial derivative is negative (and vice-versa),\n",
      "        # and magnitudes are significant, we should be cautious.\n",
      "\n",
      "        # Let's try a hybrid approach for the output `y[i]`:\n",
      "        # We'll use the Kalman filter's prediction `x_hat_minus[0, 0]` as a base.\n",
      "        # Then, we'll adjust it based on the polynomial trend to improve accuracy\n",
      "        # and responsiveness.\n",
      "\n",
      "        # The polynomial fit's prediction for the *current* point is `poly_trend`.\n",
      "        # The Kalman filter's prediction for the *current* point is `x_hat_minus[0, 0]`.\n",
      "\n",
      "        # Let's use `x_hat_minus[0, 0]` as the primary output and then\n",
      "        # apply a correction based on the polynomial trend.\n",
      "        # The correction should aim to preserve the trend and avoid false reversals.\n",
      "\n",
      "        # A potential correction term: `correction = beta * (poly_trend - x_hat_minus[0,0])`\n",
      "        # Where `beta` is a tuning parameter.\n",
      "        # If `beta` is too high, it can lead to oscillations.\n",
      "\n",
      "        # For false reversal penalty, we can modulate `beta` based on the\n",
      "        # consistency of the estimated velocities.\n",
      "        # If `kalman_velocity` and `poly_derivative_at_end` have opposite signs,\n",
      "        # we can reduce `beta` or even make it negative if `poly_trend` suggests\n",
      "        # a reversal that the Kalman filter is resisting.\n",
      "\n",
      "        # Let's try a simpler output for now:\n",
      "        # The Kalman filter's prediction `x_hat_minus[0,0]` is a good candidate for\n",
      "        # reduced lag.\n",
      "        # To improve accuracy and trend preservation, we can look at the polynomial\n",
      "        # fit's derivative.\n",
      "        # If the polynomial derivative suggests a strong trend that is different\n",
      "        # from the Kalman velocity, we might need to adjust.\n",
      "\n",
      "        # Let's use a combination of Kalman prediction and polynomial fit.\n",
      "        # The Kalman filter is state-based, providing estimates of position and velocity.\n",
      "        # The polynomial fit provides a local trend.\n",
      "\n",
      "        # A more direct approach for multi-objective:\n",
      "        # The Kalman filter already provides a smoothed estimate of the signal.\n",
      "        # To improve trend preservation and reduce lag, we can use the polynomial fit\n",
      "        # to \"guide\" the Kalman filter's update or adjust its output.\n",
      "\n",
      "        # Let's consider the polynomial trend at the end of the window (`poly_trend`)\n",
      "        # as a target for the filtered signal.\n",
      "        # The Kalman filter's prediction is `x_hat_minus[0,0]`.\n",
      "        # The difference `poly_trend - x_hat_minus[0,0]` can be an error signal.\n",
      "\n",
      "        # We can adjust the Kalman update using this error signal.\n",
      "        # For example, instead of `z`, use `z_corr = H @ x_hat_minus + K_gain * (poly_trend - H @ x_hat_minus)`.\n",
      "        # This is getting complex.\n",
      "\n",
      "        # Let's simplify: use the Kalman filter's output `x_hat[0,0]` and then\n",
      "        # post-process it to address the objectives.\n",
      "\n",
      "        # Output from Kalman filter (updated state)\n",
      "        kalman_output = x_hat[0, 0]\n",
      "\n",
      "        # Polynomial trend at the end of the window\n",
      "        # `poly_trend` is already calculated.\n",
      "\n",
      "        # Let's combine them using a weighted average, but with a mechanism to\n",
      "        # penalize false reversals.\n",
      "        # `y[i] = alpha * kalman_output + (1 - alpha) * poly_trend`\n",
      "\n",
      "        # To penalize false reversals, we can adjust `alpha` or the `poly_trend` itself.\n",
      "        # If `kalman_velocity` and `poly_derivative_at_end` have opposite signs and\n",
      "        # are significant, it's a potential false reversal.\n",
      "        # In such a case, we might want to reduce the contribution of `poly_trend`\n",
      "        # (increase `alpha`) or even suppress the `poly_trend` if it's driving\n",
      "        # the reversal.\n",
      "\n",
      "        # Let's try a simple blending with a dynamic `alpha` or a correction.\n",
      "        # `y[i] = kalman_output + correction`\n",
      "        # `correction = lambda * (poly_trend - kalman_output)`\n",
      "        # `lambda` can be adjusted.\n",
      "\n",
      "        # For false reversal penalty:\n",
      "        # If `np.sign(kalman_velocity) != np.sign(poly_derivative_at_end)`\n",
      "        # and `abs(kalman_velocity) > threshold` and `abs(poly_derivative_at_end) > threshold`:\n",
      "        #   reduce the influence of `poly_trend` or `trend_diff`.\n",
      "\n",
      "        # Let's try a simpler adjustment to the Kalman output using polynomial fit.\n",
      "        # The Kalman filter's prediction `x_hat_minus[0,0]` is a good candidate for\n",
      "        # reduced lag.\n",
      "        # We can then \"pull\" this prediction towards the polynomial trend.\n",
      "\n",
      "        # `y[i] = x_hat_minus[0,0] + beta * (poly_trend - x_hat_minus[0,0])`\n",
      "        # `beta` is a tuning parameter.\n",
      "\n",
      "        # To penalize false reversals:\n",
      "        # If the signs of `kalman_velocity` and `poly_derivative_at_end` are opposite\n",
      "        # and significant, we can reduce `beta` or even make it negative if the\n",
      "        # polynomial trend is suggesting a reversal that the Kalman filter is resisting.\n",
      "\n",
      "        # Let's use a fixed `beta` for now and tune it.\n",
      "        # `beta` around 0.1-0.3 might be a good starting point.\n",
      "        beta = 0.2\n",
      "\n",
      "        # Check for potential false reversal\n",
      "        is_potential_reversal = False\n",
      "        if window_size > 1: # Need at least two points for derivative comparison\n",
      "            # Check if Kalman velocity and polynomial derivative have opposite significant signs\n",
      "            if (np.sign(kalman_velocity) != np.sign(poly_derivative_at_end) and\n",
      "                abs(kalman_velocity) > slope_threshold and\n",
      "                abs(poly_derivative_at_end) > slope_threshold):\n",
      "                is_potential_reversal = True\n",
      "\n",
      "        # Adjust beta to penalize false reversals\n",
      "        if is_potential_reversal:\n",
      "            # Reduce the influence of the polynomial trend if it suggests a reversal\n",
      "            # that the Kalman filter is resisting.\n",
      "            # Or, if the Kalman filter is suggesting a reversal that the polynomial\n",
      "            # is resisting, we might want to trust the Kalman more (increase beta).\n",
      "            # This is tricky. Let's dampen the effect of the polynomial trend\n",
      "            # if it's a potential false reversal.\n",
      "            adjusted_beta = beta / false_reversal_penalty_factor\n",
      "        else:\n",
      "            adjusted_beta = beta\n",
      "\n",
      "        # Calculate the final output: Kalman prediction adjusted by polynomial trend\n",
      "        # We use the Kalman prediction `x_hat_minus` for reduced lag.\n",
      "        y[i] = x_hat_minus[0, 0] + adjusted_beta * (poly_trend - x_hat_minus[0, 0])\n",
      "\n",
      "        # Ensure the output doesn't become NaN if `poly_trend` is NaN (e.g., if window_data is empty)\n",
      "        if np.isnan(y[i]):\n",
      "            y[i] = x_hat_minus[0, 0] # Fallback to Kalman prediction\n",
      "\n",
      "    return y\n",
      "Iteration 5: New subsample score 0.0 is not better than old score 1.1477933721252027, skipping\n",
      "Iteration 6: Selected program 0 score: 0.39265168271364936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6: Proposed new text for program: \"\"\"\n",
      "Real-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n",
      "\n",
      "This algorithm implements a sliding window approach to filter volatile, non-stationary\n",
      "time series data while minimizing noise and preserving signal dynamics.\n",
      "It incorporates advanced techniques for multi-objective optimization.\n",
      "\"\"\"\n",
      "import numpy as np\n",
      "from scipy import signal\n",
      "from collections import deque\n",
      "from scipy.signal import savgol_filter\n",
      "\n",
      "# --- Advanced Filtering Techniques ---\n",
      "\n",
      "def adaptive_kalman_filter(x, window_size=20, process_noise=0.1, measurement_noise=0.1):\n",
      "    \"\"\"\n",
      "    Adaptive Kalman Filter for non-stationary signals.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window (used for initial state estimation)\n",
      "        process_noise: Covariance of the process noise.\n",
      "        measurement_noise: Covariance of the measurement noise.\n",
      "\n",
      "    Returns:\n",
      "        Filtered output signal.\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    # Initial state estimation from the first window\n",
      "    initial_window = x[:window_size]\n",
      "    mean_initial = np.mean(initial_window)\n",
      "    std_initial = np.std(initial_window)\n",
      "\n",
      "    # State vector [position, velocity]\n",
      "    # Initial position is the mean of the first window\n",
      "    # Initial velocity is estimated by the difference between the last and first element of the first window\n",
      "    # divided by the window size. This is a crude but simple initial guess.\n",
      "    state_hat = np.array([mean_initial, (initial_window[-1] - initial_window[0]) / window_size])\n",
      "\n",
      "    # State transition matrix (assuming constant velocity model)\n",
      "    # x_k = A * x_{k-1} + w_k\n",
      "    # [pos_k] = [1  dt] [pos_{k-1}] + [w_pos]\n",
      "    # [vel_k] = [0  1 ] [vel_{k-1}] + [w_vel]\n",
      "    dt = 1.0 # Assuming unit time step for simplicity in this context\n",
      "    A = np.array([[1, dt], [0, 1]])\n",
      "\n",
      "    # Measurement function (we observe position)\n",
      "    # z_k = H * x_k + v_k\n",
      "    H = np.array([[1, 0]])\n",
      "\n",
      "    # Covariance of the process noise\n",
      "    # Q = E[w_k * w_k^T]\n",
      "    Q = np.array([[process_noise, 0], [0, process_noise]])\n",
      "\n",
      "    # Covariance of the measurement noise\n",
      "    # R = E[v_k * v_k^T]\n",
      "    R = np.array([[measurement_noise]])\n",
      "\n",
      "    # Initial estimate of the state covariance\n",
      "    # P_hat = E[(x_k - x_hat_k) * (x_k - x_hat_k)^T]\n",
      "    P_hat = np.array([[1, 0], [0, 1]]) # A reasonable starting point\n",
      "\n",
      "    for i in range(output_length):\n",
      "        # --- Prediction Step ---\n",
      "        # Predict state estimate\n",
      "        state_hat_minus = A @ state_hat\n",
      "        # Predict covariance estimate\n",
      "        P_hat_minus = A @ P_hat @ A.T + Q\n",
      "\n",
      "        # --- Update Step ---\n",
      "        # Measurement\n",
      "        z = np.array([[x[i + window_size - 1]]]) # Current measurement\n",
      "\n",
      "        # Kalman gain\n",
      "        K_gain = P_hat_minus @ H.T @ np.linalg.inv(H @ P_hat_minus @ H.T + R)\n",
      "\n",
      "        # Update state estimate\n",
      "        state_hat = state_hat_minus + K_gain @ (z - H @ state_hat_minus)\n",
      "\n",
      "        # Update covariance estimate\n",
      "        P_hat = (np.identity(2) - K_gain @ H) @ P_hat_minus\n",
      "\n",
      "        # The filtered output is the estimated position\n",
      "        y[i] = state_hat[0]\n",
      "\n",
      "    return y\n",
      "\n",
      "\n",
      "def polynomial_trend_filter(x, window_size=20, poly_order=2):\n",
      "    \"\"\"\n",
      "    Filters signal by fitting a polynomial to a sliding window and using the\n",
      "    fitted trend. This helps preserve dynamics while smoothing noise.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window.\n",
      "        poly_order: Order of the polynomial to fit.\n",
      "\n",
      "    Returns:\n",
      "        Filtered output signal.\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    # Create x-coordinates for the polynomial fitting\n",
      "    x_coords = np.arange(window_size)\n",
      "\n",
      "    for i in range(output_length):\n",
      "        window = x[i : i + window_size]\n",
      "\n",
      "        # Fit a polynomial to the window\n",
      "        coeffs = np.polyfit(x_coords, window, poly_order)\n",
      "        poly_func = np.poly1d(coeffs)\n",
      "\n",
      "        # The filtered output is the value of the polynomial at the center of the window\n",
      "        # or at the last point of the window to minimize lag.\n",
      "        # Using the last point of the window (i.e., the current time step) to reduce lag.\n",
      "        y[i] = poly_func(window_size - 1)\n",
      "\n",
      "    return y\n",
      "\n",
      "def wavelet_denoising(x, wavelet='db1', level=1):\n",
      "    \"\"\"\n",
      "    Wavelet denoising to remove noise while preserving signal features.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples).\n",
      "        wavelet: The wavelet to use (e.g., 'db1', 'sym2').\n",
      "        level: Decomposition level.\n",
      "\n",
      "    Returns:\n",
      "        Denoised signal.\n",
      "    \"\"\"\n",
      "    from pywt import wavedec, waverec, threshold\n",
      "\n",
      "    # Perform multi-level decomposition\n",
      "    coeffs = wavedec(x, wavelet, level=level)\n",
      "\n",
      "    # Apply thresholding to detail coefficients\n",
      "    # Using soft thresholding\n",
      "    sigma = np.median(np.abs(coeffs[-1])) / 0.6745 # Estimate noise standard deviation\n",
      "    uthresh = threshold(coeffs[-1], sigma=sigma, mode='soft')\n",
      "\n",
      "    # Reconstruct the signal with thresholded coefficients\n",
      "    # Note: For real-time, this would need to be applied incrementally.\n",
      "    # For this simulation, we apply it to the whole signal.\n",
      "    # We'll only modify the detail coefficients.\n",
      "    denoised_coeffs = [coeffs[0]] + [threshold(c, sigma=sigma, mode='soft') for c in coeffs[1:]]\n",
      "    y = waverec(denoised_coeffs, wavelet)\n",
      "\n",
      "    # The output length of waverec might be slightly different due to padding.\n",
      "    # We'll truncate to the original signal length.\n",
      "    return y[:len(x)]\n",
      "\n",
      "# --- Main Processing Function ---\n",
      "\n",
      "def process_signal(input_signal, window_size=20, algorithm_type=\"kalman\", poly_order=2, wavelet_level=1):\n",
      "    \"\"\"\n",
      "    Main signal processing function that applies the selected algorithm.\n",
      "\n",
      "    Args:\n",
      "        input_signal: Input time series data\n",
      "        window_size: Window size for processing\n",
      "        algorithm_type: Type of algorithm to use (\"basic\", \"enhanced\", \"kalman\", \"poly\", \"wavelet\")\n",
      "        poly_order: Order of the polynomial for polynomial_trend_filter\n",
      "        wavelet_level: Decomposition level for wavelet_denoising\n",
      "\n",
      "    Returns:\n",
      "        Filtered signal\n",
      "    \"\"\"\n",
      "    if algorithm_type == \"enhanced\":\n",
      "        # Weighted moving average (existing enhanced filter)\n",
      "        weights = np.exp(np.linspace(-2, 0, window_size))\n",
      "        weights = weights / np.sum(weights)\n",
      "        # Pad input signal to handle edge effects for convolution-like operation\n",
      "        padded_input = np.pad(input_signal, (window_size - 1, 0), mode='edge')\n",
      "        # Use convolution for efficient weighted moving average\n",
      "        filtered_signal = np.convolve(padded_input, weights[::-1], mode='valid')\n",
      "        return filtered_signal[:len(input_signal)] # Truncate to original length\n",
      "\n",
      "    elif algorithm_type == \"kalman\":\n",
      "        return adaptive_kalman_filter(input_signal, window_size=window_size)\n",
      "\n",
      "    elif algorithm_type == \"poly\":\n",
      "        return polynomial_trend_filter(input_signal, window_size=window_size, poly_order=poly_order)\n",
      "\n",
      "    elif algorithm_type == \"wavelet\":\n",
      "        return wavelet_denoising(input_signal, level=wavelet_level)\n",
      "\n",
      "    else: # Default to basic moving average\n",
      "        if len(input_signal) < window_size:\n",
      "            raise ValueError(f\"Input signal length ({len(input_signal)}) must be >= window_size ({window_size})\")\n",
      "        output_length = len(input_signal) - window_size + 1\n",
      "        y = np.zeros(output_length)\n",
      "        for i in range(output_length):\n",
      "            window = input_signal[i : i + window_size]\n",
      "            y[i] = np.mean(window)\n",
      "        return y\n",
      "\n",
      "# --- Metrics Calculation and Signal Generation (for testing) ---\n",
      "\n",
      "def calculate_metrics(filtered_signal, clean_signal, noisy_signal, window_size):\n",
      "    \"\"\"\n",
      "    Calculate various metrics for evaluating the filtered signal.\n",
      "    \"\"\"\n",
      "    if len(filtered_signal) == 0 or len(clean_signal) < window_size:\n",
      "        return {\n",
      "            \"correlation\": 0,\n",
      "            \"noise_reduction\": 0,\n",
      "            \"slope_changes\": 0,\n",
      "            \"lag_error\": 0,\n",
      "            \"avg_error\": 0,\n",
      "            \"false_reversals\": 0,\n",
      "            \"smoothness_score\": 0,\n",
      "            \"responsiveness_score\": 0,\n",
      "            \"accuracy_score\": 0,\n",
      "            \"efficiency_score\": 1.0, # Assume efficiency is high if no processing errors\n",
      "        }\n",
      "\n",
      "    # Align signals for comparison (account for processing delay)\n",
      "    # The delay is typically window_size - 1 for simple filters.\n",
      "    # For Kalman and polynomial, the delay can be more complex.\n",
      "    # For simplicity, we'll use a fixed delay based on window_size for now,\n",
      "    # but a more precise delay estimation might be needed.\n",
      "    delay = window_size - 1\n",
      "    if len(clean_signal) < delay:\n",
      "        return {\n",
      "            \"correlation\": 0,\n",
      "            \"noise_reduction\": 0,\n",
      "            \"slope_changes\": 0,\n",
      "            \"lag_error\": 0,\n",
      "            \"avg_error\": 0,\n",
      "            \"false_reversals\": 0,\n",
      "            \"smoothness_score\": 0,\n",
      "            \"responsiveness_score\": 0,\n",
      "            \"accuracy_score\": 0,\n",
      "            \"efficiency_score\": 1.0,\n",
      "        }\n",
      "\n",
      "    aligned_clean = clean_signal[delay:]\n",
      "    aligned_noisy = noisy_signal[delay:]\n",
      "\n",
      "    # Ensure same length for all signals\n",
      "    min_length = min(len(filtered_signal), len(aligned_clean), len(aligned_noisy))\n",
      "    filtered_signal = filtered_signal[:min_length]\n",
      "    aligned_clean = aligned_clean[:min_length]\n",
      "    aligned_noisy = aligned_noisy[:min_length]\n",
      "\n",
      "    if min_length == 0:\n",
      "        return {\n",
      "            \"correlation\": 0,\n",
      "            \"noise_reduction\": 0,\n",
      "            \"slope_changes\": 0,\n",
      "            \"lag_error\": 0,\n",
      "            \"avg_error\": 0,\n",
      "            \"false_reversals\": 0,\n",
      "            \"smoothness_score\": 0,\n",
      "            \"responsiveness_score\": 0,\n",
      "            \"accuracy_score\": 0,\n",
      "            \"efficiency_score\": 1.0,\n",
      "        }\n",
      "\n",
      "    # 1. Tracking Accuracy (Correlation with clean signal)\n",
      "    correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n",
      "    accuracy_score = max(0, correlation) # Ensure non-negative\n",
      "\n",
      "    # 2. Noise Reduction\n",
      "    noise_before = np.var(aligned_noisy - aligned_clean)\n",
      "    noise_after = np.var(filtered_signal - aligned_clean)\n",
      "    noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 1e-9 else 0\n",
      "    # Clamp noise reduction to avoid values > 1 or < 0 due to edge cases\n",
      "    noise_reduction = max(0, min(1, noise_reduction))\n",
      "\n",
      "    # 3. Slope Change Minimization & False Reversal Penalty\n",
      "    # Use Savitzky-Golay filter to estimate derivatives for slope changes\n",
      "    # We need to be careful with window size and order for Savitzky-Golay\n",
      "    # to avoid introducing its own artifacts.\n",
      "    # A small window and low order should be sufficient for derivative estimation.\n",
      "    # Ensure the Savitzky-Golay window is odd and smaller than the signal length.\n",
      "    sg_window_length = min(31, min_length // 2 * 2 + 1) # Odd number, not too large\n",
      "    sg_polyorder = min(3, sg_window_length - 1) # Order less than window length\n",
      "\n",
      "    if sg_window_length > 1 and sg_polyorder > 0:\n",
      "        filtered_diff = savgol_filter(filtered_signal, sg_window_length, sg_polyorder, deriv=1)\n",
      "        clean_diff = savgol_filter(aligned_clean, sg_window_length, sg_polyorder, deriv=1)\n",
      "\n",
      "        # Slope changes: detect sign changes in the derivative\n",
      "        filtered_sign_changes = np.sum(np.diff(np.sign(filtered_diff)) != 0)\n",
      "        clean_sign_changes = np.sum(np.diff(np.sign(clean_diff)) != 0)\n",
      "\n",
      "        # False reversals: count slope changes in filtered signal that are not in clean signal\n",
      "        # This is a simplification. A more robust method would involve comparing local extrema.\n",
      "        # For now, we'll approximate false reversals as the difference in sign changes.\n",
      "        false_reversals = max(0, filtered_sign_changes - clean_sign_changes)\n",
      "\n",
      "        # Slope change minimization: aim to reduce the number of sign changes in the filtered signal\n",
      "        slope_changes = filtered_sign_changes\n",
      "\n",
      "        # Penalize false reversals heavily\n",
      "        false_reversal_penalty = false_reversals * 10 # Arbitrary high penalty\n",
      "\n",
      "    else:\n",
      "        slope_changes = 0\n",
      "        false_reversals = 0\n",
      "        false_reversal_penalty = 0\n",
      "\n",
      "\n",
      "    # 4. Lag Error Minimization\n",
      "    # This is tricky to measure directly without a ground truth of ideal lag.\n",
      "    # We can approximate it by comparing the filtered signal's peaks/troughs\n",
      "    # to the clean signal's. A simpler approach is to use the average absolute error\n",
      "    # as a proxy for lag and distortion.\n",
      "    avg_error = np.mean(np.abs(filtered_signal - aligned_clean))\n",
      "\n",
      "    # Responsiveness score can be inversely related to lag and avg_error.\n",
      "    # A higher responsiveness_score means less lag and error.\n",
      "    responsiveness_score = max(0, 1 / (1 + avg_error * 5)) # Adjust multiplier as needed\n",
      "\n",
      "    # Smoothness score (inverse of slope changes)\n",
      "    smoothness_score = max(0, 1 / (1 + slope_changes))\n",
      "\n",
      "    # Combined score based on objectives\n",
      "    # This is a heuristic combination. Weights can be tuned.\n",
      "    # We want to maximize correlation, noise_reduction, smoothness, responsiveness.\n",
      "    # We want to minimize slope_changes, lag_error, avg_error, false_reversals.\n",
      "\n",
      "    # Let's try to define composite scores that are maximized when good.\n",
      "    # Accuracy: correlation\n",
      "    # Noise Reduction: noise_reduction\n",
      "    # Smoothness: 1 / (1 + slope_changes)\n",
      "    # Responsiveness: 1 / (1 + avg_error) - this is a proxy for lag and distortion\n",
      "\n",
      "    # The feedback indicates noise_reduction is 0.0, which is a problem.\n",
      "    # The current enhanced_filter_with_trend_preservation is a weighted moving average,\n",
      "    # which should reduce noise to some extent. The calculation might be off or\n",
      "    # the signal generation doesn't leave enough residual noise after filtering.\n",
      "    # Let's ensure noise_reduction is calculated correctly and consider if the\n",
      "    # signal generation is appropriate.\n",
      "\n",
      "    # Re-evaluating noise reduction calculation:\n",
      "    # noise_before = np.var(aligned_noisy - aligned_clean) # Variance of the noise component in the noisy signal\n",
      "    # noise_after = np.var(filtered_signal - aligned_clean) # Variance of the noise component in the filtered signal\n",
      "    # This seems correct. If noise_after is not significantly smaller than noise_before,\n",
      "    # the filter is not reducing noise.\n",
      "\n",
      "    # The feedback also shows high avg_error and low correlation for some runs.\n",
      "    # This suggests the filters are not tracking well or are too laggy.\n",
      "\n",
      "    # Let's refine the scores and consider how to combine them.\n",
      "    # The goal is to minimize slope_changes, lag_error, avg_error, false_reversals.\n",
      "    # The goal is to maximize tracking_accuracy, noise_reduction.\n",
      "\n",
      "    # Simplified approach to combine objectives:\n",
      "    # We'll use the raw metrics and the 'composite_score' from the example.\n",
      "    # The example's composite_score seems to be a weighted sum of normalized metrics.\n",
      "    # We need to ensure each metric is contributing positively to the score.\n",
      "\n",
      "    # Let's try to create scores that are always between 0 and 1, where 1 is best.\n",
      "    # Correlation: already [0, 1] (or close to it)\n",
      "    # Noise Reduction: [0, 1]\n",
      "    # Smoothness: 1 / (1 + slope_changes) - could be small, normalize?\n",
      "    # Responsiveness: 1 / (1 + avg_error) - could be small, normalize?\n",
      "    # False Reversals: Should be penalized.\n",
      "\n",
      "    # A more direct approach:\n",
      "    # We want to minimize: slope_changes, lag_error, avg_error, false_reversals\n",
      "    # We want to maximize: correlation, noise_reduction\n",
      "\n",
      "    # Let's use a weighted sum of these objectives, where higher is better.\n",
      "    # We'll normalize each metric to a [0, 1] range if possible, or use inverse relationships.\n",
      "\n",
      "    # Normalize correlation and noise reduction (already in range)\n",
      "    score_accuracy = correlation\n",
      "    score_noise_reduction = noise_reduction\n",
      "\n",
      "    # Normalize smoothness (inverse of slope changes)\n",
      "    # Add a small constant to denominator to avoid division by zero if slope_changes is 0.\n",
      "    # Max slope changes observed is around 50-60. So max value for 1/(1+slope_changes) is 1.\n",
      "    score_smoothness = 1.0 / (1.0 + slope_changes)\n",
      "\n",
      "    # Normalize responsiveness (inverse of avg_error, which is proxy for lag)\n",
      "    # Avg error observed can be around 1.5. So 1/(1+1.5) = 0.4.\n",
      "    # We want responsiveness to be higher for lower error.\n",
      "    score_responsiveness = 1.0 / (1.0 + avg_error)\n",
      "\n",
      "    # False reversal penalty: this is a cost, not a score. We want to minimize it.\n",
      "    # The overall score should be maximized. So, we will subtract a penalty.\n",
      "    # A large penalty is needed.\n",
      "    false_reversal_cost = false_reversals * 5 # Heuristic multiplier\n",
      "\n",
      "    # Combine scores. The weights are crucial and need tuning.\n",
      "    # Let's assume equal importance for now, but this is a hyperparameter.\n",
      "    # We want to maximize accuracy, noise reduction, smoothness, responsiveness,\n",
      "    # and minimize false reversals.\n",
      "\n",
      "    # A simple composite score could be:\n",
      "    # composite_score = w_acc * score_accuracy + w_noise * score_noise_reduction +\n",
      "    #                   w_smooth * score_smoothness + w_resp * score_responsiveness -\n",
      "    #                   w_rev * false_reversal_cost\n",
      "\n",
      "    # The example feedback provides 'composite_score' and 'combined_score'.\n",
      "    # Let's try to replicate the spirit of these by combining the metrics.\n",
      "    # The 'combined_score' is likely the final objective.\n",
      "    # The 'composite_score' might be an intermediate or alternative metric.\n",
      "\n",
      "    # Let's try to define a combined score that prioritizes the objectives:\n",
      "    # Priority: Accuracy, Noise Reduction, Smoothness, Responsiveness, False Reversal Penalty.\n",
      "\n",
      "    # Let's consider the objectives:\n",
      "    # 1. Slope change minimization: Minimize slope_changes\n",
      "    # 2. Lag error minimization: Minimize avg_error (as proxy for lag)\n",
      "    # 3. Tracking accuracy: Maximize correlation\n",
      "    # 4. False reversal penalty: Minimize false_reversals\n",
      "\n",
      "    # A potential approach for a single score:\n",
      "    # score = w1 * correlation + w2 * noise_reduction - w3 * slope_changes - w4 * avg_error - w5 * false_reversals\n",
      "    # The weights w1-w5 need to be carefully chosen.\n",
      "\n",
      "    # Given the feedback, noise_reduction is 0.0. This suggests the current filters\n",
      "    # are not effective at noise reduction, or the test signal is too noisy for them,\n",
      "    # or the noise reduction calculation is flawed. The weighted moving average *should*\n",
      "    # reduce noise.\n",
      "\n",
      "    # Let's re-evaluate the 'enhanced_filter_with_trend_preservation' logic.\n",
      "    # It's a weighted moving average. The weights emphasize recent samples.\n",
      "    # This is essentially an Exponentially Weighted Moving Average (EWMA) if the weights\n",
      "    # were truly exponential. The current weights are linearly spaced exponents, which is\n",
      "    # a form of exponential decay. This should provide some smoothing.\n",
      "\n",
      "    # The feedback on the first run shows noise_reduction: 0.0. This is a critical failure.\n",
      "    # The second run also shows noise_reduction: 0.0.\n",
      "\n",
      "    # Let's try to implement a more robust filtering approach that is known for\n",
      "    # noise reduction and tracking. Kalman filter is a good candidate.\n",
      "    # The current implementation of `enhanced_filter_with_trend_preservation` might not be\n",
      "    # sufficient for significant noise reduction on this test signal.\n",
      "\n",
      "    # The proposed solutions in the prompt include Kalman Filters and Wavelets.\n",
      "    # Let's ensure these are properly integrated and contribute to noise reduction.\n",
      "\n",
      "    # Let's re-examine the 'enhanced_filter_with_trend_preservation' in the provided code.\n",
      "    # It uses `np.exp(np.linspace(-2, 0, window_size))` for weights. This is indeed an\n",
      "    # exponential decay. It should offer some noise reduction.\n",
      "\n",
      "    # Possible reasons for 0 noise reduction:\n",
      "    # 1. The noise level in `generate_test_signal` is too high relative to the signal.\n",
      "    # 2. The `window_size` is too small to effectively smooth the noise.\n",
      "    # 3. The `enhanced_filter_with_trend_preservation` is not sophisticated enough.\n",
      "\n",
      "    # Let's focus on improving the filtering itself.\n",
      "    # The current implementation uses a weighted moving average.\n",
      "    # We can try to improve this by:\n",
      "    # - Using a more adaptive weighting scheme.\n",
      "    # - Incorporating a Kalman filter.\n",
      "    # - Using wavelet denoising.\n",
      "\n",
      "    # Let's try to implement the Kalman filter and wavelet denoising as separate options\n",
      "    # and also try to improve the existing \"enhanced\" filter.\n",
      "\n",
      "    # The prompt asks to improve the code *within* the EVOLVE-BLOCK.\n",
      "    # The current code has `adaptive_filter` and `enhanced_filter_with_trend_preservation`.\n",
      "    # The `process_signal` function selects between them.\n",
      "\n",
      "    # Let's redefine `process_signal` to include more advanced options and\n",
      "    # refine the metrics calculation.\n",
      "\n",
      "    # For the metrics:\n",
      "    # slope_changes and false_reversals are based on `savgol_filter` derivatives.\n",
      "    # The `savgol_filter` itself can introduce lag and artifacts if not chosen carefully.\n",
      "    # The `window_size` for the main filter impacts lag. A larger window means more lag.\n",
      "\n",
      "    # Let's refine the objective functions for the metrics.\n",
      "    # We want to maximize:\n",
      "    # - Tracking Accuracy (correlation)\n",
      "    # - Noise Reduction\n",
      "\n",
      "    # We want to minimize:\n",
      "    # - Slope changes (spurious reversals)\n",
      "    # - Lag error (approximated by average absolute error)\n",
      "    # - False reversals (heavily penalized)\n",
      "\n",
      "    # Let's try to create a composite score that balances these.\n",
      "    # We'll normalize metrics to be between 0 and 1 where 1 is best.\n",
      "\n",
      "    # Normalizing correlation and noise reduction is straightforward.\n",
      "    # For slope changes and false reversals, we want to minimize them.\n",
      "    # A common technique is to use `1 / (1 + metric_value)`.\n",
      "    # For lag error (avg_error), we also want to minimize it.\n",
      "\n",
      "    # Let's define weights for each objective. These are crucial.\n",
      "    # For now, let's assume some initial weights.\n",
      "\n",
      "    # Weights (example, needs tuning):\n",
      "    w_accuracy = 0.3\n",
      "    w_noise_reduction = 0.3\n",
      "    w_smoothness = 0.2  # Inverse of slope changes\n",
      "    w_responsiveness = 0.2 # Inverse of avg_error\n",
      "\n",
      "    # The false reversal penalty needs to be incorporated as a cost.\n",
      "    # A high penalty for false reversals is essential.\n",
      "\n",
      "    # Let's calculate the scores and then the combined score.\n",
      "    # The feedback indicates noise_reduction is 0.0. This is a major issue.\n",
      "    # The current `enhanced_filter_with_trend_preservation` (weighted moving average)\n",
      "    # should provide some noise reduction. If it's not, either the noise is too high,\n",
      "    # or the filter is not aggressive enough.\n",
      "\n",
      "    # Let's try to incorporate a Kalman filter and wavelet denoising as options.\n",
      "    # The current `process_signal` function needs to be updated to call these.\n",
      "\n",
      "    # For the purpose of this evolution, I will focus on improving the filtering\n",
      "    # within `process_signal` and then ensure the metrics calculation is robust.\n",
      "\n",
      "    # Let's add Kalman filter and Wavelet denoising as algorithm types.\n",
      "\n",
      "    # Kalman Filter Specifics:\n",
      "    # - Needs tuning of process_noise (Q) and measurement_noise (R).\n",
      "    # - Initial state estimation is important.\n",
      "\n",
      "    # Wavelet Denoising Specifics:\n",
      "    # - Choice of wavelet and decomposition level.\n",
      "    # - Thresholding method.\n",
      "\n",
      "    # Let's refine the `process_signal` function to include these options.\n",
      "    # And let's ensure the metrics calculation is robust and reflects the objectives.\n",
      "\n",
      "    # The feedback on the first run shows:\n",
      "    # slope_changes: 57.0, lag_error: 1.055, avg_error: 0.947, false_reversals: 57.0, correlation: 0.783, noise_reduction: 0.0\n",
      "\n",
      "    # This indicates that the filter is not reducing noise, has some lag, but maintains decent correlation.\n",
      "    # The high slope_changes and false_reversals suggest it's too sensitive to noise.\n",
      "\n",
      "    # The second run shows:\n",
      "    # slope_changes: 53.0, lag_error: 0.300, avg_error: 1.499, false_reversals: 44.0, correlation: -0.040, noise_reduction: 0.0\n",
      "\n",
      "    # This run is worse: very low correlation, high avg_error, still no noise reduction.\n",
      "    # The lag_error is better, but that might be due to very poor tracking.\n",
      "\n",
      "    # The core problem seems to be:\n",
      "    # 1. Ineffective noise reduction (noise_reduction is consistently 0.0).\n",
      "    # 2. Balancing lag vs. smoothness/accuracy.\n",
      "    # 3. High false reversals when trying to track dynamics.\n",
      "\n",
      "    # Let's prioritize improving noise reduction and then balance the other objectives.\n",
      "\n",
      "    # Proposed improvements to `process_signal`:\n",
      "    # - Add Kalman filter as an option.\n",
      "    # - Add Wavelet denoising as an option.\n",
      "    # - Potentially refine the weighted moving average.\n",
      "\n",
      "    # Let's refine the `calculate_metrics` function to be more comprehensive.\n",
      "    # The existing `calculate_metrics` function already includes many of the requested metrics.\n",
      "    # The problem might be in the interpretation or the values themselves.\n",
      "\n",
      "    # Let's focus on improving the `process_signal` function first.\n",
      "\n",
      "    # --- Re-implementing `process_signal` with more advanced options ---\n",
      "    # The goal is to select an algorithm that balances the multi-objective requirements.\n",
      "\n",
      "    # Let's consider the options:\n",
      "    # 1. Kalman Filter: Good for state estimation and noise reduction, can be tuned for responsiveness.\n",
      "    # 2. Polynomial Trend Fitting: Can smooth noise while preserving trends, but can lag.\n",
      "    # 3. Wavelet Denoising: Excellent for noise reduction, but can be computationally intensive and might introduce artifacts if not tuned.\n",
      "    # 4. Enhanced Weighted Moving Average (current): Simple, but might not be sufficient for aggressive noise reduction.\n",
      "\n",
      "    # Let's choose a default algorithm that aims for a good balance. Kalman filter is a strong candidate.\n",
      "\n",
      "    # For the `calculate_metrics` function, let's ensure the metrics are calculated correctly and\n",
      "    # that they reflect the desired objectives.\n",
      "    # The `savgol_filter` for derivatives is a reasonable approach for slope changes.\n",
      "\n",
      "    # Let's ensure the `noise_reduction` calculation is robust.\n",
      "    # `noise_before = np.var(aligned_noisy - aligned_clean)`\n",
      "    # `noise_after = np.var(filtered_signal - aligned_clean)`\n",
      "    # If `noise_after` is not significantly less than `noise_before`, the filter isn't working.\n",
      "\n",
      "    # Let's consider the parameters passed to `process_signal`.\n",
      "    # `window_size`, `poly_order`, `wavelet_level`. These are hyperparameters.\n",
      "    # For a fixed algorithm, these would be tuned. In this evolutionary context,\n",
      "    # the algorithm choice itself is part of the evolution.\n",
      "\n",
      "    # Let's refine the `process_signal` function to offer these advanced options.\n",
      "    # The `algorithm_type` parameter will guide the choice.\n",
      "\n",
      "    # The current `enhanced_filter_with_trend_preservation` is a simple WMA.\n",
      "    # Let's keep it as an option but introduce Kalman and Wavelet.\n",
      "\n",
      "    # For the purpose of this evolutionary block, I'll provide a version of `process_signal`\n",
      "    # that includes these options. The `calculate_metrics` function will remain as is for now,\n",
      "    # assuming its logic is sound but the filtering itself needs improvement.\n",
      "\n",
      "    # Let's try to improve the `enhanced_filter_with_trend_preservation` by using a more\n",
      "    # sophisticated weighting or by integrating it with a smoothing technique.\n",
      "    # However, the prompt asks for *advanced techniques*, so let's focus on Kalman and Wavelets.\n",
      "\n",
      "    # Let's assume `process_signal` will now take `algorithm_type` as 'kalman', 'wavelet', or 'poly'.\n",
      "    # The original 'basic' and 'enhanced' might be less effective.\n",
      "\n",
      "    # Let's refine the `process_signal` function definition.\n",
      "    # We will pass `window_size` to Kalman and Polynomial filters.\n",
      "    # Wavelet denoising might not directly use `window_size` in the same way,\n",
      "    # but `level` is a parameter.\n",
      "\n",
      "    # Let's define the `process_signal` function to select between these.\n",
      "    # And let's ensure the `calculate_metrics` function is called correctly in `run_signal_processing`.\n",
      "\n",
      "    # The provided `run_signal_processing` function calls `process_signal` with `algorithm_type=\"enhanced\"`.\n",
      "    # To test the new algorithms, this would need to be changed.\n",
      "    # However, my task is to provide the *evolvable block content*.\n",
      "\n",
      "    # Let's replace the existing `adaptive_filter` and `enhanced_filter_with_trend_preservation`\n",
      "    # with more advanced implementations.\n",
      "\n",
      "    # For `adaptive_filter`, let's replace it with `adaptive_kalman_filter`.\n",
      "    # For `enhanced_filter_with_trend_preservation`, let's replace it with `polynomial_trend_filter`.\n",
      "    # And add `wavelet_denoising` as a separate option.\n",
      "\n",
      "    # The `process_signal` function will then select among these.\n",
      "\n",
      "    # Let's consider the parameters for `process_signal`:\n",
      "    # `input_signal`, `window_size`, `algorithm_type`, `poly_order`, `wavelet_level`.\n",
      "\n",
      "    # Let's ensure the function signatures are compatible.\n",
      "\n",
      "    # The `adaptive_filter` and `enhanced_filter_with_trend_preservation` are defined outside the EVOLVE-BLOCK-END.\n",
      "    # This means I should modify the `process_signal` function and potentially add new helper functions *within* the block.\n",
      "    # The prompt says \"ONLY propose the improved code that should go INSIDE this block\".\n",
      "    # This implies I should replace the existing `adaptive_filter` and `enhanced_filter_with_trend_preservation` logic.\n",
      "\n",
      "    # Let's redefine the functions within the EVOLVE-BLOCK.\n",
      "\n",
      "    # New functions to implement within the block:\n",
      "    # - `adaptive_kalman_filter`\n",
      "    # - `polynomial_trend_filter`\n",
      "    # - `wavelet_denoising` (this might require external library, assume it's available)\n",
      "\n",
      "    # The original `adaptive_filter` and `enhanced_filter_with_trend_preservation` will be replaced.\n",
      "\n",
      "    # Let's make `process_signal` the primary function that orchestrates these.\n",
      "    # It will take `algorithm_type` to choose which advanced filter to use.\n",
      "\n",
      "    # The existing `run_signal_processing` function calls `process_signal(..., algorithm_type=\"enhanced\")`.\n",
      "    # This will need to be changed to test the new algorithms.\n",
      "    # But I am only providing the evolvable block.\n",
      "\n",
      "    # Let's make the `process_signal` function robust and capable of selecting different algorithms.\n",
      "\n",
      "    # The `calculate_metrics` function is outside the EVOLVE-BLOCK, so I cannot modify it.\n",
      "    # I must assume its logic is correct and focus on improving the filtering.\n",
      "\n",
      "    # Let's reorganize the EVOLVE-BLOCK content.\n",
      "    # It should contain the main `process_signal` function and any helper functions it uses.\n",
      "\n",
      "    # I will remove the original `adaptive_filter` and `enhanced_filter_with_trend_preservation`\n",
      "    # and replace them with the advanced implementations and a unified `process_signal`.\n",
      "\n",
      "    # Let's ensure the helper functions like `adaptive_kalman_filter`, `polynomial_trend_filter`,\n",
      "    # and `wavelet_denoising` are defined within the EVOLVE-BLOCK or are accessible.\n",
      "    # Since I'm replacing the content of the block, I should include these helper functions.\n",
      "\n",
      "    # Let's refine the `adaptive_kalman_filter` to be more robust.\n",
      "    # The initial state estimation is crucial.\n",
      "    # The state transition matrix `A` and measurement matrix `H` are standard for a constant velocity model.\n",
      "    # Tuning `process_noise` and `measurement_noise` is key.\n",
      "\n",
      "    # For `polynomial_trend_filter`, the `poly_order` is important. Higher order can fit noise.\n",
      "    # Using `window_size - 1` for evaluation point reduces lag compared to the center.\n",
      "\n",
      "    # For `wavelet_denoising`, the `wavelet` and `level` are important.\n",
      "    # `scipy.signal.wavedec` and `waverec` are standard for this.\n",
      "    # The noise estimation `sigma` and thresholding method are critical.\n",
      "\n",
      "    # Let's make sure the `process_signal` function correctly dispatches to these.\n",
      "\n",
      "    # The original code had:\n",
      "    # `adaptive_filter` (SMA)\n",
      "    # `enhanced_filter_with_trend_preservation` (WMA with exponential weights)\n",
      "    # `process_signal` selects between them.\n",
      "\n",
      "    # My new `process_signal` will select among:\n",
      "    # - Kalman Filter\n",
      "    # - Polynomial Trend Filter\n",
      "    # - Wavelet Denoising\n",
      "    # - (Optionally, keep a refined WMA or SMA for comparison)\n",
      "\n",
      "    # Given the feedback, the primary issue is noise reduction and balancing lag/accuracy.\n",
      "    # Kalman and Wavelets are good for noise reduction. Polynomials for trend preservation.\n",
      "\n",
      "    # Let's assume the `run_signal_processing` function will be modified externally to test different `algorithm_type` values.\n",
      "    # My job is to provide the best possible filtering logic within the EVOLVE-BLOCK.\n",
      "\n",
      "    # Let's prioritize Kalman filter and Wavelet denoising for their noise reduction capabilities.\n",
      "    # Polynomial fitting is good for trend preservation.\n",
      "\n",
      "    # The `window_size` parameter is used by WMA, Kalman, and Polynomial.\n",
      "    # Wavelet denoising uses `level`.\n",
      "\n",
      "    # Let's integrate these into a single `process_signal` function.\n",
      "\n",
      "    # Consider the `output_length` calculation.\n",
      "    # For Kalman and Polynomial fitting, the output length is typically `len(x) - window_size + 1`.\n",
      "    # Wavelet denoising usually returns a signal of the same length as the input.\n",
      "    # This needs to be handled consistently. For now, let's aim for the output length to be `len(x) - window_size + 1` for filters that use a window, and `len(x)` for wavelet.\n",
      "    # However, the `calculate_metrics` function aligns signals based on `window_size - 1` delay.\n",
      "    # This implies the output length should be compatible with this alignment.\n",
      "    # If wavelet returns `len(x)`, it will need to be truncated or the `calculate_metrics` needs adjustment.\n",
      "    # For simplicity, let's ensure all filters produce an output of `len(x) - window_size + 1` or `len(x)` and then handle alignment.\n",
      "\n",
      "    # Let's make the output length consistent for all algorithms to simplify `calculate_metrics`.\n",
      "    # A common approach is to pad the input and then take the valid part, resulting in `len(x) - window_size + 1`.\n",
      "    # For wavelet denoising, we can truncate the output to match this length.\n",
      "\n",
      "    # Let's define the helper functions first.\n",
      "\n",
      "    # --- Helper function for Kalman Filter ---\n",
      "    # (Defined above)\n",
      "\n",
      "    # --- Helper function for Polynomial Trend Filter ---\n",
      "    # (Defined above)\n",
      "\n",
      "    # --- Helper function for Wavelet Denoising ---\n",
      "    # (Defined above)\n",
      "\n",
      "    # --- Main processing function ---\n",
      "    # This function will select the appropriate filter.\n",
      "    # The `algorithm_type` will be a string.\n",
      "\n",
      "    # Let's refine the `process_signal` function.\n",
      "    # It should handle different `algorithm_type` values.\n",
      "    # The `window_size` is a crucial parameter.\n",
      "    # `poly_order` and `wavelet_level` are specific to those algorithms.\n",
      "\n",
      "    # Example usage of `process_signal`:\n",
      "    # `process_signal(input_signal, window_size=20, algorithm_type=\"kalman\")`\n",
      "    # `process_signal(input_signal, window_size=20, algorithm_type=\"poly\", poly_order=3)`\n",
      "    # `process_signal(input_signal, window_size=20, algorithm_type=\"wavelet\", wavelet_level=2)`\n",
      "\n",
      "    # The current `run_signal_processing` uses `algorithm_type=\"enhanced\"`.\n",
      "    # If we want to test the new algorithms, we would need to change that call.\n",
      "    # However, the task is to provide the evolvable block.\n",
      "\n",
      "    # Let's ensure the provided code is self-contained within the EVOLVE-BLOCK.\n",
      "    # It should replace the existing functions.\n",
      "\n",
      "    # Let's consider the `adaptive_filter` and `enhanced_filter_with_trend_preservation`\n",
      "    # functions that are currently in the block. They will be replaced by the new logic.\n",
      "\n",
      "    # The prompt asks to improve the code. This means replacing the existing filtering logic.\n",
      "    # I will provide the improved filtering functions and the updated `process_signal`.\n",
      "\n",
      "    # Let's ensure the `process_signal` function handles the output length correctly.\n",
      "    # For Kalman and Polynomial, the output is naturally `len(x) - window_size + 1`.\n",
      "    # For Wavelet, we might need to truncate.\n",
      "\n",
      "    # Let's try to make the output length consistent with the original `adaptive_filter`\n",
      "    # which returned `len(x) - window_size + 1`.\n",
      "    # For wavelet denoising, we can truncate the output.\n",
      "\n",
      "    # Final check of the requirements:\n",
      "    # - Multi-objective optimization: addressed by choosing algorithms that balance noise reduction, tracking, and lag.\n",
      "    # - Advanced techniques: Kalman, Wavelets, Polynomial fitting.\n",
      "    # - Minimal computational latency and phase delay: Kalman and WMA are generally good. Wavelets can be more complex. Polynomial fitting can lag.\n",
      "    # - Slope change minimization, Lag error minimization, Tracking accuracy, False reversal penalty: these are metrics evaluated externally, but the choice of algorithm aims to optimize them.\n",
      "\n",
      "    # Let's make the default `algorithm_type` in `process_signal` to be 'kalman' or 'poly' for better performance.\n",
      "    # 'kalman' is a good default for adaptive filtering.\n",
      "\n",
      "    # Let's consider the `window_size` parameter. For Kalman, it's used for initial state estimation.\n",
      "    # For polynomial, it's the window for fitting.\n",
      "    # For wavelet, `window_size` is not directly used in the denoising itself, but the `level` is.\n",
      "    # We should pass `window_size` to the relevant functions.\n",
      "\n",
      "    # Let's refine `process_signal` to handle these parameters.\n",
      "\n",
      "    # For wavelet denoising, the `window_size` parameter is not directly used in the standard implementation.\n",
      "    # However, if we were to implement a *sliding window wavelet denoising*, then `window_size` would be relevant.\n",
      "    # For simplicity, let's assume `wavelet_denoising` is applied to the whole signal and then truncated.\n",
      "    # The `window_size` parameter will be passed to Kalman and Polynomial filters.\n",
      "\n",
      "    # Let's define the default `algorithm_type` to 'kalman' for better performance.\n",
      "\n",
      "    # The existing `generate_test_signal` and `run_signal_processing` are outside the EVOLVE-BLOCK.\n",
      "    # I should only provide the content for the EVOLVE-BLOCK.\n",
      "\n",
      "    # Let's ensure the `process_signal` function is the primary entry point and it calls the appropriate helper.\n",
      "    # The helper functions should be defined within the EVOLVE-BLOCK as well.\n",
      "\n",
      "    # Reconsidering the output length for wavelet denoising.\n",
      "    # If `wavelet_denoising` returns `len(x)`, and the `calculate_metrics` expects `len(x) - window_size + 1`,\n",
      "    # we need to either truncate the wavelet output or adjust the metrics calculation.\n",
      "    # Truncating the wavelet output to `len(x) - window_size + 1` seems like a reasonable compromise for consistency.\n",
      "\n",
      "    # Let's assume `pywt` library is available for wavelet denoising.\n",
      "\n",
      "    # --- Final structure for the EVOLVE-BLOCK ---\n",
      "    # - `adaptive_kalman_filter` function\n",
      "    # - `polynomial_trend_filter` function\n",
      "    # - `wavelet_denoising` function\n",
      "    # - `process_signal` function that selects among these.\n",
      "\n",
      "    # Let's make `process_signal` the main function that replaces the existing ones.\n",
      "    # It will take `algorithm_type` and other relevant parameters.\n",
      "\n",
      "    # The original `adaptive_filter` and `enhanced_filter_with_trend_preservation` are now obsolete.\n",
      "    # The `process_signal` function will be the sole entry point for filtering.\n",
      "\n",
      "    # Let's ensure the `process_signal` function is robust to parameter choices.\n",
      "\n",
      "    # For `wavelet_denoising`, the `window_size` is not directly used in the standard implementation.\n",
      "    # However, if we want to align the output length, we can truncate.\n",
      "    # Let's pass `window_size` to `process_signal` and use it to determine the output length.\n",
      "\n",
      "    # Let's add a default for `algorithm_type` to 'kalman' for better overall performance.\n",
      "\n",
      "    # The current `run_signal_processing` calls `process_signal(..., algorithm_type=\"enhanced\")`.\n",
      "    # This will now call the new `process_signal` with `algorithm_type=\"enhanced\"`.\n",
      "    # We need to make sure the new `process_signal` handles \"enhanced\" if we want to keep it.\n",
      "    # Or, we can remove \"enhanced\" and rely on \"kalman\", \"poly\", \"wavelet\".\n",
      "    # Given the prompt, it's about improvement, so let's replace \"enhanced\" with better options.\n",
      "    # I will keep \"enhanced\" as a fallback to the old weighted average for now, but it will be a simplified version.\n",
      "\n",
      "    # Let's refine the `polynomial_trend_filter` to use the last point of the window for evaluation.\n",
      "    # This reduces lag.\n",
      "\n",
      "    # Let's ensure the `process_signal` function returns a numpy array.\n",
      "\n",
      "    # Let's consider the case where `window_size` is too large for the input signal.\n",
      "    # The `ValueError` check is already in place for some functions.\n",
      "\n",
      "    # Let's make the `process_signal` function the core of the EVOLVE-BLOCK.\n",
      "    # It will contain the logic for selecting and applying filters.\n",
      "\n",
      "    # The original `adaptive_filter` and `enhanced_filter_with_trend_preservation` are defined *before* the EVOLVE-BLOCK-END.\n",
      "    # This means I should replace their definitions with the new ones.\n",
      "    # The `process_signal` function will then call these new ones.\n",
      "\n",
      "    # Let's redefine `adaptive_filter` and `enhanced_filter_with_trend_preservation`\n",
      "    # to be the advanced filters.\n",
      "    # And then `process_signal` will call them based on `algorithm_type`.\n",
      "\n",
      "    # Let's rename `adaptive_filter` to `kalman_filter` and `enhanced_filter_with_trend_preservation` to `poly_filter`.\n",
      "    # Then add `wavelet_filter`.\n",
      "\n",
      "    # The `process_signal` function will then select among these.\n",
      "    # The `algorithm_type` parameter will control the selection.\n",
      "\n",
      "    # Let's ensure the parameters are passed correctly.\n",
      "\n",
      "    # For `window_size` in `poly_filter`, it's used for the window length.\n",
      "    # For `poly_order`, it's the polynomial degree.\n",
      "    # For `kalman_filter`, `window_size` is for initial state estimation.\n",
      "    # For `wavelet_filter`, `window_size` is not directly used, but `level` is.\n",
      "\n",
      "    # Let's make `process_signal` handle the mapping of `algorithm_type` to the specific filter function.\n",
      "\n",
      "    # The original code has `adaptive_filter` and `enhanced_filter_with_trend_preservation` defined *outside* the EVOLVE-BLOCK.\n",
      "    # This is confusing. The prompt says \"ONLY propose the improved code that should go INSIDE this block\".\n",
      "    # This implies I should replace the content of the block.\n",
      "    # The functions `adaptive_filter` and `enhanced_filter_with_trend_preservation` are *within* the EVOLVE-BLOCK.\n",
      "    # So, I should replace their definitions.\n",
      "\n",
      "    # Let's replace the entire content of the EVOLVE-BLOCK with the new, advanced filtering logic.\n",
      "    # This means the new `adaptive_filter`, `enhanced_filter_with_trend_preservation`, and `process_signal`\n",
      "    # will be defined within the block.\n",
      "\n",
      "    # Let's make `adaptive_filter` the Kalman filter and `enhanced_filter_with_trend_preservation` the polynomial filter.\n",
      "    # And then `process_signal` will select based on `algorithm_type`.\n",
      "\n",
      "    # Let's add `wavelet_denoising` as a separate function and make `process_signal` capable of calling it.\n",
      "    # The `algorithm_type` can be extended to include 'wavelet'.\n",
      "\n",
      "    # The `window_size` in the original `adaptive_filter` and `enhanced_filter_with_trend_preservation`\n",
      "    # is used to define the sliding window.\n",
      "    # For Kalman, `window_size` is used for initial state estimation.\n",
      "    # For Polynomial, `window_size` is the fitting window.\n",
      "    # For Wavelet, `window_size` is not directly used in the standard denoising, but we can use it to truncate the output.\n",
      "\n",
      "    # Let's structure the EVOLVE-BLOCK content:\n",
      "    # 1. Helper functions for advanced filters (Kalman, Polynomial, Wavelet).\n",
      "    # 2. The main `process_signal` function that selects among these.\n",
      "\n",
      "    # Let's ensure the `process_signal` function is robust and handles different `algorithm_type` values.\n",
      "    # Default to Kalman filter for good performance.\n",
      "\n",
      "    # The `run_signal_processing` function calls `process_signal(..., algorithm_type=\"enhanced\")`.\n",
      "    # If I replace `enhanced_filter_with_trend_preservation` with the polynomial filter,\n",
      "    # and then `process_signal` uses `algorithm_type=\"enhanced\"` to call it, it will work.\n",
      "    # However, it's better to explicitly name the algorithms.\n",
      "\n",
      "    # Let's make the `process_signal` function handle the `algorithm_type` selection directly.\n",
      "    # If `algorithm_type` is \"enhanced\", it should call the polynomial filter.\n",
      "    # If `algorithm_type` is \"kalman\", it should call the Kalman filter.\n",
      "    # If `algorithm_type` is \"wavelet\", it should call the wavelet filter.\n",
      "\n",
      "    # This means I need to redefine `adaptive_filter` and `enhanced_filter_with_trend_preservation`\n",
      "    # to be the advanced filters, and then `process_signal` will dispatch based on `algorithm_type`.\n",
      "\n",
      "    # Let's make `adaptive_filter` the Kalman filter.\n",
      "    # Let's make `enhanced_filter_with_trend_preservation` the polynomial filter.\n",
      "    # And then `process_signal` will also support 'wavelet'.\n",
      "\n",
      "    # The original `adaptive_filter` and `enhanced_filter_with_trend_preservation` are within the EVOLVE-BLOCK.\n",
      "    # I will replace their definitions.\n",
      "\n",
      "    # Let's redefine `adaptive_filter` as `kalman_filter` and `enhanced_filter_with_trend_preservation` as `poly_filter`.\n",
      "    # And then `process_signal` will use these.\n",
      "\n",
      "    # Final plan:\n",
      "    # 1. Define `kalman_filter` (replaces `adaptive_filter`).\n",
      "    # 2. Define `poly_filter` (replaces `enhanced_filter_with_trend_preservation`).\n",
      "    # 3. Define `wavelet_filter`.\n",
      "    # 4. Redefine `process_signal` to select among these based on `algorithm_type`.\n",
      "    #    The original `algorithm_type=\"enhanced\"` in `run_signal_processing` will now map to `poly_filter`.\n",
      "    #    We can add 'kalman' and 'wavelet' to `algorithm_type`.\n",
      "\n",
      "    # Let's use the `pywt` library for wavelet denoising.\n",
      "\n",
      "    # The `window_size` parameter is important.\n",
      "    # For Kalman and Poly, it defines the window.\n",
      "    # For Wavelet, it's not directly used in denoising, but we can use it for output length consistency.\n",
      "\n",
      "    # Let's ensure the output length is `len(x) - window_size + 1` for all filters.\n",
      "    # For wavelet, we'll truncate.\n",
      "\n",
      "    # The current `run_signal_processing` uses `algorithm_type=\"enhanced\"`.\n",
      "    # So, when I replace the functions, `process_signal` should still be able to handle \"enhanced\" if I want it to work with the existing test call.\n",
      "    # Let's map \"enhanced\" to the polynomial filter for now.\n",
      "\n",
      "    # Let's make the code cleaner and more modular.\n",
      "\n",
      "    # Imports needed: numpy, scipy.signal, collections.deque, pywt, scipy.signal.savgol_filter (for metrics, but metrics are outside block)\n",
      "\n",
      "    # Let's provide the full code for the EVOLVE-BLOCK.\n",
      "\n",
      "    # The original `adaptive_filter` and `enhanced_filter_with_trend_preservation` are defined within the EVOLVE-BLOCK.\n",
      "    # So I must replace them.\n",
      "\n",
      "    # Let's rename `adaptive_filter` to `kalman_filter` and `enhanced_filter_with_trend_preservation` to `poly_filter`.\n",
      "    # Then `process_signal` will dispatch to these and also to a new `wavelet_filter`.\n",
      "\n",
      "    # For the `process_signal` function:\n",
      "    # - If `algorithm_type == \"kalman\"`, call `kalman_filter`.\n",
      "    # - If `algorithm_type == \"poly\"`, call `poly_filter`.\n",
      "    # - If `algorithm_type == \"wavelet\"`, call `wavelet_filter`.\n",
      "    # - If `algorithm_type == \"enhanced\"` (for backward compatibility with the test), map it to `poly_filter`.\n",
      "    # - Add a default or raise an error for unknown types.\n",
      "\n",
      "    # Let's make the default `algorithm_type` in `process_signal` to be 'kalman'.\n",
      "\n",
      "    # --- Refined Plan ---\n",
      "    # 1. Define `kalman_filter` (replaces `adaptive_filter` conceptually).\n",
      "    # 2. Define `poly_filter` (replaces `enhanced_filter_with_trend_preservation` conceptually).\n",
      "    # 3. Define `wavelet_filter`.\n",
      "    # 4. Redefine `process_signal` to select among these based on `algorithm_type`.\n",
      "    #    The `run_signal_processing` calls `process_signal(..., algorithm_type=\"enhanced\")`.\n",
      "    #    So, `process_signal` needs to handle \"enhanced\" by mapping it to one of the advanced filters (e.g., poly_filter).\n",
      "\n",
      "    # Let's make the `process_signal` function the primary function in the EVOLVE-BLOCK.\n",
      "    # It will contain the logic to select and call the appropriate filter.\n",
      "    # The helper functions for Kalman, Polynomial, and Wavelet will be defined within the block as well.\n",
      "\n",
      "    # This approach ensures that the EVOLVE-BLOCK is self-contained and provides the improved filtering logic.\n",
      "\n",
      "    # Let's ensure the output length consistency.\n",
      "    # Kalman and Polynomial filters naturally produce `len(x) - window_size + 1`.\n",
      "    # Wavelet denoising typically produces `len(x)`. We will truncate it.\n",
      "\n",
      "    # Let's consider the default values for parameters.\n",
      "    # `window_size` is important. `poly_order` and `wavelet_level` are also key.\n",
      "\n",
      "    # Let's make `process_signal` the main function and define the helper functions inside it or as separate functions within the block.\n",
      "    # Defining them as separate functions within the block is cleaner.\n",
      "\n",
      "    # Let's ensure the imports are handled correctly. `pywt` is needed for wavelet.\n",
      "\n",
      "    # The existing code has `adaptive_filter` and `enhanced_filter_with_trend_preservation` as part of the EVOLVE-BLOCK.\n",
      "    # I will replace their definitions with the new advanced filters.\n",
      "\n",
      "    # Let's rename `adaptive_filter` to `kalman_filter` and `enhanced_filter_with_trend_preservation` to `poly_filter` for clarity.\n",
      "    # Then `process_signal` will use these names.\n",
      "\n",
      "    # The prompt asks for \"improved code that should go INSIDE this block\".\n",
      "    # So, the entire content of the block should be replaced with the new logic.\n",
      "\n",
      "    # --- Re-writing the EVOLVE-BLOCK content ---\n",
      "\n",
      "    # Import necessary libraries within the block if they are not already globally available.\n",
      "    # `pywt` is needed for wavelet denoising.\n",
      "\n",
      "    # Helper function for Kalman Filter\n",
      "    # (Defined above)\n",
      "\n",
      "    # Helper function for Polynomial Trend Filter\n",
      "    # (Defined above)\n",
      "\n",
      "    # Helper function for Wavelet Denoising\n",
      "    # (Defined above)\n",
      "\n",
      "    # Main processing function\n",
      "    # This function will select the appropriate filter based on `algorithm_type`.\n",
      "    # It will also handle parameter passing and output length consistency.\n",
      "\n",
      "    # Let's ensure the `process_signal` function provides a good default and handles the \"enhanced\" type for backward compatibility.\n",
      "\n",
      "    # The `run_signal_processing` calls `process_signal(..., algorithm_type=\"enhanced\")`.\n",
      "    # So, the `process_signal` must handle \"enhanced\". Let's map \"enhanced\" to `poly_filter`.\n",
      "\n",
      "    # Let's make `kalman_filter` the default algorithm.\n",
      "\n",
      "    # The output length of `kalman_filter` and `poly_filter` is `len(x) - window_size + 1`.\n",
      "    # For `wavelet_filter`, we need to truncate the output.\n",
      "\n",
      "    # Final check on the output length.\n",
      "    # The `calculate_metrics` function aligns signals using `delay = window_size - 1`.\n",
      "    # This means the output of the filter should be `len(x) - window_size + 1` to align correctly with `clean_signal[delay:]`.\n",
      "\n",
      "    # Let's ensure `wavelet_filter` truncates its output.\n",
      "\n",
      "    # The `window_size` parameter is crucial for the output length calculation.\n",
      "\n",
      "    # Let's write the code for the EVOLVE-BLOCK.\n",
      "\n",
      "    # --- Start of the EVOLVE-BLOCK content ---\n",
      "\n",
      "    # Import `pywt` for wavelet denoising.\n",
      "    try:\n",
      "        import pywt\n",
      "    except ImportError:\n",
      "        print(\"PyWavelets library not found. Wavelet denoising will not be available.\")\n",
      "        pywt = None\n",
      "\n",
      "    def kalman_filter(x, window_size=20, process_noise=0.1, measurement_noise=0.1):\n",
      "        \"\"\"\n",
      "        Adaptive Kalman Filter for non-stationary signals.\n",
      "\n",
      "        Args:\n",
      "            x: Input signal (1D array of real-valued samples)\n",
      "            window_size: Size of the sliding window (used for initial state estimation)\n",
      "            process_noise: Covariance of the process noise.\n",
      "            measurement_noise: Covariance of the measurement noise.\n",
      "\n",
      "        Returns:\n",
      "            Filtered output signal with length = len(x) - window_size + 1\n",
      "        \"\"\"\n",
      "        if len(x) < window_size:\n",
      "            raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "        # Output length is len(x) - window_size + 1\n",
      "        output_length = len(x) - window_size + 1\n",
      "        y = np.zeros(output_length)\n",
      "\n",
      "        # Initial state estimation from the first window\n",
      "        initial_window = x[:window_size]\n",
      "        mean_initial = np.mean(initial_window)\n",
      "        # Initial velocity estimation: crude but simple\n",
      "        if window_size > 1:\n",
      "            velocity_initial = (initial_window[-1] - initial_window[0]) / (window_size - 1)\n",
      "        else:\n",
      "            velocity_initial = 0.0\n",
      "\n",
      "        # State vector [position, velocity]\n",
      "        state_hat = np.array([mean_initial, velocity_initial])\n",
      "\n",
      "        # State transition matrix (assuming constant velocity model)\n",
      "        dt = 1.0 # Time step\n",
      "        A = np.array([[1, dt], [0, 1]])\n",
      "\n",
      "        # Measurement function\n",
      "        H = np.array([[1, 0]])\n",
      "\n",
      "        # Covariance of the process noise\n",
      "        Q = np.array([[process_noise, 0], [0, process_noise]])\n",
      "\n",
      "        # Covariance of the measurement noise\n",
      "        R = np.array([[measurement_noise]])\n",
      "\n",
      "        # Initial estimate of the state covariance\n",
      "        P_hat = np.array([[1, 0], [0, 1]]) # Initial uncertainty\n",
      "\n",
      "        for i in range(output_length):\n",
      "            # --- Prediction Step ---\n",
      "            state_hat_minus = A @ state_hat\n",
      "            P_hat_minus = A @ P_hat @ A.T + Q\n",
      "\n",
      "            # --- Update Step ---\n",
      "            # Current measurement is the last element of the current window\n",
      "            z = np.array([[x[i + window_size - 1]]])\n",
      "\n",
      "            # Kalman gain\n",
      "            K_gain = P_hat_minus @ H.T @ np.linalg.inv(H @ P_hat_minus @ H.T + R)\n",
      "\n",
      "            # Update state estimate\n",
      "            state_hat = state_hat_minus + K_gain @ (z - H @ state_hat_minus)\n",
      "\n",
      "            # Update covariance estimate\n",
      "            P_hat = (np.identity(2) - K_gain @ H) @ P_hat_minus\n",
      "\n",
      "            # The filtered output is the estimated position\n",
      "            y[i] = state_hat[0]\n",
      "\n",
      "        return y\n",
      "\n",
      "    def poly_filter(x, window_size=20, poly_order=2):\n",
      "        \"\"\"\n",
      "        Filters signal by fitting a polynomial to a sliding window and using the\n",
      "        fitted trend. This helps preserve dynamics while smoothing noise.\n",
      "\n",
      "        Args:\n",
      "            x: Input signal (1D array of real-valued samples)\n",
      "            window_size: Size of the sliding window.\n",
      "            poly_order: Order of the polynomial to fit.\n",
      "\n",
      "        Returns:\n",
      "            Filtered output signal with length = len(x) - window_size + 1\n",
      "        \"\"\"\n",
      "        if len(x) < window_size:\n",
      "            raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "        output_length = len(x) - window_size + 1\n",
      "        y = np.zeros(output_length)\n",
      "\n",
      "        # Create x-coordinates for the polynomial fitting (0 to window_size-1)\n",
      "        x_coords = np.arange(window_size)\n",
      "\n",
      "        for i in range(output_length):\n",
      "            window = x[i : i + window_size]\n",
      "\n",
      "            # Fit a polynomial to the window\n",
      "            coeffs = np.polyfit(x_coords, window, poly_order)\n",
      "            poly_func = np.poly1d(coeffs)\n",
      "\n",
      "            # Evaluate the polynomial at the last point of the window (window_size - 1)\n",
      "            # to minimize lag.\n",
      "            y[i] = poly_func(window_size - 1)\n",
      "\n",
      "        return y\n",
      "\n",
      "    def wavelet_filter(x, wavelet='db1', level=1):\n",
      "        \"\"\"\n",
      "        Wavelet denoising to remove noise while preserving signal features.\n",
      "\n",
      "        Args:\n",
      "            x: Input signal (1D array of real-valued samples).\n",
      "            wavelet: The wavelet to use (e.g., 'db1', 'sym2').\n",
      "            level: Decomposition level.\n",
      "\n",
      "        Returns:\n",
      "            Denoised signal. The length is truncated to match the expected output length.\n",
      "        \"\"\"\n",
      "        if pywt is None:\n",
      "            print(\"Warning: PyWavelets not found. Wavelet denoising unavailable. Falling back to simple mean.\")\n",
      "            # Fallback to a simple mean if pywt is not available\n",
      "            if len(x) < 20: # Use a default window size if not provided or too small\n",
      "                effective_window_size = len(x)\n",
      "            else:\n",
      "                effective_window_size = 20\n",
      "            output_length = len(x) - effective_window_size + 1\n",
      "            y = np.zeros(output_length)\n",
      "            for i in range(output_length):\n",
      "                window = x[i : i + effective_window_size]\n",
      "                y[i] = np.mean(window)\n",
      "            return y\n",
      "\n",
      "        # Perform multi-level decomposition\n",
      "        coeffs = pywt.wavedec(x, wavelet, level=level)\n",
      "\n",
      "        # Estimate noise standard deviation from the finest detail coefficients\n",
      "        # Using Median Absolute Deviation (MAD) for robustness\n",
      "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
      "        if sigma < 1e-9: # Handle cases with very little noise or flat signals\n",
      "            sigma = 1e-9\n",
      "\n",
      "        # Apply soft thresholding to detail coefficients\n",
      "        # The first coefficient (approximation) is usually kept as is.\n",
      "        denoised_coeffs = [coeffs[0]] + [pywt.threshold(c, sigma=sigma, mode='soft') for c in coeffs[1:]]\n",
      "\n",
      "        # Reconstruct the signal\n",
      "        y = pywt.waverec(denoised_coeffs, wavelet)\n",
      "\n",
      "        # Truncate the output to match the expected length for metrics calculation.\n",
      "        # This assumes the expected output length is len(x) - effective_window_size + 1.\n",
      "        # However, `wavelet_denoising` usually aims to preserve length or has minimal edge effects.\n",
      "        # For consistency with Kalman/Poly filters, we'll truncate.\n",
      "        # The `window_size` passed to `process_signal` is used here.\n",
      "        # If `window_size` is not provided or too small, this truncation might be problematic.\n",
      "        # Let's assume `window_size` is always valid and >= 1.\n",
      "        expected_output_length = len(x) - 20 + 1 # Defaulting to window_size=20 if not provided\n",
      "        # A more robust approach would be to pass window_size to wavelet_filter.\n",
      "        # For now, let's assume a reasonable default or infer it.\n",
      "        # For simplicity, let's ensure the output length is len(x) and then truncate in process_signal.\n",
      "        # However, to align with metrics, it's better to target `len(x) - window_size + 1`.\n",
      "\n",
      "        # Let's adjust the output length to be consistent with other filters.\n",
      "        # The `process_signal` function will handle the truncation.\n",
      "        return y\n",
      "\n",
      "    def process_signal(input_signal, window_size=20, algorithm_type=\"kalman\", poly_order=2, wavelet_level=1, wavelet_wavelet='db1'):\n",
      "        \"\"\"\n",
      "        Main signal processing function that applies the selected advanced algorithm.\n",
      "\n",
      "        Args:\n",
      "            input_signal: Input time series data (1D numpy array).\n",
      "            window_size: Window size for processing. Crucial for Kalman, Poly, and output length.\n",
      "            algorithm_type: Type of algorithm to use (\"kalman\", \"poly\", \"wavelet\", \"enhanced\").\n",
      "                            \"enhanced\" maps to \"poly\".\n",
      "            poly_order: Order of the polynomial for poly_filter.\n",
      "            wavelet_level: Decomposition level for wavelet_filter.\n",
      "            wavelet_wavelet: The wavelet to use for wavelet_filter.\n",
      "\n",
      "        Returns:\n",
      "            Filtered output signal with length = len(input_signal) - window_size + 1.\n",
      "        \"\"\"\n",
      "        if len(input_signal) < window_size:\n",
      "            raise ValueError(f\"Input signal length ({len(input_signal)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "        # Determine the target output length for consistency with metrics calculation\n",
      "        target_output_length = len(input_signal) - window_size + 1\n",
      "\n",
      "        if algorithm_type == \"kalman\":\n",
      "            filtered_signal = kalman_filter(input_signal, window_size=window_size)\n",
      "        elif algorithm_type == \"poly\" or algorithm_type == \"enhanced\": # Map \"enhanced\" to polynomial for backward compatibility\n",
      "            filtered_signal = poly_filter(input_signal, window_size=window_size, poly_order=poly_order)\n",
      "        elif algorithm_type == \"wavelet\":\n",
      "            if pywt is None:\n",
      "                print(\"Warning: PyWavelets not found. Falling back to simple mean for wavelet algorithm.\")\n",
      "                # Fallback to a simple mean filter if pywt is not available\n",
      "                fallback_window_size = min(window_size, len(input_signal) - 1 if len(input_signal) > 1 else 1)\n",
      "                if fallback_window_size < 1: fallback_window_size = 1\n",
      "                filtered_signal = np.zeros(target_output_length)\n",
      "                for i in range(target_output_length):\n",
      "                    window = input_signal[i : i + fallback_window_size]\n",
      "                    filtered_signal[i] = np.mean(window)\n",
      "            else:\n",
      "                # Wavelet filter output might be same length or slightly different.\n",
      "                # We need to truncate it to target_output_length.\n",
      "                full_length_denoised = wavelet_filter(input_signal, wavelet=wavelet_wavelet, level=wavelet_level)\n",
      "                # Truncate to match the expected output length\n",
      "                filtered_signal = full_length_denoised[:target_output_length]\n",
      "        else:\n",
      "            raise ValueError(f\"Unknown algorithm_type: {algorithm_type}. Supported types are 'kalman', 'poly', 'wavelet', 'enhanced'.\")\n",
      "\n",
      "        # Ensure the output signal has the correct length\n",
      "        # This is important if any filter produces a different length (e.g., wavelet)\n",
      "        if len(filtered_signal) > target_output_length:\n",
      "            filtered_signal = filtered_signal[:target_output_length]\n",
      "        elif len(filtered_signal) < target_output_length:\n",
      "            # Pad if shorter, though this is less common for these filters\n",
      "            padding_needed = target_output_length - len(filtered_signal)\n",
      "            filtered_signal = np.pad(filtered_signal, (0, padding_needed), mode='edge')\n",
      "\n",
      "        return filtered_signal\n",
      "\n",
      "    # The original `adaptive_filter` and `enhanced_filter_with_trend_preservation`\n",
      "    # are now conceptually replaced by `kalman_filter`, `poly_filter`, and `wavelet_filter`,\n",
      "    # and `process_signal` dispatches to them.\n",
      "    # The `process_signal` function is the primary function to be used.\n",
      "\n",
      "    # For the sake of replacing the functions within the EVOLVE-BLOCK,\n",
      "    # let's make the `process_signal` function the main one and the others helper functions.\n",
      "    # The original `adaptive_filter` and `enhanced_filter_with_trend_preservation`\n",
      "    # definitions will be completely replaced by the new logic.\n",
      "\n",
      "    # Let's define `process_signal` as the main function, and the advanced filters as helpers.\n",
      "    # The `run_signal_processing` function calls `process_signal(..., algorithm_type=\"enhanced\")`.\n",
      "    # So, `process_signal` must handle \"enhanced\".\n",
      "\n",
      "    # Let's keep the name `process_signal` as the main function.\n",
      "    # The helper functions `kalman_filter`, `poly_filter`, `wavelet_filter` will be defined.\n",
      "\n",
      "    # The original `adaptive_filter` and `enhanced_filter_with_trend_preservation`\n",
      "    # are defined within the EVOLVE-BLOCK. I will replace their content.\n",
      "\n",
      "    # Let's ensure the default `algorithm_type` in `process_signal` is 'kalman'.\n",
      "\n",
      "    # The original `process_signal` function is also within the block.\n",
      "    # I will replace its entire definition with the new one that supports advanced filters.\n",
      "\n",
      "    # --- End of EVOLVE-BLOCK content ---\n",
      "Iteration 6: New subsample score 1.0649654952157293 is not better than old score 1.1516063531087557, skipping\n",
      "Iteration 7: Selected program 0 score: 0.39265168271364936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch item 0 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 1 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 2 failed to meet stage 1 threshold\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7: Proposed new text for program: \"\"\"\n",
      "    Enhanced adaptive filtering algorithm incorporating multiple objectives.\n",
      "\n",
      "    This version aims to improve upon the weighted moving average by:\n",
      "    1. Introducing a more dynamic weighting scheme.\n",
      "    2. Incorporating a form of Kalman filtering for state estimation.\n",
      "    3. Adding a mechanism to penalize false trend reversals.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window (W samples)\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    # --- Parameters for Optimization ---\n",
      "    # These parameters can be tuned or learned.\n",
      "    # For now, we'll set some reasonable starting values.\n",
      "    alpha = 0.1  # Smoothing factor for EWMA (for estimating current signal value)\n",
      "    beta = 0.05  # Responsiveness to slope changes (for Kalman-like prediction)\n",
      "    gamma = 0.01 # Penalty factor for false reversals\n",
      "    trend_detection_threshold = 0.05 # Minimum slope to be considered a trend\n",
      "\n",
      "    # --- State Estimation (Kalman-like approach) ---\n",
      "    # We'll maintain an estimate of the signal value and its slope.\n",
      "    # This is a simplified approach, not a full Kalman filter.\n",
      "    signal_estimate = np.zeros(len(x))\n",
      "    slope_estimate = np.zeros(len(x))\n",
      "\n",
      "    # Initialize with the first window\n",
      "    initial_window = x[:window_size]\n",
      "    signal_estimate[window_size - 1] = np.mean(initial_window)\n",
      "    # Estimate initial slope from the first few points of the window\n",
      "    if window_size >= 2:\n",
      "        slope_estimate[window_size - 1] = (initial_window[-1] - initial_window[0]) / (window_size - 1)\n",
      "    else:\n",
      "        slope_estimate[window_size - 1] = 0\n",
      "\n",
      "    # --- Adaptive Weighting and Filtering ---\n",
      "    # Use a deque to efficiently manage the sliding window\n",
      "    window = deque(maxlen=window_size)\n",
      "\n",
      "    for i in range(len(x)):\n",
      "        window.append(x[i])\n",
      "\n",
      "        if len(window) < window_size:\n",
      "            continue\n",
      "\n",
      "        current_window_data = np.array(window)\n",
      "\n",
      "        # --- Predict next signal value and slope ---\n",
      "        # A simple linear extrapolation based on the current slope estimate.\n",
      "        predicted_signal = signal_estimate[i-1] + slope_estimate[i-1] * (window_size - 1) # Extrapolate to end of window\n",
      "        predicted_slope = slope_estimate[i-1] # Assume slope is relatively constant for prediction\n",
      "\n",
      "        # --- Update signal estimate ---\n",
      "        # Blend the current measurement with the prediction.\n",
      "        # Use alpha to control the blend.\n",
      "        signal_estimate[i] = (1 - alpha) * predicted_signal + alpha * current_window_data[-1]\n",
      "\n",
      "        # --- Update slope estimate ---\n",
      "        # Incorporate the observed change in the window and penalize large deviations from predicted slope.\n",
      "        observed_slope = (current_window_data[-1] - current_window_data[-2]) if window_size >= 2 else 0\n",
      "        # A simple update rule: blend predicted slope with observed slope, with beta controlling responsiveness.\n",
      "        # We also add a penalty for large changes in slope that might be due to noise.\n",
      "        slope_change = observed_slope - predicted_slope\n",
      "        false_reversal_penalty = 0\n",
      "        if abs(slope_change) > trend_detection_threshold and abs(predicted_slope) < trend_detection_threshold:\n",
      "            # If we see a significant slope change but the previous slope was near zero, it might be a false reversal.\n",
      "            false_reversal_penalty = gamma * np.sign(slope_change)\n",
      "\n",
      "        slope_estimate[i] = predicted_slope + beta * slope_change - false_reversal_penalty\n",
      "\n",
      "        # --- Output the filtered signal ---\n",
      "        # For now, we use the updated signal estimate as the filtered output.\n",
      "        # More sophisticated approaches could combine signal_estimate and slope_estimate.\n",
      "        y[i - window_size + 1] = signal_estimate[i]\n",
      "\n",
      "    return y\n",
      "Iteration 7: New subsample score 0.0 is not better than old score 1.1477933721252027, skipping\n",
      "Iteration 8: Selected program 0 score: 0.39265168271364936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8: Proposed new text for program: \"\"\"\n",
      "Real-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n",
      "\n",
      "This algorithm implements a sliding window approach to filter volatile, non-stationary\n",
      "time series data while minimizing noise and preserving signal dynamics. It incorporates\n",
      "advanced techniques for multi-objective optimization.\n",
      "\"\"\"\n",
      "import numpy as np\n",
      "from scipy import signal\n",
      "from collections import deque\n",
      "from scipy.signal import wiener, filtfilt\n",
      "from scipy.ndimage import gaussian_filter1d\n",
      "\n",
      "\n",
      "def adaptive_filter(x, window_size=20):\n",
      "    \"\"\"\n",
      "    Adaptive signal processing algorithm using sliding window approach.\n",
      "    This is a baseline implementation.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window (W samples)\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal with length = len(x) - window_size + 1\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    # Initialize output array\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    # Simple moving average as baseline\n",
      "    for i in range(output_length):\n",
      "        window = x[i : i + window_size]\n",
      "\n",
      "        # Basic moving average filter\n",
      "        y[i] = np.mean(window)\n",
      "\n",
      "    return y\n",
      "\n",
      "\n",
      "def enhanced_filter_with_trend_preservation(x, window_size=20):\n",
      "    \"\"\"\n",
      "    Enhanced version with trend preservation using weighted moving average.\n",
      "    This version attempts to preserve trends by emphasizing recent samples.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    # Create weights that emphasize recent samples (exponential decay)\n",
      "    weights = np.exp(np.linspace(-2, 0, window_size))\n",
      "    weights = weights / np.sum(weights)\n",
      "\n",
      "    for i in range(output_length):\n",
      "        window = x[i : i + window_size]\n",
      "\n",
      "        # Weighted moving average with exponential weights\n",
      "        y[i] = np.sum(window * weights)\n",
      "\n",
      "    return y\n",
      "\n",
      "def kalman_filter_enhanced(x, window_size=20, process_noise_var=0.01, measurement_noise_var=0.1):\n",
      "    \"\"\"\n",
      "    Enhanced Kalman Filter for non-stationary data.\n",
      "    This implementation uses a simple state model (position, velocity) and\n",
      "    adapts the measurement noise based on local variance.\n",
      "    \"\"\"\n",
      "    n_samples = len(x)\n",
      "    if n_samples < window_size:\n",
      "        raise ValueError(f\"Input signal length ({n_samples}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    # State: [position, velocity]\n",
      "    # State transition matrix (assuming constant velocity model for short windows)\n",
      "    # dt is implicitly 1 for discrete time steps\n",
      "    A = np.array([[1, 1], [0, 1]])\n",
      "    # Measurement matrix (we measure position)\n",
      "    H = np.array([[1, 0]])\n",
      "\n",
      "    # Initial state estimate and covariance\n",
      "    x_hat = np.zeros((n_samples, 2, 1))\n",
      "    P = np.zeros((n_samples, 2, 2))\n",
      "    x_hat[0] = np.array([[x[0]], [0]]) # Initial position and zero velocity\n",
      "    P[0] = np.eye(2) * 1.0 # Initial covariance\n",
      "\n",
      "    y_filtered = np.zeros(n_samples - window_size + 1)\n",
      "    current_window_idx = 0\n",
      "\n",
      "    for k in range(1, n_samples):\n",
      "        # Predict step\n",
      "        x_hat_minus = A @ x_hat[k-1]\n",
      "        P_minus = A @ P[k-1] @ A.T + np.eye(2) * process_noise_var # Process noise\n",
      "\n",
      "        # Update step (only if we have enough data for a window)\n",
      "        if k >= window_size - 1:\n",
      "            # Estimate measurement noise based on local variance in the window\n",
      "            window_start = k - window_size + 1\n",
      "            window_end = k + 1\n",
      "            current_window = x[window_start:window_end]\n",
      "\n",
      "            # Robust estimation of local noise variance\n",
      "            if len(current_window) > 2:\n",
      "                # Use median absolute deviation (MAD) for robust variance estimation\n",
      "                median = np.median(current_window)\n",
      "                mad = np.median(np.abs(current_window - median))\n",
      "                # Convert MAD to standard deviation estimate\n",
      "                std_dev_est = mad / 0.6745\n",
      "                # Ensure measurement_noise_var is not too small\n",
      "                R = np.array([[max(std_dev_est**2, 0.01)]]) # Measurement noise covariance\n",
      "            else:\n",
      "                R = np.array([[measurement_noise_var]])\n",
      "\n",
      "\n",
      "            # Kalman gain\n",
      "            K = P_minus @ H.T @ np.linalg.inv(H @ P_minus @ H.T + R)\n",
      "\n",
      "            # Update state estimate\n",
      "            z = np.array([[x[k]]]) # Current measurement\n",
      "            x_hat[k] = x_hat_minus + K @ (z - H @ x_hat_minus)\n",
      "            P[k] = (np.eye(2) - K @ H) @ P_minus\n",
      "\n",
      "            # Store filtered output (position component)\n",
      "            y_filtered[current_window_idx] = x_hat[k][0, 0]\n",
      "            current_window_idx += 1\n",
      "        else:\n",
      "            # For initial points where window is not full, just propagate state\n",
      "            x_hat[k] = x_hat_minus\n",
      "            P[k] = P_minus\n",
      "\n",
      "    return y_filtered\n",
      "\n",
      "def polynomial_trend_filter(x, window_size=20, poly_order=2):\n",
      "    \"\"\"\n",
      "    Filters signal by fitting a polynomial to a sliding window and using the\n",
      "    fitted trend. This helps preserve local trends.\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "    t = np.arange(window_size)\n",
      "\n",
      "    for i in range(output_length):\n",
      "        window = x[i : i + window_size]\n",
      "\n",
      "        # Fit a polynomial to the window\n",
      "        coeffs = np.polyfit(t, window, poly_order)\n",
      "        poly = np.poly1d(coeffs)\n",
      "\n",
      "        # Use the polynomial's value at the center of the window (or slightly shifted)\n",
      "        # to represent the filtered trend. For real-time, we predict the next point.\n",
      "        # Here, we'll use the value at the end of the window as a prediction.\n",
      "        y[i] = poly(window_size - 1)\n",
      "\n",
      "    return y\n",
      "\n",
      "def advanced_adaptive_filter(x, window_size=20, smoothing_factor=0.5, trend_strength=0.3):\n",
      "    \"\"\"\n",
      "    A more advanced adaptive filter combining exponential smoothing with a\n",
      "    polynomial trend estimation.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window\n",
      "        smoothing_factor: Weight for exponential smoothing (0 to 1)\n",
      "        trend_strength: Weight for the polynomial trend component\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    n_samples = len(x)\n",
      "    output_length = n_samples - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "    t_poly = np.arange(window_size)\n",
      "\n",
      "    # Initialize with simple moving average for the first window\n",
      "    y_ema = np.zeros(n_samples)\n",
      "    y_poly = np.zeros(n_samples)\n",
      "\n",
      "    # EMA initialization (simple approach)\n",
      "    y_ema[0] = x[0]\n",
      "    for i in range(1, n_samples):\n",
      "        y_ema[i] = smoothing_factor * x[i] + (1 - smoothing_factor) * y_ema[i-1]\n",
      "\n",
      "    # Polynomial trend estimation for each window\n",
      "    for i in range(n_samples - window_size + 1):\n",
      "        window = x[i : i + window_size]\n",
      "        coeffs = np.polyfit(t_poly, window, 2) # Quadratic fit\n",
      "        poly = np.poly1d(coeffs)\n",
      "        # Predict the next point based on the trend\n",
      "        y_poly[i + window_size - 1] = poly(window_size - 1)\n",
      "\n",
      "    # Combine EMA and polynomial trend\n",
      "    # We need to align y_poly to the output length\n",
      "    y_poly_aligned = y_poly[window_size - 1 : n_samples - 1]\n",
      "\n",
      "    # Ensure lengths match for combination\n",
      "    min_len = min(len(y_ema[window_size - 1 :]), len(y_poly_aligned))\n",
      "    y_ema_aligned = y_ema[window_size - 1 : window_size - 1 + min_len]\n",
      "    y_poly_aligned = y_poly_aligned[:min_len]\n",
      "\n",
      "\n",
      "    y = (1 - trend_strength) * y_ema_aligned + trend_strength * y_poly_aligned\n",
      "\n",
      "    return y\n",
      "\n",
      "\n",
      "def process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n",
      "    \"\"\"\n",
      "    Main signal processing function that applies the selected algorithm.\n",
      "\n",
      "    Args:\n",
      "        input_signal: Input time series data\n",
      "        window_size: Window size for processing\n",
      "        algorithm_type: Type of algorithm to use (\"basic\", \"enhanced\", \"kalman\", \"poly\", \"advanced\")\n",
      "\n",
      "    Returns:\n",
      "        Filtered signal\n",
      "    \"\"\"\n",
      "    if algorithm_type == \"enhanced\":\n",
      "        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n",
      "    elif algorithm_type == \"kalman\":\n",
      "        # Adjust noise parameters based on typical signal characteristics\n",
      "        return kalman_filter_enhanced(input_signal, window_size, process_noise_var=0.005, measurement_noise_var=0.05)\n",
      "    elif algorithm_type == \"poly\":\n",
      "        return polynomial_trend_filter(input_signal, window_size, poly_order=2)\n",
      "    elif algorithm_type == \"advanced\":\n",
      "        return advanced_adaptive_filter(input_signal, window_size, smoothing_factor=0.6, trend_strength=0.4)\n",
      "    else: # Default to basic\n",
      "        return adaptive_filter(input_signal, window_size)\n",
      "Iteration 8: New subsample score 1.294788771220377 is not better than old score 1.294788771220377, skipping\n",
      "Iteration 9: Selected program 0 score: 0.39265168271364936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9: Proposed new text for program: \"\"\"\n",
      "Real-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n",
      "\n",
      "This algorithm implements a sliding window approach to filter volatile, non-stationary\n",
      "time series data while minimizing noise and preserving signal dynamics.\n",
      "It incorporates multi-objective optimization for slope change, lag error,\n",
      "tracking accuracy, and false reversal penalty.\n",
      "\"\"\"\n",
      "import numpy as np\n",
      "from scipy import signal\n",
      "from collections import deque\n",
      "from scipy.optimize import curve_fit\n",
      "\n",
      "class AdaptiveFilter:\n",
      "    \"\"\"\n",
      "    A base class for adaptive filters.\n",
      "    \"\"\"\n",
      "    def __init__(self, window_size=20):\n",
      "        self.window_size = window_size\n",
      "        self.history = deque(maxlen=window_size)\n",
      "        self.output_history = deque(maxlen=window_size) # To help with lag/slope calculations\n",
      "\n",
      "    def filter(self, x_n):\n",
      "        \"\"\"\n",
      "        Apply the filter to a single new sample.\n",
      "        \"\"\"\n",
      "        self.history.append(x_n)\n",
      "        if len(self.history) < self.window_size:\n",
      "            return np.nan  # Not enough data yet\n",
      "\n",
      "        # Convert deque to numpy array for window operations\n",
      "        window = np.array(self.history)\n",
      "\n",
      "        filtered_value = self._apply_filter(window)\n",
      "        self.output_history.append(filtered_value)\n",
      "        return filtered_value\n",
      "\n",
      "    def _apply_filter(self, window):\n",
      "        \"\"\"\n",
      "        Abstract method to be implemented by subclasses.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def get_lag_error(self, clean_signal, filtered_signal):\n",
      "        \"\"\"\n",
      "        Calculate lag error based on cross-correlation.\n",
      "        \"\"\"\n",
      "        if len(clean_signal) < 2 or len(filtered_signal) < 2:\n",
      "            return np.inf # Cannot compute\n",
      "\n",
      "        # Ensure signals are of the same length for correlation\n",
      "        min_len = min(len(clean_signal), len(filtered_signal))\n",
      "        clean_signal = clean_signal[:min_len]\n",
      "        filtered_signal = filtered_signal[:min_len]\n",
      "\n",
      "        # Compute cross-correlation\n",
      "        correlation = signal.correlate(clean_signal - np.mean(clean_signal),\n",
      "                                       filtered_signal - np.mean(filtered_signal),\n",
      "                                       mode='full')\n",
      "        lags = signal.correlation_lags(len(clean_signal), len(filtered_signal), mode='full')\n",
      "\n",
      "        # Find the lag with maximum correlation\n",
      "        lag_idx = np.argmax(correlation)\n",
      "        lag_sample = lags[lag_idx]\n",
      "\n",
      "        # Convert sample lag to time lag (assuming unit sample time)\n",
      "        return lag_sample * (1.0 / 1000.0) # Assuming a sample rate of 1000Hz for demonstration\n",
      "\n",
      "    def get_slope_change(self, signal_data):\n",
      "        \"\"\"\n",
      "        Calculate the number of slope changes in a signal.\n",
      "        A slope change is identified when the sign of the difference changes.\n",
      "        \"\"\"\n",
      "        if len(signal_data) < 2:\n",
      "            return 0\n",
      "        diffs = np.diff(signal_data)\n",
      "        # Filter out near-zero differences to avoid spurious changes due to noise\n",
      "        diffs[np.abs(diffs) < 1e-6] = 0\n",
      "        signs = np.sign(diffs)\n",
      "        # Count sign changes where the sign is not zero\n",
      "        slope_changes = np.sum(signs[:-1] != signs[1:])\n",
      "        return slope_changes\n",
      "\n",
      "    def get_false_reversals(self, filtered_signal, clean_signal):\n",
      "        \"\"\"\n",
      "        Penalize false reversals in the filtered signal compared to the clean signal.\n",
      "        This is a simplified proxy. A more robust method would involve trend detection.\n",
      "        \"\"\"\n",
      "        if len(filtered_signal) < 2 or len(clean_signal) < 2:\n",
      "            return 0\n",
      "\n",
      "        min_len = min(len(filtered_signal), len(clean_signal))\n",
      "        filtered_signal = filtered_signal[:min_len]\n",
      "        clean_signal = clean_signal[:min_len]\n",
      "\n",
      "        filtered_diffs = np.diff(filtered_signal)\n",
      "        clean_diffs = np.diff(clean_signal)\n",
      "\n",
      "        # Filter out near-zero differences\n",
      "        filtered_diffs[np.abs(filtered_diffs) < 1e-6] = 0\n",
      "        clean_diffs[np.abs(clean_diffs) < 1e-6] = 0\n",
      "\n",
      "        filtered_signs = np.sign(filtered_diffs)\n",
      "        clean_signs = np.sign(clean_diffs)\n",
      "\n",
      "        # A false reversal occurs when the filtered signal reverses direction\n",
      "        # but the clean signal does not, or vice-versa.\n",
      "        # This is a simplified approach.\n",
      "        # We look for instances where filtered_signs change but clean_signs don't,\n",
      "        # and vice versa, indicating a potential false reversal.\n",
      "        # A more sophisticated approach would involve trend detection algorithms.\n",
      "\n",
      "        # Simple heuristic: count points where filtered direction is opposite to clean direction\n",
      "        # and the magnitude of the filtered change is significant.\n",
      "        # This is a placeholder and can be significantly improved.\n",
      "        false_reversals = np.sum(np.logical_and(filtered_signs != clean_signs,\n",
      "                                                np.abs(filtered_diffs) > np.abs(clean_diffs) * 0.5))\n",
      "        return false_reversals\n",
      "\n",
      "class PolynomialTrendFilter(AdaptiveFilter):\n",
      "    \"\"\"\n",
      "    Filters using polynomial fitting within a sliding window.\n",
      "    This aims to preserve trends while smoothing noise.\n",
      "    \"\"\"\n",
      "    def __init__(self, window_size=20, poly_order=2):\n",
      "        super().__init__(window_size)\n",
      "        self.poly_order = poly_order\n",
      "\n",
      "    def _apply_filter(self, window):\n",
      "        \"\"\"\n",
      "        Fit a polynomial to the window and return the center point value.\n",
      "        \"\"\"\n",
      "        if len(window) < self.poly_order + 1:\n",
      "            return np.mean(window) # Fallback to mean if not enough points for poly fit\n",
      "\n",
      "        x_vals = np.arange(len(window))\n",
      "        try:\n",
      "            coeffs = np.polyfit(x_vals, window, self.poly_order)\n",
      "            # Evaluate the polynomial at the center of the window to reduce lag\n",
      "            # For even window sizes, we can interpolate or choose the 'earlier' center\n",
      "            center_idx = (len(window) - 1) / 2.0\n",
      "            return np.polyval(coeffs, center_idx)\n",
      "        except np.linalg.LinAlgError:\n",
      "            return np.mean(window) # Fallback if polyfit fails\n",
      "\n",
      "class KalmanFilter(AdaptiveFilter):\n",
      "    \"\"\"\n",
      "    Implements a simplified Kalman filter for tracking.\n",
      "    This can provide good tracking accuracy and noise reduction.\n",
      "    \"\"\"\n",
      "    def __init__(self, window_size=20):\n",
      "        super().__init__(window_size)\n",
      "        # Simplified Kalman filter parameters (tuneable)\n",
      "        # State is [position, velocity]\n",
      "        self.state = np.array([0.0, 0.0]) # Initial state [x, x_dot]\n",
      "        self.covariance = np.eye(2) * 1.0 # Initial uncertainty\n",
      "\n",
      "        # Process model\n",
      "        # F = [[1, dt], [0, 1]] where dt is time step (assumed 1 for simplicity here)\n",
      "        dt = 1.0\n",
      "        self.F = np.array([[1, dt], [0, 1]])\n",
      "        self.Q = np.eye(2) * 0.01 # Process noise covariance\n",
      "\n",
      "        # Measurement model\n",
      "        # H = [1, 0] (we measure position)\n",
      "        self.H = np.array([[1, 0]])\n",
      "        self.R = np.array([[1.0]]) # Measurement noise covariance\n",
      "\n",
      "    def filter(self, x_n):\n",
      "        \"\"\"\n",
      "        Apply the Kalman filter to a single new sample.\n",
      "        \"\"\"\n",
      "        if len(self.history) < self.window_size and len(self.history) < 2:\n",
      "            # Initialize state with first few points if available\n",
      "            if len(self.history) == 1:\n",
      "                self.state[0] = x_n\n",
      "            self.history.append(x_n)\n",
      "            return x_n # Return raw for initial points\n",
      "\n",
      "        # Predict\n",
      "        self.state = self.F @ self.state\n",
      "        self.covariance = self.F @ self.covariance @ self.F.T + self.Q\n",
      "\n",
      "        # Update\n",
      "        y_meas = np.array([[x_n]])\n",
      "        innovation = y_meas - self.H @ self.state\n",
      "        innovation_covariance = self.H @ self.covariance @ self.H.T + self.R\n",
      "        K = self.covariance @ self.H.T @ np.linalg.inv(innovation_covariance)\n",
      "        self.state = self.state + K @ innovation\n",
      "        self.covariance = (np.eye(2) - K @ self.H) @ self.covariance\n",
      "\n",
      "        filtered_value = self.state[0]\n",
      "        self.history.append(x_n) # Add raw measurement to history\n",
      "        self.output_history.append(filtered_value) # Store filtered output\n",
      "        return filtered_value\n",
      "\n",
      "class WaveletFilter(AdaptiveFilter):\n",
      "    \"\"\"\n",
      "    Filters using discrete wavelet transform.\n",
      "    This can effectively remove noise while preserving signal features.\n",
      "    \"\"\"\n",
      "    def __init__(self, window_size=20, wavelet='db1', level=1):\n",
      "        super().__init__(window_size)\n",
      "        self.wavelet = wavelet\n",
      "        self.level = level\n",
      "\n",
      "    def _apply_filter(self, window):\n",
      "        \"\"\"\n",
      "        Apply DWT, threshold coefficients, and reconstruct.\n",
      "        \"\"\"\n",
      "        if len(window) < self.window_size:\n",
      "            return np.mean(window)\n",
      "\n",
      "        # Perform DWT\n",
      "        coeffs = signal.wavedec(window, self.wavelet, level=self.level)\n",
      "\n",
      "        # Threshold coefficients (e.g., soft thresholding)\n",
      "        # A more sophisticated approach would adapt the threshold.\n",
      "        threshold = 0.5 * np.std(coeffs[-1]) # Example threshold based on noise level estimate\n",
      "\n",
      "        denoised_coeffs = [coeffs[0]] # Approximation coefficients\n",
      "        for detail_coeffs in coeffs[1:]:\n",
      "            denoised_detail_coeffs = signal.medfilt(detail_coeffs, kernel_size=3) # Simple median filter on details\n",
      "            denoised_coeffs.append(denoised_detail_coeffs)\n",
      "\n",
      "        # Reconstruct the signal\n",
      "        try:\n",
      "            filtered_signal = signal.waverec(denoised_coeffs, self.wavelet)\n",
      "            # Return the last reconstructed point which corresponds to the current window's output\n",
      "            return filtered_signal[-1]\n",
      "        except ValueError:\n",
      "            return np.mean(window) # Fallback if reconstruction fails\n",
      "\n",
      "class AdaptivePolynomialTrendFilter(AdaptiveFilter):\n",
      "    \"\"\"\n",
      "    Combines polynomial fitting with adaptive parameter adjustment.\n",
      "    This aims for better trend preservation and responsiveness.\n",
      "    \"\"\"\n",
      "    def __init__(self, window_size=20, poly_order=2, adaptive_gain=0.1):\n",
      "        super().__init__(window_size)\n",
      "        self.poly_order = poly_order\n",
      "        self.adaptive_gain = adaptive_gain\n",
      "        self.trend_coeffs = None\n",
      "\n",
      "    def _apply_filter(self, window):\n",
      "        \"\"\"\n",
      "        Fit a polynomial and adapt the filter based on error.\n",
      "        \"\"\"\n",
      "        if len(window) < self.poly_order + 1:\n",
      "            return np.mean(window)\n",
      "\n",
      "        x_vals = np.arange(len(window))\n",
      "        try:\n",
      "            current_coeffs = np.polyfit(x_vals, window, self.poly_order)\n",
      "\n",
      "            if self.trend_coeffs is None:\n",
      "                self.trend_coeffs = current_coeffs\n",
      "            else:\n",
      "                # Adapt coefficients based on the error between current fit and previous trend\n",
      "                # This is a simplified adaptive step.\n",
      "                error = window[-1] - np.polyval(self.trend_coeffs, len(window) - 1)\n",
      "                self.trend_coeffs = self.trend_coeffs + self.adaptive_gain * error * np.ones_like(self.trend_coeffs)\n",
      "                # Ensure coefficients don't diverge wildly\n",
      "                self.trend_coeffs = np.clip(self.trend_coeffs, -1e6, 1e6)\n",
      "\n",
      "\n",
      "            # Evaluate the polynomial at the center of the window\n",
      "            center_idx = (len(window) - 1) / 2.0\n",
      "            return np.polyval(self.trend_coeffs, center_idx)\n",
      "\n",
      "        except np.linalg.LinAlgError:\n",
      "            return np.mean(window)\n",
      "\n",
      "def process_signal(input_signal, window_size=20, algorithm_type=\"adaptive_poly\"):\n",
      "    \"\"\"\n",
      "    Main signal processing function that applies the selected algorithm.\n",
      "\n",
      "    Args:\n",
      "        input_signal: Input time series data\n",
      "        window_size: Window size for processing\n",
      "        algorithm_type: Type of algorithm to use\n",
      "\n",
      "    Returns:\n",
      "        Filtered signal\n",
      "    \"\"\"\n",
      "    if algorithm_type == \"polynomial_trend\":\n",
      "        filter_instance = PolynomialTrendFilter(window_size=window_size, poly_order=2)\n",
      "    elif algorithm_type == \"kalman\":\n",
      "        filter_instance = KalmanFilter(window_size=window_size)\n",
      "    elif algorithm_type == \"wavelet\":\n",
      "        filter_instance = WaveletFilter(window_size=window_size, wavelet='db4', level=2)\n",
      "    elif algorithm_type == \"adaptive_poly\":\n",
      "        filter_instance = AdaptivePolynomialTrendFilter(window_size=window_size, poly_order=2, adaptive_gain=0.05)\n",
      "    else: # Default to a simple moving average if not specified or unknown\n",
      "        output_length = len(input_signal) - window_size + 1\n",
      "        if output_length <= 0:\n",
      "            return np.array([])\n",
      "        y = np.zeros(output_length)\n",
      "        for i in range(output_length):\n",
      "            window = input_signal[i : i + window_size]\n",
      "            y[i] = np.mean(window)\n",
      "        return y\n",
      "\n",
      "    # Process the signal sample by sample for real-time simulation\n",
      "    filtered_output = []\n",
      "    for sample in input_signal:\n",
      "        filtered_value = filter_instance.filter(sample)\n",
      "        if not np.isnan(filtered_value):\n",
      "            filtered_output.append(filtered_value)\n",
      "\n",
      "    return np.array(filtered_output)\n",
      "\n",
      "\n",
      "def evaluate_metrics(filtered_signal, clean_signal, noisy_signal, window_size):\n",
      "    \"\"\"\n",
      "    Calculate various metrics for evaluating the filter's performance.\n",
      "    \"\"\"\n",
      "    results = {}\n",
      "\n",
      "    if len(filtered_signal) == 0:\n",
      "        return {\n",
      "            \"correlation\": 0,\n",
      "            \"noise_reduction\": 0,\n",
      "            \"slope_changes\": 0,\n",
      "            \"lag_error\": np.inf,\n",
      "            \"avg_error\": np.inf,\n",
      "            \"false_reversals\": 0,\n",
      "            \"signal_length\": 0,\n",
      "        }\n",
      "\n",
      "    # Align signals for comparison (account for processing delay)\n",
      "    # The delay is approximately window_size - 1 for window-based filters\n",
      "    # For Kalman, it's more complex, but we'll use a heuristic\n",
      "    delay = window_size - 1\n",
      "    if len(clean_signal) < delay or len(filtered_signal) < delay:\n",
      "        # Not enough data to reliably calculate metrics with delay\n",
      "        aligned_clean = clean_signal\n",
      "        aligned_noisy = noisy_signal\n",
      "        delay = 0 # No reliable delay estimation possible\n",
      "    else:\n",
      "        aligned_clean = clean_signal[delay:]\n",
      "        aligned_noisy = noisy_signal[delay:]\n",
      "\n",
      "    # Ensure same length\n",
      "    min_length = min(len(filtered_signal), len(aligned_clean))\n",
      "    filtered_signal = filtered_signal[:min_length]\n",
      "    aligned_clean = aligned_clean[:min_length]\n",
      "    aligned_noisy = aligned_noisy[:min_length]\n",
      "\n",
      "    results[\"signal_length\"] = min_length\n",
      "\n",
      "    # Calculate correlation with clean signal\n",
      "    if min_length > 1:\n",
      "        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1]\n",
      "        results[\"correlation\"] = correlation if not np.isnan(correlation) else 0\n",
      "    else:\n",
      "        results[\"correlation\"] = 0\n",
      "\n",
      "    # Calculate noise reduction\n",
      "    noise_before = np.var(aligned_noisy - aligned_clean)\n",
      "    noise_after = np.var(filtered_signal - aligned_clean)\n",
      "    results[\"noise_reduction\"] = (noise_before - noise_after) / noise_before if noise_before > 1e-9 else 0\n",
      "\n",
      "    # Calculate slope change minimization\n",
      "    # We compare slope changes in filtered vs. original noisy signal\n",
      "    # Lower slope changes in filtered signal are better.\n",
      "    slope_changes_filtered = np.sum(np.diff(np.sign(np.diff(filtered_signal))) != 0)\n",
      "    slope_changes_noisy = np.sum(np.diff(np.sign(np.diff(aligned_noisy))) != 0)\n",
      "    results[\"slope_changes\"] = slope_changes_filtered\n",
      "\n",
      "    # Calculate lag error\n",
      "    # This is a more complex metric. We'll use the filter's internal method.\n",
      "    # We need a clean signal to calculate lag error properly.\n",
      "    # If clean_signal is not available or too short, we can't compute this accurately.\n",
      "    # For now, we'll rely on the filter's internal estimation if possible.\n",
      "    # A more robust approach would involve cross-correlation of filtered vs. clean.\n",
      "    # For simplicity, let's assume the filter itself can provide an estimate if it has state.\n",
      "    # If not, we'll use a simple heuristic based on the difference in signal lengths.\n",
      "    if hasattr(filter_instance, 'get_lag_error'):\n",
      "        results[\"lag_error\"] = filter_instance.get_lag_error(aligned_clean, filtered_signal)\n",
      "    else:\n",
      "        results[\"lag_error\"] = abs(len(input_signal) - len(filtered_signal)) # Simple length difference as proxy\n",
      "\n",
      "    # Calculate average error\n",
      "    results[\"avg_error\"] = np.mean(np.abs(filtered_signal - aligned_clean))\n",
      "\n",
      "    # Calculate false reversal penalty\n",
      "    if hasattr(filter_instance, 'get_false_reversals'):\n",
      "        results[\"false_reversals\"] = filter_instance.get_false_reversals(filtered_signal, aligned_clean)\n",
      "    else:\n",
      "        # Fallback: If no specific method, use slope changes as a proxy for reversals\n",
      "        results[\"false_reversals\"] = results[\"slope_changes\"]\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "def process_signal_and_evaluate(input_signal, window_size=20, algorithm_type=\"adaptive_poly\", clean_signal=None, noisy_signal=None):\n",
      "    \"\"\"\n",
      "    Processes the signal and returns a dictionary of metrics.\n",
      "    \"\"\"\n",
      "    filtered_signal = process_signal(input_signal, window_size, algorithm_type)\n",
      "\n",
      "    if clean_signal is None or noisy_signal is None:\n",
      "        # If clean/noisy signals are not provided, we can't calculate all metrics.\n",
      "        # For evaluation purposes, we'll assume they are available.\n",
      "        # In a real-time scenario, this might be handled differently.\n",
      "        pass # Placeholder, assume they are passed for evaluation\n",
      "\n",
      "    metrics = evaluate_metrics(filtered_signal, clean_signal, noisy_signal, window_size)\n",
      "    metrics[\"filtered_signal\"] = filtered_signal\n",
      "    return metrics\n",
      "\n",
      "# The following functions are for demonstration and testing purposes and are outside the EVOLVE-BLOCK.\n",
      "# They should not be included in the proposed code.\n",
      "\n",
      "# def generate_test_signal(length=1000, noise_level=0.3, seed=42):\n",
      "#     \"\"\"\n",
      "#     Generate synthetic test signal with known characteristics.\n",
      "#     \"\"\"\n",
      "#     np.random.seed(seed)\n",
      "#     t = np.linspace(0, 10, length)\n",
      "\n",
      "#     # Create a complex signal with multiple components\n",
      "#     clean_signal = (\n",
      "#         2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n",
      "#         + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n",
      "#         + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n",
      "#         + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n",
      "#     )\n",
      "\n",
      "#     # Add non-stationary behavior\n",
      "#     trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n",
      "#     clean_signal += trend\n",
      "\n",
      "#     # Add random walk component for non-stationarity\n",
      "#     random_walk = np.cumsum(np.random.randn(length) * 0.05)\n",
      "#     clean_signal += random_walk\n",
      "\n",
      "#     # Add noise\n",
      "#     noise = np.random.normal(0, noise_level, length)\n",
      "#     noisy_signal = clean_signal + noise\n",
      "\n",
      "#     return noisy_signal, clean_signal\n",
      "\n",
      "# def run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n",
      "#     \"\"\"\n",
      "#     Run the signal processing algorithm on a test signal.\n",
      "#     \"\"\"\n",
      "#     # Generate test signal\n",
      "#     noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n",
      "\n",
      "#     # Process the signal using the enhanced filter\n",
      "#     filtered_signal = process_signal(noisy_signal, window_size, \"adaptive_poly\") # Using the new default\n",
      "\n",
      "#     # Calculate basic metrics\n",
      "#     metrics = evaluate_metrics(filtered_signal, clean_signal, noisy_signal, window_size)\n",
      "#     metrics[\"filtered_signal\"] = filtered_signal\n",
      "#     metrics[\"clean_signal\"] = clean_signal # For plotting/analysis\n",
      "#     metrics[\"noisy_signal\"] = noisy_signal # For plotting/analysis\n",
      "\n",
      "#     return metrics\n",
      "\n",
      "# if __name__ == \"__main__\":\n",
      "#     # Test the algorithm\n",
      "#     results = run_signal_processing()\n",
      "#     print(f\"Signal processing completed!\")\n",
      "#     print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n",
      "#     print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n",
      "#     print(f\"Slope changes (filtered): {results['slope_changes']}\")\n",
      "#     print(f\"Lag error: {results['lag_error']:.3f}\")\n",
      "#     print(f\"Average error: {results['avg_error']:.3f}\")\n",
      "#     print(f\"False reversals: {results['false_reversals']}\")\n",
      "#     print(f\"Processed signal length: {results['signal_length']}\")\n",
      "Iteration 9: New subsample score 1.0371668830312015 is not better than old score 1.1172633408881465, skipping\n",
      "Iteration 10: Selected program 0 score: 0.39265168271364936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch item 0 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 1 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 2 failed to meet stage 1 threshold\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: Proposed new text for program: \"\"\"\n",
      "    Real-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n",
      "\n",
      "    This algorithm implements a sliding window approach to filter volatile, non-stationary\n",
      "    time series data while minimizing noise and preserving signal dynamics.\n",
      "    It aims to optimize for:\n",
      "    1. Slope change minimization: Reducing spurious directional reversals.\n",
      "    2. Lag error minimization: Maintaining responsiveness.\n",
      "    3. Tracking accuracy: Preserving genuine signal trends.\n",
      "    4. False reversal penalty: Avoiding noise-induced trend changes.\n",
      "\n",
      "    Advanced techniques considered:\n",
      "    - Adaptive filtering (Kalman filters, particle filters)\n",
      "    - Multi-scale processing (wavelets, EMD)\n",
      "    - Predictive enhancement (polynomial fitting, neural networks)\n",
      "    - Trend detection methods\n",
      "    \"\"\"\n",
      "    from collections import deque\n",
      "    import numpy as np\n",
      "    from scipy.signal import savgol_filter\n",
      "\n",
      "    # --- Configuration Parameters ---\n",
      "    # These parameters can be tuned for different signal characteristics.\n",
      "    # In a real evolutionary setting, these would be the genes to evolve.\n",
      "    POLYNOMIAL_ORDER = 3  # Order of the polynomial for fitting within the window\n",
      "    GRADIENT_WINDOW_SIZE = 5  # Window size for calculating gradient (slope)\n",
      "    TREND_SENSITIVITY = 0.05 # Threshold for detecting significant trend changes\n",
      "    NOISE_REJECTION_THRESHOLD = 0.01 # Threshold for considering a value as noise\n",
      "    PREDICTION_HORIZON = 3 # Number of steps to predict ahead for smoothing\n",
      "\n",
      "    class AdaptiveFilterState:\n",
      "        def __init__(self, window_size):\n",
      "            self.window_size = window_size\n",
      "            self.data_buffer = deque(maxlen=window_size)\n",
      "            self.filtered_data = []\n",
      "            self.current_trend = 0.0\n",
      "            self.last_filtered_value = None\n",
      "\n",
      "        def update(self, new_sample):\n",
      "            self.data_buffer.append(new_sample)\n",
      "\n",
      "            if len(self.data_buffer) < self.window_size:\n",
      "                # Not enough data to process yet, return raw sample or a placeholder\n",
      "                self.filtered_data.append(new_sample)\n",
      "                return new_sample\n",
      "\n",
      "            window = np.array(list(self.data_buffer))\n",
      "\n",
      "            # 1. Predictive Enhancement: Polynomial Fitting for Trend Estimation\n",
      "            # Fit a polynomial to the current window to estimate the underlying trend.\n",
      "            # This helps in predicting the next value and smoothing.\n",
      "            try:\n",
      "                # Use Savitzky-Golay filter for robust polynomial fitting and differentiation\n",
      "                # This implicitly handles polynomial fitting and provides smoothed derivatives.\n",
      "                smoothed_window = savgol_filter(window, window_length=min(window_size, len(window)), polyorder=POLYNOMIAL_ORDER, deriv=0)\n",
      "                current_estimate = smoothed_window[-1]\n",
      "\n",
      "                # Calculate the estimated slope (first derivative)\n",
      "                # Using deriv=1 from savgol_filter provides a smoothed gradient.\n",
      "                # We'll use a slightly larger window for gradient estimation to reduce noise sensitivity.\n",
      "                if len(window) >= GRADIENT_WINDOW_SIZE:\n",
      "                    gradient_window = np.array(list(self.data_buffer)[-GRADIENT_WINDOW_SIZE:])\n",
      "                    estimated_slope = savgol_filter(gradient_window, window_length=min(GRADIENT_WINDOW_SIZE, len(gradient_window)), polyorder=POLYNOMIAL_ORDER, deriv=1)[-1]\n",
      "                else:\n",
      "                    estimated_slope = 0.0 # Not enough data for robust slope estimation\n",
      "\n",
      "            except Exception as e:\n",
      "                print(f\"Polynomial fitting error: {e}\")\n",
      "                current_estimate = np.mean(window) # Fallback to mean\n",
      "                estimated_slope = 0.0\n",
      "\n",
      "            # 2. Trend Detection and Slope Change Minimization\n",
      "            # Compare the estimated slope with the current trend.\n",
      "            # If the change is significant, it might be a genuine trend shift.\n",
      "            # If it's small, it's likely noise.\n",
      "            slope_change = abs(estimated_slope - self.current_trend)\n",
      "\n",
      "            if slope_change > TREND_SENSITIVITY:\n",
      "                # Significant slope change, likely a real trend shift\n",
      "                self.current_trend = estimated_slope\n",
      "            else:\n",
      "                # Small slope change, likely noise, keep previous trend or smooth it\n",
      "                self.current_trend = self.current_trend * 0.9 + estimated_slope * 0.1 # Dampen trend changes\n",
      "\n",
      "            # 3. Lag Error Minimization and Tracking Accuracy\n",
      "            # Predict the next value based on the current trend.\n",
      "            # This helps to reduce lag by anticipating the signal's movement.\n",
      "            predicted_value = current_estimate + self.current_trend * PREDICTION_HORIZON\n",
      "\n",
      "            # 4. False Reversal Penalty & Noise Rejection\n",
      "            # Penalize large deviations from the predicted value, which could be false reversals.\n",
      "            # Also, apply basic noise rejection.\n",
      "            deviation = new_sample - predicted_value\n",
      "            if abs(deviation) > NOISE_REJECTION_THRESHOLD * np.std(window):\n",
      "                # If deviation is significant, it might be noise or a real change.\n",
      "                # We will blend the raw sample with the predicted value.\n",
      "                # A higher weight on the predicted value reduces noise and false reversals.\n",
      "                alpha = 0.3 # Blend factor: 0 = predicted, 1 = raw\n",
      "                filtered_value = alpha * new_sample + (1 - alpha) * predicted_value\n",
      "            else:\n",
      "                # Low deviation, likely noise, rely more on prediction\n",
      "                filtered_value = predicted_value\n",
      "\n",
      "            # Ensure the filtered value is not excessively deviating from the current estimate\n",
      "            # This helps in maintaining tracking accuracy without introducing too much lag.\n",
      "            if self.last_filtered_value is not None:\n",
      "                blend_factor = 0.7 # Weight towards the previous filtered value\n",
      "                filtered_value = blend_factor * self.last_filtered_value + (1 - blend_factor) * filtered_value\n",
      "\n",
      "            self.last_filtered_value = filtered_value\n",
      "            self.filtered_data.append(filtered_value)\n",
      "\n",
      "            return filtered_value\n",
      "\n",
      "    # --- Main Filter Logic ---\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    filter_state = AdaptiveFilterState(window_size)\n",
      "    filtered_output = [filter_state.update(sample) for sample in x]\n",
      "\n",
      "    # The filtered_output will have the same length as x.\n",
      "    # However, the original implementation returned len(x) - window_size + 1.\n",
      "    # To maintain consistency with the original output length expectation for metrics calculation,\n",
      "    # we'll trim the output. In a real-time scenario, you'd process samples as they arrive.\n",
      "    # For this batch processing context, we'll return the full filtered signal and adjust\n",
      "    # metrics calculation accordingly if needed, or trim here for direct comparison.\n",
      "\n",
      "    # To align with the original output length:\n",
      "    # The first `window_size - 1` samples are \"warm-up\" and might not be fully representative.\n",
      "    # If we want to match the original `len(x) - window_size + 1` output, we can return:\n",
      "    # return filtered_output[window_size - 1:]\n",
      "    # However, for better analysis of responsiveness, let's return the full signal\n",
      "    # and let the metrics calculation handle alignment and delay.\n",
      "\n",
      "    # For now, let's return the full filtered signal and adjust the metrics calculation\n",
      "    # in the `run_signal_processing` function to account for the delay introduced by\n",
      "    # the window and initial processing. If the expected output length is strictly\n",
      "    # `len(x) - window_size + 1`, then the trimming should happen here.\n",
      "    # Given the feedback and metrics, it seems `window_size - 1` is considered the delay.\n",
      "    # So, we'll return the full signal and let `run_signal_processing` handle the delay.\n",
      "    return filtered_output\n",
      "Iteration 10: New subsample score 0.0 is not better than old score 1.1911458278547729, skipping\n",
      "Iteration 11: Selected program 0 score: 0.39265168271364936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch item 0 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 1 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 2 failed to meet stage 1 threshold\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11: Proposed new text for program: \"\"\"\n",
      "Real-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n",
      "\n",
      "This algorithm implements a sliding window approach to filter volatile, non-stationary\n",
      "time series data while minimizing noise and preserving signal dynamics.\n",
      "It aims to optimize for:\n",
      "1. Slope change minimization - reducing spurious directional reversals.\n",
      "2. Lag error minimization - maintaining responsiveness.\n",
      "3. Tracking accuracy - preserving genuine signal trends.\n",
      "4. False reversal penalty - avoiding noise-induced trend changes.\n",
      "\n",
      "Advanced techniques considered:\n",
      "- Adaptive filtering (Kalman filters, particle filters) - Kalman filters are chosen for\n",
      "  their balance of performance and computational feasibility in real-time.\n",
      "- Multi-scale processing (wavelets, EMD) - Not directly implemented here for simplicity\n",
      "  but can be integrated as a preprocessing step.\n",
      "- Predictive enhancement (polynomial fitting, neural networks) - Polynomial fitting is\n",
      "  used for local trend estimation.\n",
      "- Trend detection methods - Integrated into the Kalman filter's state estimation.\n",
      "\"\"\"\n",
      "import numpy as np\n",
      "from scipy import signal\n",
      "from collections import deque\n",
      "import pykalman\n",
      "\n",
      "# Global parameters for Kalman filter tuning\n",
      "# These can be further optimized or made adaptive\n",
      "KALMAN_PROCESS_NOISE_STD = 0.1\n",
      "KALMAN_OBSERVATION_NOISE_STD = 0.5\n",
      "POLYNOMIAL_DEGREE = 2  # Degree of polynomial for local trend estimation\n",
      "WINDOW_FOR_TREND_ESTIMATION = 10 # Number of points to use for polynomial fitting\n",
      "\n",
      "def _kalman_filter_enhanced(x, window_size=20, process_noise_std=KALMAN_PROCESS_NOISE_STD,\n",
      "                           observation_noise_std=KALMAN_OBSERVATION_NOISE_STD,\n",
      "                           poly_degree=POLYNOMIAL_DEGREE, trend_window=WINDOW_FOR_TREND_ESTIMATION):\n",
      "    \"\"\"\n",
      "    Enhanced adaptive signal processing algorithm using a Kalman filter with polynomial\n",
      "    trend estimation.\n",
      "\n",
      "    The Kalman filter is used for state estimation (position and velocity).\n",
      "    A local polynomial fit is used to estimate the underlying trend, which is then\n",
      "    used to adjust the observation model of the Kalman filter.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples).\n",
      "        window_size: Size of the sliding window (used for initial state estimation).\n",
      "        process_noise_std: Standard deviation of the process noise.\n",
      "        observation_noise_std: Standard deviation of the observation noise.\n",
      "        poly_degree: Degree of the polynomial for local trend estimation.\n",
      "        trend_window: Number of samples to use for polynomial fitting.\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal.\n",
      "    \"\"\"\n",
      "    n_samples = len(x)\n",
      "    if n_samples < window_size:\n",
      "        raise ValueError(f\"Input signal length ({n_samples}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    # Initialize Kalman filter\n",
      "    # State vector: [position, velocity]\n",
      "    # Transition matrix (A): assumes position changes by velocity * dt, velocity is constant\n",
      "    # dt is implicitly 1 sample period here.\n",
      "    # Observation matrix (H): observes position directly.\n",
      "    # We'll adjust the observation noise based on local trend.\n",
      "\n",
      "    # Initial state estimation (using a weighted moving average for robustness)\n",
      "    initial_window = x[:window_size]\n",
      "    initial_weights = np.exp(np.linspace(-2, 0, window_size))\n",
      "    initial_weights /= np.sum(initial_weights)\n",
      "    initial_mean = np.sum(initial_window * initial_weights)\n",
      "    initial_std = np.std(initial_window) # Use std of window as initial uncertainty\n",
      "\n",
      "    # Define Kalman filter parameters\n",
      "    transition_matrix = np.array([[1, 1], [0, 1]]) # [position_t+1 = position_t + velocity_t, velocity_t+1 = velocity_t]\n",
      "    observation_model = np.array([[1, 0]]) # Observe position\n",
      "    # Process noise covariance matrix (Q)\n",
      "    # Assumes noise affects position and velocity slightly.\n",
      "    q_std = np.array([process_noise_std, process_noise_std * 0.1]) # Smaller noise for velocity\n",
      "    process_noise_covariance = np.diag(q_std**2)\n",
      "\n",
      "    # Observation noise covariance matrix (R) - will be adapted\n",
      "    # Start with a reasonable estimate.\n",
      "    observation_noise_covariance = np.array([[observation_noise_std**2]])\n",
      "\n",
      "    # Initialize Kalman filter state and covariance\n",
      "    initial_state_mean = np.array([initial_mean, 0.0]) # Assume initial velocity is 0\n",
      "    initial_state_covariance = np.diag([initial_std**2, (initial_std*0.5)**2]) # Higher uncertainty in velocity initially\n",
      "\n",
      "    # Use pykalman for efficient Kalman filtering\n",
      "    # The 'filtered_state_means' will contain the estimated states over time.\n",
      "    # We need to adjust the observation noise dynamically.\n",
      "\n",
      "    # This implementation will use a forward pass of the Kalman filter.\n",
      "    # For real-time, a receding horizon or a fixed-lag smoother might be considered,\n",
      "    # but a standard Kalman filter provides a good balance of latency and performance.\n",
      "\n",
      "    filtered_state_means = np.zeros((n_samples, 2))\n",
      "    filtered_state_covariances = np.zeros((n_samples, 2, 2))\n",
      "    observations = np.zeros(n_samples)\n",
      "\n",
      "    # Initialize for the first few points where trend estimation is not reliable\n",
      "    # We'll use a simpler approach for the initial phase.\n",
      "    # For simplicity, let's use a weighted moving average for the first `trend_window` samples\n",
      "    # and then switch to the Kalman filter with trend estimation.\n",
      "\n",
      "    y = np.zeros(n_samples)\n",
      "\n",
      "    # Initial phase using weighted moving average\n",
      "    for i in range(min(n_samples, trend_window)):\n",
      "        current_window = x[max(0, i - trend_window + 1):i+1]\n",
      "        current_weights = np.exp(np.linspace(-2, 0, len(current_window)))\n",
      "        current_weights /= np.sum(current_weights)\n",
      "        y[i] = np.sum(current_window * current_weights)\n",
      "\n",
      "    # Kalman filter phase\n",
      "    # Initialize the filter with the last estimated state from the WMA\n",
      "    current_mean_wma = y[min(n_samples-1, trend_window-1)]\n",
      "    current_std_wma = np.std(x[max(0, trend_window - trend_window):min(n_samples, trend_window)]) if trend_window > 1 else np.std(x[:min(n_samples, trend_window)])\n",
      "    initial_state_mean = np.array([current_mean_wma, 0.0])\n",
      "    initial_state_covariance = np.diag([current_std_wma**2 if current_std_wma > 0 else 1.0, (current_std_wma*0.5)**2 if current_std_wma > 0 else 1.0])\n",
      "\n",
      "    kf = pykalman.KalmanFilter(\n",
      "        transition_matrices=transition_matrix,\n",
      "        observation_matrices=observation_model,\n",
      "        observation_offsets=np.array([0.0]), # No offset\n",
      "        initial_state_mean=initial_state_mean,\n",
      "        initial_state_covariance=initial_state_covariance,\n",
      "        transition_covariance=process_noise_covariance,\n",
      "        observation_covariance=observation_noise_covariance # Initial value, will be updated\n",
      "    )\n",
      "\n",
      "    # Store observations and filtered states\n",
      "    filtered_state_means[min(n_samples-1, trend_window-1)] = initial_state_mean\n",
      "    filtered_state_covariances[min(n_samples-1, trend_window-1)] = initial_state_covariance\n",
      "    observations[min(n_samples-1, trend_window-1)] = current_mean_wma\n",
      "\n",
      "    for i in range(trend_window, n_samples):\n",
      "        # Estimate local trend using polynomial fitting\n",
      "        # Ensure enough points for polynomial fitting\n",
      "        start_idx = max(0, i - trend_window)\n",
      "        end_idx = i + 1\n",
      "        current_segment = x[start_idx:end_idx]\n",
      "        t_segment = np.arange(len(current_segment))\n",
      "\n",
      "        try:\n",
      "            # Fit polynomial to estimate trend\n",
      "            poly_coeffs = np.polyfit(t_segment, current_segment, poly_degree)\n",
      "            poly_trend = np.polyval(poly_coeffs, len(current_segment) - 1) # Trend at the current point\n",
      "\n",
      "            # Calculate the deviation of the current observation from the estimated trend\n",
      "            # This deviation can be used to estimate the local observation noise.\n",
      "            # A larger deviation suggests higher noise.\n",
      "            # We can scale the observation noise based on the variance of the residuals\n",
      "            # or simply the magnitude of the current observation relative to the trend.\n",
      "\n",
      "            # Simple approach: If the current observation is far from the trend, increase observation noise.\n",
      "            # This helps the filter to be less sensitive to outliers.\n",
      "            observation_error = x[i] - poly_trend\n",
      "            # A heuristic to adjust observation noise:\n",
      "            # If error is large, increase noise. If error is small, decrease noise (but not too much).\n",
      "            # We want to balance responsiveness to new information with stability.\n",
      "            # Let's use a factor that's inversely proportional to the signal's local variance,\n",
      "            # but also considers the deviation from the trend.\n",
      "\n",
      "            # Calculate local variance more robustly.\n",
      "            local_variance = np.var(current_segment) if len(current_segment) > 1 else 1.0\n",
      "            # Adjust observation noise based on local variance and deviation from trend\n",
      "            # The goal is to make the filter more responsive when the signal is more volatile.\n",
      "            # If local_variance is high, we want to trust the observation more (lower observation noise).\n",
      "            # If the observation deviates significantly from the trend, it might be noise, so we increase noise.\n",
      "            # This is a delicate balance.\n",
      "\n",
      "            # Let's try a simpler adaptive observation noise based on local variance.\n",
      "            # Higher local variance -> lower observation noise to track faster changes.\n",
      "            # Lower local variance -> higher observation noise to smooth more.\n",
      "            # A common approach is R = sigma_obs^2 * local_variance_scaling_factor\n",
      "            # Or, R = sigma_obs^2 * (1 + error_magnitude / local_variance)\n",
      "            # Let's try to make observation noise inversely proportional to local variance,\n",
      "            # but bounded.\n",
      "\n",
      "            adjusted_obs_noise_std = observation_noise_std * (1.0 / (np.sqrt(local_variance) + 1e-6))\n",
      "            # Clamp the noise to avoid extreme values\n",
      "            adjusted_obs_noise_std = np.clip(adjusted_obs_noise_std, observation_noise_std * 0.1, observation_noise_std * 5.0)\n",
      "\n",
      "            # We can also penalize large deviations from the trend by increasing R further.\n",
      "            # This is a penalty for false reversals.\n",
      "            deviation_penalty = np.abs(observation_error) / (np.sqrt(local_variance) + 1e-6)\n",
      "            # Scale this penalty to influence R.\n",
      "            # If deviation_penalty is high, R increases.\n",
      "            adjusted_obs_noise_std *= (1.0 + deviation_penalty * 0.1) # Heuristic scaling\n",
      "\n",
      "            kf.observation_covariance = np.array([[adjusted_obs_noise_std**2]])\n",
      "\n",
      "            # Update Kalman filter with the current observation\n",
      "            # The observation is the raw input signal x[i].\n",
      "            kf.smooth(x[i:i+1]) # Use smooth for potentially better accuracy, though it's more computationally intensive. For real-time, 'filter' is preferred.\n",
      "            # For real-time, we should use filter:\n",
      "            kf.filter(x[i:i+1])\n",
      "\n",
      "            # The filtered state mean is [estimated_position, estimated_velocity]\n",
      "            filtered_state_means[i] = kf.smooth_state_means[-1] # Use smooth for accuracy, or filter_state_means[-1] for latency\n",
      "            filtered_state_covariances[i] = kf.smooth_state_covariances[-1] # or filter_state_covariances[-1]\n",
      "            observations[i] = x[i]\n",
      "\n",
      "            # The output y[i] is the estimated position from the Kalman filter\n",
      "            y[i] = filtered_state_means[i, 0] # Estimated position\n",
      "\n",
      "        except np.linalg.LinAlgError:\n",
      "            # Handle cases where polynomial fitting might fail (e.g., insufficient unique points)\n",
      "            # Fallback to a simpler method or previous estimate\n",
      "            y[i] = y[i-1] if i > 0 else x[i] # Fallback to previous output or current input\n",
      "\n",
      "    # The current implementation of process_signal expects output length = len(x) - window_size + 1\n",
      "    # However, Kalman filters typically output for the same length as input.\n",
      "    # We need to align the output. The delay is related to the initial estimation and\n",
      "    # the smoothing process. For simplicity, let's assume the Kalman filter output\n",
      "    # is aligned such that y[i] corresponds to the filtered estimate for x[i].\n",
      "    # The `run_signal_processing` function handles alignment.\n",
      "\n",
      "    # If we want to match the output length of the original `adaptive_filter` and `enhanced_filter_with_trend_preservation`\n",
      "    # which is len(x) - window_size + 1, we would need to truncate or adjust.\n",
      "    # However, a Kalman filter naturally produces an output for each input sample.\n",
      "    # Let's return the full-length output and let the caller handle alignment.\n",
      "    return y\n",
      "\n",
      "\n",
      "def process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n",
      "    \"\"\"\n",
      "    Main signal processing function that applies the selected algorithm.\n",
      "\n",
      "    Args:\n",
      "        input_signal: Input time series data\n",
      "        window_size: Window size for processing (used for initial Kalman state estimation)\n",
      "        algorithm_type: Type of algorithm to use (\"basic\", \"enhanced\", or \"kalman_trend\")\n",
      "\n",
      "    Returns:\n",
      "        Filtered signal\n",
      "    \"\"\"\n",
      "    if algorithm_type == \"enhanced\":\n",
      "        # Keep the existing enhanced filter for comparison or as an option\n",
      "        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n",
      "    elif algorithm_type == \"kalman_trend\":\n",
      "        # Use the new Kalman filter with trend estimation\n",
      "        return _kalman_filter_enhanced(input_signal, window_size=window_size)\n",
      "    else: # Default to basic moving average\n",
      "        return adaptive_filter(input_signal, window_size)\n",
      "\n",
      "# The original adaptive_filter and enhanced_filter_with_trend_preservation are kept\n",
      "# outside the EVOLVE-BLOCK for compatibility if needed.\n",
      "# However, the goal is to replace the core logic within the block.\n",
      "\n",
      "# The `run_signal_processing` and `generate_test_signal` functions are outside\n",
      "# the EVOLVE-BLOCK and will remain as is.\n",
      "Iteration 11: New subsample score 0.0 is not better than old score 1.182136384345812, skipping\n",
      "Iteration 12: Selected program 0 score: 0.39265168271364936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch item 0 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 1 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 2 failed to meet stage 1 threshold\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12: Proposed new text for program: \"\"\"\n",
      "Real-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n",
      "\n",
      "This algorithm implements a sliding window approach to filter volatile, non-stationary\n",
      "time series data while minimizing noise and preserving signal dynamics.\n",
      "It incorporates advanced techniques to optimize for slope change minimization, lag error,\n",
      "tracking accuracy, and false reversal penalty.\n",
      "\"\"\"\n",
      "import numpy as np\n",
      "from scipy import signal\n",
      "from collections import deque\n",
      "import pywt  # For wavelet decomposition\n",
      "from scipy.signal import savgol_filter # For polynomial fitting\n",
      "\n",
      "\n",
      "class AdaptiveFilter:\n",
      "    \"\"\"\n",
      "    Base class for adaptive filters.\n",
      "    \"\"\"\n",
      "    def __init__(self, window_size=20):\n",
      "        self.window_size = window_size\n",
      "        self.history = deque(maxlen=window_size)\n",
      "\n",
      "    def process(self, sample):\n",
      "        self.history.append(sample)\n",
      "        if len(self.history) < self.window_size:\n",
      "            return np.mean(self.history) # Or some other initialization strategy\n",
      "        return self._filter_window(np.array(list(self.history)))\n",
      "\n",
      "    def _filter_window(self, window):\n",
      "        raise NotImplementedError\n",
      "\n",
      "class WeightedMovingAverage(AdaptiveFilter):\n",
      "    \"\"\"\n",
      "    Enhanced version with trend preservation using weighted moving average.\n",
      "    Emphasizes recent samples with exponential weights.\n",
      "    \"\"\"\n",
      "    def __init__(self, window_size=20):\n",
      "        super().__init__(window_size)\n",
      "        self.weights = np.exp(np.linspace(-2, 0, window_size))\n",
      "        self.weights /= np.sum(self.weights)\n",
      "\n",
      "    def _filter_window(self, window):\n",
      "        return np.sum(window * self.weights)\n",
      "\n",
      "class KalmanFilterWrapper:\n",
      "    \"\"\"\n",
      "    A wrapper for a simplified Kalman Filter approach for demonstration.\n",
      "    This is a basic implementation and can be significantly expanded.\n",
      "    \"\"\"\n",
      "    def __init__(self, initial_state, initial_covariance, process_noise, measurement_noise, window_size=20):\n",
      "        self.state = np.array(initial_state)\n",
      "        self.covariance = np.array(initial_covariance)\n",
      "        self.process_noise = np.array(process_noise)\n",
      "        self.measurement_noise = np.array(measurement_noise)\n",
      "        self.window_size = window_size\n",
      "        self.history = deque(maxlen=window_size)\n",
      "        self.initialized = False\n",
      "\n",
      "    def predict(self):\n",
      "        # Simple prediction: assuming state is the signal value and its derivative\n",
      "        # State transition matrix (assuming constant velocity model for simplicity)\n",
      "        # F = [[1, dt], [0, 1]] where dt is the time step. For simplicity, assume dt=1.\n",
      "        dt = 1\n",
      "        F = np.array([[1, dt], [0, 1]])\n",
      "        self.state = F @ self.state\n",
      "        self.covariance = F @ self.covariance @ F.T + self.process_noise\n",
      "\n",
      "    def update(self, measurement):\n",
      "        # Kalman Gain\n",
      "        H = np.array([[1, 0]]) # Measurement model: we measure the state value\n",
      "        K_numerator = self.covariance @ H.T\n",
      "        K_denominator = H @ self.covariance @ H.T + self.measurement_noise\n",
      "        K = K_numerator / K_denominator\n",
      "\n",
      "        # Update state\n",
      "        y = measurement - H @ self.state\n",
      "        self.state = self.state + K * y\n",
      "        self.covariance = (np.eye(self.covariance.shape[0]) - K @ H) @ self.covariance\n",
      "\n",
      "    def process(self, measurement):\n",
      "        if not self.initialized:\n",
      "            # Initialize state and covariance based on first few samples\n",
      "            self.history.append(measurement)\n",
      "            if len(self.history) == self.window_size:\n",
      "                # A more robust initialization would use RANSAC or similar\n",
      "                # For simplicity, let's fit a line to the first few points\n",
      "                x_vals = np.arange(self.window_size)\n",
      "                y_vals = np.array(list(self.history))\n",
      "                coeffs = np.polyfit(x_vals, y_vals, 1)\n",
      "                self.state = np.array([coeffs[0], coeffs[1]]) # [slope, intercept] - reversed for model\n",
      "                self.covariance = np.eye(2) * 1e-3 # Small initial covariance\n",
      "                self.initialized = True\n",
      "            else:\n",
      "                return np.mean(self.history) # Return mean until initialized\n",
      "\n",
      "        self.predict()\n",
      "        self.update(measurement)\n",
      "        return self.state[0] # Return the estimated signal value\n",
      "\n",
      "class PolynomialTrendFilter:\n",
      "    \"\"\"\n",
      "    Filters using polynomial fitting within a sliding window.\n",
      "    \"\"\"\n",
      "    def __init__(self, window_size=20, poly_order=2):\n",
      "        self.window_size = window_size\n",
      "        self.poly_order = poly_order\n",
      "        self.history = deque(maxlen=window_size)\n",
      "\n",
      "    def process(self, sample):\n",
      "        self.history.append(sample)\n",
      "        if len(self.history) < self.window_size:\n",
      "            return np.mean(self.history)\n",
      "\n",
      "        window = np.array(list(self.history))\n",
      "        x_vals = np.arange(self.window_size)\n",
      "\n",
      "        # Use Savitzky-Golay filter for smoothing and derivative estimation\n",
      "        # It fits a polynomial to the data in each window\n",
      "        # For simplicity, we'll use it to get a smoothed value\n",
      "        smoothed_window = savgol_filter(window, self.window_size, self.poly_order)\n",
      "        return smoothed_window[-1] # Return the last smoothed point\n",
      "\n",
      "class MultiScaleFilter:\n",
      "    \"\"\"\n",
      "    A conceptual placeholder for multi-scale processing (e.g., Wavelets, EMD).\n",
      "    This implementation uses a simple wavelet decomposition and reconstruction.\n",
      "    \"\"\"\n",
      "    def __init__(self, window_size=20, wavelet='db1', level=1):\n",
      "        self.window_size = window_size\n",
      "        self.wavelet = wavelet\n",
      "        self.level = level\n",
      "        self.history = deque(maxlen=window_size)\n",
      "\n",
      "    def process(self, sample):\n",
      "        self.history.append(sample)\n",
      "        if len(self.history) < self.window_size:\n",
      "            return np.mean(self.history)\n",
      "\n",
      "        window = np.array(list(self.history))\n",
      "        coeffs = pywt.wavedec(window, self.wavelet, level=self.level)\n",
      "\n",
      "        # Reconstruct with modified coefficients (e.g., thresholding noise coefficients)\n",
      "        # For simplicity, we'll just reconstruct without modification here.\n",
      "        # A real implementation would analyze noise and detail coefficients.\n",
      "        reconstructed_window = pywt.waverec(coeffs, self.wavelet)\n",
      "\n",
      "        # Ensure the output is of the correct length and shape\n",
      "        if len(reconstructed_window) > 0:\n",
      "            return reconstructed_window[-1]\n",
      "        else:\n",
      "            return np.mean(self.history) # Fallback\n",
      "\n",
      "\n",
      "class AdvancedAdaptiveFilter:\n",
      "    \"\"\"\n",
      "    Combines multiple advanced techniques for robust filtering.\n",
      "    Optimizes for slope change minimization, lag error, tracking accuracy,\n",
      "    and false reversal penalty.\n",
      "    \"\"\"\n",
      "    def __init__(self, window_size=20, poly_order=2, kalman_process_noise=1e-4, kalman_measurement_noise=1e-2):\n",
      "        self.window_size = window_size\n",
      "        self.poly_order = poly_order\n",
      "        self.history = deque(maxlen=window_size)\n",
      "        self.time_step = 1 # Assuming unit time steps for simplicity\n",
      "\n",
      "        # Initialize components\n",
      "        self.wma_filter = WeightedMovingAverage(window_size)\n",
      "        self.poly_filter = PolynomialTrendFilter(window_size, poly_order)\n",
      "        # Simplified Kalman Filter initialization\n",
      "        # State: [signal_value, signal_derivative]\n",
      "        # Initial state and covariance need careful tuning based on expected signal dynamics\n",
      "        self.kalman_filter = KalmanFilterWrapper(\n",
      "            initial_state=[0, 0],\n",
      "            initial_covariance=np.eye(2) * 1e1, # High initial uncertainty\n",
      "            process_noise=np.array([[kalman_process_noise, 0], [0, kalman_process_noise * 0.1]]), # Tune process noise\n",
      "            measurement_noise=np.array([[kalman_measurement_noise]]), # Tune measurement noise\n",
      "            window_size=window_size\n",
      "        )\n",
      "        # Multi-scale filter (e.g., Wavelet) - conceptual, can be integrated more deeply\n",
      "        self.wavelet_filter = MultiScaleFilter(window_size=window_size, wavelet='db4', level=1)\n",
      "\n",
      "        self.current_estimate = 0\n",
      "        self.last_estimate = 0\n",
      "        self.slope = 0\n",
      "        self.last_slope = 0\n",
      "        self.false_reversal_penalty_factor = 0.5 # Adjust to penalize false reversals more or less\n",
      "\n",
      "    def process(self, sample):\n",
      "        self.history.append(sample)\n",
      "\n",
      "        if len(self.history) < self.window_size:\n",
      "            # Initialize with a simple moving average or first sample until window is full\n",
      "            self.current_estimate = np.mean(self.history) if self.history else 0\n",
      "            return self.current_estimate\n",
      "\n",
      "        # --- Multi-objective strategy ---\n",
      "\n",
      "        # 1. Weighted Moving Average for initial smoothing and trend emphasis\n",
      "        wma_output = self.wma_filter.process(sample)\n",
      "\n",
      "        # 2. Polynomial Trend Filter for capturing local trends\n",
      "        poly_output = self.poly_filter.process(sample)\n",
      "\n",
      "        # 3. Kalman Filter for state estimation (signal value and its derivative)\n",
      "        kalman_output = self.kalman_filter.process(sample)\n",
      "\n",
      "        # 4. Wavelet Filter (conceptual) - could be used for noise thresholding or feature extraction\n",
      "        # For this implementation, we'll use its output as another candidate, but its integration\n",
      "        # is more complex and might involve analyzing coefficient levels.\n",
      "        wavelet_output = self.wavelet_filter.process(sample)\n",
      "\n",
      "        # --- Combining estimates ---\n",
      "        # A weighted combination of different filter outputs.\n",
      "        # Weights can be dynamically adjusted based on signal characteristics or performance metrics.\n",
      "        # For now, a fixed set of weights is used.\n",
      "\n",
      "        # Prioritize Kalman and Polynomial for better state tracking and derivative estimation\n",
      "        # WMA provides a good baseline smoothing\n",
      "        # Wavelet can help with noise reduction but might introduce phase shifts if not handled carefully.\n",
      "\n",
      "        # Simple weighted average of outputs. More sophisticated methods could use Bayesian fusion.\n",
      "        # Weights can be tuned. Higher weight for Kalman/Poly for accuracy and responsiveness.\n",
      "        combined_estimate = (\n",
      "            0.4 * kalman_output +\n",
      "            0.3 * poly_output +\n",
      "            0.2 * wma_output +\n",
      "            0.1 * wavelet_output\n",
      "        )\n",
      "\n",
      "        # --- Slope Change Minimization and False Reversal Penalty ---\n",
      "        # Estimate current slope based on the combined estimate\n",
      "        current_slope = combined_estimate - self.current_estimate # Simple difference as proxy for slope\n",
      "\n",
      "        # Penalize sharp changes in slope (false reversals)\n",
      "        # If the sign of the slope has changed drastically without a significant change in the signal, penalize it.\n",
      "        # This is a heuristic penalty. More rigorous methods would involve change point detection.\n",
      "        if self.last_slope != 0 and current_slope != 0:\n",
      "            if np.sign(current_slope) != np.sign(self.last_slope):\n",
      "                # Check if the signal magnitude change justifies the slope reversal\n",
      "                # If not, penalize by reducing the magnitude of the current slope or adjusting the estimate.\n",
      "                # This is a simplified penalty mechanism.\n",
      "                # A more robust approach might involve comparing the current estimate to a prediction based on the last slope.\n",
      "                if abs(current_slope) < abs(self.last_slope) * self.false_reversal_penalty_factor:\n",
      "                    # Penalize: reduce the magnitude of the current slope to avoid abrupt reversal\n",
      "                    current_slope = np.sign(current_slope) * abs(self.last_slope) * self.false_reversal_penalty_factor\n",
      "\n",
      "        # Update current estimate using the adjusted slope\n",
      "        self.current_estimate = self.current_estimate + current_slope\n",
      "        self.last_slope = current_slope\n",
      "        self.last_estimate = self.current_estimate # Update for the next iteration\n",
      "\n",
      "        return self.current_estimate\n",
      "\n",
      "\n",
      "def process_signal(input_signal, window_size=20, algorithm_type=\"advanced\"):\n",
      "    \"\"\"\n",
      "    Main signal processing function that applies the selected algorithm.\n",
      "\n",
      "    Args:\n",
      "        input_signal: Input time series data\n",
      "        window_size: Window size for processing\n",
      "        algorithm_type: Type of algorithm to use (\"basic\", \"enhanced\", or \"advanced\")\n",
      "\n",
      "    Returns:\n",
      "        Filtered signal\n",
      "    \"\"\"\n",
      "    if algorithm_type == \"enhanced\":\n",
      "        return EnhancedFilter(window_size).filter(input_signal)\n",
      "    elif algorithm_type == \"advanced\":\n",
      "        return AdvancedAdaptiveFilter(window_size=window_size).process_sequence(input_signal)\n",
      "    else: # \"basic\"\n",
      "        return BasicMovingAverage(window_size).filter(input_signal)\n",
      "\n",
      "class BasicMovingAverage:\n",
      "    def __init__(self, window_size):\n",
      "        self.window_size = window_size\n",
      "        self.history = deque(maxlen=window_size)\n",
      "\n",
      "    def filter(self, x):\n",
      "        output = []\n",
      "        for sample in x:\n",
      "            self.history.append(sample)\n",
      "            if len(self.history) < self.window_size:\n",
      "                output.append(np.mean(self.history) if self.history else 0)\n",
      "            else:\n",
      "                output.append(np.mean(self.history))\n",
      "        return np.array(output)\n",
      "\n",
      "class EnhancedFilter:\n",
      "    def __init__(self, window_size):\n",
      "        self.window_size = window_size\n",
      "        self.history = deque(maxlen=window_size)\n",
      "        self.weights = np.exp(np.linspace(-2, 0, window_size))\n",
      "        self.weights /= np.sum(self.weights)\n",
      "\n",
      "    def filter(self, x):\n",
      "        output = []\n",
      "        for sample in x:\n",
      "            self.history.append(sample)\n",
      "            if len(self.history) < self.window_size:\n",
      "                output.append(np.mean(self.history) if self.history else 0)\n",
      "            else:\n",
      "                window = np.array(list(self.history))\n",
      "                output.append(np.sum(window * self.weights))\n",
      "        return np.array(output)\n",
      "\n",
      "# Add a method to AdvancedAdaptiveFilter to process a sequence\n",
      "def process_sequence(self, input_signal):\n",
      "    filtered_output = []\n",
      "    for sample in input_signal:\n",
      "        filtered_output.append(self.process(sample))\n",
      "    return np.array(filtered_output)\n",
      "\n",
      "AdvancedAdaptiveFilter.process_sequence = process_sequence\n",
      "Iteration 12: New subsample score 0.0 is not better than old score 1.2604457589997677, skipping\n",
      "Iteration 13: Selected program 0 score: 0.39265168271364936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch item 0 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 1 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 2 failed to meet stage 1 threshold\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13: Proposed new text for program: \"\"\"\n",
      "Real-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n",
      "\n",
      "This algorithm implements a sliding window approach to filter volatile, non-stationary\n",
      "time series data while minimizing noise and preserving signal dynamics.\n",
      "It aims to optimize for:\n",
      "1. Slope change minimization - reducing spurious directional reversals.\n",
      "2. Lag error minimization - maintaining responsiveness.\n",
      "3. Tracking accuracy - preserving genuine signal trends.\n",
      "4. False reversal penalty - avoiding noise-induced trend changes.\n",
      "\n",
      "Advanced techniques are considered: adaptive filtering, multi-scale processing,\n",
      "predictive enhancement, and trend detection methods.\n",
      "\"\"\"\n",
      "import numpy as np\n",
      "from scipy import signal\n",
      "from collections import deque\n",
      "from scipy.signal import savgol_filter\n",
      "\n",
      "class AdaptiveFilter:\n",
      "    \"\"\"\n",
      "    A base class for adaptive filters, providing common functionality.\n",
      "    \"\"\"\n",
      "    def __init__(self, window_size=20):\n",
      "        self.window_size = window_size\n",
      "        self.history = deque(maxlen=window_size)\n",
      "        self.output_buffer = deque()\n",
      "\n",
      "    def process(self, sample):\n",
      "        \"\"\"Processes a single sample.\"\"\"\n",
      "        self.history.append(sample)\n",
      "        if len(self.history) == self.window_size:\n",
      "            filtered_sample = self._filter_window(list(self.history))\n",
      "            self.output_buffer.append(filtered_sample)\n",
      "            return filtered_sample\n",
      "        return None\n",
      "\n",
      "    def _filter_window(self, window):\n",
      "        \"\"\"Abstract method to filter a window of samples.\"\"\"\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def get_output(self):\n",
      "        \"\"\"Returns the filtered output as a numpy array.\"\"\"\n",
      "        return np.array(self.output_buffer)\n",
      "\n",
      "class SavitzkyGolayFilter(AdaptiveFilter):\n",
      "    \"\"\"\n",
      "    Savitzky-Golay filter for smoothing and preserving signal shape.\n",
      "    This is a good starting point for reducing noise while maintaining dynamics.\n",
      "    \"\"\"\n",
      "    def __init__(self, window_size=20, polyorder=3):\n",
      "        super().__init__(window_size)\n",
      "        if window_size % 2 == 0:\n",
      "            raise ValueError(\"window_size must be odd for Savitzky-Golay filter.\")\n",
      "        self.polyorder = polyorder\n",
      "        if self.polyorder >= self.window_size:\n",
      "            raise ValueError(\"polyorder must be less than window_size.\")\n",
      "\n",
      "    def _filter_window(self, window):\n",
      "        # Savitzky-Golay filter is applied to a window.\n",
      "        # For real-time, we can apply it to the current window.\n",
      "        # The derivative can be used to estimate slope, which is useful for\n",
      "        # slope change minimization and false reversal penalty.\n",
      "        # However, for simplicity and to avoid excessive complexity in this evolution,\n",
      "        # we'll focus on the smoothed output first.\n",
      "        # The polyfit and polyval approach is more computationally intensive for real-time.\n",
      "        # Savitzky-Golay is a good balance.\n",
      "        try:\n",
      "            # Ensure window is a numpy array for savgol_filter\n",
      "            window_np = np.array(window)\n",
      "            # The window needs to be centered. For real-time, we use the last `window_size` samples.\n",
      "            # savgol_filter expects the entire signal, so we simulate it by applying to the window.\n",
      "            # For a true real-time implementation, one might pre-compute filter coefficients.\n",
      "            # For this simulation, we'll apply it to the current window.\n",
      "            # The output will be shorter than the window, so we take the last element.\n",
      "            # A more robust real-time approach would involve managing coefficients and state.\n",
      "            # For this evolutionary step, we simulate by fitting a polynomial to the window.\n",
      "            # This is a simplification for demonstration.\n",
      "            coeffs = np.polyfit(np.arange(self.window_size), window_np, self.polyorder)\n",
      "            smoothed_value = np.polyval(coeffs, self.window_size - 1) # Evaluate at the end of the window\n",
      "            return smoothed_value\n",
      "        except Exception as e:\n",
      "            # Fallback to simple mean if savgol fails (e.g., insufficient data points)\n",
      "            print(f\"Savitzky-Golay failed: {e}. Falling back to mean.\")\n",
      "            return np.mean(window)\n",
      "\n",
      "class KalmanFilter(AdaptiveFilter):\n",
      "    \"\"\"\n",
      "    A simplified Kalman Filter implementation for demonstration.\n",
      "    Kalman filters are excellent for tracking and noise reduction in linear systems.\n",
      "    This implementation is a basic example and might need tuning for specific signals.\n",
      "    \"\"\"\n",
      "    def __init__(self, window_size=20, process_noise=0.1, measurement_noise=1.0):\n",
      "        super().__init__(window_size)\n",
      "        # State is [position, velocity]\n",
      "        self.state = np.zeros(2)\n",
      "        # State transition matrix (assuming constant velocity model)\n",
      "        # F = [[1, dt], [0, 1]] where dt is implicitly 1 for sample-by-sample\n",
      "        self.F = np.array([[1, 1], [0, 1]])\n",
      "        # Measurement matrix (we measure position)\n",
      "        self.H = np.array([[1, 0]])\n",
      "        # Process noise covariance\n",
      "        self.Q = np.array([[process_noise, 0], [0, process_noise]])\n",
      "        # Measurement noise covariance\n",
      "        self.R = np.array([[measurement_noise]])\n",
      "        # Initial state covariance\n",
      "        self.P = np.eye(2) * 1000\n",
      "        self.first_sample = True\n",
      "\n",
      "    def _filter_window(self, window):\n",
      "        # For real-time, we process samples one by one.\n",
      "        # This `_filter_window` method is a simplification.\n",
      "        # In a true real-time Kalman, we'd update state with each new sample.\n",
      "        # Here, we'll simulate by processing the window sequentially.\n",
      "        current_window_state = np.copy(self.state)\n",
      "        current_window_P = np.copy(self.P)\n",
      "\n",
      "        for i, sample in enumerate(window):\n",
      "            if self.first_sample and i == 0:\n",
      "                # Initialize state with the first measurement\n",
      "                self.state = np.array([sample, 0])\n",
      "                self.P = np.array([[self.R[0,0], 0], [0, 1000]]) # Reset covariance\n",
      "                self.first_sample = False\n",
      "                continue\n",
      "\n",
      "            # Prediction step\n",
      "            self.state = self.F @ self.state\n",
      "            self.P = self.F @ self.P @ self.F.T + self.Q\n",
      "\n",
      "            # Update step\n",
      "            y = sample - self.H @ self.state\n",
      "            S = self.H @ self.P @ self.H.T + self.R\n",
      "            K = self.P @ self.H.T @ np.linalg.inv(S)\n",
      "            self.state = self.state + K @ y\n",
      "            self.P = (np.eye(self.P.shape[0]) - K @ self.H) @ self.P\n",
      "\n",
      "        # The filtered output is the estimated position\n",
      "        return self.state[0]\n",
      "\n",
      "    def process(self, sample):\n",
      "        \"\"\"Processes a single sample for real-time operation.\"\"\"\n",
      "        if self.first_sample:\n",
      "            # Initialize state with the first measurement\n",
      "            self.state = np.array([sample, 0])\n",
      "            self.P = np.array([[self.R[0,0], 0], [0, 1000]]) # Reset covariance\n",
      "            self.first_sample = False\n",
      "            # We don't have a full window yet, so no output.\n",
      "            return None\n",
      "\n",
      "        # Prediction step\n",
      "        self.state = self.F @ self.state\n",
      "        self.P = self.F @ self.P @ self.F.T + self.Q\n",
      "\n",
      "        # Update step\n",
      "        y = sample - self.H @ self.state\n",
      "        S = self.H @ self.P @ self.H.T + self.R\n",
      "        K = self.P @ self.H.T @ np.linalg.inv(S)\n",
      "        self.state = self.state + K @ y\n",
      "        self.P = (np.eye(self.P.shape[0]) - K @ self.H) @ self.P\n",
      "\n",
      "        # The filtered output is the estimated position\n",
      "        filtered_sample = self.state[0]\n",
      "        self.output_buffer.append(filtered_sample)\n",
      "        return filtered_sample\n",
      "\n",
      "\n",
      "def adaptive_filter(x, window_size=20, algorithm_type=\"kalman\", **kwargs):\n",
      "    \"\"\"\n",
      "    Adaptive signal processing algorithm using sliding window approach with advanced filters.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window (W samples)\n",
      "        algorithm_type: Type of algorithm to use (\"savitzky_golay\", \"kalman\", \"weighted_mean\")\n",
      "        **kwargs: Additional arguments for specific filter types.\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal with length = len(x) - window_size + 1\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    if algorithm_type == \"kalman\":\n",
      "        # For Kalman, we need to process sample by sample.\n",
      "        # Initialize the filter and process the entire signal.\n",
      "        # This is a simplification for batch processing. A true real-time would be an object.\n",
      "        filter_instance = KalmanFilter(window_size=window_size, **kwargs)\n",
      "        y = np.array([filter_instance.process(sample) for sample in x])\n",
      "        # Remove None values from the beginning where the window wasn't full\n",
      "        return y[y != None]\n",
      "\n",
      "    elif algorithm_type == \"savitzky_golay\":\n",
      "        if window_size % 2 == 0:\n",
      "            print(\"Warning: Savitzky-Golay window size should be odd. Adjusting to next odd number.\")\n",
      "            window_size += 1\n",
      "        filter_instance = SavitzkyGolayFilter(window_size=window_size, **kwargs.get(\"polyorder\", 3))\n",
      "        # For real-time, we would process sample by sample. For batch, we can use savgol_filter directly.\n",
      "        # However, to align with the `process_signal` function structure, we will use the class.\n",
      "        # This is a slight deviation from pure real-time simulation in this batch processing context.\n",
      "        # A more accurate simulation would iterate and call process.\n",
      "        # For now, we use the class to demonstrate its logic.\n",
      "        # A direct savgol_filter on the whole signal would be more efficient for batch.\n",
      "        # Let's simulate by processing each window.\n",
      "        y = np.zeros(len(x) - window_size + 1)\n",
      "        for i in range(len(x) - window_size + 1):\n",
      "            window = x[i : i + window_size]\n",
      "            # Using polyfit/polyval as a proxy for savgol_filter for simplicity in this context.\n",
      "            # A direct savgol_filter call would be more appropriate if the whole signal is available.\n",
      "            try:\n",
      "                coeffs = np.polyfit(np.arange(window_size), window, kwargs.get(\"polyorder\", 3))\n",
      "                y[i] = np.polyval(coeffs, window_size - 1)\n",
      "            except:\n",
      "                y[i] = np.mean(window) # Fallback\n",
      "        return y\n",
      "\n",
      "    elif algorithm_type == \"weighted_mean\":\n",
      "        # Enhanced version with trend preservation using weighted moving average.\n",
      "        # This is similar to the original 'enhanced_filter_with_trend_preservation'.\n",
      "        # We can make the weights adaptive or more sophisticated.\n",
      "        # For now, keep the exponential weights but make them tunable.\n",
      "        weights = np.exp(np.linspace(-kwargs.get(\"decay\", 2.0), 0, window_size))\n",
      "        weights = weights / np.sum(weights)\n",
      "\n",
      "        y = np.zeros(len(x) - window_size + 1)\n",
      "        for i in range(len(x) - window_size + 1):\n",
      "            window = x[i : i + window_size]\n",
      "            y[i] = np.sum(window * weights)\n",
      "        return y\n",
      "\n",
      "    else:\n",
      "        raise ValueError(f\"Unknown algorithm_type: {algorithm_type}\")\n",
      "\n",
      "\n",
      "def process_signal(input_signal, window_size=20, algorithm_type=\"kalman\", **kwargs):\n",
      "    \"\"\"\n",
      "    Main signal processing function that applies the selected algorithm.\n",
      "\n",
      "    Args:\n",
      "        input_signal: Input time series data\n",
      "        window_size: Window size for processing\n",
      "        algorithm_type: Type of algorithm to use (\"savitzky_golay\", \"kalman\", \"weighted_mean\")\n",
      "        **kwargs: Additional arguments for specific filter types.\n",
      "\n",
      "    Returns:\n",
      "        Filtered signal\n",
      "    \"\"\"\n",
      "    # Default to Kalman for better tracking and noise reduction\n",
      "    if algorithm_type not in [\"savitzky_golay\", \"kalman\", \"weighted_mean\"]:\n",
      "        print(f\"Warning: Invalid algorithm_type '{algorithm_type}'. Defaulting to 'kalman'.\")\n",
      "        algorithm_type = \"kalman\"\n",
      "\n",
      "    # Adjust window_size for Savitzky-Golay if it's even\n",
      "    if algorithm_type == \"savitzky_golay\" and window_size % 2 == 0:\n",
      "        window_size += 1\n",
      "\n",
      "    return adaptive_filter(input_signal, window_size, algorithm_type, **kwargs)\n",
      "Warning: Invalid algorithm_type 'enhanced'. Defaulting to 'kalman'.\n",
      "Warning: Invalid algorithm_type 'enhanced'. Defaulting to 'kalman'.\n",
      "Warning: Invalid algorithm_type 'enhanced'. Defaulting to 'kalman'.\n",
      "Iteration 13: New subsample score 0.0 is not better than old score 1.1172633408881465, skipping\n",
      "Iteration 14: Selected program 0 score: 0.39265168271364936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14: Proposed new text for program: \"\"\"\n",
      "Real-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n",
      "\n",
      "This algorithm implements a sliding window approach to filter volatile, non-stationary\n",
      "time series data while minimizing noise and preserving signal dynamics.\n",
      "It incorporates advanced techniques to optimize for:\n",
      "1. Slope change minimization\n",
      "2. Lag error minimization\n",
      "3. Tracking accuracy\n",
      "4. False reversal penalty\n",
      "\n",
      "This version introduces a Kalman filter for adaptive state estimation and a polynomial\n",
      "fitting approach for trend extrapolation within the sliding window.\n",
      "\"\"\"\n",
      "import numpy as np\n",
      "from collections import deque\n",
      "from scipy.signal import savgol_filter\n",
      "\n",
      "class KalmanFilter:\n",
      "    \"\"\"\n",
      "    A simple Kalman Filter implementation for tracking a 1D signal.\n",
      "    Assumes a constant velocity model.\n",
      "    \"\"\"\n",
      "    def __init__(self, initial_state, initial_covariance, process_noise, measurement_noise):\n",
      "        # State vector: [position, velocity]\n",
      "        self.state = np.array(initial_state, dtype=np.float64)\n",
      "        # State covariance matrix\n",
      "        self.covariance = np.array(initial_covariance, dtype=np.float64)\n",
      "\n",
      "        # Process model: x_k = F * x_{k-1} + w_k\n",
      "        # F = [[1, dt], [0, 1]] (assuming dt=1 for simplicity in this context)\n",
      "        self.process_model = np.array([[1, 1], [0, 1]], dtype=np.float64)\n",
      "        # Process noise covariance\n",
      "        self.process_noise_covariance = np.array([[process_noise[0], 0], [0, process_noise[1]]], dtype=np.float64)\n",
      "\n",
      "        # Measurement model: z_k = H * x_k + v_k\n",
      "        # H = [1, 0] (we only measure position)\n",
      "        self.measurement_model = np.array([[1, 0]], dtype=np.float64)\n",
      "        # Measurement noise covariance\n",
      "        self.measurement_noise_covariance = np.array([[measurement_noise]], dtype=np.float64)\n",
      "\n",
      "    def predict(self):\n",
      "        \"\"\"Predict the next state and covariance.\"\"\"\n",
      "        # Predict state\n",
      "        self.state = self.process_model @ self.state\n",
      "        # Predict covariance\n",
      "        self.covariance = self.process_model @ self.covariance @ self.process_model.T + self.process_noise_covariance\n",
      "\n",
      "    def update(self, measurement):\n",
      "        \"\"\"Update the state and covariance based on a measurement.\"\"\"\n",
      "        # Kalman gain\n",
      "        innovation_covariance = self.measurement_model @ self.covariance @ self.measurement_model.T + self.measurement_noise_covariance\n",
      "        kalman_gain = self.covariance @ self.measurement_model.T @ np.linalg.inv(innovation_covariance)\n",
      "\n",
      "        # Update state\n",
      "        self.state = self.state + kalman_gain @ (measurement - self.measurement_model @ self.state)\n",
      "        # Update covariance\n",
      "        self.covariance = (np.eye(self.state.shape[0]) - kalman_gain @ self.measurement_model) @ self.covariance\n",
      "\n",
      "    def get_state(self):\n",
      "        \"\"\"Get the current estimated state.\"\"\"\n",
      "        return self.state\n",
      "\n",
      "    def get_filtered_value(self):\n",
      "        \"\"\"Get the filtered position estimate.\"\"\"\n",
      "        return self.state[0]\n",
      "\n",
      "def adaptive_filter_kalman_poly(x, window_size=20, poly_order=2, process_noise_pos=0.1, process_noise_vel=0.01, measurement_noise=1.0):\n",
      "    \"\"\"\n",
      "    Adaptive signal processing algorithm using a sliding window with Kalman filtering\n",
      "    and polynomial fitting for enhanced trend preservation and noise reduction.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window (W samples)\n",
      "        poly_order: Order of the polynomial to fit for trend estimation.\n",
      "        process_noise_pos: Process noise for position in Kalman filter.\n",
      "        process_noise_vel: Process noise for velocity in Kalman filter.\n",
      "        measurement_noise: Measurement noise in Kalman filter.\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal with length = len(x) - window_size + 1\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    # Initialize Kalman Filter for each window start.\n",
      "    # Initial state assumes the first sample is the position and velocity is zero.\n",
      "    # This initialization will be reset as we slide the window.\n",
      "    kf = None\n",
      "\n",
      "    for i in range(output_length):\n",
      "        window = x[i : i + window_size]\n",
      "\n",
      "        # Initialize or re-initialize Kalman Filter for the current window's start\n",
      "        # This is crucial for adapting to non-stationarity.\n",
      "        if kf is None:\n",
      "            kf = KalmanFilter(\n",
      "                initial_state=[window[0], 0],\n",
      "                initial_covariance=[[1.0, 0], [0, 1.0]],\n",
      "                process_noise=[process_noise_pos, process_noise_vel],\n",
      "                measurement_noise=measurement_noise\n",
      "            )\n",
      "        else:\n",
      "            # Re-initialize with the first element of the current window\n",
      "            kf.state = np.array([window[0], 0], dtype=np.float64)\n",
      "            kf.covariance = np.array([[1.0, 0], [0, 1.0]], dtype=np.float64)\n",
      "\n",
      "        # Apply Kalman Filter to the current window\n",
      "        filtered_window = np.zeros(window_size)\n",
      "        for j in range(window_size):\n",
      "            kf.predict()\n",
      "            kf.update(window[j])\n",
      "            filtered_window[j] = kf.get_filtered_value()\n",
      "\n",
      "        # Use polynomial fitting on the filtered window to get a smoother trend\n",
      "        # and predict the value at the center of the window (or end for minimal lag)\n",
      "        # We fit a polynomial to the filtered window and evaluate it at the last point\n",
      "        # to minimize lag error while leveraging the smoothed data.\n",
      "        time_points = np.arange(window_size)\n",
      "        try:\n",
      "            coeffs = np.polyfit(time_points, filtered_window, poly_order)\n",
      "            poly_func = np.poly1d(coeffs)\n",
      "            # Evaluate at the end of the window to reduce lag\n",
      "            y[i] = poly_func(window_size - 1)\n",
      "        except np.linalg.LinAlgError:\n",
      "            # Fallback to the mean of the filtered window if polynomial fitting fails\n",
      "            y[i] = np.mean(filtered_window)\n",
      "        except Exception as e:\n",
      "            # Fallback to the mean of the filtered window for any other error\n",
      "            print(f\"Polynomial fitting error: {e}. Falling back to mean.\")\n",
      "            y[i] = np.mean(filtered_window)\n",
      "\n",
      "    return y\n",
      "\n",
      "def process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n",
      "    \"\"\"\n",
      "    Main signal processing function that applies the selected algorithm.\n",
      "\n",
      "    Args:\n",
      "        input_signal: Input time series data\n",
      "        window_size: Window size for processing\n",
      "        algorithm_type: Type of algorithm to use (\"basic\", \"enhanced\", or \"kalman_poly\")\n",
      "\n",
      "    Returns:\n",
      "        Filtered signal\n",
      "    \"\"\"\n",
      "    if algorithm_type == \"enhanced\":\n",
      "        # The original enhanced filter is a weighted moving average.\n",
      "        # We'll keep it as an option but prefer the new one.\n",
      "        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n",
      "    elif algorithm_type == \"kalman_poly\":\n",
      "        # Use the new Kalman Filter and polynomial fitting approach\n",
      "        return adaptive_filter_kalman_poly(input_signal, window_size=window_size)\n",
      "    else:\n",
      "        # Fallback to the basic moving average filter\n",
      "        return adaptive_filter(input_signal, window_size)\n",
      "\n",
      "# Keep the original adaptive_filter and enhanced_filter_with_trend_preservation\n",
      "# as they might be needed for comparison or as fallbacks, but the primary\n",
      "# focus for improvement is the new kalman_poly method.\n",
      "\n",
      "def adaptive_filter(x, window_size=20):\n",
      "    \"\"\"\n",
      "    Adaptive signal processing algorithm using sliding window approach.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window (W samples)\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal with length = len(x) - window_size + 1\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    # Initialize output array\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    # Simple moving average as baseline\n",
      "    for i in range(output_length):\n",
      "        window = x[i : i + window_size]\n",
      "\n",
      "        # Basic moving average filter\n",
      "        y[i] = np.mean(window)\n",
      "\n",
      "    return y\n",
      "\n",
      "\n",
      "def enhanced_filter_with_trend_preservation(x, window_size=20):\n",
      "    \"\"\"\n",
      "    Enhanced version with trend preservation using weighted moving average.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    # Create weights that emphasize recent samples\n",
      "    weights = np.exp(np.linspace(-2, 0, window_size))\n",
      "    weights = weights / np.sum(weights)\n",
      "\n",
      "    for i in range(output_length):\n",
      "        window = x[i : i + window_size]\n",
      "\n",
      "        # Weighted moving average with exponential weights\n",
      "        y[i] = np.sum(window * weights)\n",
      "\n",
      "    return y\n",
      "Iteration 14: New subsample score 1.3468393901855285 is not better than old score 1.3468393901855285, skipping\n",
      "Iteration 15: Selected program 0 score: 0.39265168271364936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Batch item 0 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 1 failed to meet stage 1 threshold\n",
      "WARNING:root:Batch item 2 failed to meet stage 1 threshold\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15: Proposed new text for program: \"\"\"\n",
      "    Real-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n",
      "\n",
      "    This algorithm implements a sliding window approach to filter volatile, non-stationary\n",
      "    time series data while minimizing noise and preserving signal dynamics.\n",
      "    It aims to optimize for:\n",
      "    1. Slope change minimization: Reducing spurious directional reversals.\n",
      "    2. Lag error minimization: Maintaining responsiveness.\n",
      "    3. Tracking accuracy: Preserving genuine signal trends.\n",
      "    4. False reversal penalty: Avoiding noise-induced trend changes.\n",
      "    \"\"\"\n",
      "    import numpy as np\n",
      "    from scipy import signal\n",
      "    from collections import deque\n",
      "\n",
      "    # --- Advanced Filtering Techniques ---\n",
      "\n",
      "    def kalman_filter_adaptive(x, initial_state, initial_covariance, process_noise, measurement_noise, window_size=20):\n",
      "        \"\"\"\n",
      "        Implements an adaptive Kalman filter for real-time signal processing.\n",
      "\n",
      "        Args:\n",
      "            x: Input signal (1D array of real-valued samples)\n",
      "            initial_state: Initial state vector [position, velocity]\n",
      "            initial_covariance: Initial error covariance matrix\n",
      "            process_noise: Process noise covariance matrix [ [q_pos, 0], [0, q_vel] ]\n",
      "            measurement_noise: Measurement noise variance (scalar)\n",
      "            window_size: Size of the sliding window (for conceptual alignment, not direct use in KF math)\n",
      "\n",
      "        Returns:\n",
      "            Filtered output signal\n",
      "        \"\"\"\n",
      "        n_samples = len(x)\n",
      "        if n_samples < window_size:\n",
      "            raise ValueError(f\"Input signal length ({n_samples}) must be >= window_size ({window_size})\")\n",
      "\n",
      "        # State vector: [position, velocity]\n",
      "        state = np.array(initial_state)\n",
      "        covariance = np.array(initial_covariance)\n",
      "\n",
      "        # State transition matrix (assuming constant velocity model for simplicity)\n",
      "        # dt is implicitly 1 for discrete time steps\n",
      "        dt = 1\n",
      "        A = np.array([[1, dt], [0, 1]])\n",
      "\n",
      "        # Measurement matrix (we only measure position)\n",
      "        H = np.array([[1, 0]])\n",
      "\n",
      "        filtered_output = np.zeros(n_samples - window_size + 1)\n",
      "        output_idx = 0\n",
      "\n",
      "        for i in range(window_size - 1, n_samples): # Start processing after the initial window is \"filled\" conceptually\n",
      "            z = x[i] # Current measurement\n",
      "\n",
      "            # --- Kalman Filter Prediction Step ---\n",
      "            # Predict state\n",
      "            state = A @ state\n",
      "            # Predict covariance\n",
      "            covariance = A @ covariance @ A.T + process_noise\n",
      "\n",
      "            # --- Kalman Filter Update Step ---\n",
      "            # Calculate Kalman Gain\n",
      "            K = covariance @ H.T @ np.linalg.inv(H @ covariance @ H.T + measurement_noise)\n",
      "\n",
      "            # Update state\n",
      "            state = state + K @ (z - H @ state)\n",
      "\n",
      "            # Update covariance\n",
      "            covariance = (np.identity(len(state)) - K @ H) @ covariance\n",
      "\n",
      "            # Store filtered position\n",
      "            if output_idx < len(filtered_output):\n",
      "                filtered_output[output_idx] = state[0]\n",
      "                output_idx += 1\n",
      "\n",
      "        return filtered_output\n",
      "\n",
      "    # --- Multi-Scale Processing (conceptual placeholder for now) ---\n",
      "    # For this problem, direct wavelet/EMD implementation can be complex for real-time\n",
      "    # and might increase latency. We'll focus on adaptive filters and polynomial fitting.\n",
      "\n",
      "    # --- Predictive Enhancement ---\n",
      "\n",
      "    def polynomial_trend_filter(x, window_size=20, poly_order=2):\n",
      "        \"\"\"\n",
      "        Filters signal by fitting a polynomial to a sliding window and using its central point.\n",
      "        This helps preserve trends while smoothing noise.\n",
      "\n",
      "        Args:\n",
      "            x: Input signal (1D array of real-valued samples)\n",
      "            window_size: Size of the sliding window\n",
      "            poly_order: Order of the polynomial to fit\n",
      "\n",
      "        Returns:\n",
      "            Filtered output signal\n",
      "        \"\"\"\n",
      "        if len(x) < window_size:\n",
      "            raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "        output_length = len(x) - window_size + 1\n",
      "        y = np.zeros(output_length)\n",
      "\n",
      "        for i in range(output_length):\n",
      "            window = x[i : i + window_size]\n",
      "            # Create time points for the window (centered around 0 for better fitting)\n",
      "            t_window = np.arange(window_size) - (window_size - 1) / 2.0\n",
      "\n",
      "            # Fit polynomial\n",
      "            coeffs = np.polyfit(t_window, window, poly_order)\n",
      "            poly = np.poly1d(coeffs)\n",
      "\n",
      "            # Evaluate the polynomial at the center of the window.\n",
      "            # This point represents the smoothed value for the current time step.\n",
      "            y[i] = poly(0) # Center of the window is t=0\n",
      "\n",
      "        return y\n",
      "\n",
      "    # --- Trend Detection and False Reversal Penalty ---\n",
      "    # These will be integrated into the main processing logic.\n",
      "\n",
      "    def enhanced_adaptive_filter_optimized(x, window_size=20, poly_order=2, process_noise_std=0.1, measurement_noise_std=0.5):\n",
      "        \"\"\"\n",
      "        An enhanced adaptive filter that combines polynomial fitting for trend preservation\n",
      "        with a Kalman filter for noise reduction and responsiveness.\n",
      "\n",
      "        Args:\n",
      "            x: Input signal (1D array of real-valued samples)\n",
      "            window_size: Size of the sliding window. This influences the polynomial fitting.\n",
      "            poly_order: Order of the polynomial to fit for trend estimation.\n",
      "            process_noise_std: Standard deviation of the process noise for the Kalman filter.\n",
      "            measurement_noise_std: Standard deviation of the measurement noise for the Kalman filter.\n",
      "\n",
      "        Returns:\n",
      "            Filtered output signal\n",
      "        \"\"\"\n",
      "        n_samples = len(x)\n",
      "        if n_samples < window_size:\n",
      "            raise ValueError(f\"Input signal length ({n_samples}) must be >= window_size ({window_size})\")\n",
      "\n",
      "        # Initialize Kalman Filter components\n",
      "        # State: [position, velocity]\n",
      "        initial_state = [x[0], 0.0] # Initial guess for position and velocity\n",
      "        initial_covariance = np.diag([1.0, 1.0]) # Initial uncertainty\n",
      "        process_noise = np.diag([process_noise_std**2, (process_noise_std/2)**2]) # Process noise covariance\n",
      "        measurement_noise = measurement_noise_std**2 # Measurement noise variance\n",
      "\n",
      "        # Initialize output array\n",
      "        # The output length will be consistent with the polynomial_trend_filter\n",
      "        output_length = n_samples - window_size + 1\n",
      "        y_poly_trend = np.zeros(output_length)\n",
      "        y_kalman = np.zeros(output_length)\n",
      "        y_combined = np.zeros(output_length)\n",
      "\n",
      "        # --- Polynomial Trend Estimation ---\n",
      "        # This part is similar to polynomial_trend_filter but integrated\n",
      "        for i in range(output_length):\n",
      "            window = x[i : i + window_size]\n",
      "            t_window = np.arange(window_size) - (window_size - 1) / 2.0\n",
      "            coeffs = np.polyfit(t_window, window, poly_order)\n",
      "            poly = np.poly1d(coeffs)\n",
      "            y_poly_trend[i] = poly(0)\n",
      "\n",
      "        # --- Kalman Filter Application ---\n",
      "        # We'll apply the Kalman filter to the original noisy signal,\n",
      "        # but align its output to match the length of the polynomial filter.\n",
      "        # This requires careful indexing.\n",
      "        kf_state = np.array(initial_state)\n",
      "        kf_covariance = np.array(initial_covariance)\n",
      "        dt = 1\n",
      "        A = np.array([[1, dt], [0, 1]])\n",
      "        H = np.array([[1, 0]])\n",
      "\n",
      "        for i in range(window_size - 1, n_samples): # Start after initial window conceptually\n",
      "            z = x[i] # Current measurement\n",
      "\n",
      "            # Prediction\n",
      "            kf_state = A @ kf_state\n",
      "            kf_covariance = A @ kf_covariance @ A.T + process_noise\n",
      "\n",
      "            # Update\n",
      "            K = kf_covariance @ H.T @ np.linalg.inv(H @ kf_covariance @ H.T + measurement_noise)\n",
      "            kf_state = kf_state + K @ (z - H @ kf_state)\n",
      "            kf_covariance = (np.identity(len(kf_state)) - K @ H) @ kf_covariance\n",
      "\n",
      "            # Store Kalman filtered position, aligning with the polynomial filter's output length\n",
      "            if i - (window_size - 1) < output_length:\n",
      "                y_kalman[i - (window_size - 1)] = kf_state[0]\n",
      "\n",
      "        # --- Combined Filtering and False Reversal Penalty ---\n",
      "        # We'll use a weighted average of the polynomial trend and Kalman filter output.\n",
      "        # The weights can be adaptive or fixed. For simplicity and to avoid excessive latency,\n",
      "        # we'll start with a fixed weighting scheme.\n",
      "        # A higher weight on polynomial trend preserves dynamics, while a higher weight\n",
      "        # on Kalman filter reduces noise.\n",
      "\n",
      "        # Simple adaptive weighting based on signal volatility (e.g., local variance)\n",
      "        # This is a basic approach; more sophisticated methods could be used.\n",
      "        local_variance_window = min(window_size, 10) # Use a smaller window for variance calculation\n",
      "        variance = np.zeros(output_length)\n",
      "        for i in range(output_length):\n",
      "            start_idx = max(0, i - local_variance_window // 2)\n",
      "            end_idx = min(output_length, i + local_variance_window // 2 + 1)\n",
      "            if end_idx - start_idx > 1:\n",
      "                variance[i] = np.var(y_kalman[start_idx:end_idx]) # Use Kalman output for variance to be less noisy\n",
      "\n",
      "        # Normalize variance to a 0-1 range for weighting\n",
      "        max_variance = np.max(variance)\n",
      "        min_variance = np.min(variance)\n",
      "        if max_variance > min_variance:\n",
      "            normalized_variance = (variance - min_variance) / (max_variance - min_variance)\n",
      "        else:\n",
      "            normalized_variance = np.zeros_like(variance)\n",
      "\n",
      "        # Weighting scheme:\n",
      "        # If variance is high (volatile), rely more on Kalman (noise reduction).\n",
      "        # If variance is low (stable), rely more on polynomial trend (dynamics preservation).\n",
      "        # We want to avoid false reversals, so we penalize sharp changes.\n",
      "        # A simple approach is to slightly favor the smoother output (Kalman) when variance is high.\n",
      "\n",
      "        # Let's use a simple linear interpolation for weights, but with a bias towards\n",
      "        # the smoother output during high volatility.\n",
      "        # weight_poly = 1 - normalized_variance  # Higher variance -> lower poly weight\n",
      "        # weight_kalman = normalized_variance   # Higher variance -> higher kalman weight\n",
      "\n",
      "        # To penalize false reversals, we can adjust the weights.\n",
      "        # If the polynomial trend shows a sharp reversal and Kalman is smoother,\n",
      "        # we lean towards Kalman.\n",
      "        # A more direct approach to false reversal penalty is to look at the slope\n",
      "        # of the polynomial trend and compare it to the Kalman output's implied slope.\n",
      "\n",
      "        # Let's combine them with a bias towards the Kalman filter's smoothness,\n",
      "        # especially when the polynomial trend is highly volatile.\n",
      "        # We'll use a dynamic weighting that favors Kalman when variance is high.\n",
      "        # A simple heuristic:\n",
      "        # weight_poly = (1 - normalized_variance) * 0.5 + 0.25 # Base weight + variance adjustment\n",
      "        # weight_kalman = normalized_variance * 0.5 + 0.25\n",
      "\n",
      "        # A more direct way to penalize false reversals is to ensure the combined output\n",
      "        # doesn't flip direction too quickly. We can achieve this by ensuring the combined\n",
      "        # output's slope is a weighted average of the polynomial and Kalman slopes,\n",
      "        # or by directly limiting the rate of change.\n",
      "\n",
      "        # For now, let's use a weighted average that is influenced by variance,\n",
      "        # and then apply a post-processing step to penalize rapid slope changes if needed.\n",
      "\n",
      "        # Simple weighted average:\n",
      "        alpha = 0.5 # Default weight, can be adjusted\n",
      "        if max_variance > 0:\n",
      "             # Adjust alpha based on variance. Higher variance -> more weight on Kalman (smoother)\n",
      "             alpha = 0.5 + 0.3 * normalized_variance # Range [0.5, 0.8]\n",
      "        else:\n",
      "            alpha = 0.5 # Default if variance is zero\n",
      "\n",
      "        weight_poly = 1 - alpha\n",
      "        weight_kalman = alpha\n",
      "\n",
      "        y_combined = weight_poly * y_poly_trend + weight_kalman * y_kalman\n",
      "\n",
      "        # --- Post-processing for Slope Change Minimization and False Reversal Penalty ---\n",
      "        # This is a crucial step to address the multi-objective requirements.\n",
      "        # We'll analyze the slope of the combined signal and the polynomial trend.\n",
      "        # If the polynomial trend shows a reversal that is not supported by the Kalman,\n",
      "        # we can smooth it out.\n",
      "\n",
      "        # Calculate slopes (using central differences for better accuracy)\n",
      "        poly_slope = np.gradient(y_poly_trend)\n",
      "        kalman_slope = np.gradient(y_kalman)\n",
      "        combined_slope = np.gradient(y_combined)\n",
      "\n",
      "        # Identify potential false reversals: where poly_slope changes sign significantly\n",
      "        # but kalman_slope does not, or where combined_slope is too abrupt.\n",
      "        # A simple penalty: if the combined slope is very different from the Kalman slope\n",
      "        # (which is expected to be smoother), we can adjust it.\n",
      "\n",
      "        final_y = np.copy(y_combined)\n",
      "        for i in range(1, output_length - 1):\n",
      "            # If polynomial trend reverses sharply, and Kalman is smoother, favor Kalman.\n",
      "            # This is a simplified heuristic.\n",
      "            if np.sign(poly_slope[i]) != np.sign(poly_slope[i-1]) and \\\n",
      "               np.sign(kalman_slope[i]) == np.sign(kalman_slope[i-1]) and \\\n",
      "               abs(poly_slope[i]) > abs(kalman_slope[i]) * 1.5: # Heuristic threshold\n",
      "                # Adjust the combined output to be closer to the Kalman output at this point\n",
      "                final_y[i] = y_kalman[i]\n",
      "\n",
      "            # Also, ensure the combined slope doesn't change too rapidly.\n",
      "            # If the change in combined slope is extreme, smooth it out.\n",
      "            if abs(combined_slope[i] - combined_slope[i-1]) > abs(kalman_slope[i] - kalman_slope[i-1]) * 2.0:\n",
      "                 # Interpolate between the previous and next points, or nudge towards Kalman\n",
      "                 final_y[i] = (y_combined[i-1] + y_combined[i+1]) / 2.0 # Simple smoothing\n",
      "\n",
      "        return final_y\n",
      "\n",
      "\n",
      "    def process_signal(input_signal, window_size=20, algorithm_type=\"enhanced_adaptive\"):\n",
      "        \"\"\"\n",
      "        Main signal processing function that applies the selected algorithm.\n",
      "\n",
      "        Args:\n",
      "            input_signal: Input time series data\n",
      "            window_size: Window size for processing\n",
      "            algorithm_type: Type of algorithm to use (\"basic\", \"enhanced\", \"enhanced_adaptive\")\n",
      "\n",
      "        Returns:\n",
      "            Filtered signal\n",
      "        \"\"\"\n",
      "        if algorithm_type == \"enhanced_adaptive\":\n",
      "            # Parameters for enhanced_adaptive_filter_optimized\n",
      "            # These can be tuned based on signal characteristics and performance metrics\n",
      "            poly_order = 2\n",
      "            process_noise_std = 0.05 # Smaller process noise for smoother tracking\n",
      "            measurement_noise_std = 0.3 # Higher measurement noise to emphasize Kalman's role\n",
      "\n",
      "            return enhanced_adaptive_filter_optimized(\n",
      "                input_signal,\n",
      "                window_size=window_size,\n",
      "                poly_order=poly_order,\n",
      "                process_noise_std=process_noise_std,\n",
      "                measurement_noise_std=measurement_noise_std\n",
      "            )\n",
      "        elif algorithm_type == \"enhanced\":\n",
      "            return enhanced_filter_with_trend_preservation(input_signal, window_size)\n",
      "        else: # \"basic\"\n",
      "            return adaptive_filter(input_signal, window_size)\n",
      "Iteration 15: New subsample score 0.0 is not better than old score 1.3253188024574332, skipping\n",
      "Iteration 16: Selected program 0 score: 0.39265168271364936\n",
      "Iteration 16: Proposed new text for program: \"\"\"\n",
      "Real-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n",
      "\n",
      "This algorithm implements a sliding window approach to filter volatile, non-stationary\n",
      "time series data while minimizing noise and preserving signal dynamics.\n",
      "It incorporates advanced techniques for multi-objective optimization:\n",
      "(1) Slope change minimization - reducing spurious directional reversals,\n",
      "(2) Lag error minimization - maintaining responsiveness,\n",
      "(3) Tracking accuracy - preserving genuine signal trends, and\n",
      "(4) False reversal penalty - avoiding noise-induced trend changes.\n",
      "\"\"\"\n",
      "import numpy as np\n",
      "from scipy import signal\n",
      "from collections import deque\n",
      "from scipy.signal import savgol_filter # For smoothing and derivative estimation\n",
      "\n",
      "class AdaptiveFilterConfig:\n",
      "    \"\"\"Configuration parameters for the adaptive filter.\"\"\"\n",
      "    def __init__(self,\n",
      "                 window_size=20,\n",
      "                 poly_order=3,\n",
      "                 diff_order=1,\n",
      "                 noise_threshold_factor=1.5,\n",
      "                 trend_persistence_threshold=0.05,\n",
      "                 kalman_process_noise=1e-5,\n",
      "                 kalman_measurement_noise=1e-3,\n",
      "                 kalman_initial_estimate=0.1,\n",
      "                 kalman_initial_covariance=1.0):\n",
      "        self.window_size = window_size\n",
      "        self.poly_order = poly_order # Order of the polynomial for fitting\n",
      "        self.diff_order = diff_order # Order of the derivative to estimate slope\n",
      "        self.noise_threshold_factor = noise_threshold_factor # Factor to determine noise level\n",
      "        self.trend_persistence_threshold = trend_persistence_threshold # Threshold for trend persistence\n",
      "        self.kalman_process_noise = kalman_process_noise # Process noise for Kalman filter\n",
      "        self.kalman_measurement_noise = kalman_measurement_noise # Measurement noise for Kalman filter\n",
      "        self.kalman_initial_estimate = kalman_initial_estimate # Initial state estimate for Kalman filter\n",
      "        self.kalman_initial_covariance = kalman_initial_covariance # Initial covariance for Kalman filter\n",
      "\n",
      "def adaptive_filter(x, config: AdaptiveFilterConfig):\n",
      "    \"\"\"\n",
      "    Adaptive signal processing algorithm using a sliding window with polynomial fitting\n",
      "    and Kalman filtering for improved noise reduction and trend preservation.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        config: AdaptiveFilterConfig object with algorithm parameters.\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal with length = len(x) - window_size + 1\n",
      "    \"\"\"\n",
      "    if len(x) < config.window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({config.window_size})\")\n",
      "\n",
      "    output_length = len(x) - config.window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "    \n",
      "    # Initialize Kalman Filter states\n",
      "    # State vector: [signal_value, slope]\n",
      "    # We'll use a simplified 1D Kalman filter for each window for demonstration\n",
      "    # A more robust implementation would track state across windows or use a multi-dimensional KF\n",
      "    \n",
      "    # For simplicity, we'll implement a polynomial-based approach with adaptive thresholding\n",
      "    # and a more direct approach to slope and reversal detection.\n",
      "    \n",
      "    # Using Savitzky-Golay filter for smoothing and derivative estimation\n",
      "    # This provides a good balance between noise reduction and preserving dynamics.\n",
      "    # The window length of Savitzky-Golay should be odd.\n",
      "    sg_window_length = config.window_size if config.window_size % 2 != 0 else config.window_size - 1\n",
      "    if sg_window_length < 3:\n",
      "        sg_window_length = 3 # Minimum window length for Savitzky-Golay\n",
      "\n",
      "    # Apply Savitzky-Golay filter for smoothing\n",
      "    smoothed_signal = savgol_filter(x, sg_window_length, config.poly_order)\n",
      "\n",
      "    # Estimate the first derivative (slope)\n",
      "    # We need to be careful about the output length of derivative calculation\n",
      "    # If we use savgol_filter with deriv=1, it returns a smoothed derivative.\n",
      "    # To align with the sliding window output, we can calculate it within the loop\n",
      "    # or use a method that produces a signal of compatible length.\n",
      "    \n",
      "    # For a sliding window approach that outputs len(x) - window_size + 1,\n",
      "    # we can process segments and then combine.\n",
      "    # Alternatively, we can consider the output of savgol_filter as a representation\n",
      "    # of the filtered signal, with some delay.\n",
      "\n",
      "    # Let's adapt the output to match the expected length by processing segments.\n",
      "    # This approach will have latency, but it's a common trade-off.\n",
      "    \n",
      "    # For a real-time system, processing each new sample and updating the filter state\n",
      "    # is more appropriate. However, given the current structure, we'll simulate\n",
      "    # by processing windows.\n",
      "\n",
      "    # Re-evaluating the output length requirement: \"y: Filtered output signal with length = len(x) - window_size + 1\"\n",
      "    # This implies that the output at index `i` corresponds to the window `x[i : i + window_size]`.\n",
      "    # This is a direct sliding window implementation.\n",
      "\n",
      "    # Let's use a combination of Savitzky-Golay for smoothing within the window\n",
      "    # and then analyze trends within that window.\n",
      "\n",
      "    for i in range(output_length):\n",
      "        window = x[i : i + config.window_size]\n",
      "        \n",
      "        # Apply Savitzky-Golay filter to the current window for smoothing\n",
      "        # polyorder should be less than window_length\n",
      "        current_poly_order = min(config.poly_order, len(window) - 1)\n",
      "        if current_poly_order < 0: current_poly_order = 0\n",
      "        \n",
      "        smoothed_window = savgol_filter(window, window_length=len(window), polyorder=current_poly_order, deriv=0)\n",
      "        \n",
      "        # Estimate slope within the window using the derivative of the smoothed signal\n",
      "        # We'll use the derivative of the smoothed window to estimate the slope at the end of the window\n",
      "        # to decide on the filtered output value.\n",
      "        \n",
      "        # Calculate the derivative of the smoothed window\n",
      "        # The output length of derivative is usually window_length - deriv_order * (poly_order + 1)\n",
      "        # For simplicity, let's use the derivative of the smoothed signal for the *entire* input `x`\n",
      "        # and then extract the relevant part. This introduces a fixed delay but simplifies implementation.\n",
      "        \n",
      "        # Let's try a different approach: fit a polynomial to the window and use its value at the end.\n",
      "        # This directly addresses the sliding window output requirement.\n",
      "        \n",
      "        # Fit a polynomial of specified order to the window\n",
      "        # We need at least poly_order + 1 points for fitting\n",
      "        if len(window) > config.poly_order:\n",
      "            # Use numpy.polyfit for polynomial fitting\n",
      "            # The x-values for fitting are just indices within the window\n",
      "            window_indices = np.arange(len(window))\n",
      "            coeffs = np.polyfit(window_indices, window, config.poly_order)\n",
      "            poly = np.poly1d(coeffs)\n",
      "            \n",
      "            # The filtered output is the value of the polynomial at the last point of the window\n",
      "            # which corresponds to index len(window) - 1.\n",
      "            # However, to better represent the \"center\" or \"end\" of the filtered value\n",
      "            # for the current window, we can evaluate at the last index of the window.\n",
      "            # To minimize lag, we might evaluate at the middle or use a forward-looking approach.\n",
      "            # Given the output length requirement, we'll evaluate at the end.\n",
      "            \n",
      "            # A common strategy for trend preservation is to use the polynomial value at the\n",
      "            # end of the window, or a weighted average of polynomial values across the window.\n",
      "            # For this problem, let's use the polynomial's value at the last point of the window.\n",
      "            # This will introduce some lag.\n",
      "            \n",
      "            # To reduce lag, we can consider the value at the center of the window,\n",
      "            # but this doesn't align with the output length requirement.\n",
      "            # The output length `len(x) - window_size + 1` means `y[i]` is the filtered\n",
      "            # value corresponding to the window `x[i:i+window_size]`.\n",
      "            # The most natural interpretation for `y[i]` is the filtered value\n",
      "            # representing the *end* of that window, which is `x[i + window_size - 1]`.\n",
      "            \n",
      "            # Let's use the polynomial fit value at the last point of the window.\n",
      "            # This means `y[i]` is the polynomial prediction for `x[i + window_size - 1]`.\n",
      "            \n",
      "            # To incorporate slope change minimization and false reversal penalty:\n",
      "            # We can analyze the derivative of the fitted polynomial.\n",
      "            \n",
      "            # Calculate the derivative of the fitted polynomial\n",
      "            poly_deriv = np.polyder(poly, config.diff_order)\n",
      "            \n",
      "            # Estimate the slope at the end of the window\n",
      "            current_slope = poly_deriv(len(window) - 1)\n",
      "            \n",
      "            # Heuristic for determining if a reversal is spurious or genuine:\n",
      "            # Compare the current slope with the previous slope.\n",
      "            # If the slope changes sign drastically and the magnitude of the change is large,\n",
      "            # it might be a genuine trend change. If it's small, it might be noise.\n",
      "            \n",
      "            # We need to store previous slope and previous filtered value.\n",
      "            if i == 0:\n",
      "                # Initialize previous values for the first window\n",
      "                prev_filtered_value = smoothed_window[-1] if len(smoothed_window) > 0 else np.mean(window)\n",
      "                prev_slope = current_slope\n",
      "                \n",
      "                # For the first output point, we can use the smoothed value or mean of the first window.\n",
      "                # To align with the output length, this `y[0]` corresponds to `x[0:window_size]`.\n",
      "                # We'll use the polynomial prediction at the end of the window.\n",
      "                y[i] = poly(len(window) - 1)\n",
      "            else:\n",
      "                # Compare current slope with previous slope\n",
      "                # A simple check for significant slope change\n",
      "                if np.sign(current_slope) != np.sign(prev_slope) and abs(current_slope - prev_slope) > config.trend_persistence_threshold * abs(prev_slope):\n",
      "                    # Potential significant trend change or reversal\n",
      "                    # If the magnitude of the current value is also changing significantly,\n",
      "                    # we trust the change. Otherwise, it might be noise.\n",
      "                    # This is where false reversal penalty comes in.\n",
      "                    \n",
      "                    # A more direct approach for false reversal penalty:\n",
      "                    # If the current filtered value is very close to the previous filtered value,\n",
      "                    # and the slope changes direction, it's likely a false reversal.\n",
      "                    \n",
      "                    # Let's use a threshold on the difference between consecutive filtered values.\n",
      "                    # If the difference is small but the slope reversed, penalize it.\n",
      "                    \n",
      "                    current_filtered_value = poly(len(window) - 1)\n",
      "                    \n",
      "                    # False reversal penalty: if the change in filtered value is small\n",
      "                    # but the slope reversed, we might want to suppress the reversal.\n",
      "                    # This is a heuristic.\n",
      "                    \n",
      "                    # For now, let's focus on basic trend preservation and noise reduction.\n",
      "                    # The polynomial fitting itself handles some of the trend preservation.\n",
      "                    \n",
      "                    y[i] = current_filtered_value\n",
      "                    \n",
      "                    prev_slope = current_slope\n",
      "                    prev_filtered_value = current_filtered_value\n",
      "                else:\n",
      "                    # Slope is consistent or change is not significant.\n",
      "                    # Use the polynomial prediction.\n",
      "                    y[i] = poly(len(window) - 1)\n",
      "                    prev_slope = current_slope\n",
      "                    prev_filtered_value = y[i]\n",
      "        else:\n",
      "            # Not enough points to fit the polynomial, fall back to mean or smoothed value\n",
      "            y[i] = smoothed_window[-1] if len(smoothed_window) > 0 else np.mean(window)\n",
      "            if i > 0:\n",
      "                prev_slope = 0 # Assume zero slope if fitting failed\n",
      "                prev_filtered_value = y[i]\n",
      "\n",
      "\n",
      "    # Post-processing to enforce some constraints if needed, e.g., smoothing the output `y`\n",
      "    # or applying a final pass to penalize false reversals.\n",
      "\n",
      "    # One way to penalize false reversals and reduce slope changes:\n",
      "    # If the output `y` shows a sharp reversal that's not supported by the overall trend,\n",
      "    # we can smooth it further or adjust it.\n",
      "\n",
      "    # Let's consider a simple post-processing step to penalize rapid oscillations\n",
      "    # that might be noise-induced reversals.\n",
      "    # We can use another Savitzky-Golay filter on the output `y`.\n",
      "    \n",
      "    # The output length of `y` is `output_length`.\n",
      "    # We need a window for this post-processing that is smaller than `output_length`.\n",
      "    post_process_window_size = min(max(3, config.window_size // 4), output_length - 1)\n",
      "    if post_process_window_size % 2 == 0:\n",
      "        post_process_window_size -= 1\n",
      "    if post_process_window_size < 3:\n",
      "        post_process_window_size = 3\n",
      "\n",
      "    if output_length >= post_process_window_size:\n",
      "        # Apply a mild smoothing to the output to reduce high-frequency reversals\n",
      "        # and enforce smoothness.\n",
      "        y_smoothed_final = savgol_filter(y, window_length=post_process_window_size, polyorder=config.poly_order - 1, deriv=0)\n",
      "        \n",
      "        # Re-evaluate the output based on the smoothed version.\n",
      "        # This is a trade-off: it reduces responsiveness but improves smoothness.\n",
      "        # We need to balance this.\n",
      "        \n",
      "        # For now, let's return the direct polynomial fit output.\n",
      "        # Further refinement would involve adaptive window sizing or more sophisticated Kalman/particle filters.\n",
      "        \n",
      "        # Let's re-evaluate the metrics and objectives.\n",
      "        # Slope change minimization: The polynomial fitting helps.\n",
      "        # Lag error minimization: Evaluating at the end of the window introduces lag.\n",
      "        # Tracking accuracy: Polynomial fitting aims for this.\n",
      "        # False reversal penalty: This is the hardest part.\n",
      "\n",
      "        # A more direct way to penalize false reversals is to compare the current\n",
      "        # slope with the slope of the smoothed signal over a slightly longer window.\n",
      "        # If the short-term slope (from polynomial) is very different from the long-term slope,\n",
      "        # it might be a false reversal.\n",
      "\n",
      "        # Let's refine the polynomial fitting approach:\n",
      "        # Instead of just using the last point, we can use a weighted average of\n",
      "        # polynomial predictions within the window, or the value at the center.\n",
      "        # But the output length constraint is strict.\n",
      "\n",
      "        # For the output `y[i]`, it represents the filtered value for the window `x[i : i + window_size]`.\n",
      "        # This value should ideally be the \"best estimate\" for the signal at time `i + window_size - 1`.\n",
      "        \n",
      "        # Let's consider a Kalman filter approach for real-time adaptation.\n",
      "        # State: [signal_value, slope]\n",
      "        # Measurement: the current sample `x[t]`\n",
      "        # Prediction:\n",
      "        # x_hat(t|t-1) = F * x_hat(t-1|t-1) + B * u(t)\n",
      "        # P(t|t-1) = F * P(t-1|t-1) * F' + Q\n",
      "        # Update:\n",
      "        # K(t) = P(t|t-1) * H' * (H * P(t|t-1) * H' + R)^-1\n",
      "        # x_hat(t|t) = x_hat(t|t-1) + K(t) * (z(t) - H * x_hat(t|t-1))\n",
      "        # P(t|t) = (I - K(t) * H) * P(t|t-1)\n",
      "\n",
      "        # This requires maintaining state across samples, which is different from the current window-based processing.\n",
      "        # Given the structure, we'll stick to window-based processing but try to improve the within-window estimation.\n",
      "\n",
      "        # Alternative within-window processing:\n",
      "        # For each window `x[i : i + window_size]`:\n",
      "        # 1. Smooth the window using Savitzky-Golay.\n",
      "        # 2. Estimate the slope at the end of the smoothed window.\n",
      "        # 3. Use a weighted average of the smoothed window's end value and a prediction based on the slope.\n",
      "        # 4. Apply a penalty if a reversal is detected and the magnitude of change is small.\n",
      "\n",
      "        # Let's focus on a robust polynomial fitting approach for the window.\n",
      "        \n",
      "        # For y[i], it represents the filtered value for the window x[i : i + window_size].\n",
      "        # This value should be an estimate of the signal at time `i + window_size - 1`.\n",
      "        # Using the polynomial prediction at `len(window) - 1` is a reasonable first step.\n",
      "\n",
      "        # To minimize slope changes and false reversals:\n",
      "        # We can analyze the derivative of the fitted polynomial.\n",
      "        # If the derivative changes sign and the magnitude of the value change is small,\n",
      "        # we can \"hold\" the previous value or apply a weaker update.\n",
      "\n",
      "        # Let's introduce a state to track previous slope and filtered value.\n",
      "        \n",
      "        # Initialize state for the first window\n",
      "        if i == 0:\n",
      "            if len(window) > config.poly_order:\n",
      "                window_indices = np.arange(len(window))\n",
      "                coeffs = np.polyfit(window_indices, window, config.poly_order)\n",
      "                poly = np.poly1d(coeffs)\n",
      "                y[i] = poly(len(window) - 1)\n",
      "                \n",
      "                # Calculate initial slope\n",
      "                if config.poly_order >= 1:\n",
      "                    poly_deriv = np.polyder(poly, config.diff_order)\n",
      "                    prev_slope = poly_deriv(len(window) - 1)\n",
      "                else:\n",
      "                    prev_slope = 0\n",
      "                \n",
      "                prev_filtered_value = y[i]\n",
      "            else:\n",
      "                y[i] = np.mean(window) # Fallback\n",
      "                prev_slope = 0\n",
      "                prev_filtered_value = y[i]\n",
      "        else:\n",
      "            # Process subsequent windows\n",
      "            current_window = x[i : i + config.window_size]\n",
      "            \n",
      "            if len(current_window) > config.poly_order:\n",
      "                window_indices = np.arange(len(current_window))\n",
      "                coeffs = np.polyfit(window_indices, current_window, config.poly_order)\n",
      "                poly = np.poly1d(coeffs)\n",
      "                \n",
      "                current_filtered_value = poly(len(current_window) - 1)\n",
      "                \n",
      "                current_slope = 0\n",
      "                if config.poly_order >= 1:\n",
      "                    poly_deriv = np.polyder(poly, config.diff_order)\n",
      "                    current_slope = poly_deriv(len(current_window) - 1)\n",
      "                \n",
      "                # Multi-objective optimization:\n",
      "                # 1. Slope change minimization: penalize large changes in slope.\n",
      "                # 2. Lag error minimization: use polynomial value at end of window.\n",
      "                # 3. Tracking accuracy: polynomial fitting aims for this.\n",
      "                # 4. False reversal penalty: if slope reverses and value change is small.\n",
      "\n",
      "                # Heuristic for false reversal penalty:\n",
      "                # If the slope changes sign and the absolute difference between current and previous\n",
      "                # filtered values is small relative to the overall signal dynamics or noise level.\n",
      "                \n",
      "                # Estimate noise level in the current window\n",
      "                window_mean = np.mean(current_window)\n",
      "                window_std = np.std(current_window)\n",
      "                estimated_noise_level = window_std # A simple estimate\n",
      "\n",
      "                # Check for reversal and small value change\n",
      "                if (np.sign(current_slope) != np.sign(prev_slope) and\n",
      "                    abs(current_filtered_value - prev_filtered_value) < config.noise_threshold_factor * estimated_noise_level):\n",
      "                    \n",
      "                    # This looks like a potential false reversal. Penalize it by\n",
      "                    # either holding the previous value or taking a less aggressive step.\n",
      "                    # For now, let's just use the previous filtered value to suppress the reversal.\n",
      "                    # This is a strong penalty. A softer penalty would be a weighted average.\n",
      "                    y[i] = prev_filtered_value\n",
      "                    # Keep the previous slope to avoid repeated penalization in the next step if it's still noisy\n",
      "                    # Or update the slope to the current one, assuming the noise will be filtered out.\n",
      "                    # Let's update the slope but use the previous value for output.\n",
      "                    prev_slope = current_slope\n",
      "                else:\n",
      "                    # No significant false reversal detected, or it's a genuine trend change.\n",
      "                    y[i] = current_filtered_value\n",
      "                    prev_slope = current_slope\n",
      "                    \n",
      "                prev_filtered_value = y[i]\n",
      "\n",
      "            else:\n",
      "                # Not enough points for polynomial fitting, fall back to mean\n",
      "                y[i] = np.mean(current_window)\n",
      "                prev_slope = 0\n",
      "                prev_filtered_value = y[i]\n",
      "                \n",
      "    # Final smoothing pass to ensure overall smoothness and reduce minor oscillations\n",
      "    # This is a trade-off between responsiveness and smoothness.\n",
      "    # The window size for this final smoothing should be carefully chosen.\n",
      "    # Using a smaller window than the initial one.\n",
      "    \n",
      "    final_smooth_window_size = min(max(3, config.window_size // 3), output_length - 1)\n",
      "    if final_smooth_window_size % 2 == 0:\n",
      "        final_smooth_window_size -= 1\n",
      "    if final_smooth_window_size < 3:\n",
      "        final_smooth_window_size = 3\n",
      "\n",
      "    if output_length >= final_smooth_window_size:\n",
      "        y_final = savgol_filter(y, window_length=final_smooth_window_size, polyorder=config.poly_order - 1, deriv=0)\n",
      "        return y_final\n",
      "    else:\n",
      "        return y # Return unsmoothed if output is too short for final smoothing\n",
      "\n",
      "def enhanced_filter_with_trend_preservation(x, window_size=20):\n",
      "    \"\"\"\n",
      "    Enhanced version with trend preservation using polynomial fitting and adaptive logic.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal\n",
      "    \"\"\"\n",
      "    # Configure the adaptive filter\n",
      "    config = AdaptiveFilterConfig(window_size=window_size,\n",
      "                                  poly_order=3, # Moderate polynomial order\n",
      "                                  diff_order=1,\n",
      "                                  noise_threshold_factor=1.5, # Sensitivity to noise for reversal detection\n",
      "                                  trend_persistence_threshold=0.1) # Threshold for significant slope change\n",
      "    \n",
      "    return adaptive_filter(x, config)\n",
      "\n",
      "\n",
      "def process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n",
      "    \"\"\"\n",
      "    Main signal processing function that applies the selected algorithm.\n",
      "\n",
      "    Args:\n",
      "        input_signal: Input time series data\n",
      "        window_size: Window size for processing\n",
      "        algorithm_type: Type of algorithm to use (\"basic\" or \"enhanced\")\n",
      "\n",
      "    Returns:\n",
      "        Filtered signal\n",
      "    \"\"\"\n",
      "    if algorithm_type == \"enhanced\":\n",
      "        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n",
      "    else:\n",
      "        # Fallback to a simpler method if not enhanced\n",
      "        # For simplicity, let's return a smoothed version using Savitzky-Golay\n",
      "        # if the 'basic' type is requested.\n",
      "        # This is not an adaptive filter in the strict sense but provides a baseline.\n",
      "        \n",
      "        # Ensure window_size is odd for Savitzky-Golay\n",
      "        sg_window = window_size if window_size % 2 != 0 else window_size - 1\n",
      "        if sg_window < 3: sg_window = 3\n",
      "        \n",
      "        if len(input_signal) < sg_window:\n",
      "            return np.mean(input_signal) # Fallback for very short signals\n",
      "        \n",
      "        # Basic smoothing with Savitzky-Golay, polyorder 3\n",
      "        return savgol_filter(input_signal, window_length=sg_window, polyorder=3, deriv=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GEPA Optimization:  95%|| 95/100 [04:42<00:14,  2.98s/rollouts]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16: New subsample score 0.8819203957618702 is not better than old score 0.9397379105559157, skipping\n",
      "\n",
      "Best score: 0.3927\n",
      "Best candidate index: 0\n",
      "Total candidates evaluated: 1\n",
      "Total metric calls: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run GEPA optimization\n",
    "result = optimize(\n",
    "    seed_candidate=seed_candidate,\n",
    "    trainset=trainset,\n",
    "    adapter=adapter,\n",
    "    max_metric_calls=100,  # Budget for evaluation calls - adjust as needed\n",
    "    display_progress_bar=True,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "best_score = result.val_aggregate_scores[result.best_idx]\n",
    "print(f\"\\nBest score: {best_score:.4f}\")\n",
    "print(f\"Best candidate index: {result.best_idx}\")\n",
    "print(f\"Total candidates evaluated: {len(result.candidates)}\")\n",
    "print(f\"Total metric calls: {result.total_metric_calls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EVOLVE-BLOCK-START\n",
      "\"\"\"\n",
      "Real-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n",
      "\n",
      "This algorithm implements a sliding window approach to filter volatile, non-stationary\n",
      "time series data while minimizing noise and preserving signal dynamics.\n",
      "\"\"\"\n",
      "import numpy as np\n",
      "from scipy import signal\n",
      "from collections import deque\n",
      "\n",
      "\n",
      "def adaptive_filter(x, window_size=20):\n",
      "    \"\"\"\n",
      "    Adaptive signal processing algorithm using sliding window approach.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window (W samples)\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal with length = len(x) - window_size + 1\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    # Initialize output array\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    # Simple moving average as baseline\n",
      "    for i in range(output_length):\n",
      "        window = x[i : i + window_size]\n",
      "\n",
      "        # Basic moving average filter\n",
      "        y[i] = np.mean(window)\n",
      "\n",
      "    return y\n",
      "\n",
      "\n",
      "def enhanced_filter_with_trend_preservation(x, window_size=20):\n",
      "    \"\"\"\n",
      "    Enhanced version with trend preservation using weighted moving average.\n",
      "\n",
      "    Args:\n",
      "        x: Input signal (1D array of real-valued samples)\n",
      "        window_size: Size of the sliding window\n",
      "\n",
      "    Returns:\n",
      "        y: Filtered output signal\n",
      "    \"\"\"\n",
      "    if len(x) < window_size:\n",
      "        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n",
      "\n",
      "    output_length = len(x) - window_size + 1\n",
      "    y = np.zeros(output_length)\n",
      "\n",
      "    # Create weights that emphasize recent samples\n",
      "    weights = np.exp(np.linspace(-2, 0, window_size))\n",
      "    weights = weights / np.sum(weights)\n",
      "\n",
      "    for i in range(output_length):\n",
      "        window = x[i : i + window_size]\n",
      "\n",
      "        # Weighted moving average with exponential weights\n",
      "        y[i] = np.sum(window * weights)\n",
      "\n",
      "    return y\n",
      "\n",
      "\n",
      "def process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n",
      "    \"\"\"\n",
      "    Main signal processing function that applies the selected algorithm.\n",
      "\n",
      "    Args:\n",
      "        input_signal: Input time series data\n",
      "        window_size: Window size for processing\n",
      "        algorithm_type: Type of algorithm to use (\"basic\" or \"enhanced\")\n",
      "\n",
      "    Returns:\n",
      "        Filtered signal\n",
      "    \"\"\"\n",
      "    if algorithm_type == \"enhanced\":\n",
      "        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n",
      "    else:\n",
      "        return adaptive_filter(input_signal, window_size)\n",
      "# EVOLVE-BLOCK-END\n",
      "\n",
      "\n",
      "def generate_test_signal(length=1000, noise_level=0.3, seed=42):\n",
      "    \"\"\"\n",
      "    Generate synthetic test signal with known characteristics.\n",
      "\n",
      "    Args:\n",
      "        length: Length of the signal\n",
      "        noise_level: Standard deviation of noise to add\n",
      "        seed: Random seed for reproducibility\n",
      "\n",
      "    Returns:\n",
      "        Tuple of (noisy_signal, clean_signal)\n",
      "    \"\"\"\n",
      "    np.random.seed(seed)\n",
      "    t = np.linspace(0, 10, length)\n",
      "\n",
      "    # Create a complex signal with multiple components\n",
      "    clean_signal = (\n",
      "        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n",
      "        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n",
      "        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n",
      "        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n",
      "    )\n",
      "\n",
      "    # Add non-stationary behavior\n",
      "    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n",
      "    clean_signal += trend\n",
      "\n",
      "    # Add random walk component for non-stationarity\n",
      "    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n",
      "    clean_signal += random_walk\n",
      "\n",
      "    # Add noise\n",
      "    noise = np.random.normal(0, noise_level, length)\n",
      "    noisy_signal = clean_signal + noise\n",
      "\n",
      "    return noisy_signal, clean_signal\n",
      "\n",
      "\n",
      "def run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n",
      "    \"\"\"\n",
      "    Run the signal processing algorithm on a test signal.\n",
      "\n",
      "    Returns:\n",
      "        Dictionary containing results and metrics\n",
      "    \"\"\"\n",
      "    # Generate test signal\n",
      "    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n",
      "\n",
      "    # Process the signal\n",
      "    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n",
      "\n",
      "    # Calculate basic metrics\n",
      "    if len(filtered_signal) > 0:\n",
      "        # Align signals for comparison (account for processing delay)\n",
      "        delay = window_size - 1\n",
      "        aligned_clean = clean_signal[delay:]\n",
      "        aligned_noisy = noisy_signal[delay:]\n",
      "\n",
      "        # Ensure same length\n",
      "        min_length = min(len(filtered_signal), len(aligned_clean))\n",
      "        filtered_signal = filtered_signal[:min_length]\n",
      "        aligned_clean = aligned_clean[:min_length]\n",
      "        aligned_noisy = aligned_noisy[:min_length]\n",
      "\n",
      "        # Calculate correlation with clean signal\n",
      "        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n",
      "\n",
      "        # Calculate noise reduction\n",
      "        noise_before = np.var(aligned_noisy - aligned_clean)\n",
      "        noise_after = np.var(filtered_signal - aligned_clean)\n",
      "        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n",
      "\n",
      "        return {\n",
      "            \"filtered_signal\": filtered_signal,\n",
      "            \"clean_signal\": aligned_clean,\n",
      "            \"noisy_signal\": aligned_noisy,\n",
      "            \"correlation\": correlation,\n",
      "            \"noise_reduction\": noise_reduction,\n",
      "            \"signal_length\": min_length,\n",
      "        }\n",
      "    else:\n",
      "        return {\n",
      "            \"filtered_signal\": [],\n",
      "            \"clean_signal\": [],\n",
      "            \"noisy_signal\": [],\n",
      "            \"correlation\": 0,\n",
      "            \"noise_reduction\": 0,\n",
      "            \"signal_length\": 0,\n",
      "        }\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Test the algorithm\n",
      "    results = run_signal_processing()\n",
      "    print(f\"Signal processing completed!\")\n",
      "    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n",
      "    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n",
      "    print(f\"Processed signal length: {results['signal_length']}\")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final evolved program\n",
    "best_program_text = result.best_candidate.get(\"program\", \"\")\n",
    "complete_program = adapter._construct_complete_program(best_program_text)\n",
    "print(complete_program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Results Comparison with OpenEvolve\n",
    "\n",
    "Let's compare GEPA's results with the original OpenEvolve results from their signal processing example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEPA Best Candidate Detailed Metrics:\n",
      "==================================================\n",
      "composite_score: 0.4406\n",
      "slope_changes: 55.0000\n",
      "lag_error: 1.0024\n",
      "correlation: 0.8563\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import tempfile\n",
    "\n",
    "# Re-evaluate the best candidate to get detailed metrics\n",
    "evaluator_path = project_path / \"evaluator.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"evaluator\", evaluator_path)\n",
    "if spec is None or spec.loader is None:\n",
    "    raise ImportError(f\"Could not load evaluator module from {evaluator_path}\")\n",
    "evaluator_module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(evaluator_module)\n",
    "evaluate = evaluator_module.evaluate\n",
    "\n",
    "# Use the adapter's _construct_complete_program method\n",
    "best_program_text = result.best_candidate.get(\"program\", \"\")\n",
    "complete_program = adapter._construct_complete_program(best_program_text)\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n",
    "    f.write(complete_program)\n",
    "    temp_program_path = f.name\n",
    "\n",
    "# Evaluate on first signal pair\n",
    "best_result = evaluate(temp_program_path, trainset[0])\n",
    "gepa_metrics = best_result.metrics\n",
    "\n",
    "gepa_detailed = {\n",
    "    \"composite_score\": gepa_metrics.get(\"composite_score\", 0.0),\n",
    "    \"slope_changes\": gepa_metrics.get(\"slope_changes\", 0.0),\n",
    "    \"correlation\": gepa_metrics.get(\"correlation\", 0.0),\n",
    "    \"lag_error\": gepa_metrics.get(\"lag_error\", 0.0),\n",
    "}\n",
    "\n",
    "import os\n",
    "\n",
    "os.unlink(temp_program_path)\n",
    "\n",
    "print(\"GEPA Best Candidate Detailed Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "for metric, value in gepa_detailed.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenEvolve Best Candidate Detailed Metrics:\n",
      "==================================================\n",
      "composite_score: 0.3712\n",
      "slope_changes: 322.8000\n",
      "correlation: 0.1470\n",
      "lag_error: 0.9140\n"
     ]
    }
   ],
   "source": [
    "# OpenEvolve metrics listed in their README.md\n",
    "openevolve_metrics = {\n",
    "    \"composite_score\": 0.3712,\n",
    "    \"slope_changes\": 322.8,\n",
    "    \"correlation\": 0.147,\n",
    "    \"lag_error\": 0.914,\n",
    "}\n",
    "print(\"OpenEvolve Best Candidate Detailed Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "for metric, value in openevolve_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
