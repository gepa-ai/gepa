{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial: Using EvolveAdapter with OpenEvolve Projects\n",
        "\n",
        "This tutorial demonstrates how to adapt an existing OpenEvolve project to work with GEPA's `EvolveAdapter`. The key change required is modifying your `evaluate` function to accept a **batch of training data instances** and return a **list of evaluation results** (one per instance), rather than returning a single result.\n",
        "\n",
        "> **Note:** This tutorial is also available as a standalone Python script: `tutorial_evolve_adapter.py`. You can run it directly instead of using this notebook.\n",
        "\n",
        "## Why This Change?\n",
        "\n",
        "GEPA's optimization engine works with batches of data to:\n",
        "- Provide per-instance feedback for better program refinement\n",
        "- Support minibatch-based optimization strategies\n",
        "\n",
        "## Example Project: Function Minimization\n",
        "\n",
        "We'll use the **function minimization** example from OpenEvolve [examples/function_minimization](https://github.com/algorithmicsuperintelligence/openevolve/tree/main/examples/function_minimization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Key Changes Required\n",
        "\n",
        "Here are the **three main changes** you need to make to your `evaluate` function:\n",
        "\n",
        "### Change 1: Function Signature\n",
        "**Before:**\n",
        "```python\n",
        "def evaluate(program_path: str) -> EvaluationResult:\n",
        "```\n",
        "\n",
        "**After:**\n",
        "```python\n",
        "def evaluate(program_path: str, batch: list) -> list[EvaluationResult]:\n",
        "```\n",
        "- Add `batch` parameter\n",
        "- Change return type from `EvaluationResult` to `list[EvaluationResult]`\n",
        "\n",
        "### Change 2: Loop Over Batch Items\n",
        "**Before:** The function runs the program multiple times internally and aggregates results.\n",
        "\n",
        "**After:** Loop over each batch item and evaluate the program for each one:\n",
        "```python\n",
        "results = []\n",
        "for batch_item in batch:\n",
        "    # Evaluate program for this specific batch item\n",
        "    # ... evaluation logic ...\n",
        "    results.append(EvaluationResult(...))  # One result per batch item\n",
        "return results\n",
        "```\n",
        "\n",
        "### Change 3: Extract Parameters from Batch Items\n",
        "**Before:** Hard-coded problem parameters (e.g., `GLOBAL_MIN_X = -1.704`)\n",
        "\n",
        "**After:** Extract parameters from each batch item:\n",
        "```python\n",
        "GLOBAL_MIN_X = batch_item.get(\"global_min_x\", -1.704)\n",
        "GLOBAL_MIN_Y = batch_item.get(\"global_min_y\", 0.678)\n",
        "# ... etc\n",
        "```\n",
        "\n",
        "## Complete Modified Evaluate Function\n",
        "\n",
        "Here's the complete modified evaluate function with all changes applied. For this function minimization project example, each batch item represents a different function minimization problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modified evaluator.py for EvolveAdapter\n",
        "# For this example, each batch item represents a different function minimization problem\n",
        "# \n",
        "# KEY CHANGES MADE:\n",
        "# 1. Added 'batch: list' parameter to function signature\n",
        "# 2. Changed return type to 'list[EvaluationResult]'\n",
        "# 3. Added loop: 'for batch_item in batch:'\n",
        "# 4. Extract parameters from batch_item instead of hard-coding\n",
        "# 5. Return list of results instead of single result\n",
        "\n",
        "import importlib.util\n",
        "import numpy as np\n",
        "import time\n",
        "import concurrent.futures\n",
        "import traceback\n",
        "from openevolve.evaluation_result import EvaluationResult\n",
        "\n",
        "\n",
        "def run_with_timeout(func, args=(), kwargs={}, timeout_seconds=5):\n",
        "    \"\"\"\n",
        "    Run a function with a timeout using concurrent.futures\n",
        "\n",
        "    Args:\n",
        "        func: Function to run\n",
        "        args: Arguments to pass to the function\n",
        "        kwargs: Keyword arguments to pass to the function\n",
        "        timeout_seconds: Timeout in seconds\n",
        "\n",
        "    Returns:\n",
        "        Result of the function or raises TimeoutError\n",
        "    \"\"\"\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
        "        future = executor.submit(func, *args, **kwargs)\n",
        "        try:\n",
        "            return future.result(timeout=timeout_seconds)\n",
        "        except concurrent.futures.TimeoutError:\n",
        "            raise TimeoutError(f\"Function timed out after {timeout_seconds} seconds\")\n",
        "\n",
        "\n",
        "def safe_float(value):\n",
        "    \"\"\"Convert a value to float safely.\"\"\"\n",
        "    try:\n",
        "        return float(value)\n",
        "    except (TypeError, ValueError):\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "# CHANGE 1: Added 'batch' parameter, changed return type to list\n",
        "def evaluate(program_path: str, batch: list) -> list[EvaluationResult]:\n",
        "    \"\"\"\n",
        "    Evaluate the program on a batch of function minimization problems.\n",
        "    \n",
        "    Args:\n",
        "        program_path: Path to the program file to evaluate\n",
        "        batch: List of dicts, each containing:\n",
        "            - 'global_min_x': Target x coordinate\n",
        "            - 'global_min_y': Target y coordinate  \n",
        "            - 'global_min_value': Target function value\n",
        "            - 'bounds': Tuple of (min, max) bounds for search space\n",
        "            - 'function_name': Optional name for the function\n",
        "    \n",
        "    Returns:\n",
        "        List of EvaluationResult objects, one per batch item\n",
        "    \"\"\"\n",
        "    # CHANGE 2: Initialize results list to collect one result per batch item\n",
        "    results = []\n",
        "    \n",
        "    # Load the program once (shared across all batch items)\n",
        "    try:\n",
        "        spec = importlib.util.spec_from_file_location(\"program\", program_path)\n",
        "        program = importlib.util.module_from_spec(spec)\n",
        "        spec.loader.exec_module(program)\n",
        "        \n",
        "        if not hasattr(program, \"run_search\"):\n",
        "            # Return error results for all batch items\n",
        "            for _ in batch:\n",
        "                results.append(EvaluationResult(\n",
        "                    metrics={\"combined_score\": 0.0, \"error\": 1.0},\n",
        "                    artifacts={\"error\": \"Missing run_search function\"}\n",
        "                ))\n",
        "            return results\n",
        "    except Exception as e:\n",
        "        # Return error results for all batch items\n",
        "        for _ in batch:\n",
        "            results.append(EvaluationResult(\n",
        "                metrics={\"combined_score\": 0.0, \"error\": 1.0},\n",
        "                artifacts={\"error\": str(e), \"traceback\": traceback.format_exc()}\n",
        "            ))\n",
        "        return results\n",
        "    \n",
        "    # CHANGE 3: Loop over each batch item\n",
        "    for batch_item in batch:\n",
        "        try:\n",
        "            # CHANGE 4: Extract problem parameters from batch_item instead of hard-coding\n",
        "            # In the original, these were hard-coded constants like:\n",
        "            #   GLOBAL_MIN_X = -1.704\n",
        "            # Now we get them from each batch item:\n",
        "            GLOBAL_MIN_X = batch_item.get(\"global_min_x\", -1.704)\n",
        "            GLOBAL_MIN_Y = batch_item.get(\"global_min_y\", 0.678)\n",
        "            GLOBAL_MIN_VALUE = batch_item.get(\"global_min_value\", -1.519)\n",
        "            bounds = batch_item.get(\"bounds\", (-5, 5))\n",
        "            \n",
        "            # Run multiple trials for this specific problem\n",
        "            num_trials = 10\n",
        "            x_values = []\n",
        "            y_values = []\n",
        "            values = []\n",
        "            distances = []\n",
        "            times = []\n",
        "            success_count = 0\n",
        "            \n",
        "            for trial in range(num_trials):\n",
        "                try:\n",
        "                    start_time = time.time()\n",
        "                    \n",
        "                    # Run the program (it should use the bounds from batch_item)\n",
        "                    # Note: The program may need to be modified to accept bounds as parameter\n",
        "                    result = run_with_timeout(program.run_search, timeout_seconds=5)\n",
        "                    \n",
        "                    # Handle different result formats\n",
        "                    if isinstance(result, tuple):\n",
        "                        if len(result) == 3:\n",
        "                            x, y, value = result\n",
        "                        elif len(result) == 2:\n",
        "                            x, y = result\n",
        "                            # Calculate function value\n",
        "                            value = np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n",
        "                        else:\n",
        "                            continue\n",
        "                    else:\n",
        "                        continue\n",
        "                    \n",
        "                    end_time = time.time()\n",
        "                    \n",
        "                    # Validate results\n",
        "                    x = safe_float(x)\n",
        "                    y = safe_float(y)\n",
        "                    value = safe_float(value)\n",
        "                    \n",
        "                    if (np.isnan(x) or np.isnan(y) or np.isnan(value) or\n",
        "                        np.isinf(x) or np.isinf(y) or np.isinf(value)):\n",
        "                        continue\n",
        "                    \n",
        "                    # Calculate metrics for this trial\n",
        "                    x_diff = x - GLOBAL_MIN_X\n",
        "                    y_diff = y - GLOBAL_MIN_Y\n",
        "                    distance_to_global = np.sqrt(x_diff**2 + y_diff**2)\n",
        "                    \n",
        "                    x_values.append(x)\n",
        "                    y_values.append(y)\n",
        "                    values.append(value)\n",
        "                    distances.append(distance_to_global)\n",
        "                    times.append(end_time - start_time)\n",
        "                    success_count += 1\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    continue\n",
        "            \n",
        "            # If all trials failed, return error result\n",
        "            if success_count == 0:\n",
        "                results.append(EvaluationResult(\n",
        "                    metrics={\n",
        "                        \"value_score\": 0.0,\n",
        "                        \"distance_score\": 0.0,\n",
        "                        \"reliability_score\": 0.0,\n",
        "                        \"combined_score\": 0.0,\n",
        "                        \"error\": 1.0\n",
        "                    },\n",
        "                    artifacts={\"error\": \"All trials failed\"}\n",
        "                ))\n",
        "                continue\n",
        "            \n",
        "            # Calculate aggregated metrics for this batch item\n",
        "            avg_value = float(np.mean(values))\n",
        "            avg_distance = float(np.mean(distances))\n",
        "            \n",
        "            # Convert to scores (higher is better)\n",
        "            value_score = float(1.0 / (1.0 + abs(avg_value - GLOBAL_MIN_VALUE)))\n",
        "            distance_score = float(1.0 / (1.0 + avg_distance))\n",
        "            reliability_score = float(success_count / num_trials)\n",
        "            \n",
        "            # Calculate combined score\n",
        "            base_score = 0.5 * value_score + 0.3 * distance_score + 0.2 * reliability_score\n",
        "            \n",
        "            # Apply solution quality multiplier\n",
        "            if avg_distance < 0.5:\n",
        "                solution_quality_multiplier = 1.5\n",
        "            elif avg_distance < 1.5:\n",
        "                solution_quality_multiplier = 1.2\n",
        "            elif avg_distance < 3.0:\n",
        "                solution_quality_multiplier = 1.0\n",
        "            else:\n",
        "                solution_quality_multiplier = 0.7\n",
        "            \n",
        "            combined_score = float(base_score * solution_quality_multiplier)\n",
        "            \n",
        "            # Create artifacts\n",
        "            artifacts = {\n",
        "                \"convergence_info\": f\"Converged in {num_trials} trials with {success_count} successes\",\n",
        "                \"best_position\": f\"Final position: x={x_values[-1]:.4f}, y={y_values[-1]:.4f}\",\n",
        "                \"average_distance_to_global\": f\"{avg_distance:.4f}\",\n",
        "                \"search_efficiency\": f\"Success rate: {reliability_score:.2%}\"\n",
        "            }\n",
        "            \n",
        "            results.append(EvaluationResult(\n",
        "                metrics={\n",
        "                    \"value_score\": value_score,\n",
        "                    \"distance_score\": distance_score,\n",
        "                    \"reliability_score\": reliability_score,\n",
        "                    \"combined_score\": combined_score,\n",
        "                },\n",
        "                artifacts=artifacts\n",
        "            ))\n",
        "            \n",
        "        except Exception as e:\n",
        "            # Return error result for this batch item\n",
        "            results.append(EvaluationResult(\n",
        "                metrics={\"combined_score\": 0.0, \"error\": 1.0},\n",
        "                artifacts={\n",
        "                    \"error\": str(e),\n",
        "                    \"traceback\": traceback.format_exc()\n",
        "                }\n",
        "            ))\n",
        "    \n",
        "    # CHANGE 5: Return list of results (one per batch item) instead of single result\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Changes\n",
        "\n",
        "Here's a quick reference of what changed:\n",
        "\n",
        "| Aspect | Original | Modified for EvolveAdapter |\n",
        "|--------|----------|---------------------------|\n",
        "| **Function signature** | `evaluate(program_path: str)` | `evaluate(program_path: str, batch: list)` |\n",
        "| **Return type** | `EvaluationResult` | `list[EvaluationResult]` |\n",
        "| **Structure** | Single evaluation, aggregated result | Loop over batch, one result per item |\n",
        "| **Parameters** | Hard-coded constants | Extracted from `batch_item` |\n",
        "| **Return statement** | `return EvaluationResult(...)` | `return [EvaluationResult(...), ...]` |\n",
        "\n",
        "## Step 2: Modifying Cascade Evaluation Functions\n",
        "\n",
        "If your project uses cascade evaluation, you must also modify your cascade evaluation functions (`evaluate_stage1`, `evaluate_stage2`, `evaluate_stage3`) to accept batch parameters and return lists, just like the main `evaluate` function.\n",
        "\n",
        "### Why Cascade Functions Need Batch Support\n",
        "\n",
        "When cascade evaluation is enabled, `EvolveAdapter` uses `CascadeEvaluationStrategy`, which calls your stage functions with a `batch` parameter. These functions must:\n",
        "1. Accept `batch: list` as a parameter\n",
        "2. Return `list[EvaluationResult]` (one result per batch item)\n",
        "3. Loop over batch items and process each one\n",
        "\n",
        "### Example: Modified `evaluate_stage1`\n",
        "\n",
        "**Before:**\n",
        "```python\n",
        "def evaluate_stage1(program_path: str) -> EvaluationResult:\n",
        "    # ... evaluation logic for single instance ...\n",
        "    return EvaluationResult(metrics={...}, artifacts={...})\n",
        "```\n",
        "\n",
        "**After:**\n",
        "```python\n",
        "def evaluate_stage1(program_path: str, batch: list) -> list[EvaluationResult]:\n",
        "    results = []\n",
        "    \n",
        "    # Load program once (shared across batch items)\n",
        "    # ... load program ...\n",
        "    \n",
        "    # Loop over each batch item\n",
        "    for batch_item in batch:\n",
        "        # Extract parameters from batch_item if needed\n",
        "        GLOBAL_MIN_X = batch_item.get(\"global_min_x\", -1.704)\n",
        "        # ... evaluation logic for this batch item ...\n",
        "        results.append(EvaluationResult(metrics={...}, artifacts={...}))\n",
        "    \n",
        "    return results\n",
        "```\n",
        "\n",
        "### Example: Modified `evaluate_stage2`\n",
        "\n",
        "**Before:**\n",
        "```python\n",
        "def evaluate_stage2(program_path: str) -> EvaluationResult:\n",
        "    # Full evaluation as in the main evaluate function\n",
        "    return evaluate(program_path)\n",
        "```\n",
        "\n",
        "**After:**\n",
        "```python\n",
        "def evaluate_stage2(program_path: str, batch: list) -> list[EvaluationResult]:\n",
        "    # Full evaluation as in the main evaluate function\n",
        "    return evaluate(program_path, batch)\n",
        "```\n",
        "\n",
        "### Key Points for Cascade Functions:\n",
        "\n",
        "1. **Same signature pattern**: All stage functions should follow the same pattern as `evaluate`:\n",
        "   - Add `batch: list` parameter\n",
        "   - Change return type to `list[EvaluationResult]`\n",
        "   - Loop over batch items\n",
        "   - Return a list of results\n",
        "\n",
        "2. **One result per batch item**: Ensure `len(results) == len(batch)`\n",
        "\n",
        "3. **Error handling**: If a stage fails for a specific batch item, return an error `EvaluationResult` for that item rather than raising an exception\n",
        "\n",
        "## Step 3: (Optional) Modify Config System Message\n",
        "\n",
        "You may need to modify the `system_message` in your `config.yaml` to match your newly-modified batch-based evaluation setup. For example, if your original OpenEvolve project had a hard-coded problem in the system prompt, you should remove or generalize it.\n",
        "\n",
        "**Example for function minimization:**\n",
        "- **Before (hard-coded)**: `\"You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.\"`\n",
        "- **After (generalized)**: `\"You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.\"`\n",
        "\n",
        "## Step 4: Using EvolveAdapter\n",
        "\n",
        "Here is how to use `EvolveAdapter` with GEPA's optimization engine.\n",
        "\n",
        "The modified function minimization example is located in the `tutorial_example/` directory\n",
        "   - This includes the modified `evaluator.py` as well as `config.yaml` and `initial_program.py`\n",
        "   - The `project_path` in Step 4 should point to the `tutorial_example` directory\n",
        "   - If running the notebook from a different location, adjust the path accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gepa in /Users/angelahe/miniconda3/lib/python3.12/site-packages (0.0.7)\n",
            "Requirement already satisfied: openevolve in /Users/angelahe/miniconda3/lib/python3.12/site-packages (0.2.15)\n",
            "Requirement already satisfied: numpy in /Users/angelahe/miniconda3/lib/python3.12/site-packages (2.2.4)\n",
            "Requirement already satisfied: scipy in /Users/angelahe/miniconda3/lib/python3.12/site-packages (1.16.2)\n",
            "Requirement already satisfied: pyyaml in /Users/angelahe/miniconda3/lib/python3.12/site-packages (6.0.3)\n",
            "Requirement already satisfied: litellm in /Users/angelahe/miniconda3/lib/python3.12/site-packages (1.77.5)\n",
            "Requirement already satisfied: openai>=1.0.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from openevolve) (2.1.0)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from openevolve) (4.67.1)\n",
            "Requirement already satisfied: flask in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from openevolve) (3.1.2)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from litellm) (3.12.15)\n",
            "Requirement already satisfied: click in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from litellm) (8.3.0)\n",
            "Requirement already satisfied: fastuuid>=0.13.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from litellm) (0.13.5)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from litellm) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from litellm) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from litellm) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from litellm) (4.25.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from litellm) (2.11.9)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from litellm) (1.1.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from litellm) (0.11.0)\n",
            "Requirement already satisfied: tokenizers in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from litellm) (0.22.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (1.20.1)\n",
            "Requirement already satisfied: anyio in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (4.9.0)\n",
            "Requirement already satisfied: certifi in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (1.0.7)\n",
            "Requirement already satisfied: idna in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (3.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.27.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from openai>=1.0.0->openevolve) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from openai>=1.0.0->openevolve) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from openai>=1.0.0->openevolve) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from openai>=1.0.0->openevolve) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from tiktoken>=0.7.0->litellm) (2025.9.18)\n",
            "Requirement already satisfied: requests>=2.26.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from tiktoken>=0.7.0->litellm) (2.32.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from flask->openevolve) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from flask->openevolve) (2.2.0)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from flask->openevolve) (3.1.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from tokenizers->litellm) (0.35.3)\n",
            "Requirement already satisfied: filelock in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (2025.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (24.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (1.1.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/angelahe/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required dependencies\n",
        "%pip install gepa openevolve numpy scipy pyyaml litellm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gepa.adapters.generic_evolve_adapter'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mgepa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mgepa\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01madapters\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric_evolve_adapter\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric_evolve_adapter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvolveAdapter\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Path to your modified OpenEvolve project directory\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# This should contain: config.yaml, evaluator.py, initial_program.py\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# The modified function minimization example is in the tutorial_example/ directory\u001b[39;00m\n\u001b[32m      8\u001b[39m project_path = Path(\u001b[33m\"\u001b[39m\u001b[33myour-project-path\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gepa.adapters.generic_evolve_adapter'"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from gepa import optimize\n",
        "from gepa.adapters.generic_evolve_adapter.generic_evolve_adapter import EvolveAdapter\n",
        "\n",
        "# Path to your modified OpenEvolve project directory\n",
        "# This should contain: config.yaml, evaluator.py, initial_program.py\n",
        "# The modified function minimization example is in the tutorial_example/ directory\n",
        "project_path = Path(\"your-project-path\")\n",
        "\n",
        "# Create the adapter\n",
        "adapter = EvolveAdapter(\n",
        "    path=project_path\n",
        ")\n",
        "\n",
        "# Define training data (for this example, a batch of function minimization problems)\n",
        "# Each item represents a different problem instance\n",
        "trainset = [\n",
        "    {\n",
        "        \"global_min_x\": -1.704,\n",
        "        \"global_min_y\": 0.678,\n",
        "        \"global_min_value\": -1.519,\n",
        "        \"bounds\": (-5, 5),\n",
        "        \"function_name\": \"sin_cos_function\"\n",
        "    },\n",
        "    {\n",
        "        \"global_min_x\": 0.0,\n",
        "        \"global_min_y\": 0.0,\n",
        "        \"global_min_value\": 0.0,\n",
        "        \"bounds\": (-3, 3),\n",
        "        \"function_name\": \"quadratic_function\"\n",
        "    },\n",
        "    # Add more problem instances as needed\n",
        "]\n",
        "\n",
        "# Read initial program\n",
        "with open(project_path / \"initial_program.py\", \"r\") as f:\n",
        "    initial_program = f.read()\n",
        "\n",
        "# Define seed candidate (the program to evolve)\n",
        "seed_candidate = {\n",
        "    \"program\": initial_program\n",
        "}\n",
        "\n",
        "# Run GEPA optimization\n",
        "result = optimize(\n",
        "    seed_candidate=seed_candidate,\n",
        "    trainset=trainset,\n",
        "    adapter=adapter,\n",
        "    max_metric_calls=60,  # Budget for evaluation calls -  adjust as needed\n",
        "    display_progress_bar=True\n",
        ")\n",
        "\n",
        "# Get the best score (GEPAResult doesn't have best_score, use val_aggregate_scores[best_idx])\n",
        "best_score = result.val_aggregate_scores[result.best_idx]\n",
        "print(f\"Best score: {best_score}\")\n",
        "print(f\"Best candidate index: {result.best_idx}\")\n",
        "print(f\"Total candidates evaluated: {len(result.candidates)}\")\n",
        "print(f\"Total metric calls: {result.total_metric_calls}\")\n",
        "\n",
        "# The evolved program is in result.best_candidate[\"program\"]\n",
        "print(f\"\\nBest candidate program:\")\n",
        "print(result.best_candidate.get(\"program\", \"N/A\")[:500] + \"...\" if len(result.best_candidate.get(\"program\", \"\")) > 500 else result.best_candidate.get(\"program\", \"N/A\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Reference: Before vs After\n",
        "\n",
        "### Original OpenEvolve Evaluate Function:\n",
        "```python\n",
        "def evaluate(program_path: str) -> EvaluationResult:\n",
        "    # Hard-coded problem parameters\n",
        "    GLOBAL_MIN_X = -1.704\n",
        "    GLOBAL_MIN_Y = 0.678\n",
        "    GLOBAL_MIN_VALUE = -1.519\n",
        "    \n",
        "    # Run program multiple times (trials) and aggregate\n",
        "    x_values = []\n",
        "    y_values = []\n",
        "    values = []\n",
        "    for trial in range(num_trials):  # e.g., 10 trials\n",
        "        result = program.run_search()\n",
        "        x, y, value = result\n",
        "        x_values.append(x)\n",
        "        y_values.append(y)\n",
        "        values.append(value)\n",
        "    \n",
        "    # Aggregate across all trials\n",
        "    avg_value = np.mean(values)\n",
        "    avg_distance = np.mean(distances)\n",
        "    # ... calculate aggregated metrics ...\n",
        "    \n",
        "    # Return single aggregated result\n",
        "    return EvaluationResult(metrics={...}, artifacts={...})\n",
        "```\n",
        "\n",
        "### Modified for EvolveAdapter:\n",
        "```python\n",
        "def evaluate(program_path: str, batch: list) -> list[EvaluationResult]:\n",
        "    results = []\n",
        "    \n",
        "    # Loop over each batch item\n",
        "    for batch_item in batch:\n",
        "        # Extract parameters from batch item\n",
        "        GLOBAL_MIN_X = batch_item.get(\"global_min_x\", -1.704)\n",
        "        GLOBAL_MIN_Y = batch_item.get(\"global_min_y\", 0.678)\n",
        "        GLOBAL_MIN_VALUE = batch_item.get(\"global_min_value\", -1.519)\n",
        "        \n",
        "        # Run program multiple times for this batch item\n",
        "        for trial in range(num_trials):\n",
        "            result = program.run_search()\n",
        "            # ... aggregate results for this batch item ...\n",
        "        \n",
        "        # Append one result per batch item\n",
        "        results.append(EvaluationResult(metrics={...}, artifacts={...}))\n",
        "    \n",
        "    # Return list of results (one per batch item)\n",
        "    return results\n",
        "```\n",
        "\n",
        "## Notes:\n",
        "\n",
        "1. **Batch Structure**: Each item in the batch should represent a distinct evaluation instance.\n",
        "\n",
        "2. **Return Format**: The function must return a **list** of `EvaluationResult` objects, with `len(results) == len(batch)`.\n",
        "\n",
        "3. **Error Handling**: If evaluation fails for a specific batch item, return an `EvaluationResult` with error metrics rather than raising an exception.\n",
        "\n",
        "4. **Metrics**: Each `EvaluationResult` should include a `\"combined_score\"` metric in its `metrics` dict, as this is used by GEPA for optimization.\n",
        "\n",
        "5. **Artifacts**: Use the `artifacts` field to store additional information. The adapter automatically uses these artifacts to create feedback for program improvement.\n",
        "\n",
        "6. **Cascade Evaluation**: If your project uses cascade evaluation, remember to modify **all** stage functions (`evaluate_stage1`, `evaluate_stage2`, etc.) to accept `batch` and return `list[EvaluationResult]`. See Step 3.5 for details.\n",
        "\n",
        "## Next Steps:\n",
        "\n",
        "1. Modify your `evaluator.py` to accept `batch` parameter and return `list[EvaluationResult]`\n",
        "2. **If using cascade evaluation**: Modify all stage functions (`evaluate_stage1`, `evaluate_stage2`, etc.) to accept `batch` and return lists\n",
        "3. Test with a small batch to ensure everything works\n",
        "4. Run GEPA optimization with your adapted project"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
