{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial: Using EvolveAdapter with OpenEvolve Projects\n",
        "\n",
        "This tutorial demonstrates how to adapt an existing OpenEvolve project to work with GEPA's `EvolveAdapter`. The key change required is modifying your `evaluate` function to accept a **batch of training data instances** and return a **list of evaluation results** (one per instance), rather than returning a single result.\n",
        "\n",
        "> **Note:** This tutorial is also available as a standalone Python script: `tutorial_evolve_adapter.py`. You can run it directly instead of using this notebook.\n",
        "\n",
        "## Why This Change?\n",
        "\n",
        "GEPA's optimization engine works with batches of data to:\n",
        "- Provide per-instance feedback for better program refinement\n",
        "- Support minibatch-based optimization strategies\n",
        "\n",
        "## Example Project: Function Minimization\n",
        "\n",
        "We'll use the **function minimization** example from OpenEvolve [examples/function_minimization](https://github.com/algorithmicsuperintelligence/openevolve/tree/main/examples/function_minimization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Key Changes Required\n",
        "\n",
        "Here are the **three main changes** you need to make to your `evaluate` function:\n",
        "\n",
        "### Change 1: Function Signature\n",
        "**Before:**\n",
        "```python\n",
        "def evaluate(program_path: str) -> EvaluationResult:\n",
        "```\n",
        "\n",
        "**After:**\n",
        "```python\n",
        "def evaluate(program_path: str, batch: list) -> list[EvaluationResult]:\n",
        "```\n",
        "- Add `batch` parameter\n",
        "- Change return type from `EvaluationResult` to `list[EvaluationResult]`\n",
        "\n",
        "### Change 2: Loop Over Batch Items\n",
        "**Before:** The function runs the program multiple times internally and aggregates results.\n",
        "\n",
        "**After:** Loop over each batch item and evaluate the program for each one:\n",
        "```python\n",
        "results = []\n",
        "for batch_item in batch:\n",
        "    # Evaluate program for this specific batch item\n",
        "    # ... evaluation logic ...\n",
        "    results.append(EvaluationResult(...))  # One result per batch item\n",
        "return results\n",
        "```\n",
        "\n",
        "### Change 3: Extract Parameters from Batch Items\n",
        "**Before:** Hard-coded problem parameters (e.g., `GLOBAL_MIN_X = -1.704`)\n",
        "\n",
        "**After:** Extract parameters from each batch item:\n",
        "```python\n",
        "GLOBAL_MIN_X = batch_item.get(\"global_min_x\", -1.704)\n",
        "GLOBAL_MIN_Y = batch_item.get(\"global_min_y\", 0.678)\n",
        "# ... etc\n",
        "```\n",
        "\n",
        "## Complete Modified Evaluate Function\n",
        "\n",
        "Here's the complete modified evaluate function with all changes applied. For this function minimization project example, each batch item represents a different function minimization problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modified evaluator.py for EvolveAdapter\n",
        "# For this example, each batch item represents a different function minimization problem\n",
        "# \n",
        "# KEY CHANGES MADE:\n",
        "# 1. Added 'batch: list' parameter to function signature\n",
        "# 2. Changed return type to 'list[EvaluationResult]'\n",
        "# 3. Added loop: 'for batch_item in batch:'\n",
        "# 4. Extract parameters from batch_item instead of hard-coding\n",
        "# 5. Return list of results instead of single result\n",
        "\n",
        "import importlib.util\n",
        "import numpy as np\n",
        "import time\n",
        "import concurrent.futures\n",
        "import traceback\n",
        "from openevolve.evaluation_result import EvaluationResult\n",
        "\n",
        "\n",
        "def run_with_timeout(func, args=(), kwargs={}, timeout_seconds=5):\n",
        "    \"\"\"\n",
        "    Run a function with a timeout using concurrent.futures\n",
        "\n",
        "    Args:\n",
        "        func: Function to run\n",
        "        args: Arguments to pass to the function\n",
        "        kwargs: Keyword arguments to pass to the function\n",
        "        timeout_seconds: Timeout in seconds\n",
        "\n",
        "    Returns:\n",
        "        Result of the function or raises TimeoutError\n",
        "    \"\"\"\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
        "        future = executor.submit(func, *args, **kwargs)\n",
        "        try:\n",
        "            return future.result(timeout=timeout_seconds)\n",
        "        except concurrent.futures.TimeoutError:\n",
        "            raise TimeoutError(f\"Function timed out after {timeout_seconds} seconds\")\n",
        "\n",
        "\n",
        "def safe_float(value):\n",
        "    \"\"\"Convert a value to float safely.\"\"\"\n",
        "    try:\n",
        "        return float(value)\n",
        "    except (TypeError, ValueError):\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "# CHANGE 1: Added 'batch' parameter, changed return type to list\n",
        "def evaluate(program_path: str, batch: list) -> list[EvaluationResult]:\n",
        "    \"\"\"\n",
        "    Evaluate the program on a batch of function minimization problems.\n",
        "    \n",
        "    Args:\n",
        "        program_path: Path to the program file to evaluate\n",
        "        batch: List of dicts, each containing:\n",
        "            - 'global_min_x': Target x coordinate\n",
        "            - 'global_min_y': Target y coordinate  \n",
        "            - 'global_min_value': Target function value\n",
        "            - 'bounds': Tuple of (min, max) bounds for search space\n",
        "            - 'function_name': Optional name for the function\n",
        "    \n",
        "    Returns:\n",
        "        List of EvaluationResult objects, one per batch item\n",
        "    \"\"\"\n",
        "    # CHANGE 2: Initialize results list to collect one result per batch item\n",
        "    results = []\n",
        "    \n",
        "    # Load the program once (shared across all batch items)\n",
        "    try:\n",
        "        spec = importlib.util.spec_from_file_location(\"program\", program_path)\n",
        "        program = importlib.util.module_from_spec(spec)\n",
        "        spec.loader.exec_module(program)\n",
        "        \n",
        "        if not hasattr(program, \"run_search\"):\n",
        "            # Return error results for all batch items\n",
        "            for _ in batch:\n",
        "                results.append(EvaluationResult(\n",
        "                    metrics={\"combined_score\": 0.0, \"error\": 1.0},\n",
        "                    artifacts={\"error\": \"Missing run_search function\"}\n",
        "                ))\n",
        "            return results\n",
        "    except Exception as e:\n",
        "        # Return error results for all batch items\n",
        "        for _ in batch:\n",
        "            results.append(EvaluationResult(\n",
        "                metrics={\"combined_score\": 0.0, \"error\": 1.0},\n",
        "                artifacts={\"error\": str(e), \"traceback\": traceback.format_exc()}\n",
        "            ))\n",
        "        return results\n",
        "    \n",
        "    # CHANGE 3: Loop over each batch item\n",
        "    for batch_item in batch:\n",
        "        try:\n",
        "            # CHANGE 4: Extract problem parameters from batch_item instead of hard-coding\n",
        "            # In the original, these were hard-coded constants like:\n",
        "            #   GLOBAL_MIN_X = -1.704\n",
        "            # Now we get them from each batch item:\n",
        "            GLOBAL_MIN_X = batch_item.get(\"global_min_x\", -1.704)\n",
        "            GLOBAL_MIN_Y = batch_item.get(\"global_min_y\", 0.678)\n",
        "            GLOBAL_MIN_VALUE = batch_item.get(\"global_min_value\", -1.519)\n",
        "            bounds = batch_item.get(\"bounds\", (-5, 5))\n",
        "            \n",
        "            # Run multiple trials for this specific problem\n",
        "            num_trials = 10\n",
        "            x_values = []\n",
        "            y_values = []\n",
        "            values = []\n",
        "            distances = []\n",
        "            times = []\n",
        "            success_count = 0\n",
        "            \n",
        "            for trial in range(num_trials):\n",
        "                try:\n",
        "                    start_time = time.time()\n",
        "                    \n",
        "                    # Run the program (it should use the bounds from batch_item)\n",
        "                    # Note: The program may need to be modified to accept bounds as parameter\n",
        "                    result = run_with_timeout(program.run_search, timeout_seconds=5)\n",
        "                    \n",
        "                    # Handle different result formats\n",
        "                    if isinstance(result, tuple):\n",
        "                        if len(result) == 3:\n",
        "                            x, y, value = result\n",
        "                        elif len(result) == 2:\n",
        "                            x, y = result\n",
        "                            # Calculate function value\n",
        "                            value = np.sin(x) * np.cos(y) + np.sin(x * y) + (x**2 + y**2) / 20\n",
        "                        else:\n",
        "                            continue\n",
        "                    else:\n",
        "                        continue\n",
        "                    \n",
        "                    end_time = time.time()\n",
        "                    \n",
        "                    # Validate results\n",
        "                    x = safe_float(x)\n",
        "                    y = safe_float(y)\n",
        "                    value = safe_float(value)\n",
        "                    \n",
        "                    if (np.isnan(x) or np.isnan(y) or np.isnan(value) or\n",
        "                        np.isinf(x) or np.isinf(y) or np.isinf(value)):\n",
        "                        continue\n",
        "                    \n",
        "                    # Calculate metrics for this trial\n",
        "                    x_diff = x - GLOBAL_MIN_X\n",
        "                    y_diff = y - GLOBAL_MIN_Y\n",
        "                    distance_to_global = np.sqrt(x_diff**2 + y_diff**2)\n",
        "                    \n",
        "                    x_values.append(x)\n",
        "                    y_values.append(y)\n",
        "                    values.append(value)\n",
        "                    distances.append(distance_to_global)\n",
        "                    times.append(end_time - start_time)\n",
        "                    success_count += 1\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    continue\n",
        "            \n",
        "            # If all trials failed, return error result\n",
        "            if success_count == 0:\n",
        "                results.append(EvaluationResult(\n",
        "                    metrics={\n",
        "                        \"value_score\": 0.0,\n",
        "                        \"distance_score\": 0.0,\n",
        "                        \"reliability_score\": 0.0,\n",
        "                        \"combined_score\": 0.0,\n",
        "                        \"error\": 1.0\n",
        "                    },\n",
        "                    artifacts={\"error\": \"All trials failed\"}\n",
        "                ))\n",
        "                continue\n",
        "            \n",
        "            # Calculate aggregated metrics for this batch item\n",
        "            avg_value = float(np.mean(values))\n",
        "            avg_distance = float(np.mean(distances))\n",
        "            \n",
        "            # Convert to scores (higher is better)\n",
        "            value_score = float(1.0 / (1.0 + abs(avg_value - GLOBAL_MIN_VALUE)))\n",
        "            distance_score = float(1.0 / (1.0 + avg_distance))\n",
        "            reliability_score = float(success_count / num_trials)\n",
        "            \n",
        "            # Calculate combined score\n",
        "            base_score = 0.5 * value_score + 0.3 * distance_score + 0.2 * reliability_score\n",
        "            \n",
        "            # Apply solution quality multiplier\n",
        "            if avg_distance < 0.5:\n",
        "                solution_quality_multiplier = 1.5\n",
        "            elif avg_distance < 1.5:\n",
        "                solution_quality_multiplier = 1.2\n",
        "            elif avg_distance < 3.0:\n",
        "                solution_quality_multiplier = 1.0\n",
        "            else:\n",
        "                solution_quality_multiplier = 0.7\n",
        "            \n",
        "            combined_score = float(base_score * solution_quality_multiplier)\n",
        "            \n",
        "            # Create artifacts\n",
        "            artifacts = {\n",
        "                \"convergence_info\": f\"Converged in {num_trials} trials with {success_count} successes\",\n",
        "                \"best_position\": f\"Final position: x={x_values[-1]:.4f}, y={y_values[-1]:.4f}\",\n",
        "                \"average_distance_to_global\": f\"{avg_distance:.4f}\",\n",
        "                \"search_efficiency\": f\"Success rate: {reliability_score:.2%}\"\n",
        "            }\n",
        "            \n",
        "            results.append(EvaluationResult(\n",
        "                metrics={\n",
        "                    \"value_score\": value_score,\n",
        "                    \"distance_score\": distance_score,\n",
        "                    \"reliability_score\": reliability_score,\n",
        "                    \"combined_score\": combined_score,\n",
        "                },\n",
        "                artifacts=artifacts\n",
        "            ))\n",
        "            \n",
        "        except Exception as e:\n",
        "            # Return error result for this batch item\n",
        "            results.append(EvaluationResult(\n",
        "                metrics={\"combined_score\": 0.0, \"error\": 1.0},\n",
        "                artifacts={\n",
        "                    \"error\": str(e),\n",
        "                    \"traceback\": traceback.format_exc()\n",
        "                }\n",
        "            ))\n",
        "    \n",
        "    # CHANGE 5: Return list of results (one per batch item) instead of single result\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Changes\n",
        "\n",
        "Here's a quick reference of what changed:\n",
        "\n",
        "| Aspect | Original | Modified for EvolveAdapter |\n",
        "|--------|----------|---------------------------|\n",
        "| **Function signature** | `evaluate(program_path: str)` | `evaluate(program_path: str, batch: list)` |\n",
        "| **Return type** | `EvaluationResult` | `list[EvaluationResult]` |\n",
        "| **Structure** | Single evaluation, aggregated result | Loop over batch, one result per item |\n",
        "| **Parameters** | Hard-coded constants | Extracted from `batch_item` |\n",
        "| **Return statement** | `return EvaluationResult(...)` | `return [EvaluationResult(...), ...]` |\n",
        "\n",
        "## Step 2: Modifying Cascade Evaluation Functions\n",
        "\n",
        "If your project uses cascade evaluation, you must also modify your cascade evaluation functions (`evaluate_stage1`, `evaluate_stage2`, `evaluate_stage3`) to accept batch parameters and return lists, just like the main `evaluate` function.\n",
        "\n",
        "### Why Cascade Functions Need Batch Support\n",
        "\n",
        "When cascade evaluation is enabled, `EvolveAdapter` uses `CascadeEvaluationStrategy`, which calls your stage functions with a `batch` parameter. These functions must:\n",
        "1. Accept `batch: list` as a parameter\n",
        "2. Return `list[EvaluationResult]` (one result per batch item)\n",
        "3. Loop over batch items and process each one\n",
        "\n",
        "### Example: Modified `evaluate_stage1`\n",
        "\n",
        "**Before:**\n",
        "```python\n",
        "def evaluate_stage1(program_path: str) -> EvaluationResult:\n",
        "    # ... evaluation logic for single instance ...\n",
        "    return EvaluationResult(metrics={...}, artifacts={...})\n",
        "```\n",
        "\n",
        "**After:**\n",
        "```python\n",
        "def evaluate_stage1(program_path: str, batch: list) -> list[EvaluationResult]:\n",
        "    results = []\n",
        "    \n",
        "    # Load program once (shared across batch items)\n",
        "    # ... load program ...\n",
        "    \n",
        "    # Loop over each batch item\n",
        "    for batch_item in batch:\n",
        "        # Extract parameters from batch_item if needed\n",
        "        GLOBAL_MIN_X = batch_item.get(\"global_min_x\", -1.704)\n",
        "        # ... evaluation logic for this batch item ...\n",
        "        results.append(EvaluationResult(metrics={...}, artifacts={...}))\n",
        "    \n",
        "    return results\n",
        "```\n",
        "\n",
        "### Example: Modified `evaluate_stage2`\n",
        "\n",
        "**Before:**\n",
        "```python\n",
        "def evaluate_stage2(program_path: str) -> EvaluationResult:\n",
        "    # Full evaluation as in the main evaluate function\n",
        "    return evaluate(program_path)\n",
        "```\n",
        "\n",
        "**After:**\n",
        "```python\n",
        "def evaluate_stage2(program_path: str, batch: list) -> list[EvaluationResult]:\n",
        "    # Full evaluation as in the main evaluate function\n",
        "    return evaluate(program_path, batch)\n",
        "```\n",
        "\n",
        "### Key Points for Cascade Functions:\n",
        "\n",
        "1. **Same signature pattern**: All stage functions should follow the same pattern as `evaluate`:\n",
        "   - Add `batch: list` parameter\n",
        "   - Change return type to `list[EvaluationResult]`\n",
        "   - Loop over batch items\n",
        "   - Return a list of results\n",
        "\n",
        "2. **One result per batch item**: Ensure `len(results) == len(batch)`\n",
        "\n",
        "3. **Error handling**: If a stage fails for a specific batch item, return an error `EvaluationResult` for that item rather than raising an exception\n",
        "\n",
        "## Step 3: (Optional) Modify Config System Message\n",
        "\n",
        "You may need to modify the `system_message` in your `config.yaml` to match your newly-modified batch-based evaluation setup. For example, if your original OpenEvolve project had a hard-coded problem in the system prompt, you should remove or generalize it.\n",
        "\n",
        "**Example for function minimization:**\n",
        "- **Before (hard-coded)**: `\"You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. The function is f(x, y) = sin(x) * cos(y) + sin(x*y) + (x^2 + y^2)/20. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.\"`\n",
        "- **After (generalized)**: `\"You are an expert programmer specializing in optimization algorithms. Your task is to improve a function minimization algorithm to find the global minimum of a complex function with many local minima. Focus on improving the search_algorithm function to reliably find the global minimum, escaping local minima that might trap simple algorithms.\"`\n",
        "\n",
        "## Step 4: Using EvolveAdapter\n",
        "\n",
        "Here is how to use `EvolveAdapter` with GEPA's optimization engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "%pip install gepa openevolve numpy scipy pyyaml litellm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from gepa import optimize\n",
        "from gepa.adapters.evolve_adapter.evolve_adapter import EvolveAdapter\n",
        "\n",
        "# Path to your modified OpenEvolve project directory\n",
        "# This should contain: config.yaml, evaluator.py, initial_program.py\n",
        "project_path = Path(\"your-project-path\")\n",
        "\n",
        "# Create the adapter\n",
        "adapter = EvolveAdapter(\n",
        "    path=project_path\n",
        ")\n",
        "\n",
        "# Define training data (for this example, a batch of function minimization problems)\n",
        "# Each item represents a different problem instance\n",
        "trainset = [\n",
        "    {\n",
        "        \"global_min_x\": -1.704,\n",
        "        \"global_min_y\": 0.678,\n",
        "        \"global_min_value\": -1.519,\n",
        "        \"bounds\": (-5, 5),\n",
        "        \"function_name\": \"sin_cos_function\"\n",
        "    },\n",
        "    {\n",
        "        \"global_min_x\": 0.0,\n",
        "        \"global_min_y\": 0.0,\n",
        "        \"global_min_value\": 0.0,\n",
        "        \"bounds\": (-3, 3),\n",
        "        \"function_name\": \"quadratic_function\"\n",
        "    },\n",
        "    # Add more problem instances as needed\n",
        "]\n",
        "\n",
        "# Read initial program\n",
        "with open(project_path / \"initial_program.py\", \"r\") as f:\n",
        "    initial_program = f.read()\n",
        "\n",
        "# Define seed candidate (the program to evolve)\n",
        "seed_candidate = {\n",
        "    \"program\": initial_program\n",
        "}\n",
        "\n",
        "# Run GEPA optimization\n",
        "result = optimize(\n",
        "    seed_candidate=seed_candidate,\n",
        "    trainset=trainset,\n",
        "    adapter=adapter,\n",
        "    max_metric_calls=60,  # Budget for evaluation calls -  adjust as needed\n",
        "    display_progress_bar=True\n",
        ")\n",
        "\n",
        "# Get the best score (GEPAResult doesn't have best_score, use val_aggregate_scores[best_idx])\n",
        "best_score = result.val_aggregate_scores[result.best_idx]\n",
        "print(f\"Best score: {best_score}\")\n",
        "print(f\"Best candidate index: {result.best_idx}\")\n",
        "print(f\"Total candidates evaluated: {len(result.candidates)}\")\n",
        "print(f\"Total metric calls: {result.total_metric_calls}\")\n",
        "\n",
        "# The evolved program is in result.best_candidate[\"program\"]\n",
        "print(f\"\\nBest candidate program:\")\n",
        "print(result.best_candidate.get(\"program\", \"N/A\")[:500] + \"...\" if len(result.best_candidate.get(\"program\", \"\")) > 500 else result.best_candidate.get(\"program\", \"N/A\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Reference: Before vs After\n",
        "\n",
        "### Original OpenEvolve Evaluate Function:\n",
        "```python\n",
        "def evaluate(program_path: str) -> EvaluationResult:\n",
        "    # Hard-coded problem parameters\n",
        "    GLOBAL_MIN_X = -1.704\n",
        "    GLOBAL_MIN_Y = 0.678\n",
        "    GLOBAL_MIN_VALUE = -1.519\n",
        "    \n",
        "    # Run program multiple times (trials) and aggregate\n",
        "    x_values = []\n",
        "    y_values = []\n",
        "    values = []\n",
        "    for trial in range(num_trials):  # e.g., 10 trials\n",
        "        result = program.run_search()\n",
        "        x, y, value = result\n",
        "        x_values.append(x)\n",
        "        y_values.append(y)\n",
        "        values.append(value)\n",
        "    \n",
        "    # Aggregate across all trials\n",
        "    avg_value = np.mean(values)\n",
        "    avg_distance = np.mean(distances)\n",
        "    # ... calculate aggregated metrics ...\n",
        "    \n",
        "    # Return single aggregated result\n",
        "    return EvaluationResult(metrics={...}, artifacts={...})\n",
        "```\n",
        "\n",
        "### Modified for EvolveAdapter:\n",
        "```python\n",
        "def evaluate(program_path: str, batch: list) -> list[EvaluationResult]:\n",
        "    results = []\n",
        "    \n",
        "    # Loop over each batch item\n",
        "    for batch_item in batch:\n",
        "        # Extract parameters from batch item\n",
        "        GLOBAL_MIN_X = batch_item.get(\"global_min_x\", -1.704)\n",
        "        GLOBAL_MIN_Y = batch_item.get(\"global_min_y\", 0.678)\n",
        "        GLOBAL_MIN_VALUE = batch_item.get(\"global_min_value\", -1.519)\n",
        "        \n",
        "        # Run program multiple times for this batch item\n",
        "        for trial in range(num_trials):\n",
        "            result = program.run_search()\n",
        "            # ... aggregate results for this batch item ...\n",
        "        \n",
        "        # Append one result per batch item\n",
        "        results.append(EvaluationResult(metrics={...}, artifacts={...}))\n",
        "    \n",
        "    # Return list of results (one per batch item)\n",
        "    return results\n",
        "```\n",
        "\n",
        "## Notes:\n",
        "\n",
        "1. **Batch Structure**: Each item in the batch should represent a distinct evaluation instance.\n",
        "\n",
        "2. **Return Format**: The function must return a **list** of `EvaluationResult` objects, with `len(results) == len(batch)`.\n",
        "\n",
        "3. **Error Handling**: If evaluation fails for a specific batch item, return an `EvaluationResult` with error metrics rather than raising an exception.\n",
        "\n",
        "4. **Metrics**: Each `EvaluationResult` should include a `\"combined_score\"` metric in its `metrics` dict, as this is used by GEPA for optimization.\n",
        "\n",
        "5. **Artifacts**: Use the `artifacts` field to store additional information. The adapter automatically uses these artifacts to create feedback for program improvement.\n",
        "\n",
        "6. **Cascade Evaluation**: If your project uses cascade evaluation, remember to modify **all** stage functions (`evaluate_stage1`, `evaluate_stage2`, etc.) to accept `batch` and return `list[EvaluationResult]`. See Step 3.5 for details.\n",
        "\n",
        "## Next Steps:\n",
        "\n",
        "1. Modify your `evaluator.py` to accept `batch` parameter and return `list[EvaluationResult]`\n",
        "2. **If using cascade evaluation**: Modify all stage functions (`evaluate_stage1`, `evaluate_stage2`, etc.) to accept `batch` and return lists\n",
        "3. Test with a small batch to ensure everything works\n",
        "4. Run GEPA optimization with your adapted project"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
